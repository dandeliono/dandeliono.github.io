[{"id":"2d734239bc109a5925e2725830e859da","title":"Maven 私服 Nexus 升级实录","content":"Maven 私服 Nexus 升级实录摘要\n本文介绍 Mavne 私服 Nexus 升级的全过程，从 3.29.2-02 升级到 3.83.2-01\n\nNexus官网\n\nOpenJdk下载地址\n\nNexus系统配置要求\n\n\n升级过程说明\nNexus 从 3.71.x 开始，不再支持 OrientDB，后续版本仅支持 H2 和 PostgreSQL ，根据官网说明，3.70.x 以下的版本需要将 Nexus 先升级到 3.70.x 的最新版本，然后使用官方提供的数据库迁移工具，将数据库迁移到 H2 后，再升级到 3.71.x 以后的版本\n\n从 3.29.2-02 升级到 nexus-3.70.4-02安装 OpenJDK\n需要先安装好 OpenJDK，原因是Nexus的数据库迁移工具仅支持 OpenJDK，不支持 Oracle JDK，我这里选择安装 OpenJDK11\n\n123456cd /usr/localcurl -O https://mirrors.tuna.tsinghua.edu.cn/Adoptium/11/jdk/x64/linux/OpenJDK11U-jdk_x64_linux_hotspot_11.0.28_6.tar.gztar -zxvf OpenJDK11U-jdk_x64_linux_hotspot_11.0.28_6.tar.gzln -s /usr/local/jdk-11.0.28+6/bin/java /usr/bin/javaexport JAVA_HOME=/usr/local/jdk-11.0.28+6export PATH=$JAVA_HOME/bin:$PATH\n\n安装 nexus-3.70.x\n目前官网发布的nexus-3.70.x的最新版本为 nexus-3.70.4-02，下载页面，其对应的数据库迁移工具也可以从该页面下载。\n\n这里我们选择 Java 11 的版本，升级安装与第一次安装方式一样。\n\n\n1234567891011121314su - nexussudo systemctl stop nexuscurl -O https://download.sonatype.com/nexus/3/nexus-3.70.4-02-java11-unix.tar.gztar -zxvf nexus-3.70.4-02-java11-unix.tar.gzrm -f ~/nexus3ln -s /usr/local/nexus-3.70.4-02 ~/nexus3sudo systemctl start nexus\n\n迁移数据到 H2\n下载数据库迁移工具\n\n1234mkdir ~/backupcd ~/backupcurl -O https://download.sonatype.com/nexus/nxrm3-migrator/nexus-db-migrator-3.70.4-02.jar\n\n\n登录 Nexus 导出数据: 设置 -&gt; System -&gt; Tasks -&gt; Create task -&gt; Admin - Export databases for backup\n\n\n任务创建后点击Run，即可在 /home/nexus/backup 目录下看到备份文件\n\n\n123456ll /home/nexus/backup-rw-rw-r-- 1 nexus nexus   121066 Sep  8 07:25 analytics-2025-09-08-07-25-57-3.70.4-02.bak-rw-rw-r-- 1 nexus nexus 19349428 Sep  8 07:25 component-2025-09-08-07-25-57-3.70.4-02.bak-rw-rw-r-- 1 nexus nexus   266208 Sep  8 07:25 config-2025-09-08-07-25-57-3.70.4-02.bak-rw-r--r-- 1 nexus nexus 56809625 Sep  8 06:41 nexus-db-migrator-3.70.4-02.jar-rw-rw-r-- 1 nexus nexus   132802 Sep  8 07:25 security-2025-09-08-07-25-57-3.70.4-02.bak\n\n\n使用迁移工具生成H2数据库文件，官网参考资料: Migrating From OrientDB to H2\n\n12345678sudo systemctl stop nexuscd /home/nexus/backupjava -Xmx2G -Xms2G -XX:+UseG1GC -jar nexus-db-migrator-3.70.4-02.jar --migration_type=h2\n\n\n运行成功后会生成 nexus.mv.db，将其移动到 /home/nexus/sonatype-work/nexus3/db/ 目录下\n\n1mv nexus.mv.db /home/nexus/sonatype-work/nexus3/db/\n\n\n编辑/home/nexus/sonatype-work/nexus3/etc/nexus.properties 文件，添加如下内容\n\n12nexus.datastore.enabled=true\n\n\n启动 nexus，此时我们就完成了 从 3.29.2-02 到 nexus-3.70.4-02 的升级\n\n12sudo systemctl start nexus\n\n从 nexus-3.70.4-02 升级到 nexus-3.83.2-01\n这个升级就比较简单了，和我们此前的升级方式是一样的，下载解压后替换安装目录即可，这里要注意，从nexus-3.71.0+开始仅支持jdk17，所以需要提前安装好jdk17，另外从nexus-3.78.0开始，Nexus 内置了openjdk17，所以不需要再额外安装jdk。\n\nnexus-3.83.2-01 是目前的最新版，最新版下载页面，历史版本下载页面地址\n\n\n1234567891011sudo systemctl stop nexuscd ~curl -O https://download.sonatype.com/nexus/3/nexus-3.83.2-01-linux-x86_64.tar.gztar -zxvf nexus-3.83.2-01-linux-x86_64.tar.gzrm -f nexus3ln -s nexus-3.83.2-01 nexus3sudo systemctl start nexus\n\n\n\n\n\n","slug":"Maven 私服 Nexus 升级实录","date":"2025-09-28T16:12:32.000Z","categories_index":"nexus","tags_index":"https,com,sonatype","author_index":"dandeliono"},{"id":"4944cfccd2356072e58570e06c390077","title":"定制Ubuntu安装镜像的两种方法","content":"定制Ubuntu安装镜像的两种方法1. 准备工作1234- 确保你有一个Ubuntu工作站或虚拟机来进行定制工作。- 确保你的系统已经更新：    ```bash    sudo apt update &amp;&amp; sudo apt upgrade\n\n12345672\\. **安装必要的工具**---------------```go```bashsudo apt install squashfs-tools genisoimage isolinux xorriso\n\n12345673\\. **获取官方Ubuntu ISO**----------------------```null前往Ubuntu官网下载你想要定制的版本的ISO文件。\n\n4. 挂载ISO文件123```bashmkdir ~/isosudo mount -o loop ubuntu-18.04.6-live-server-amd64.iso ~/iso\n\n123456785\\. **复制ISO内容**---------------```bash```bashmkdir ~/livecdcp -rT ~/iso ~/livecd\n\n12345676\\. **解压文件系统 (第二次做的时候，可以忽略这一步)**--------------------------------```go```bashsudo unsquashfs -d ~/squashfs ~/livecd/casper/filesystem.squashfs\n\n12345677\\. **chroot到解压后的文件系统**-----------------------```perl```bashsudo chroot ~/squashfs\n\n在chroot环境中，你可以安装&#x2F;卸载软件，更改系统设置，添加&#x2F;删除用户等。\n1234567891011128\\. **进行自定义**-------------```less- 例如，[安装新软件](https:```bashsudo apt-get updatesudo apt-get install -y gcc-7 g++-7 make cmake unzip vimsudo apt-get updatesudo apt-get install -y pkg-config zlib1g-dev libnuma-dev libdrm-dev libudev-dev\n\n\n例如，file：12exitsudo cp /home/hcsw/*.zip ~/home/hcsw/unsquashfs/home/hcsw\n\n12345679\\. **退出chroot环境**------------------```go```bashexit\n\n1234567810\\. **重新创建文件系统**-----------------```bash```bashsudo rm ~/livecd/casper/filesystem.squashfssudo mksquashfs ~/squashfs ~/livecd/casper/filesystem.squashfs\n\n1234567811\\. **更新文件的MD5值**------------------```bash```bashsudo rm ~/livecd/md5sum.txtsudo sh -c &quot;cd ~/livecd &amp;&amp; find . -type f -print0 | xargs -0 md5sum &gt; md5sum.txt&quot;\n\n123456712\\. **创建新的ISO**----------------```bash```bashsudo xorriso -as mkisofs -D -r -V &quot;Custom Ubuntu&quot; -cache-inodes -J -l -b isolinux/isolinux.bin -c isolinux/boot.cat -no-emul-boot -boot-load-size 4 -boot-info-table -o custom-ubuntu.iso ~/livecd\n\n123456789101112131415161718192021222324现在你应该有一个自定义的Ubuntu ISO文件在指定的路径上。此ISO文件可以用于创建启动USB或用于虚拟机。**注意**：定制ISO时要小心，确保不要引入安全风险或使系统不稳定。在应用到生产环境之前，务必充分测试定制的版本。Cubic（Custom Ubuntu ISO Creator）是一个基于GUI的工具，用于创建定制的Ubuntu livecd镜像。它允许用户在一个chroot环境中轻松地启动、自定义和重新包装系统。下面是使用Cubic定制Ubuntu的基本步骤：1\\. **准备工作**------------确保你有一个Ubuntu Desktop工作站或虚拟机来进行定制工作。1\\. **安装Cubic**：----------------在Ubuntu上，您可以使用以下命令安装Cubic：```bashsudo add-apt-repository ppa:cubic-wizard/releasesudo apt updatesudo apt install cubic\n\n2. 启动Cubic：从应用程序菜单启动Cubic，或在终端中输入cubic。\n3. 选择原始ISO：当Cubic启动时，它会提示您选择一个原始Ubuntu ISO文件。浏览到您的ISO文件，然后点击“Next”。\n4. 指定工作目录：Cubic将提取ISO的内容到这个工作目录。默认的工作目录应该没问题，但您可以根据需要更改它。\n5. 定制：点击“Next”后，Cubic将开始提取ISO的内容。完成后，它将在一个chroot环境中启动一个终端。在这个环境中，您可以：\n\n安装、卸载软件包\n更改系统设置\n编辑、添加或删除文件\n\n当您完成所有更改并退出终端时，Cubic将继续下一步。\n6. 修改ISO引导选项（可以忽略此步）：在这里，您可以修改livecd的引导选项、背景图像、启动菜单文本等。\n7. ISO信息（可以忽略此步）：在这里，您可以为您的定制ISO提供一个新的卷标、发布注释等。\n8. 生成ISO：点击“生成”按钮，Cubic将开始打包所有内容，并创建一个新的ISO文件。完成后，您将看到一个链接，指向新创建的ISO文件。\n9. 完成：关闭Cubic，您现在可以测试或分发您的定制Ubuntu ISO。\n注意：在生产环境中使用您的定制ISO之前，确保充分测试所有功能，以确保它的稳定性和安全性。\n","slug":"LINUX/定制Ubuntu安装镜像的两种方法","date":"2025-08-07T16:44:08.000Z","categories_index":"ISO,LINUX","tags_index":"sudo,bash,Cubic","author_index":"dandeliono"},{"id":"deb7ca296fb4175b782a15a32d428fc0","title":"常见的bpftrace脚本","content":"常见的bpftrace脚本跟踪 tracepoint kfree_skb ​123456789101112131415161718192021222324252627282930313233343536373839404142#!/usr/bin/bpftracetracepoint:skb:kfree_skb &#123; $skb = (struct sk_buff *)args-&gt;skbaddr; $iph = (struct iphdr *)($skb-&gt;head + $skb-&gt;network_header); if ($iph-&gt;protocol == IPPROTO_ICMP) &#123; $icmph = (struct icmphdr *)($skb-&gt;head + $skb-&gt;transport_header); printf(&quot;TIME:%s PID/TID: %d/%d &quot;, strftime(&quot;%H:%M:%S:%f&quot;, nsecs), pid, tid); printf(&quot;COMM: %s DEV: %s\\n&quot;, comm, $skb-&gt;dev-&gt;name); printf(&quot;ICMP %s-&gt;%s &quot;, ntop($iph-&gt;saddr), ntop($iph-&gt;daddr)); printf(&quot;id:%d seq:%d &quot;, bswap($icmph-&gt;un.echo.id), bswap($icmph-&gt;un.echo.sequence)); printf(&quot;\\n\\n&quot;); // printf(&quot;%s&quot;, kstack); &#125; else if ($iph-&gt;protocol == IPPROTO_TCP) &#123; $tcph = (struct tcphdr *)($skb-&gt;head + $skb-&gt;transport_header); printf(&quot;TIME:%s PID/TID: %d/%d &quot;, strftime(&quot;%H:%M:%S:%f&quot;, nsecs), pid, tid); printf(&quot;COMM: %s DEV: %s \\n&quot;, comm, $skb-&gt;dev-&gt;name); printf(&quot;TCP %s:%d&quot;, ntop($iph-&gt;saddr), bswap($tcph-&gt;source)); printf(&quot; -&gt; %s:%d &quot;, ntop($iph-&gt;daddr), bswap($tcph-&gt;dest)); printf(&quot;FLAGS: [&quot;); if ($tcph-&gt;syn) &#123; printf(&quot;S&quot;); &#125; if ($tcph-&gt;fin) &#123; printf(&quot;F&quot;); &#125; if ($tcph-&gt;rst) &#123; printf(&quot;R&quot;); &#125; if ($tcph-&gt;psh) &#123; printf(&quot;P&quot;); &#125; if ($tcph-&gt;ack) &#123; printf(&quot;.&quot;); &#125; printf(&quot;] &quot;); $seq = bswap($tcph-&gt;seq); $ack = bswap($tcph-&gt;ack_seq); printf(&quot;SEQ: %ld ACK: %ld &quot;, $seq, $ack); printf(&quot;\\n\\n&quot;); // printf(&quot;%s&quot;, kstack); &#125;&#125;\n\n跟踪 tcp reset ​1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#!/usr/bin/bpftrace#define AF_INET   2#define AF_INET6 10kprobe:tcp_v4_send_reset &#123; $sk = (struct sock *)arg0; $skb = (struct sk_buff *)arg1; $iph = (struct iphdr *)($skb-&gt;head + $skb-&gt;network_header); // printf(&quot;%p %p %d\\n&quot;, $skb, $skb-&gt;head, $skb-&gt;network_header); if ($iph-&gt;protocol == IPPROTO_TCP) &#123; $tcph = (struct tcphdr *)($skb-&gt;head + $skb-&gt;transport_header); printf(&quot;PASSIVE RESET TIME:%s PID/TID: %d/%d &quot;, strftime(&quot;%H:%M:%S:%f&quot;, nsecs), pid, tid); printf(&quot;COMM: %s DEV: %s \\n&quot;, comm, $skb-&gt;dev-&gt;name); printf(&quot;TCP %s:%d&quot;, ntop($iph-&gt;saddr), bswap($tcph-&gt;source)); printf(&quot; -&gt; %s:%d &quot;, ntop($iph-&gt;daddr), bswap($tcph-&gt;dest)); printf(&quot;FLAGS: [&quot;); if ($tcph-&gt;syn) &#123; printf(&quot;S&quot;); &#125; if ($tcph-&gt;fin) &#123; printf(&quot;F&quot;); &#125; if ($tcph-&gt;rst) &#123; printf(&quot;R&quot;); &#125; if ($tcph-&gt;psh) &#123; printf(&quot;P&quot;); &#125; if ($tcph-&gt;ack) &#123; printf(&quot;.&quot;); &#125; printf(&quot;] &quot;); $seq = bswap($tcph-&gt;seq); $ack = bswap($tcph-&gt;ack_seq); printf(&quot;SEQ: %ld ACK: %ld &quot;, $seq, $ack); printf(&quot;\\n\\n&quot;); // printf(&quot;%s&quot;, kstack); &#125;&#125;kprobe:tcp_send_active_reset &#123; $sk = (struct sock *)arg0; $lport = $sk-&gt;__sk_common.skc_num; printf(&quot;%d %d\\n&quot;, $lport, $sk-&gt;__sk_common.skc_num); $dport = $sk-&gt;__sk_common.skc_dport; $dport = bswap($dport); $family = $sk-&gt;__sk_common.skc_family; $saddr = ntop(0); $daddr = ntop(0); if ($family == AF_INET) &#123; $saddr = ntop(AF_INET, $sk-&gt;__sk_common.skc_rcv_saddr); $daddr = ntop(AF_INET, $sk-&gt;__sk_common.skc_daddr); &#125; else &#123; // AF_INET6 $saddr = ntop(AF_INET6, $sk-&gt;__sk_common.skc_v6_rcv_saddr.in6_u.u6_addr8); $daddr = ntop(AF_INET6, $sk-&gt;__sk_common.skc_v6_daddr.in6_u.u6_addr8); &#125; printf(&quot;ACTIVE RESET TIME:%s PID/TID: %d/%d &quot;, strftime(&quot;%H:%M:%S:%f&quot;, nsecs), pid, tid); printf(&quot;%s:%d -&gt; %s:%d&quot;, $saddr, $lport, $daddr, $dport); printf(&quot;\\n\\n&quot;); printf(&quot;%s&quot;, kstack);&#125;\n","slug":"LINUX/常见的bpftrace脚本","date":"2025-08-05T17:34:11.000Z","categories_index":"跟踪,LINUX","tags_index":"tracepoint,kfree,skb","author_index":"dandeliono"},{"id":"7c083edc735a96bb85e82e86710ce6ee","title":"Linux etc shadow 文件学习笔记","content":"Linux &#x2F;etc&#x2F;shadow 文件学习笔记Linux系统下，创建的用户信息如ID，家目录，默认shell保存在&#x2F;etc&#x2F;passwd下，该文件每行的第二位（以冒号分隔）表示密码。但真正的密码其实被加密后放在&#x2F;etc&#x2F;shadow里，&#x2F;etc&#x2F;passwd里只显示为x。本文主要介绍shadow文件各字段含义和相关的密码生命周期配置。\n本文内使用的Linux环境是centos7.3, 操作时间是2017年10月12号\n&#x2F;etc&#x2F;shadow每行和&#x2F;etc&#x2F;passwd一一对应，命令pwconv根据&#x2F;etc&#x2F;passwd生成。每行由9个字段（以冒号分隔）组成，如下是每个字段的含义：\n1. 登录名: ​如 root， 通过它，唯一匹配&#x2F;etc&#x2F;passwd中的一行\n2. 加密后的密码： ​通常的格式为: $X$ZZZZZZX为数字，表示不同的加密算法，具体如下：\n\n\n\nID\nMethod\n\n\n\n1\nMD5\n\n\n2a\nBlowfish\n\n\n5\nSHA-256\n\n\n6\nSHA-512\n\n\n第二个星号后面的ZZZZ为加密后的密文, 具体是通过glibc里的crypt函数来实现加密。文章最后会提供一段程序，通过盐值来加密明文。可以通过man 3 crypt了解函数使用方法\n如果该字段 是 ! 或者 *， 表示该用户无法用密码登录系统.但可以通过其他方式登录。如果该字段以感叹号!开头，其余是通常的格式，则系统认为密码被锁， ssh登录时即使输入正确的密码，也会拒绝登录. usermod -L username 就是使用该原理。usermod -U username 功能是解锁\n3. 密码最后修改时间： ​通过chage -d XX user可以设置该字段值为从1970 1月1号至改密码时的天数。0表示用户需要在下次登录时修改密码空值表示关闭密码有效期功能，即密码永远有效如下是设置为0时的系统行为\n1234567891011121314151617181920[root@linux /root]# grep test /etc/shadowtest:$6$IFqUZWcW$lEDn9cLw:17450:0:99999:7:::[root@linux /root]# chage -d 0 test[root@linux /root]# grep test /etc/shadowtest:$6$IFqUZWcW$lEDn9cLw:0:0:99999:7:::$ ssh test@XX.XX.XX.XXtest@XX.XX.XX.XX&#x27;s password:You are required to change your password immediately (root enforced)Last login: Wed Oct 11 23:09:07 2017 from XX.XX.XX.XXWARNING: Your password has expired.You must change your password now and login again!Changing password for user test.Changing password for test.(current) UNIX password:New password:Retype new password:passwd: all authentication tokens updated successfully.Connection to XX.XX.XX.XX closed.$然后用新密码就可以登录了， 这个可以用于管理员强制普通用户修改密码\n\n4. 最小时间间隔： ​通过chage -m XXX user可以设置该字段两次修改口令之间所需的最小天数。空或者0表示没有限制\n5. 最大时间间隔： ​通过chage -M XXX user可以设置该字段两次修改口令之间所需的最大天数。一旦超过，意味着密码过期.空表示没有限制.\n如果该值小于最小时间间隔，则用户无法修改密码,如下演示其行为:\n123456789[root@linux /root]# grep test /etc/shadowtest:$6$xxFzM1X0$GzIbWsFIqhhcJ:17450:5:4:7:::[root@linux /root]#[test@linux ~]$ passwdChanging password for user test.Changing password for test.(current) UNIX password:You must wait longer to change your passwordpasswd: Authentication token manipulation error\n\n6. 警告天数： ​通过chage -W XXX user可以设置该字段在密码过期前（即 密码最后修改时间 + 最大时间间隔），提前多少天通知用户. 此时仍可以正常登陆，只是多了一行提示空或者0表示无警告.如下演示告警信息:\n12345678[root@linux /root]# grep test /etc/shadowtest:$6$xxFzM1X0$GzIbWsFIqhh:17450:0:6:7:::[root@linux /root]#$ ssh test@XX.XX.XX.XXtest@XX.XX.XX.XX&#x27;s password:Warning: your password will expire in 6 daysLast login: Wed Oct 11 23:36:04 2017 from XX.XX.XX.XX[test@linux ~]$\n\n7. 非活动周期: ​通过chage -I XXX user可以设置该字段表示密码过期后，多少天内用户仍可以正常登陆，但要求立即修改密码。 一旦超过该天数，系统会拒绝用户登陆空值或者0表示没有非活动期，一旦密码过期直接拒绝登陆\n如下演示进入非活动期\n1234567891011121314151617181920212223[root@linux /root]# chage -l testLast password change\t\t\t\t\t: Oct 01, 2017Password expires\t\t\t\t\t: Oct 02, 2017Password inactive\t\t\t\t\t: Nov 01, 2017Account expires\t\t\t\t\t\t: neverMinimum number of days between password change\t\t: 0Maximum number of days between password change\t\t: 1Number of days of warning before password expires\t: 7[root@linux /root]# grep test /etc/shadowtest:$6$xxFzM1X0$GzIbWsFIqhhcJ:17440:0:1:7:30::$ ssh test@XX.XX.XX.XXtest@XX.XX.XX.XX&#x27;s password:You are required to change your password immediately (password aged)Last login: Wed Oct 11 23:51:05 2017 from XX.XX.XX.XXWARNING: Your password has expired.You must change your password now and login again!Changing password for user test.Changing password for test.(current) UNIX password:New password:Retype new password:passwd: all authentication tokens updated successfully.Connection to XX.XX.XX.XX closed.\n\n如下演示超过非活动期:\n1234567891011121314[root@linux /root]# chage -l testLast password change\t\t\t\t\t: Oct 01, 2017Password expires\t\t\t\t\t: Oct 02, 2017Password inactive\t\t\t\t\t: Oct 02, 2017Account expires\t\t\t\t\t\t: neverMinimum number of days between password change\t\t: 0Maximum number of days between password change\t\t: 1Number of days of warning before password expires\t: 7[root@linux /root]# grep test /etc/shadowtest:$6$wODB1.oE$39TBytc.5y0OkKn:17440:0:1:7:0::$ ssh test@10.211.55.9test@10.211.55.9&#x27;s password:Your account has expired; please contact your system administratorConnection closed by 10.211.55.9\n\n8. 用户过期时间： ​通过chage -E XXX user可以设置该字段用户过期时间，值表示为自19701月1号起的天数.密码过期后，用户只是无法使用密码登陆，还可以用其他方式。 一旦用户过期，任何方式都无法用该用户登陆空值表示永远不会过期0值不建议使用。解释取决于程序本身\n9. 其他为保留字段，为将来扩展功能用 ​自己写代码实现加密(C 和 Python实现) ​c\n12345678910#include &lt;crypt.h&gt;#include &lt;stdio.h&gt;int main(int argc, char *argv[])&#123; if(argc!=3) return -1; char *buf = crypt((const char *)argv[1], (const char *)argv[2]); printf(&quot;salt: %s, crypt: %s\\n&quot;, argv[2], buf); return 0;&#125;\n\n12345#gcc -g a.c -lcrypt #grep root /etc/shadow root:$6$DEgVEU0T$mwlTGb/nTtvpIJcoIy2t9xNMgv0.IT34WLvQ7VmbWJP9rU8Ysp9JyJ8I8PxEleGPWoirdbk4VKbhtCg6P.sm1.:17450:0:99999:7::: #./a.out abc \\$6\\$DEgVEU0T salt: $6$DEgVEU0T, crypt: $6$DEgVEU0T$mwlTGb/nTtvpIJcoIy2t9xNMgv0.IT34WLvQ7VmbWJP9rU8Ysp9JyJ8I8PxEleGPWoirdbk4VKbhtCg6P.sm1.\n\nPython 2.7 自带crypt模块，它是C库 crypt的binding， 实现更简单。代码如下：\n1234567[root@abc ~]# pythonPython 2.7.5 (default, Nov  6 2016, 00:28:07)[GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; import crypt&gt;&gt;&gt; crypt.crypt(&quot;abc&quot;, &quot;$6$DEgVEU0T&quot;)&#x27;$6$DEgVEU0T$mwlTGb/nTtvpIJcoIy2t9xNMgv0.IT34WLvQ7VmbWJP9rU8Ysp9JyJ8I8PxEleGPWoirdbk4VKbhtCg6P.sm1.&#x27;\n\n密码相关一些配置 ​&#x2F;etc&#x2F;login.defs 用来存放一些与创建用户和密码相关的配置信息当使用useradd创建新用户时，系统会读取该文件，然后写入&#x2F;etc&#x2F;shadows.和密码相关的只要是 PASS_MAX_DAYS，PASS_MIN_DAYS，PASS_MIN_LEN，PASS_WARN_AGE，ENCRYPT_METHOD 这几个参数UMASK用来定义默认的新建文件权限\n123456789101112131415161718[root@linux /root]# grep ^[^#] /etc/login.defsMAIL_DIR\t/var/spool/mailPASS_MAX_DAYS\t99999PASS_MIN_DAYS\t0PASS_MIN_LEN\t5PASS_WARN_AGE\t7UID_MIN                  1000UID_MAX                 60000SYS_UID_MIN               201SYS_UID_MAX               999GID_MIN                  1000GID_MAX                 60000SYS_GID_MIN               201SYS_GID_MAX               999CREATE_HOME\tyesUMASK           077USERGROUPS_ENAB yesENCRYPT_METHOD SHA512\n\n\n\n\n\n\n\n\n\n\n参考：man 8 pwconvman 5 shadowman 3 crypt\n","slug":"LINUX/Linux etc shadow 文件学习笔记","date":"2025-08-05T17:33:14.000Z","categories_index":"etc,LINUX","tags_index":"通过,chage,user","author_index":"dandeliono"},{"id":"dbdd74b0108262782d34164391ad385e","title":"Linux 下常用命令与技巧汇总","content":"Linux 下常用命令与技巧汇总收集自己常用到的linux命令与技巧,方便后续查找.不定期更新.\n1. 查询系统准确的启动时间 ​bash\n12date -d &quot;$(awk -F. &#x27;&#123;print $1&#125;&#x27; /proc/uptime) second ago&quot; +&quot;%Y-%m-%d %H:%M:%S&quot;2018-01-17 22:27:55\n\nbash\n12who -b system boot 2018-01-17 22:27\n\nbash\n12uptime -s2019-09-04 00:40:58\n\n2. 查询所有进程的准确启动时间和运行时长 ​-e 表示查询所有进程-o 表示按指定的格式输出lstart 为进程启动时间etime可以显示已经运行了多长时间args 为具体的命令行参数\nbash\n1ps -e -o pid,lstart,etime,args\n\nps -efL 加 -L 则显示线程\nTOP5 线程最多的进程列表\nbash\n1ps -eL -o pid,ppid,state,command | sort | uniq -c | sort -n -k1 | tail -5\n\n3. 设置时间 ​bash\n1date -s &quot;dd/mm/yyyy hh:mm:ss&quot;\n\n也可以在centos7下使用\nbash\n1timedatectl set-time &quot;2012-10-30 18:17:16&quot;\n\n4. 同步OS与硬件时间 ​将OS系统时间同步到硬件(RTC)\n将硬件时间同步到OS\n5. 查看硬件信息 ​bash\n123456lscpu #查看CPU信息lspci #查看PCI信息lsusb #查看外设USBlsscsi #查看SCSI设备lsblk #查看块设备dmidecode -t memory #查内存槽位信息. 不跟 -t , 查全量processor, Memory, BIOS\n\n6 锁定文件,不被任何程序修改 ​chattr 可以改变文件扩展属性, chattr +i 可以防止任何程序修改该文件. 即使有root权限.常用于锁定dns配置,防止dhcp程序自动修改. lsattr 查看当前的扩展属性. man chattr 查看所有参数含义.\nbash\n1chattr +i /etc/resolv.conf\n\n7. stat 查看文件详细元数据信息, 特别是inode ​bash\n12345678910$ stat messages File: ‘messages’ Size: 0 Blocks: 0 IO Block: 4096 regular empty fileDevice: fd01h/64769d Inode: 528438 Links: 1Access: (0600/-rw-------)  Uid: ( 0/ root)   Gid: ( 0/ root)Context: system_u:object_r:var_log_t:s0Access: 2018-01-14 21:32:01.977218590 +0800Modify: 2018-01-14 21:32:01.977218590 +0800Change: 2018-01-14 21:32:01.977218590 +0800 Birth: -\n\n8. xxd查看二进制文件 ​-s 指定起始位置(不指定则从0开始), -l 指定打印多少个字节\nbash\n1xxd -s 1 -l 5 /etc/hosts\n\n当将js,或其他文本类信息嵌入到C程序时, xxd -i 非常实用.\nbash\n1234567$ xxd a.js0000000: 7661 7220 6120 3d20 310a var a = 1.$ xxd -i a.jsunsigned char a_js[] = &#123; 0x76, 0x61, 0x72, 0x20, 0x61, 0x20, 0x3d, 0x20, 0x31, 0x0a&#125;;unsigned int a_js_len = 10;\n\n9. 批量修改文件名 ​用法:rename 原字符串 目标字符串 文件含义:原字符串：将文件名需要替换的字符串；目标字符串：将文件名中含有的原字符替换成目标字符串；文件：指定要改变文件名的文件列\n1234rename支持通配符?    可替代单个字符*    可替代多个字符[charset]    可替代charset集中的任意单个字符\n\n将所有已yum开头的文件的后缀名从.yumtx改为.txt\nbash\n1rename .yumtx .txt yum*\n\n10. df 查询文件系统挂载点信息 ​-h 使容量的大小显示更人性化 -T 显示文件系统类型\nbash\n12345678910$ df -hTFilesystem Type Size Used Avail Use% Mounted on/dev/mapper/VolGroup-lv_root ext4 50G 8.0G 39G 18% /devtmpfs devtmpfs 484M 0 484M 0% /devtmpfs tmpfs 495M 0 495M 0% /dev/shmtmpfs tmpfs 495M 460K 494M 1% /runtmpfs tmpfs 495M 0 495M 0% /sys/fs/cgroup/dev/sda1 ext4 477M 234M 215M 53% /boot/dev/mapper/VolGroup-lv_home ext4 12G 41M 11G 1% /hometmpfs tmpfs 99M 0 99M 0% /run/user/0\n\n-i 显示inode容量\nbash\n12345678910$ df -hiFilesystem Inodes IUsed IFree IUse% Mounted on/dev/mapper/VolGroup-lv_root 3.2M 170K 3.0M 6% /devtmpfs 121K 389 121K 1% /devtmpfs 124K 1 124K 1% /dev/shmtmpfs 124K 498 124K 1% /runtmpfs 124K 16 124K 1% /sys/fs/cgroup/dev/sda1 126K 358 125K 1% /boot/dev/mapper/VolGroup-lv_home 740K 31 740K 1% /hometmpfs 124K 1 124K 1% /run/user/0\n\n11. du 命令 ​单位是KB 如下获取&#x2F;tmp下所有文件的大小, 并按降序排列\nbash\n12345du -xs /tmp/* | sort -rn -k18 systemd-private-415928f8a9ee431abb4fdb7f6265aa87-chronyd.service-uw9sq74 test4 empty4 a.tar.gz\n\n12. find删除查找到的文件 ​find命令经常会用到, -type f表示只返回文件, -exec 可将已找到的结果作为标准输入执行其他命令.\nbash\n12find &lt;PATH&gt; -type f -exec rm &#123;&#125; \\; # 逐个删除查找到的文件, 将 path 改为实际要查找的目录find &lt;PATH&gt; -type f -exec rm &#123;&#125; + # 一次性删除查找到的文件\n\nbash\n1234567891011121314151617181920find [PATH] [option] [action] -mtime n : n为数字，意思为在n天之前的“一天内”被更改过的文件； -mtime +n : 列出在n天之前（不含n天本身）被更改过的文件名； -mtime -n : 列出在n天之内（含n天本身）被更改过的文件名； -newer file : 列出比file还要新的文件名 例如： find /root -mtime 0 # 在当前目录下查找今天之内有改动的文件   与文件权限及名称有关的参数： -name filename ：找出文件名为filename的文件 -size [+-]SIZE ：找出比SIZE还要大（+）或小（-）的文件 -tpye TYPE ：查找文件的类型为TYPE的文件，TYPE的值主要有：一般文件（f)、设备文件（b、c）、  目录（d）、连接文件（l）、socket（s）、FIFO管道文件（p）； -perm mode ：查找文件权限刚好等于mode的文件，mode用数字表示，如0755； -perm -mode ：查找文件权限必须要全部包括mode权限的文件，mode用数字表示 -perm +mode ：查找文件权限包含任一mode的权限的文件，mode用数字表示 例如： find / -name passwd # 查找文件名为passwd的文件 find . -perm 0755 # 查找当前目录中文件权限的0755的文件 find . -size +12k # 查找当前目录中大于12KB的文件，注意c表示byte\n\n13. xargs 命令 ​该命令可以将一个命令的输出作为参数传递给另一个命令。区别于管道符|是将将输出作为另一个命令的标准输入传递.\nbash\n1find /tmp -name *.png -type f | xargs tar -cvzf images.tar.gz\n\n默认是将内容放到参数的最后面, 如果要放到指定位置,需要使用 -i 和 &#123;&#125;如下所示，将第一个命令的输出放到&#123;&#125;出现的位置\nbash\n1ls /etc/*.conf | xargs -i cp &#123;&#125; /home/likegeeks/Desktop/out\n\n14. grep查询文本 ​在文件中查找字符串(不区分大小写)\nbash\n1grep -i &quot;the&quot; /etc/hosts\n\n输出成功匹配的行，以及该行之后的三行. -B 表示前三行. -C 指前后三行\nbash\n1grep -A 3 &quot;localhost&quot; /etc/hosts\n\n在一个文件夹中递归查询包含指定字符串的文件\n查找不包含 127 的所有行\nbash\n1grep -v &quot;127&quot; /etc/hosts\n\n递归搜索时忽略二进制文件,加参数-I\nbash\n1grep -r &quot;abc&quot; -I /etc\n\n打印被搜索的文件名\nbash\n1grep -H &quot;abc&quot; /etc/fstab\n\n15. tail 查看文件指定行信息 ​显示最后3行记录\nbash\n1234$ tail -3 /var/log/yum.log-20180101Dec 10 00:35:55 Installed: libvirt-debuginfo-3.2.0-14.el7_4.3.x86_64Dec 10 18:52:40 Installed: cloud-init-0.7.9-9.el7.centos.2.x86_64Dec 10 19:50:30 Erased: cloud-init-0.7.9-9.el7.centos.2.x86_64\n\n从第770行开始显示\nbash\n123$ tail -n +770 /var/log/yum.log-20180101Dec 10 18:52:40 Installed: cloud-init-0.7.9-9.el7.centos.2.x86_64Dec 10 19:50:30 Erased: cloud-init-0.7.9-9.el7.centos.2.x86_64\n\n16. 批量创建目录 ​bash\n1mkdir -p new_folder/&#123;folder_1,folder_2,folder_3,folder_4,folder_5&#125;\n\n17. echo 相关命令 ​显示当前使用哪个shell\n显示最近一个命令执行的结果码\n18. nohup与标准输出,标准错误输出 ​nohup配和&amp;可以让进程在后台运行, 如果没有显示指定, 默认将标准输出和错误输出重定向到 nohup.out1&gt;/dev/null 首先表示标准输出重定向到空设备文件，也就是不输出任何信息到终端，不显示任何信息2&gt;&amp;1 表示标准错误输出重定向等同于标准输出，因为之前标准输出已经重定向到了空设备文件，所以标准错误输出也重定向到空设备文件常用使用方式为:\nbash\n1nohup COMMAND &gt;output.log 2&gt;&amp;1 &amp;\n\n19. Linux模块的安装和卸载 ​bash\n123456lsmod #显示当前装入的内核模块modinfo module_name #显示模块信息modprobe -c #显示模块的配置信息 modprobe --show-depends module_name #显示模块的依赖信息modprobe module_name #手动加载模块rmmod module_name #卸载模块\n\nsystemd 读取 &#x2F;etc&#x2F;modules-load.d&#x2F; 中的配置加载额外的内核模块。配置文件名称通常为 /etc/modules-load.d/&lt;program&gt;.conf。如：\n12$ cat /etc/modules-load.d/bonding.confbonding\n\n使用 &#x2F;etc&#x2F;modprobe.d&#x2F;中的文件来配置传递参数，如:\nbash\n12$ /etc/modprobe.d/bonding.confoptions bonding mode=1\n\n别名\nbash\n123$ cat /etc/modprobe.d/myalias.conf# Lets you use &#x27;mymod&#x27; in MODULES, instead of &#x27;really_long_module_name&#x27;alias mymod really_long_module_name\n\n如果模块直接编译进内核，也可以通过启动管理器(GRUB, LILO 或 Syslinux)的内核行加入参数： modname.parametername=parametercontents\n列出直接编译到内核的模块列表\nbash\n12345$ cat /lib/modules/$(uname -r)/modules.builtinkernel/lib/zlib_deflate/zlib_deflate.kokernel/lib/zlib_inflate/zlib_inflate.kokernel/lib/crc16.kokernel/lib/crc32.ko\n\n20. column格式化输出 ​可以让一些命令的输出看起来更舒服些. 例如blkid,mount,cat /etc/fstab.\nbash\n123456789101112$ blkid/dev/sda1: UUID=&quot;04f8e14d-906d-475a-bacc-7d4be966f670&quot; TYPE=&quot;ext4&quot;/dev/sda2: UUID=&quot;8pXbJg-c7bG-CzS4-Xs9Y-SdBD-TYOJ-0r9hgF&quot; TYPE=&quot;LVM2_member&quot;/dev/mapper/VolGroup-lv_swap: UUID=&quot;c058935d-34e0-403c-82f6-435b1c8194ce&quot; TYPE=&quot;swap&quot;/dev/mapper/VolGroup-lv_root: UUID=&quot;e9fe20ee-5d6f-4610-98ca-7cbed5d012ad&quot; TYPE=&quot;ext4&quot;/dev/mapper/VolGroup-lv_home: UUID=&quot;43578ba0-a809-4274-931d-b92f881196ad&quot; TYPE=&quot;ext4&quot;$ blkid | column -t/dev/sda1: UUID=&quot;04f8e14d-906d-475a-bacc-7d4be966f670&quot; TYPE=&quot;ext4&quot;/dev/sda2: UUID=&quot;8pXbJg-c7bG-CzS4-Xs9Y-SdBD-TYOJ-0r9hgF&quot; TYPE=&quot;LVM2_member&quot;/dev/mapper/VolGroup-lv_swap: UUID=&quot;c058935d-34e0-403c-82f6-435b1c8194ce&quot; TYPE=&quot;swap&quot;/dev/mapper/VolGroup-lv_root: UUID=&quot;e9fe20ee-5d6f-4610-98ca-7cbed5d012ad&quot; TYPE=&quot;ext4&quot;/dev/mapper/VolGroup-lv_home: UUID=&quot;43578ba0-a809-4274-931d-b92f881196ad&quot; TYPE=&quot;ext4&quot;\n\n-s 参数指定可以指定分隔符. 默认是空格\nbash\n123$ grep -E &quot;sshd|qemu&quot; /etc/passwd | column -t -s:sshd x 74 74 Privilege-separated SSH /var/empty/sshd /sbin/nologinqemu x 107 107 qemu user / /sbin/nologin\n\n21. 通过pid查看进程的环境变量信息 ​使用strings, 可以格式化打印\nbash\n12345$ strings /proc/1158/environLANG=en_US.UTF-8PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/binNOTIFY_SOCKET=/run/systemd/notifySSH_USE_STRONG_RNG=0\n\n22. 通过pid查看进程对应可执行文件的绝对路径 ​bash\n1readlink /proc/[pid]/exe\n\n23. 通过pid查看进程的当前工作目录 ​24. lsof 一切皆文件 ​-p [PID] 只显示该进程打开的所有文件. 不带参数显示所有已打开的文件-d 对FD有效, 用于筛选文件列表. ^txt 显示除txt 其他所有类型的文件. 1 显示所有fd为1的文件. 可以使用,逗号连接多个选择-a 表示两个参数都必须满足 (AND)。如果没有 -a 标志，缺省的情况是显示匹配任何一个参数 (OR) 的文件-n 阻止网络地址转换-P 阻止端口号到端口名的转换-i protocol:@ip:port protocol 包括 tcp 和 udp. 显示符合该地址的文件列表 -u s s为用户名或者用户ID, 选择该用户下的文件\nlsof [name]name是 mount point或者文件系统对应的设备文件, 则显示在该文件系统上打开的所有文件列表name是 文件夹(非mount point), 则显示所有将该文件夹作为正常文件打开的列表. 例如 cwd, rtd. 如果 +d 打印所有在该目录下已打开的文件,但不递归查找子目录. +D 则允许递归查找\n谁在使用 &#x2F;var&#x2F;log&#x2F;audit&#x2F;audit.log\nbash\n123$ lsof /var/log/audit/audit.logCOMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEauditd 669 root 5w REG 253,1 2027785 524474 /var/log/audit/audit.log\n\n显示文件系统/下所有已打开的文件列表. 和fuser /效果一样\n所有在&#x2F;var下已打开的文件\nbash\n123456$ lsof +D /var/spool/COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEmaster 1297 root cwd DIR 253,1 4096 524402 /var/spool/postfixmaster 1297 root 10uW REG 253,1 33 524591 /var/spool/postfix/pid/master.pidqmgr 1301 postfix cwd DIR 253,1 4096 524402 /var/spool/postfixpickup 4178 postfix cwd DIR 253,1 4096 524402 /var/spool/postfix\n\n列出tcpdump这个用户打开的所有文件\nbash\n12345678910111213141516171819$ lsof -u tcpdumpCOMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEtcpdump 17905 tcpdump cwd DIR 253,1 4096 2752513 /roottcpdump 17905 tcpdump rtd DIR 253,1 4096 2 /tcpdump 17905 tcpdump txt REG 253,1 929928 1323976 /usr/sbin/tcpdumptcpdump 17905 tcpdump mem REG 253,1 62184 1328951 /usr/lib64/libnss_files-2.17.sotcpdump 17905 tcpdump mem REG 0,7 231120 socket:[231120] (stat: No such file or directory)tcpdump 17905 tcpdump mem REG 253,1 90664 1311459 /usr/lib64/libz.so.1.2.7tcpdump 17905 tcpdump mem REG 253,1 19776 1328757 /usr/lib64/libdl-2.17.sotcpdump 17905 tcpdump mem REG 253,1 2127336 1314561 /usr/lib64/libc-2.17.sotcpdump 17905 tcpdump mem REG 253,1 266496 1312167 /usr/lib64/libpcap.so.1.5.3tcpdump 17905 tcpdump mem REG 253,1 2512448 1312014 /usr/lib64/libcrypto.so.1.0.2ktcpdump 17905 tcpdump mem REG 253,1 23968 1311919 /usr/lib64/libcap-ng.so.0.0.0tcpdump 17905 tcpdump mem REG 253,1 164264 1311451 /usr/lib64/ld-2.17.sotcpdump 17905 tcpdump 0u CHR 136,1 0t0 4 /dev/pts/1tcpdump 17905 tcpdump 1u CHR 136,1 0t0 4 /dev/pts/1tcpdump 17905 tcpdump 2u CHR 136,1 0t0 4 /dev/pts/1tcpdump 17905 tcpdump 3u pack 231120 0t0 ALL type=SOCK_RAWtcpdump 17905 tcpdump 4w REG 253,1 864256 2756467 /root/abc2\n\n25. fuser 查找访问文件系统的进程 ​-v 显示具体的进程名和用户\nbash\n1234567$ fuser -v / USER PID ACCESS COMMAND/: root kernel mount / root 1 .rc.. systemd root 2 .rc.. kthreadd root 3 .rc.. ksoftirqd/0 root 5 .rc.. kworker/0:0H\n\n-k kill掉访问该文件系统的所有进程.慎用. 先使用上述命令查询\n26. dd 测试磁盘或者文件读写 ​高危命令, of一定要指向正确的文件, 不要指 /, /dev/vda, /dev/vda1等系统重要设备.该命令要在测试环境验证充分.if 表示从哪个设备&#x2F;文件读of 表示写到哪个设备&#x2F;文件bs 表示一次读写多少字节. 也可以使用 1K, 1M这样带单位的count 表示最多读写多少次. 总的读写量为 bs * count/dev/zero 可以无限读取\\0\nbash\n1dd if=/dev/zero of=/tmp/abc.txt bs=1M count=10\n\n27. rpm 安装&#x2F;更新&#x2F;卸载软件包 ​查询系统已安装的所有软件包\n查询某个文件所属的软件包名. 文件必须是绝对路径\nbash\n1rpm -qf /etc/ssh/sshd_config\n\nbash\n1rpm -qf `which strace`\n\n查询包所含有的文件\n查询包里的配置文件\nbash\n1rpm -qc openssh-server\n\n查询包里的文档\nbash\n1rpm -qd openssh-server\n\n查询软件包里的脚本信息\nbash\n1234567891011121314151617181920212223$ rpm -q --scripts openssh-serverpreinstall scriptlet (using /bin/sh):getent group sshd &gt;/dev/null || groupadd -g 74 -r sshd || :getent passwd sshd &gt;/dev/null || \\ useradd -c &quot;Privilege-separated SSH&quot; -u 74 -g sshd \\ -s /sbin/nologin -r -d /var/empty/sshd sshd 2&gt; /dev/null || :postinstall scriptlet (using /bin/sh):if [ $1 -eq 1 ] ; then # Initial installation systemctl preset sshd.service sshd.socket &gt;/dev/null 2&gt;&amp;1 || :fipreuninstall scriptlet (using /bin/sh):if [ $1 -eq 0 ] ; then # Package removal, not upgrade systemctl --no-reload disable sshd.service sshd.socket &gt; /dev/null 2&gt;&amp;1 || : systemctl stop sshd.service sshd.socket &gt; /dev/null 2&gt;&amp;1 || :fipostuninstall scriptlet (using /bin/sh):systemctl daemon-reload &gt;/dev/null 2&gt;&amp;1 || :if [ $1 -ge 1 ] ; then # Package upgrade, not uninstall systemctl try-restart sshd.service &gt;/dev/null 2&gt;&amp;1 || :fi\n\n查询包,自定义输出的格式和地段\nbash\n12345rpm -qa --queryformat &quot;%-35&#123;NAME&#125; %&#123;VERSION&#125; %&#123;RELEASE&#125; %&#123;INSTALLTIME:date&#125;\\n&quot;jzlib 1.1.1 6.el7 Thu 27 Jun 2019 11:30:01 AM HKTnss 3.36.0 7.1.el7_6 Tue 11 Jun 2019 11:13:42 PM HKThamcrest 1.3 6.el7 Thu 27 Jun 2019 11:30:01 AM HKTdhcp-libs 4.2.5 68.el7.centos.1 Tue 11 Jun 2019 11:13:43 PM HKT\n\n查询当前包的changelog. 常用于查看已解决的CVE列表\nbash\n1rpm -q --changelog openssh-server\n\n查询包所能提供的CAPABILITY\nbash\n1rpm -q --provides glibc\n\n查询哪些包依赖某个CAPABILITY\nbash\n1rpm -q --whatrequires [CAPABILITY]\n\n查询包所依赖的CAPABILITY\n查询包所有提供的CAPABILITY\nbash\n1rpm -q --whatprovides [CAPABILITY]\n\n校验当前包与原始状态的差别.5 – MD5 校验和S – 文件长度L – 符号链接T – 文件修改日期D – 设备U – 用户G – 用户组M – 模式 (包含许可和文件类型)? – 不可读文件如下标志文件的md5, 文件长度, 修改日志有变化\nbash\n12$ rpm -V openssh-serverS.5....T. c /etc/ssh/sshd_config\n\n安装本地包 -vh 获得一个详细的安装进程 --nodeps 忽略依赖关系\n卸载包\n升级包\n查询本地包的信息,要加-p\n28. yum 安装&#x2F;更新&#x2F;卸载软件包 ​bash\n12345678910111213yum install a #安装软件包a   (加上-y选项，可以在安装软件包时，不弹出是否继续的提示)yum install xxx --downloadonly --downloaddir=/xxx #只下载,不安装.yum reinstall xxx #重装yum remove a #卸载软件包ayum groups list #查看已安装的软件组和可用的软件组yum groups install &quot;Basic Web Server&quot; #安装软件组yum groups remove &quot;Basic Web Server&quot; #卸载软件组yum info a #查看软件包a的相关信息，如大小，版本等...yum update a #更新软件包ayum update #整体更新所有可更新的软件包yum provides 文件或目录 #查看文件由哪个rpm包提供的yum search tree #从仓库中搜索关键词为tree的包yum history #查看yum运行历史记录\n\n\n\n\n\n\n\n\n\n\n查找某个文件属于哪个包的方法有:如果文件已经安装在本机, 则rpm -qf xxx如果文件没有在本机安装, 则yum provides XXX XXX 可以是文件名, 也可以使用glob这样的通配符去查找比如安装某rpm包, 提示文件没找到, 则可以使用 yum provices xxx. 比如文件名为 abc.txt, 用yum provides *abc.txt去查找\n查询当前在repo里指定包的所有可用版本, 可用于升级到特定版本. yum update systemd默认升级到最新版\nbash\n1234567$ yum list --showduplicates systemdInstalled Packagessystemd.x86_64 219-42.el7_4.1 @updatesAvailable Packagessystemd.x86_64 219-42.el7 basesystemd.x86_64 219-42.el7_4.1 updatessystemd.x86_64 219-42.el7_4.4 updates\n\n升级指定包到特定的版本\nbash\n1yum install systemd-219-42.el7_4.4\n\n具体查看某次事务中安装,更新,删除的包列表\nbash\n123456789101112131415161718192021$ yum history info 307Loaded plugins: auto-update-debuginfo, fastestmirrorTransaction ID : 307Begin time : Sat Dec 9 20:24:11 2017Begin rpmdb : 700:b36eb7acc22b3ab4b107097d0b740dc6bfd84a58End time : 20:24:12 2017 (1 seconds)End rpmdb : 701:df2089f95d28d6f1f19561b39352bf3dd7a98c75User : root &lt;root&gt;Return-Code : SuccessCommand Line : install qemu-kvmTransaction performed with: Installed rpm-4.11.3-25.el7.x86_64 @base Installed yum-3.4.3-154.el7.centos.noarch @base Installed yum-plugin-fastestmirror-1.1.31-42.el7.noarch @basePackages Altered: Updated qemu-img-10:1.5.3-141.el7_4.2.x86_64 @updates Update 10:1.5.3-141.el7_4.4.x86_64 @updates Install qemu-kvm-10:1.5.3-141.el7_4.4.x86_64 @updates Updated qemu-kvm-common-10:1.5.3-141.el7_4.2.x86_64 @updates Update 10:1.5.3-141.el7_4.4.x86_64 @updateshistory info\n\n查询某个软件包的所有历史变更记录\nbash\n1234567891011$ yum history list cloud-initLoaded plugins: auto-update-debuginfo, fastestmirrorID | Command line | Date and time | Action(s) | Altered------------------------------------------------------------------------------- 312 | erase cloud-init | 2018-01-11 00:07 | Erase | 1 311 | install cloud-init | 2018-01-11 00:01 | Install | 1 310 | erase cloud-init | 2017-12-10 19:50 | Erase | 1 309 | install cloud-init | 2017-12-10 18:52 | Install | 1 306 | erase cloud-init | 2017-12-09 13:09 | Erase | 1 301 | install cloud-init | 2017-10-14 11:49 | Install | 5history list\n\n查询包的依赖关系\nbash\n123456789101112131415$ yum deplist stracepackage: strace.x86_64 4.12-4.el7 dependency: /bin/sh provider: bash.x86_64 4.2.46-29.el7_4 dependency: libc.so.6(GLIBC_2.15)(64bit) provider: glibc.x86_64 2.17-196.el7_4.2 dependency: libgcc_s.so.1()(64bit) provider: libgcc.x86_64 4.8.5-16.el7_4.1 dependency: libgcc_s.so.1(GCC_3.0)(64bit) provider: libgcc.x86_64 4.8.5-16.el7_4.1 dependency: libgcc_s.so.1(GCC_3.3.1)(64bit) provider: libgcc.x86_64 4.8.5-16.el7_4.1 dependency: rtld(GNU_HASH) provider: glibc.x86_64 2.17-196.el7_4.2 provider: glibc.i686 2.17-196.el7_4.2\n\n29. 解压缩命令 ​tar.gz\nbash\n1234tar -xzvf abc.tar.gz #解压缩tar -xzvf abc.tar.gz -C /path #解压缩到制动的path目录下tar -tzvf abc.tar.gz #不解压,只查看文件列表tar -czvf abc.tar.gz a.txt b.txt #将 a.txt 和 b.txt 压缩为 abc.tar.gz\n\n.gz\nbash\n12gzip -d FileName.gz #解压gzip FileName #压缩\n\n.bz .bz2\nbash\n1bzip2 -d FileName.bz2 #解压\n\n.tar.bz .tar.bz2\nbash\n1tar jxvf FileName.tar.bz2 #解压\n\n.Z\nbash\n12uncompress FileName.Z #解压compress FileName #压缩\n\n.zip\nbash\n12unzip FileName.zip #解压zip FileName.zip DirName #压缩\n\n.rar\nbash\n12rar x FileName.rar #解压rar a FileName.rar DirName #压缩\n\n.rpm\nbash\n1rpm2cpio FileName.rpm | cpio -div #解包\n\n.deb\nbash\n1ar p FileName.deb data.tar.gz | tar -xzvf #解包\n\n30. systemd 管理命令 ​bash\n1234567891011121314151617181920212223systemctl -t help #列出所有的单元类型systemctl --type &quot;unit&quot; #查看指定单元类型的状况, 替换 unit 为 &quot;mount&quot;, &quot;service&quot;, &quot;socket&quot;等systemctl --failed #查看所有加载失败的单元信息systemctl status sshd #查看sshd服务单元状况systemctl start sshd #启动sshd服务单元systemctl stop sshd #停止sshd服务单元systemctl restart sshd #重启sshd服务单元systemctl enable sshd #配置sshd服务单元开机自动启动systemctl disable sshd #配置sshd服务单元开机不启动systemctl reload sshd #重新加载sshd服务单元的配置文件systemctl mask sshd #彻底屏蔽sshd服务单元systemctl unmask sshd #取消屏蔽sshd服务单元systemctl list-units #列出当前所有的单元, 这是 systemctl 的默认命令systemctl get-default #查看系统默认启动的Targetsystemctl set-default multi-user.target # 设置系统默认的target为 多用户模式systemctl list-dependencies multi-user.target #查看该target下所有的unitsystemctl list-dependencies sshd # 查看一个service所依赖的所有unitsystemctl list-dependencies --reverse sshd # 查看哪个unit依赖该服务systemctl list-dependencies --before sshd # 查看所有unit after sshd, 也就是 sshd 配置里的before=systemctl list-dependencies --after sshd # 查看所有unit before sshdsystemctl cat sshd #查看service的配置文件systemctl show sshd #查看service的配置参数, 不带 service则显示默认的配置. 可用来查询nofile, noproc等限制资源参数systemctl list-jobs #查看有哪些job仍在运行或者等待运行\n\n\n\n\n\n\n\n\n\n\n&#x2F;etc&#x2F;security&#x2F;limits.conf无法设置systemd服务的资源限制. 参见:http://smilejay.com/2016/06/centos-7-systemd-conf-limits/\n列出当前旧的LSB脚本\nbash\n1234$ systemctl | grep LSB network.service loaded active running LSB: Bring up/down networking prl-x11.service loaded active exited LSB: Autostart script for Parallels service prltoolsd.service loaded active running LSB: Autostart script for guest tools service.\n\n查询systemd 版本\nbash\n123$ systemctl --versionsystemd 219+PAM +AUDIT +SELINUX +IMA -APPARMOR +SMACK +SYSVINIT +UTMP +LIBCRYPTSETUP +GCRYPT +GNUTLS +ACL +XZ -LZ4 -SECCOMP +BLKID +ELFUTILS +KMOD +IDN\n\n查看启动耗时\nbash\n12$ systemd-analyzeStartup finished in 579ms (kernel) + 2.964s (initrd) + 8.899s (userspace) = 12.442s\n\n查看每个服务的启动耗时,并打印TOP5\nbash\n123456$ systemd-analyze blame | head -5 4.949s NetworkManager-wait-online.service 3.379s network.service 987ms postfix.service 656ms dev-mapper-VolGroup\\x2dlv_root.device 591ms lvm2-monitor.service\n\n打印最耗时的一条启动链, 后面跟指定的service, 则打印该服务的启动链\nbash\n12345678910111213141516171819202122$ systemd-analyze critical-chainThe time after the unit is active or started is printed after the &quot;@&quot; character.The time the unit takes to start is printed after the &quot;+&quot; character.multi-user.target @8.863s└─postfix.service @7.864s +987ms └─network.target @7.856s └─NetworkManager.service @2.459s +64ms └─network-pre.target @2.458s └─firewalld.service @1.934s +523ms └─polkit.service @1.724s +203ms └─basic.target @1.721s └─sockets.target @1.721s └─iscsiuio.socket @1.721s └─sysinit.target @1.709s └─systemd-update-utmp.service @1.700s +8ms └─auditd.service @1.426s +272ms └─systemd-tmpfiles-setup.service @1.358s +58ms └─rhel-import-state.service @1.303s +52ms └─local-fs.target @1.299s └─home.mount @1.283s +15ms └─systemd-fsck@dev-mapper-VolGroup\\x2dlv_home.service @1.266s +14ms └─dev-mapper-VolGroup\\x2dlv_home.device @1.265s\n\n生成启动瀑布图, 使用chrome浏览器打开 a.svg. 分析启动性能更加直观\nbash\n1systemd-analyze plot &gt;a.svg\n\n启动瀑布图\njournalctl 相关操作\nbash\n12345678910journalctl # 查看所有日志（默认情况下 ，显示所有可以查看的日志）journalctl -k # 查看内核日志（不显示应用日志）journalctl -b # 查看系统本次启动的日志 journalctl -b -1 # 查看上一次启动的日志（需更改设置）journalctl -f # 实时滚动显示最新日志journalctl /usr/lib/systemd/systemd # 查看指定可执行文件的日志, 必须指定绝对路径journalctl _PID=1 # 查看指定进程的日志journalctl -u sshd # 查看某个 Unit 的日志journalctl --disk-usage # 显示日志占据的硬盘空间journalctl --list-boots # 显示每次启动的id和时间\n\n查看指定时间的日志\nbash\n12345678journalctl --since &quot;2012-10-30 18:17:16&quot;journalctl --since &quot;2020-10-01&quot;journalctl --since &quot;20 min ago&quot;journalctl --since &quot;-5m&quot;journalctl --since &quot;-5d&quot;journalctl --since yesterdayjournalctl --since &quot;2015-01-10&quot; --until &quot;2015-01-11 03:00&quot;journalctl --since 09:00 --until &quot;1 hour ago&quot;\n\n时间格式可参考man journalctl或者man 7 systemd.time\n31. Ubuntu&#x2F;Debian 检查已经安装好的软件包的更新日志 ​bash\n12zless /usr/share/doc/&lt;package -name&gt;/changelog.Debian.gzzless /usr/share/doc/&lt;package -name&gt;/changelog.gz\n\n32. 搜索含有指定字符的手册页 ​bash\n1234$ man -k logrotatedh_installlogrotate (1) - install logrotate config fileslogrotate (8) - rotates, compresses, and mails system logslogrotate.conf (5) - rotates, compresses, and mails system logs\n\n33. 快速删除大量小文件 ​使用rsync, 速度快但占用大量IO, 业务量低时使用. 高危命令\nbash\n12mkdir empty_dirrsync -a --delete empty_dir/ yourdirectory/\n\n使用find, 速度慢但消耗IO少. rm会遇到Argument list too long报错\nbash\n1find /tmp -type f -exec rm &#123;&#125; \\; #删除/tmp目录下所有文件\n\n34. sysctl 管理系统参数 ​bash\n123sysctl -a #打印当前参数sysctl -w net.ipv4.tcp_fin_timeout=30 #实时更新一个系统参数, 高危. 要验证好sysctl -p #读取/etc/sysctl.conf和 /etc/sysctl.d/下配置文件信息,使其生效\n\n35. 清空文件内容 ​假设文件名为 abc.txt, 以下方法都可以清空该文件的内容\nbash\n12&gt; abc.txtcp /dev/null abc.txt\n\n36. 查询系统调用 ​跟踪命令ls -rlt的系统调用, 将信息输出到a.txt\nbash\n1strace -ftT -o a.txt ls -rlt\n\n统计命令进行的系统调用信息\n跟踪具体的进程\n过滤系统调用\nbash\n12strace -e trace=open lsstrace -e trace=open,write ls\n\n37. 查看磁盘uuid和文件系统类型 ​bash\n123456$ lsblk -fNAME FSTYPE LABEL UUID MOUNTPOINTvda└─vda1 ext3 32236b41-fcde-460e-8c34-ba50515b33f2 /$ blkid/dev/vda1: UUID=&quot;32236b41-fcde-460e-8c34-ba50515b33f2&quot; TYPE=&quot;ext3&quot;\n\n38. 创建软连接 ​bash\n1ln -s regular_file softlink 创建软连接\n\n39. history查看历史记录 ​如果直接执行history没有显示时间戳,可以使用下面的命令\nbash\n12HISTTIMEFORMAT=&quot;%d/%m/%y %T&quot;history\n\n40. 查看进程的父子调用关系 ​-a 显示命令行参数-p 显示pid信息. {}表示线程-s 显示特定进程的所有父进程，不加该参数显示特定进程的所有子进程\nbash\n123456789101112131415$ pstree -psystemd(1)─┬─agetty(1176) ├─auditd(669)───&#123;auditd&#125;(670) ├─avahi-daemon(701)───avahi-daemon(720) ├─chronyd(721) ├─crond(1169) ├─dbus-daemon(702)───&#123;dbus-daemon&#125;(730) ├─dhclient(4673) ├─dnsmasq(1344)───dnsmasq(1345) ├─firewalld(776)───&#123;firewalld&#125;(952) ├─gssproxy(714)─┬─&#123;gssproxy&#125;(724) │ ├─&#123;gssproxy&#125;(725) │ ├─&#123;gssproxy&#125;(726) │ ├─&#123;gssproxy&#125;(727) │ └─&#123;gssproxy&#125;(728)\n\nbash\n12345678910# pstree -ap 3598bash,3598 └─pstree,3941 -ap 3598# pstree -aps 3598systemd,1 --switched-root --system --deserialize 30 └─sshd,869 └─sshd,3595 └─sshd,3597 └─bash,3598 └─pstree,3942 -aps 3598\n\n41. rsync文件 ​rsync [OPTION]… SRC [SRC]… DEST在指定复制源时，路径是否有最后的 “&#x2F;” 有不同的含义，例如：&#x2F;data ：表示将整个 &#x2F;data 目录复制到目标目录&#x2F;data&#x2F; ：表示将 &#x2F;data&#x2F; 目录中的所有内容复制到目标目录--stats : 输出文件传输的状态--progress : 输出文件传输的进度-a归档模式, 递归且保留符号链接，保留权限信息，时间戳，以及owner,group信息-z 打开压缩功能-v verbose更多打印信息本地同步\nbash\n1rsync -azv /var/opt/installation/inventory/ /root/temp/\n\n从本地到远端\nbash\n1rsync -avz /root/temp/ thegeekstuff@192.168.200.10:/home/thegeekstuff/temp/\n\n42. 查询的动态库链接信息 ​查询python这个程序依赖的所有动态库\nbash\n12345678910$ ldd `which python` linux-vdso.so.1 =&gt; (0x00007fffb1fb4000) libpython2.7.so.1.0 =&gt; /lib64/libpython2.7.so.1.0 (0x00007f5dee201000) libpthread.so.0 =&gt; /lib64/libpthread.so.0 (0x00007f5dedfe5000) libdl.so.2 =&gt; /lib64/libdl.so.2 (0x00007f5dedde0000) libutil.so.1 =&gt; /lib64/libutil.so.1 (0x00007f5dedbdd000) libm.so.6 =&gt; /lib64/libm.so.6 (0x00007f5ded8db000) libc.so.6 =&gt; /lib64/libc.so.6 (0x00007f5ded517000) /lib64/ld-linux-x86-64.so.2 (0x0000561b940b0000)[root@linux /tmp/new_folder]#\n\n43. 查询ascii编码表 ​速查编码信息\n44. iptable 内置防火墙 ​iptables内置了4个表，即raw表、filter表、nat表和mangle表，默认操作filter表.\n规则表之间的优先顺序如下：\n123456 Raw      --&gt;    mangle       -&gt;        nat    --&gt;      filterprerouting          prerouting          prerouting        inputoutput              postrouting         postrouting       output input               output            forward output forward\n\n查询表中的规则,当时打印匹配该规则的包和字节数\nREJECT 拦阻该数据包，并返回数据包通知对方，可以返回的数据包有几个选择：ICMP port-unreachable、ICMP echo-reply 或是tcp-reset（这个数据包包会要求对方关闭联机），进行完此处理动作后，将不再比对其它规则，直接中断过滤程序。 范例如下：\nbash\n1iptables -A INPUT -p TCP --dport 22 -j REJECT --reject-with ICMP echo-reply\n\nDROP 丢弃数据包不予处理，进行完此处理动作后，将不再比对其它规则，直接中断过滤程序。 REDIRECT 将封包重新导向到另一个端口（PNAT），进行完此处理动作后，将会继续比对其它规则。这个功能可以用来实作透明代理 或用来保护web 服务器。例如：\nbash\n1iptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT--to-ports 8081\n\nMASQUERADE 改写封包来源IP为防火墙的IP，可以指定port 对应的范围，进行完此处理动作后，直接跳往下一个规则链（mangle:postrouting）。这个功能与SNAT略有不同，当进行IP 伪装时，不需指定要伪装成哪个 IP，IP 会从网卡直接读取，当使用拨接连线时，IP 通常是由ISP公司的DHCP服务器指派的，这个时候MASQUERADE特别有用。范例如下：\nbash\n1iptables -t nat -A POSTROUTING -p TCP -j MASQUERADE --to-ports 21000-31000\n\nLOG 将数据包相关信息纪录在 &#x2F;var&#x2F;log 中，详细位置请查阅 &#x2F;etc&#x2F;syslog.conf 配置文件，进行完此处理动作后，将会继续比对其它规则。例如：\nbash\n1iptables -A INPUT -p tcp -j LOG --log-prefix &quot;input packet&quot;\n\nSNAT 改写封包来源IP为某特定IP或IP范围，可以指定port对应的范围，进行完此处理动作后，将直接跳往下一个规则炼（mangle:postrouting）.范例如下：\nbash\n1iptables -t nat -A POSTROUTING -p tcp -o eth0 -j SNAT --to-source 192.168.10.15-192.168.10.160:2100-3200\n\nDNAT 改写数据包包目的地 IP 为某特定 IP 或 IP 范围，可以指定 port 对应的范围，进行完此处理动作后，将会直接跳往下一个规则链（filter:input 或 filter:forward）。范例如下：\nbash\n1iptables -t nat -A PREROUTING -p tcp -d 15.45.23.67 --dport 80 -j DNAT --to-destination 192.168.10.1-192.168.10.10:80-100\n\nMIRROR 镜像数据包，也就是将来源 IP与目的地IP对调后，将数据包返回，进行完此处理动作后，将会中断过滤程序。 QUEUE 中断过滤程序，将封包放入队列，交给其它程序处理。透过自行开发的处理程序，可以进行其它应用，例如：计算联机费用…….等。 RETURN 结束在目前规则链中的过滤程序，返回主规则链继续过滤，如果把自订规则炼看成是一个子程序，那么这个动作，就相当于提早结束子程序并返回到主程序中。 MARK 将封包标上某个代号，以便提供作为后续过滤的条件判断依据，进行完此处理动作后，将会继续比对其它规则。范例如下：\nbash\n1iptables -t mangle -A PREROUTING -p tcp --dport 22 -j MARK --set-mark 22\n\n清空iptables规则\nbash\n12345678# set ACCEPT for default actioniptables -P INPUT ACCEPTiptables -P FORWARD ACCEPTiptables -P OUTPUT ACCEPT# flush all chainsiptables -F# delete all  user-defined chainsiptables -X\n\n丢弃TCP访问21端口的包\nbash\n1iptables -A INPUT -p tcp --dport 21 -j DROP\n\n加--line-numbers参数可以打印规则序号. 删除时用\nbash\n1iptables -nvL --line-numbers\n\n删除INPUT链里的第三个规则\nhttps://blog.csdn.net/htmlxx/article/details/51412750这里记录了一些常用的iptables规则操作\n45. tcpdump 抓包 ​常用表达式:非 : ! or “not” (去掉双引号)且 : &amp;&amp; or “and”或 : || or “or”-nn 不转换地址和端口号 -v 打印详情\n抓取所有经过eth1，地址是192.168.1.1且端口号为25的所有包\nbash\n1tcpdump -i eth1 host 192.168.1.1 and port 25\n\n-c count 抓取指定数目的包后退出\nbash\n1tcpdump -i eth0 -c 1000\n\n-C file_size 循环抓包,当文件达到指定的大小后,将包写入到新文件.举例如下:\nbash\n1234tcpdump -i eth0 -C 1 -w abc$ ls -rlt abc*-rw-r--r--. 1 tcpdump tcpdump 1001078 Jan 14 23:33 abc-rw-r--r--. 1 tcpdump tcpdump 647168 Jan 14 23:33 abc1\n\n抓取本机eth0网卡发往公网ip的数据包\nbash\n1tcpdump -n -i eth0 &#x27;not net 10.0.0.0/8 and not net 192.168.0.0/16 and not net 172.16.0.0/12&#x27;\n\n46. DNS查询工具 ​指定DNS服务器递归查\nbash\n123456789101112131415161718$ dig www.baidu.com @8.8.8.8; &lt;&lt;&gt;&gt; DiG 9.9.4-RedHat-9.9.4-51.el7 &lt;&lt;&gt;&gt; www.baidu.com @8.8.8.8;; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 4712;; flags: qr rd ra; QUERY: 1, ANSWER: 3, AUTHORITY: 0, ADDITIONAL: 1;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 512;; QUESTION SECTION:;www.baidu.com.\t\t\tIN\tA;; ANSWER SECTION:www.baidu.com.\t\t1043\tIN\tCNAME\twww.a.shifen.com.www.a.shifen.com.\t218\tIN\tA\t220.181.112.244www.a.shifen.com.\t218\tIN\tA\t220.181.111.188;; Query time: 78 msec;; SERVER: 8.8.8.8#53(8.8.8.8);; WHEN: Mon Jan 15 00:03:09 HKT 2018;; MSG SIZE  rcvd: 101\n\n反解析\nbash\n12345$ dig -x 74.125.135.105;; QUESTION SECTION:;105.135.125.74.in-addr.arpa. IN PTR;; ANSWER SECTION:105.135.125.74.in-addr.arpa. 83205 IN PTR ni-in-f105.1e100.net.\n\n全过程迭代跟踪\nbash\n1dig +trace www.baidu.com\n\n47. NTP时间同步 ​ntpdate 手工校准时间, 加 -q 只查询, 不同步时间, 加 -d 打开Debug模式, 但不真正更新时间\nbash\n12$ ntpdate ntp1.aliyun.com15 Jan 23:18:51 ntpdate[18981]: adjust time server 182.92.12.11 offset 0.000643 sec\n\n安装ntpd时查询ntp同步状态的命令为ntpq -p如果使用chronyd(centos7默认)是命令为chronyc sources -v\nbash\n1234567891011121314$ chronyc sources -v210 Number of sources = 2 .-- Source mode &#x27;^&#x27; = server, &#x27;=&#x27; = peer, &#x27;#&#x27; = local clock. / .- Source state &#x27;*&#x27; = current synced, &#x27;+&#x27; = combined , &#x27;-&#x27; = not combined,| / &#x27;?&#x27; = unreachable, &#x27;x&#x27; = time may be in error, &#x27;~&#x27; = time too variable.|| .- xxxx [ yyyy ] +/- zzzz|| Reachability register (octal) -. | xxxx = adjusted offset,|| Log2(Polling interval) --. | | yyyy = measured offset,|| \\ | | zzzz = estimated error.|| | | \\MS Name/IP address Stratum Poll Reach LastRx Last sample===============================================================================^* time5.aliyun.com 2 6 37 41 +2394ns[+1057us] +/- 15ms^- 120.25.115.19 2 6 37 40 -2559us[-2559us] +/- 67ms\n\n48. netstat查询网络连接信息 ​查看所有tcp下监听端口, p表示打印相应的进程\n查看所有已经建立的连接\n查看路由表\n查看网络统计信息进程\n-i 查网卡驱动信息\nbash\n1234567891011$ ethtool -i eth0driver: virtio_netversion: 1.0.0firmware-version:expansion-rom-version:bus-info: 0000:00:05.0supports-statistics: nosupports-test: nosupports-eeprom-access: nosupports-register-dump: nosupports-priv-flags: no\n\n不跟参数,显示速率,全双工模式等信息(虚拟机下没有)Link 为yes表示 网线连接正常\nbash\n123$ ethtool eth0Settings for eth0: Link detected: yes\n\n50. ip 网络维护 ​启用接口\n禁用接口\n设置接口MAC地址（设置前请先禁用接口）\n1ip link set &lt;接口名&gt; address &lt;值&gt;\n\n设置接口MTU（设置前请先禁用接口） 例：把ens33的MTU改成9000并检查。\n1234567# ip link show dev ens33 #修改前2: ens33: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc pfifo_fast state DOWN mode DEFAULT qlen 1000 link/ether 88:32:9b:ca:3f:49 brd ff:ff:ff:ff:ff:ff# ip link set ens33 mtu 9000# ip link show dev ens33  #修改后2: ens33: &lt;BROADCAST,MULTICAST&gt; mtu 9000 qdisc pfifo_fast state DOWN mode DEFAULT qlen 1000 link/ether 88:32:9b:ca:3f:49 brd ff:ff:ff:ff:ff:ff\n\n查看网卡的详细信息, 比如是bridge还是veth等类型\nbash\n12ip --details linkip link show type bridge #显示所有的桥接设备\n\n下面脚本可以显示每个链路的类型, 除了以太网\nbash\n12345678910!/bin/bash# Arguments: $1: Interface (&#x27;grep&#x27;-regexp).# Static list of types (from `ip link help`):TYPES=(bond bond_slave bridge dummy gre gretap ifb ip6gre ip6gretap ip6tnl ipip ipoib ipvlan macvlan macvtap nlmon sit vcan veth vlan vti vxlan tun tap)iface=&quot;$1&quot;for type in &quot;$&#123;TYPES[@]&#125;&quot;; do ip link show type &quot;$&#123;type&#125;&quot; | grep -E &#x27;^[0-9]+:&#x27; | cut -d &#x27;:&#x27; -f 2 | sed &#x27;s|^[[:space:]]*||&#x27; | while read _if; do echo &quot;$&#123;_if&#125;:$&#123;type&#125;&quot; done | grep &quot;^$&#123;iface&#125;&quot;done\n\n查看网卡状态和对应的IP信息\n给接口eth0赋予地址10.211.55.10 掩码是255.255.255.0(24代表掩码中1的个数)，广播地址是10.211.55.255\nbash\n1ip addr add 10.211.55.10/24 dev eth0\n\n删除eth0上的 10.211.55.10\nbash\n1ip addr del 10.211.55.10/24 dev eth0\n\n查看路由表信息\nbash\n1234$ ip routedefault via 10.211.55.1 dev eth010.211.55.0/24 dev eth0 proto kernel scope link src 10.211.55.9192.168.122.0/24 dev virbr0 proto kernel scope link src 192.168.122.1\n\n查询目的地为192.168.1.1时选择的路由情况\nbash\n123$ ip route get 192.168.1.1192.168.1.1 via 10.211.55.1 dev eth0 src 10.211.55.9 cache\n\n增加默认路由\nbash\n1ip route add default via 20.0.0.1\n\n策略性路由相对于传统的路由算法主要是引入了多路由表以及规则的概念。例如一个子网通过一个路由器与外界相连，路由器与外界有两条线路相连，其中一条的速度比较快，一条的速度比较慢。对于子网内的大多数用户来说对速度并没有特殊的要求，所以可以让他们用比较慢的路由；但是子网内有一些特殊的用户却是对速度的要求比较苛刻，所以他们需要使用速度比较快的路由。如果使用一张路由表上述要求是无法实现的，而如果根据源地址或其它参数，对不同的用户使用不同的路由表，这样就可以大大提高路由器的性能。优先级别越高的规则越先匹配（数值越小优先级别越高）。\n系统内置3张表:\n123表255 本地路由表（Local table）本地接口地址，广播地址，已及NAT地址都放在这个表。该路由表由系统自动维护，管理员不能直接修改。表254 主路由表（Main table）如果没有指明路由所属的表，所有的路由都默认都放在这个表里，route）所添加的路由都会加到这个表。表253 默认路由表（Default table）一般来说默认的路由都放在这张表，但是如果特别指明放的也可以是所有的网关路由。\n\n查看路由规则, 32766表示优先级, 越小优先级越高.\nbash\n1234$ ip rule list0: from all lookup local32766: from all lookup main32767: from all lookup default\n\n创建一个新的路由表, 原地址为192.168.2.0/24的消息全部路由到这张表里\nbash\n12$ echo &quot;250 test&quot; &gt;&gt; /etc/iproute2/rt_tables$ ip rule add from 192.168.2.0/24 table test\n\n上面的配置如果要持久化, 可以将命令写入/etc/rc.d/rc.local, 并给该文件可执行权限. chmod +x /etc/rc.d/rc.local\n查看某一张表里的路由信息\nbash\n1$ ip route list table main\n\n规则匹配的对象是所有的数据包，动作是选用路由表1的路由，这条规则的优先级是32800\nbash\n1$ ip rule add from 0/0 table 1 pref 32800\n\n规则匹配的对象是IP为192.168.3.112，tos等于0x10的包，使用路由表2，这条规则的优先级是1500\nbash\n1ip rule add from 192.168.3.112/32 [tos 0x10] table ２ pref 1500\n\n在table test里添加一条默认路由\nbash\n1ip route add default via 182.169.1.1 table test\n\n上面的规则是以源地址为关键字，作为是否匹配的依据的。除了源地址外，还可以用以下的信息：\n12345from -- 源地址to -- 目的地址（这里是选择规则时使用，查找路由表时也使用）tos -- IP包头的TOS（type of sevice）域dev -- 物理接口fwmark -- 防火墙参数\n\n采取的动作除了指定表，还可以指定下面的动作：\n12345Table 指明所使用的表Nat 透明网关Action prohibit 丢弃该包，并发送 COMM.ADM.PROHIITED的ICMP信息Reject 单纯丢弃该包Unreachable丢弃该包，并发送 NET UNREACHABLE的ICMP信息\n\n实际场景中尽量使用ip rule to xxxx table xxx针对目的IP指定路由表. 这样无论是收到请求后回复,或者主动发起请求都能匹配\n具体的语法可参考man ip-rule和man ip-routehttps://www.redhat.com/sysadmin/beginners-guide-network-troubleshooting-linuxhttp://www.microhowto.info/troubleshooting/troubleshooting_the_routing_table.html\nip netns操作https://www.cnblogs.com/sparkdev/p/9253409.html\n51. who 查看当前登录用户 ​bash\n12$ whoroot pts/0 2018-01-17 22:28 (10.211.55.2)\n\n52. 使用iftop查看主机实时网络流量 ​类似top这样的交互式界面监控网卡的实时流量,反向解析IP,同时还显示具体每个连接接受和发送的流量注意显示流量的单位为b. 例如 8Mb = 1MB. 所有流量值都是 per second按n关闭主机名解析, 按N关闭端口号解析, 按p显示连接的端口号iftop -f &quot;port 22&quot;过滤只显示端口为22的连接的流量信息, 过滤语法跟tcpdump一样直接运行iftop, 得到如下信息:\n\n\n\n\n\n\n\n\n\n第一行类似刻度尺的刻度，为显示流量图形的长条作标尺用的。&lt;&#x3D; &#x3D;&gt;这两个左右箭头，表示的是流量的方向。TX：发送流量RX：接收流量TOTAL：总流量Cumm：运行iftop到目前时间的总流量peak：流量峰值rates：分别表示过去 2s 10s 40s 的平均流量\n\n53. Bash相关环境变量 ​历史信息显示时间格式如下设置后, 使用history就会显示具体命令的具体执行时间.可以直接将其写在 .bashrc文件里\nbash\n1HISTTIMEFORMAT=&quot;%d/%m/%y %T &quot;\n\nPS1变量 设置登录提示符, 如下带颜色显示当前用户, 主机名和当前目录的绝对路径.\nbash\n1PS1=&quot;[\\e[1;31m\\u@\\h \\e[0;34m$&#123;PWD&#125;\\e[0m]\\\\$ &quot;\n\n54. 查本机的公网IP ​通常我们的主机都在内网, 访问互联网是都是换成公网IP后去连接的如下是查询我们主机的公网IP\n\n打开百度 www.baidu.com\n输出两个字符 ip, 敲回车\n新页面显示的本机IP就是公网IP\n\n55. 查询linux系统的一些限制信息 ​如下文章详细列举了redhat&#x2F;centos各版本操作系统的重要限制信息.比如ext4限制, 最大cpu, 最大Memoryhttps://access.redhat.com/articles/rhel-limits\n56. 判断虚拟化类型的N种方法 ​bash\n12$ dmidecode -s system-manufacturerOpenStack Foundation\n\nbash\n12$ systemd-detect-virtkvm\n\n57. cpu信息解读 ​bash\n12345678910111213141516171819202122232425$ lscpuArchitecture: x86_64 // 架构 CPU op-mode(s): 32-bit, 64-bit // 位数Byte Order: Little Endian // 小端CPU(s):                2                   // 逻辑cpu数On-line CPU(s) list: 0,1Thread(s) per core:    1                   // 一个核有一个线程. Core(s) per socket:    2                   // 一个插槽有两个核Socket(s):             1                   // 总共一个插槽NUMA node(s): 1Vendor ID: GenuineIntelCPU family: 6Model: 69Model name: Intel(R) Core(TM) i5-4260U CPU @ 1.40GHzStepping: 1CPU MHz: 2000.000BogoMIPS: 4000.00Hypervisor vendor: KVM // 虚拟化类型Virtualization type: full // 全虚拟化L1d cache: 32KL1i cache: 32KL2 cache: 256KL3 cache: 3072KNUMA node0 CPU(s): 0,1............\n\n58. taskset指定进程的CPU亲和性 ​返回的mask为十六进制, 3 代表 0x11 即绑定在cpu 0 和 1 上面. 最低位代表第一个cputaskset -p pid 返回pid对应进程当前的亲和性\nbash\n12$ taskset -p 691pid 691&#x27;s current affinity mask: 3\n\ntaskset -p mask pid设置进程的cpu亲和性也可以使用数字代替mask, 比如 taskset -c 0,1 -p 1 表示为pid为1的进程绑定cpu0和1\ntaskset 1 ls 执行ls命令是直接绑定到cpu0上.\n/proc/[pid]/status也可以查询, 举例:\n\n\n\n\n\n\n\n\n\nCpus_allowed: 3Cpus_allowed_list: 0-1解释:Cpus_allowed:3指出该进程可以使用CPU的亲和性掩码,因为我们指定为两块CPU,所以这里就是3,如果该进程指定为4个CPU(如果有话),这里就是F(1111).Cpus_allowed_list:0-1指出该进程可以使用CPU的列表,这里是0-1.\n进程运行时设置的亲和性并不影响其他线程, 如果要对java这样的多线程设置, 需要ps -efL 找到所有线程, 然后逐一设置taskset 1 java启动java命令时的绑定, 会应用到所有线程, 因为cpu的亲和性具有继承性\n59. Linux系统最大文件数量 ​ulimit -n返回当前user单进程可打开的最大进程数/proc/[pid]/limits 查询该进程的资源限制数据ls -rlt /proc/[pid]/fd | wc -l 查询该进程当前已打开的文件数lsof -p [pid] | wc -l 也可查询, 但要过滤掉cwd, 动态库等ulimit -n unlimited是无效的, 进程级的文件句柄不支持unlimited, 最大值为 /proc/sys/fs/nr_open, 该值默认为1048576.\nbash\n123456# ulimit -n 1048577-bash: ulimit: op]en files: cannot modify limit: Operation not permitted# ulimit -n unlimited-bash: ulimit: open files: cannot modify limit: Operation not permitted# cat /proc/sys/fs/nr_open1048576\n\n在centos7上nr_open的最大值可调整为2147483584\nbash\n12345# sysctl -w fs.nr_open=2147483585sysctl: setting key &quot;fs.nr_open&quot;: Invalid argumentfs.nr_open = 2147483585# sysctl -w fs.nr_open=2147483584fs.nr_open = 2147483584\n\n查询整个OS当前的文件数使用情况:file-max显示所有进程的最大文件数, 可通过echo 100000 &gt; /proc/sys/fs/file-max提高\nbash\n12$ cat /proc/sys/fs/file-max97703\n\nfile-nr是只读的, 第一个字段代表已分配(即已打开)文件数, 第二个字段从2.6开始一直为0, 第三个字段等于file-max\nbash\n12$ cat /proc/sys/fs/file-nr1344 0 97703\n\n列出每个进程的已打开的文件数, 用于找到当前哪个进程占用最多文件数-d 用于排除cwd, 动态库之类的信息-n 不用协议解析, 加快命令运行速度lsof -n -d0-999999 | awk &#39;&#123;print $2&#125;&#39; | sort | uniq -c | sort -k1 -n\n60. ss 查询socket连接信息 ​类似于netstat, 使用了netlink, 性能更好\nbash\n123456ss -s #显示各socket的统计信息ss -ntlp #-n 不解析, -t 显示tcp连接  -l 显示监听socket, -p 显示使用的进程名ss -t -a #查看所有tcp连接, 不带-a则指显示 Established 连接ss -to #查看tcp的keepalive信息ss -ti #查看tcp的内部信息,比如拥塞算法,rto,rtt,cwnd,ssthresh等ss -tm #查看tcp的内存使用信息, 比如收发缓冲区的大小\n\n还有一些高级的过滤用法\nbash\n12345678910ss -A raw,packet_raw -a -p #查看raw socket的信息ss -tn state listening #显示处于listening状态的连接ss -tn state established #显示处于established状态的连接ss -tn state listening &#x27;( sport == 22 )&#x27; #显示处于listening状态且源端口为22的连接ss -tn &#x27;( sport == 22 )&#x27; #显示源端口为22的连接ss -tn &#x27;( sport != 22 )&#x27; #显示源端口不为22的连接ss -tn &#x27;( sport == 22 || dport == 22 )&#x27; #显示源端口为22或者目标端口为22的连接ss -tn &#x27;( dst 10.211.55.2 &amp;&amp; dport == 58181 )&#x27; #显示目标ip为10.211.55.2且目标端口为58181的连接ss -tn &#x27;( src 127.0.0.1 )&#x27; #显示源ip为 127.0.0.1的连接ss -tn &#x27;( ! dst 10.211.55.2 &amp;&amp; dport != 58181 )&#x27; #显示目标ip不是10.211.55.2且目标端口不是58181的连接\n\nstate后面支持的TCP状态如下:\nc\n1234567891011121314static const char * const sstate_namel[] = &#123;&quot;UNKNOWN&quot;,[SS_ESTABLISHED] = &quot;established&quot;,[SS_SYN_SENT] = &quot;syn-sent&quot;,[SS_SYN_RECV] = &quot;syn-recv&quot;,[SS_FIN_WAIT1] = &quot;fin-wait-1&quot;,[SS_FIN_WAIT2] = &quot;fin-wait-2&quot;,[SS_TIME_WAIT] = &quot;time-wait&quot;,[SS_CLOSE] = &quot;unconnected&quot;,[SS_CLOSE_WAIT] = &quot;close-wait&quot;,[SS_LAST_ACK] = &quot;last-ack&quot;,[SS_LISTEN] = &quot;listening&quot;,[SS_CLOSING] = &quot;closing&quot;,&#125;;\n\n更多使用介绍请参考:https://www.cyberciti.biz/tips/linux-investigate-sockets-network-connections.htmlhttps://man7.org/linux/man-pages/man8/ss.8.htmlhttps://github.com/shemminger/iproute2/blob/main/misc/ssfilter.y\n61. 消除用户被锁的错误登录记录 ​通常管理员会配置pam_faillock或者pam_tally2, 当用户使用错误密码满足一定条件, 就将用户锁定一段时间. 如下是即时清除错误记录的命令\nbash\n12faillock --resetpam_tally2 -r -u [username]\n\n62. 估计RSS总和的大小 ​bash\n1ps aux | awk &#x27;&#123;sum+=$6&#125; END &#123;print sum / 1024&#125;&#x27;\n\n如下脚本也可以:\nbash\n1234567891011$ cat rss.sh#/bin/bashfor PROC in `ls /proc/|grep &quot;^[0-9]&quot;`do if [ -f /proc/$PROC/statm ]; then TEP=`cat /proc/$PROC/statm | awk &#x27;&#123;print ($2)&#125;&#x27;` RSS=`expr $RSS + $TEP` fidoneRSS=`expr $RSS \\* 4`echo $RSS&quot;KB&quot;\n\n63. 文件系统修复 ​针对xfs, 使用xfs_repair命令:-n 表示不修复, 只扫描并显示错误\nbash\n1xfs_repair -n -v /dev/mapper/vg-home\n\n不带-n, 执行修复. -v表示显示详情\nbash\n1xfs_repair -v /dev/mapper/vg-home\n\n最后方法：损失部分数据的修复方法根据打印消息，修复失败时：先执行xfs_repair -L &#x2F;dev&#x2F;sdd(清空日志，会丢失文件)，再执行xfs_repair &#x2F;dev&#x2F;sdd，再执行xfs_check &#x2F;dev&#x2F;sdd 检查文件系统是否修复成功。\n\n\n\n\n\n\n\n\n\n-L是修复xfs文件系统的最后手段，慎重选择，它会清空日志，会丢失用户数据和文件。\n64. 非交互式修改密码 ​bash\n1echo &quot;Linux@123&quot; |passwd --stdin root\n\n66. udev管理 ​查询设备的信息, 主要是环境变量\nbash\n12345678910111213udevadm info /dev/sda1P: /devices/pci0000:00/0000:00:1f.2/ata1/host0/target0:0:0/0:0:0:0/block/sda/sda1N: sda1S: disk/by-id/ata-CentOS_Linux-0_SSD_FFACPKN8NC9MK3EX3N4R-part1S: disk/by-uuid/5de1c8df-9b03-4830-9b56-b96a2290b78fE: DEVLINKS=/dev/disk/by-id/ata-CentOS_Linux-0_SSD_FFACPKN8NC9MK3EX3N4R-part1 /dev/disk/by-uuid/5de1c8df-9b03-4830-9b56-b96a2290b78fE: DEVNAME=/dev/sda1E: DEVPATH=/devices/pci0000:00/0000:00:1f.2/ata1/host0/target0:0:0/0:0:0:0/block/sda/sda1E: DEVTYPE=partitionE: ID_ATA=1E: ID_ATA_ROTATION_RATE_RPM=0E: ID_ATA_SATA=1E: ID_ATA_SATA_SIGNAL_RATE_GEN1=1\n\n加参数-a, 主要是设备的属性.\nbash\n12345678910111213udevadm info -a /dev/sda1P: /devices/pci0000:00/0000:00:1f.2/ata1/host0/target0:0:0/0:0:0:0/block/sda/sda1N: sda1S: disk/by-id/ata-CentOS_Linux-0_SSD_FFACPKN8NC9MK3EX3N4R-part1S: disk/by-uuid/5de1c8df-9b03-4830-9b56-b96a2290b78fE: DEVLINKS=/dev/disk/by-id/ata-CentOS_Linux-0_SSD_FFACPKN8NC9MK3EX3N4R-part1 /dev/disk/by-uuid/5de1c8df-9b03-4830-9b56-b96a2290b78fE: DEVNAME=/dev/sda1E: DEVPATH=/devices/pci0000:00/0000:00:1f.2/ata1/host0/target0:0:0/0:0:0:0/block/sda/sda1E: DEVTYPE=partitionE: ID_ATA=1E: ID_ATA_ROTATION_RATE_RPM=0E: ID_ATA_SATA=1E: ID_ATA_SATA_SIGNAL_RATE_GEN1=1\n\nudevadmin monitor用于实时监控udev实践,定位热插拔设备时特别有用.udevadmin test xxxx用于测试匹配设备的规则执行动作.xxx可以是/sys/class/block/sda,/sys/class/net/eth0, 也可以是/sys/devices下面的设备\n修改/etc/udev/udev.conf里的udev_log=&quot;debug&quot;可以打开更详细的日志使用journalctl -u systemd-udevd查看\n67. 一条命令启动web服务器 ​查询设备的信息, 主要是环境变量\nbash\n1python -m SimpleHTTPServer 8000\n\n68. dpkg,apt-get包管理命令 ​查询所有包\n查询某个文件属于哪个包\nbash\n12$ dpkg -S /usr/bin/growpartcloud-guest-utils: /usr/bin/growpart\n\n查询使用apt-get安装的历史记录\nbash\n1$ cat /var/log/apt/history.log\n\n69. 主机安全入侵盘查 ​检查LD_PRELOAD环境变量使用vi打开/etc/ld.so.preload这个文件是否有内容使用cat /proc/$$/mountinfo 或者cat /proc/mounts 看proc 是否被挂载到其他目录使用strace检查普通命令的系统调用如下个三个安全相关的分析案例:https://www.anquanke.com/post/id/160843https://superuser.com/questions/1183037/what-is-does-ld-so-preload-dohttps://www.jianshu.com/p/31e487daa79d\n70. top命令 ​top默认3秒刷新一次信息, 导致一些即时启动并结束的进程无法观察到, 可以用如下参数指定间隔:\n上面是以100ms为间隔, 也可进入交互模式后按d进行动态调整交互模式下按1显示每个cpu的使用率信息, 按M 进程按内存排序, 按H显示线程信息\ntop进程级的默认字段如果不够, 可以按f选择要显示的字段,比如last cpu used, 然后按q返回显示f后不仅可以增加,删除,调整字段顺序, 还可以按s将当前选中的字段作为排序的字段. 比如按内存排序如果要过滤进程, 比如只显示cmdline 包含字符abc的进程, 按o后输入COMMAND=abc, 碰到java这种进程, 可以按c显示完整的命令行\n查看单个进程的所有线程信息, 对java程序特别有用\nbash\n1234567891011121314151617$ top -Hp 4592Threads: 11 total, 0 running, 11 sleeping, 0 stopped, 0 zombie%Cpu(s):  0.0 us,  0.0 sy,  0.0 ni, 98.4 id,  0.0 wa,  1.6 hi,  0.0 si,  0.0 stMiB Mem : 7809.8 total, 5711.5 free, 910.8 used, 1187.6 buff/cacheMiB Swap: 0.0 total, 0.0 free, 0.0 used. 6748.4 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 4592 root 20 0 1810424 372332 14820 S 0.0 4.7 0:06.87 gopls 4593 root 20 0 1810424 372332 14820 S 0.0 4.7 0:06.01 gopls 4594 root 20 0 1810424 372332 14820 S 0.0 4.7 0:00.68 gopls 4595 root 20 0 1810424 372332 14820 S 0.0 4.7 0:07.53 gopls 4596 root 20 0 1810424 372332 14820 S 0.0 4.7 0:00.58 gopls 4597 root 20 0 1810424 372332 14820 S 0.0 4.7 0:00.00 gopls 4598 root 20 0 1810424 372332 14820 S 0.0 4.7 0:06.96 gopls 4605 root 20 0 1810424 372332 14820 S 0.0 4.7 0:06.64 gopls 4644 root 20 0 1810424 372332 14820 S 0.0 4.7 0:06.70 gopls 4647 root 20 0 1810424 372332 14820 S 0.0 4.7 0:00.01 gopls 4818 root 20 0 1810424 372332 14820 S 0.0 4.7 0:00.27 gopls\n\n71. ping命令 ​-i指定两次ping之前的间隔, 默认是1s\nbash\n1ping -i 0.1 www.163.com\n\n-s指定ping包的大小, 默认是64bytes\nbash\n1234$ ping -s 1024 www.163.comPING z163ipv6.v.bsgslb.cn (117.23.1.15) 1024(1052) bytes of data.1032 bytes from 117.23.1.15: icmp_seq=1 ttl=128 time=7.03 ms1032 bytes from 117.23.1.15: icmp_seq=2 ttl=128 time=7.77 ms\n\n72. ping和curl常见的网络报错 ​ping ip 报错：connect: Network is unreachable 原因是OS里没有相关路由导致From 192.168.100.1 icmp_seq=1 Destination Port Unreachable 原因是收到icmp reponse, 端口不可达， 中间设备或者iptable reject\ncur ip 一些报错:curl: (7) Failed to connect to 114.114.114.114: Network is unreachable 原因是OS里没有相关路由导致 很快返回curl: (7) Failed connect to 114.114.114.114:80; Connection refused 因为Curl收到icmp 应答消息 80端口不可达， 可能是中间设备iptable发送或者收到服务端rst消息， 80端口没有处于监听状态\n73. 一些常用的内存统计命令 ​统计所有进程占用的物理内存\nbash\n123# 使用 grep 查找 Pss 指标后，再用 awk 计算累加值$ grep Pss /proc/[1-9]*/smaps | awk &#x27;&#123;total+=$2&#125;; END &#123;printf &quot;%d kB\\n&quot;, total &#125;&#x27;391266 kB\n\n有时smaps文件会看到没有读写权限的内存段,但有p这个权限,即是内存,往往发生在libc这样的共享库和java程序.如下文章解释了具体的目的, java程序的情况下经常占用了很大的内存, 只要是为了预留一段连续的内存快这些内存是不会计算到Rss或者Pss里面, 但却实实在在占用了内存.https://unix.stackexchange.com/questions/226283/shared-library-mappings-in-proc-pid-mapshttps://unix.stackexchange.com/questions/353676/what-is-the-purpose-of-seemingly-unusable-memory-mappings-in-linux/353685#353685\nbash\n12347fae7db9f000-7fae7dc8f000 r-xp 00000000 08:05 536861 /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.207fae7dc8f000-7fae7de8f000 ---p 000f0000 08:05 536861 /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.207fae7de8f000-7fae7de97000 r--p 000f0000 08:05 536861 /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.207fae7de97000-7fae7de99000 rw-p 000f8000 08:05 536861 /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.20\n\n这篇文章详细地介绍了/proc/meminfo里的每一个字段的含义和一些有用的公式.http://linuxperf.com/?p=142\n74. 不同OS的文件编码 ​linux显示文件内容的编码方式\nbash\n12$ file -i old.txtold.txt: text/plain; charset=us-ascii\n\n将文件abc.txt 编����方式从iso8859-1改为utf-8\nbash\n1iconv -f iso8859-1 -t utf-8 abc.txt &gt; abc.txt.utf8\n\n讲当前文件夹所有文件的文件名编码从GBK改为UTF-8, -r表示递归所有子文件夹\nbash\n1convmv -f GBk -t UTF-8 --notest -r *，这\n\n参考: http://kuring.me/post/windows_linux_code&#x2F;\n75. 一些shell循环命令 ​bash\n123while true; do cat /proc/stat | grep &quot;cpu &quot; ; sleep 30; donefor n in &#123;1..1000&#125;; do touch a$n; rm -rf a$n; donefor file in $(ls -1 /proc/[1-9]*/status); do grep Name $file; done\n\n76. dhcp持续请求配置 ​Network管理时, 如果要在dhcp获取ip失败后不断重试,则在ifcfg-ethX文件里增加PERSISTENT_DHCLIENT=yes默认是1分钟后超时, 不重试,日志打印no dhcpoffers received如果加参数, 1分钟超时, 然后一个随机的时间间隔后再次重试. 一直重试到成功获取ip,日志打印如下:\nbash\n12no dhcpoffers receivedno working leases in persistent database - sleeping\n\n如果底层网卡有down, up动作, dhcp进程还在, 只显示一条报错信息receive_packet failed on eth0: Network is down当通过dhcp获取ip失败后, 如果配置了持续获取, 那么下次发起请求的时间间隔是个随机值,大致为150s~450s之间.具体的代码如下, retry_interval默认是300s,且不可调整https://github.com/42wim/isc-dhcp/blob/f54a146c7fe88889d60f0c1aa8e6f04707f95223/client/dhclient.c\nc\n123456789101112log_info (&quot;No working leases in persistent database - sleeping.&quot;);script_init(client, &quot;FAIL&quot;, (struct string_list *)0);if (client -&gt; alias)script_write_params(client, &quot;alias_&quot;, client -&gt; alias);script_go(client);client -&gt; state = S_INIT;tv.tv_sec = cur_tv.tv_sec + ((client-&gt;config-&gt;retry_interval + 1) / 2 +(random() % client-&gt;config-&gt;retry_interval));tv.tv_usec = ((tv.tv_sec - cur_tv.tv_sec) &gt; 1) ?random() % 1000000 : cur_tv.tv_usec;add_timeout(&amp;tv, state_init, client, 0, 0);detach ();\n\nNetworkManager管理时, 两个方法配置持久化:使用类似命令修改nmcli connection modify enps1s0 ipv4.dhcp-timeout infinity在ifcfg-ethX里增加IPV4_DHCP_TIMEOUT=2147483647参考 https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux&#x2F;7&#x2F;html&#x2F;networking_guide&#x2F;configuring_the_dhcp_client_behavior\n77. 解析&#x2F;proc&#x2F;[pid]&#x2F;status里关于信号的字段 ​bash\n12345678910111213141516#read -p &quot;PID=&quot; pidpid=$1cat /proc/$pid/status|egrep &#x27;(Sig|Shd)(Pnd|Blk|Ign|Cgt)&#x27;|while read name mask;do bin=$(echo &quot;ibase=16; obase=2; $&#123;mask^^*&#125;&quot;|bc) echo -n &quot;$name $mask $bin &quot; i=1 while [[ $bin -ne 0 ]];do if [[ $&#123;bin:(-1)&#125; -eq 1 ]];then kill -l $i | tr &#x27;\\n&#x27; &#x27; &#x27; fi bin=$&#123;bin::-1&#125; set $((i++)) done echodone# vim:et:sw=4:ts=4:sts=4:\n\n参考链接: https://stackoverflow.com/questions/4155483/proc-pidstatus-sigign-field\n78. 审计谁杀了进程 ​bash\n12auditctl -a always,exit -F arch=b64 -F a1=15 -S kill -k log_killauditctl -a always,exit -F arch=b64 -F a1=9 -S kill -k log_kill\n\n参考: https://jotdownux.wordpress.com/2016/01/23/whos-killing-that-process-whos-dumping-prelink-files-in-tmp-linux-auditd-to-the-rescue/\n79. 修改字符集的命令 ​bash\n123localectl set-locale LANG=en_US.UTF-8localectl set-locale LANG=zh_CN.UTF-8localectl list-locales\n\n80. bash脚本的退出码 ​脚本退出码是有特殊含义的,比如常见的127代表脚本里的命令没找到,130代表用户按了ctrl+c导致脚本停止\nbash\n12345678$ abc-bash: abc: command not found$ echo $?127$ sleep 1000^C$ echo $?130\n\n更详细的退出码含义请参见 https://tldp.org/LDP/abs/html/exitcodes.html\n81. 模拟不断增加内存致使触发oom的程序 ​c\n123456789101112131415161718#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;int main (void) &#123; int n = 0; while (1) &#123; if (malloc(1&lt;&lt;20) == NULL) &#123; printf(&quot;malloc failure after %d MiB\\n&quot;, n); return 0; &#125; printf (&quot;got %d MiB\\n&quot;, ++n); &#125; &#125; $ gcc memtest1.c $ ./a.out got 570528 MiB got 570529 MiB got 570530 MiB got 570531 MiBKilled\n\n具体的详细测试步骤: https://access.redhat.com/solutions/47692\n82. vi实现十六进制编辑功能 ​vim配合xxd对文件进行十六进制的编辑,达到类似UltraEdit的效果\nbash\n1234vi abc.txt:%!xxd修改完后,运行下面的命令. 注意左边改动会生效,右边的文本改动不生效:%!xxd -r\n\n更详细的请看 https://www.cnblogs.com/meibenjin/archive/2012/12/06/2806396.html\n83. iproute软件包里的一些常用命令 ​ifstat打印网卡的一些统计信息,类似ifconfig里的输出,但每次运行时输出从上次运行之后这段时间的统计,而ifconfig是打印的从网卡up以来的累积值-a则忽略历史文件,打印从网卡up后的累积值,-j表示输出格式是json.方便二次处理\nbash\n123456789101112$ ifstat#kernelInterface RX Pkts/Rate TX Pkts/Rate RX Data/Rate TX Data/Rate RX Errs/Drop TX Errs/Drop RX Over/Rate TX Coll/Ratelo 14 0 14 0 819 0 819 0 0 0 0 0 0 0 0 0eth0 25 0 18 0 2058 0 2784 0 0 0 0 0 0 0 0 0virbr0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0docker0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\nnstat打印协议栈里的一些统计计数,类似netstat -s的输出,与内核代码里的相关变量名更贴近,容易分析比较-r则忽略历史文件,打印从网卡up后的累积值,-z表示非0值的字段也输出\nbash\n12345678910$ nstat#kernelIpInReceives 8297 0.0IpInDelivers 8297 0.0IpOutRequests 7789 0.0IcmpInMsgs 6 0.0IcmpInErrors 3 0.0IcmpInDestUnreachs 6 0.0IcmpOutMsgs 6 0.0IcmpOutDestUnreachs 6 0.0\n\nTcpExtListenOverflows和TcpExtListenDrops 代表全队列或者半队列满了TcpInCsumErrors代表接受的包在tcp层的checksum校验不通过,被丢弃\n所有的计数项解释 https://www.kernel.org/doc/html/latest/networking/snmp_counter.html\nlnstat周期性输入一些内核统计数据, -d显示支持的具体文件和项\nbash\n1234567891011121314151617181920212223242526272829303132$ lnstat -d/proc/net/stat/nf_conntrack: 1: entries 2: searched 3: found 4: new 5: invalid 6: ignore 7: delete 8: delete_list 9: insert 10: insert_failed 11: drop 12: early_drop 13: icmp_error 14: expect_new 15: expect_create 16: expect_delete 17: search_restart/proc/net/stat/ndisc_cache: 1: entries 2: allocs 3: destroys 4: hash_grows 5: lookups 6: hits 7: res_failed 8: rcv_probes_mcast 9: rcv_probes_ucast 10: periodic_gc_runs 11: forced_gc_runs 12: unresolved_discards\n\n-k可输出特定项的信息\nbash\n1234567$ lnstat -k &quot;entries,insert_failed&quot;nf_connt|nf_connt| entries|insert_f| | ailed| 7| 0| 7| 0| 7| 0|\n\n这里的entris和conntrack -L的信息是一样的\ntc可以模拟网络延迟,限速等功能, 下面是查看当前策略的命令\nbash\n12$ tc -s qdisc$ tc -s qdisc ls dev eth0\n\n84. 链接跟踪 ​conntrack -L可显示当前的所有链接信息, 需要安装conntrack-tools后才可以使用\nbash\n123$ conntrack -Ltcp 6 430950 ESTABLISHED src=10.211.55.22 dst=10.211.55.2 sport=22 dport=51568 src=10.211.55.2 dst=10.211.55.22 sport=51568 dport=22 [ASSURED] mark=0 use=1tcp 6 5 TIME_WAIT src=10.211.55.22 dst=113.142.161.250 sport=48530 dport=80 src=113.142.161.250 dst=10.211.55.22\n\n查看当前链接跟踪表里的数目\nbash\n12$ cat /proc/sys/net/netfilter/nf_conntrack_count4\n\n查看当前生效的最大值\nbash\n12$ cat /proc/sys/net/netfilter/nf_conntrack_max65536\n\n如果内核出现nf_conntrack: table full, dropping packet., 则需要增加链接跟踪表的最大连接数\nbash\n1sysctl -w net.nf_conntrack_max=xxxxx\n\n还可以通过在加载模块时指定hashsize解决\nhttps://access.redhat.com/solutions/972673 介绍了针对匹配的包不进行连接跟踪的方法更详细的一些文章:https://blog.csdn.net/u010472499/article/details/78292811https://access.redhat.com/solutions/8721https://access.redhat.com/solutions/974723https://access.redhat.com/solutions/8721\nsysctl -a | grep nf_ 查看和链接跟踪所有的内核参数具体解释: https://www.kernel.org/doc/Documentation/networking/nf_conntrack-sysctl.txt\n下面的规则是用于跟踪对应连接的TCP状态, 每次连接都有自己的超时时间, 如果配置不当会导致丢包.比如ESTABLISHED的超时时间时5天, 如果改为5分钟, 且这5分钟该TCP没有任何报文交互, 则 iptables会将该连接置为invalid.这样即使用ss或者netstat查看该连接仍是ESTABLISHED, 任何发到该连接的报文全部会被丢弃.\nbash\n10 0 ACCEPT all -- * virbr0 0.0.0.0/0 192.168.122.0/24 ctstate RELATED,ESTABLISHED\n\n具体的信息见`man iptables-extensions\n一些介绍链接跟踪,iptables的文章https://www.cnblogs.com/liushaodong/archive/2013/02/26/2933593.htmlhttp://arthurchiao.art/blog/conntrack-design-and-implementation/\n85. 一些汇编语法 ​如下文章介绍了x86_64下许多寄存器的用途http://abcdxyzk.github.io/blog/2012/11/23/assembly-args/\n86. 使用crash分析内核coredump的一些文章 ​https://irmbor.co.rs/~dspalovic&#x2F;assets&#x2F;docsOracle&#x2F;E41138&#x2F;html&#x2F;ch10s02.htmlhttps://www.slideshare.net/PaulVNovarese/linux-crash-dump-capture-and-analysishttps://serverfault.com/questions/475721/how-to-use-kdump-crash-to-investigate-an-oom-issuehttps://www.redhat.com/archives/crash-utility/2012-December/msg00029.html\n最主要还是看crash的官方主页: https://crash-utility.github.io/\n87. Linux内核模块相关 ​使用weak-modules实现外部开发的内核模块在多个内核(KABI兼容的情况下)都可以使用, 这篇文章介绍了具体的方法https://www.cnblogs.com/xingmuxin/p/9092344.html\n88. ssh连接保活配置 ​编辑 &#x2F;etc&#x2F;ssh&#x2F;sshd_config, 添加如下两行, 重启sshd服务生效\nbash\n12ClientAliveInterval 60 ClientAliveCountMax 3\n\n89. 使用auditd审计功能 ​直接使用systemctl restart auditd重启auditd是失败.\nbash\n123$ systemctl restart auditdFailed to restart auditd.service: Operation refused, unit auditd.service may be requested by dependency only (it is configured to refuse manual start/stop).See system logs and &#x27;systemctl status auditd.service&#x27; for details.\n\n可以使用service auditd status/start/stop/restart参见: https://access.redhat.com/solutions/2664811\n启动&#x2F;重启auditd 报错”Not able to start Auditd service with pid file exists error”原因是有其他程序占用了kernel的audit功能, 可以使用auditctl -s查看其他程序的pidhttps://access.redhat.com/solutions/5431221\n/var/log/audit/audit.log里的日志格式, 默认不够人性化, 具体字段含义参见:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux&#x2F;7&#x2F;html&#x2F;security_guide&#x2F;sec-understanding_audit_log_files\n打印今天的审计摘要\nbash\n1234567891011121314151617$ aureport --start 01/10/2020 07:00:00 --end 01/10/2020 19:00:00Summary Report======================Range of time in logs: 04/27/2019 20:30:36.683 - 01/01/1970 08:00:00.000Selected time for report: 01/10/2020 07:00:00 - 01/10/2020 19:00:00Number of changes in configuration: 0Number of changes to accounts, groups, or roles: 0Number of logins: 0Number of failed logins: 0Number of authentications: 0Number of failed authentications: 0Number of users: 0Number of terminals: 0Number of host names: 0Number of executables: 0Number of commands: 0Number of files: 0\n\n昨天到今天这个时间段里的审计摘要\nbash\n1234567891011121314151617$ aureport --start yesterday --end todaySummary Report======================Range of time in logs: 04/27/2019 20:30:36.683 - 10/01/2020 13:10:43.415Selected time for report: 09/30/2020 00:00:00 - 10/01/2020 13:12:35Number of changes in configuration: 46Number of changes to accounts, groups, or roles: 0Number of logins: 25Number of failed logins: 2Number of authentications: 80Number of failed authentications: 1Number of users: 2Number of terminals: 16Number of host names: 3Number of executables: 14Number of commands: 11Number of files: 2\n\n从昨天开始事件统计摘要, -i让输出更人性化.\nbash\n1234567891011121314$ aureport --start yesterday -e -i --summaryEvent Summary Report======================total type======================14485 CRYPTO_KEY_USER8076 USER_START7639 USER_END6762 CRED_ACQ6761 USER_ACCT6753 LOGIN6704 CRED_REFR6527 CRED_DISP4966 NETFILTER_CFG\n\n统计可执行文件的摘要\nbash\n1234567891011121314151617181920$ aureport -x -i --summaryExecutable Summary Report=================================total file=================================33791 /usr/sbin/sshd31880 /usr/sbin/crond7432 /usr/lib/systemd/systemd4370 /usr/sbin/xtables-multi651 ?627 /usr/bin/python2.7608 /usr/bin/login309 /usr/bin/kmod232 /usr/lib/systemd/systemd-update-utmp204 /102 /usr/sbin/libvirtd94 /usr/sbin/tcpdump60 /usr/bin/su50 /usr/bin/dockerd-current42 /usr/sbin/groupadd\n\n显示每个audit日志记录的对应时间段\nbash\n123456$ aureport -tLog Time Range Report=====================/var/log/audit/audit.log.2: 04/27/2019 20:30:36.683 - 12/01/2019 10:40:01.857/var/log/audit/audit.log.1: 12/01/2019 10:40:01.858 - 06/08/2020 12:20:01.343/var/log/audit/audit.log: 06/08/2020 12:20:01.358 - 10/01/2020 13:42:52.455\n\n搜索今天所有的日志\nbash\n1$ ausearch --start today -i\n\n搜索今天所有的事件为系统调用的日志所有的事件列表参见: https://access.redhat.com/articles/4409591\nbash\n1ausearch --start today -i -m syscall\n\n搜索今天所有的关键字为xxxx的日志\nbash\n1ausearch --start today -i -k xxxx\n\n搜索今天所有的涉及文件&#x2F;etc&#x2F;passwd的日志\nbash\n1ausearch --start today -i -f /etc/passwd\n\n其他一些参考资料:https://cloud.tencent.com/developer/article/1359606\n90. 分析磁盘性能 ​blktrace -d &#x2F;dev&#x2F;vda blkparse -i vda -d vda.blktrace.bin btt -i vda.blktrace.bin -l vda.d2c_latency\n参考:https://developer.aliyun.com/article/698568https://tunnelix.com/debugging-disk-issues-with-blktrace-blkparse-btrace-and-btt-in-linux-environment/\niostat里await和svctm相关解释https://access.redhat.com/solutions/2039133https://access.redhat.com/articles/524353https://access.redhat.com/solutions/112613\n91. Grub相关问题 ​CentOS 6 和7 如何重装grubhttps://access.redhat.com/solutions/1521\n92. SLAB相关的知识 ​统计slab内各项的内存占用并降序排列TOP 10\nbash\n1awk &#x27;&#123;printf &quot;  %6i MB %s \\n&quot;,$6*$15/256,$1&#125;&#x27; /proc/slabinfo | sort -nrk1 | head -10\n\n93. nfs相关的知识 ​nfs Debughttps://access.redhat.com/solutions/262213https://access.redhat.com/solutions/1460313https://access.redhat.com/solutions/4253191https://access.redhat.com/solutions/2948091https://access.redhat.com/solutions/3765711https://access.redhat.com/solutions/28211\nnfs, rpc相关的Debug开关https://wiki.archlinux.org/index.php/NFS/Troubleshooting#RPC_debug_flags\n94. fork创建进程时的几个报错说明 ​有两个常见的报错:\nbash\n1211: Resource temporarily unavailable12: Cannot allocate memory\n\n在fork等创建进程的操作里, 当达到nproc的限制， 即用户的最大进程数， 报错Resource temporarily unavailable （这条规则对root用户无效） 如果在文件里对nproc配置为ulimited 这个值是根据内存算出来的， 公式可参考https://thelinuxcluster.com/2020/05/14/how-is-the-nproc-hard-limit-calculated-and-how-do-we-change-the-value-on-centos-7/\n当达到系统级参数pid_max限制，无法分配出pid时， 报错为Cannot allocate memory\nsysctl kernel.pid_max查询当前的最大值, pid_max默认是32768， 64位系统， 最大可配置为4百万左右kernel.threads-max 也控制系统能创建的最大进程&#x2F;线程数, 因为它的默认值非常大. 一般都没关注过. 总是kernel.pid_max先超出限制.如下是相关代码: linux-3.10.0-957.21.3.el7\\include\\linux\\threads.h\nc\n12345678910/* * This controls the default maximum pid allocated to a process */#define PID_MAX_DEFAULT (CONFIG_BASE_SMALL ? 0x1000 : 0x8000)/* * A maximum of 4 million PIDs should be enough for a while. * [NOTE: PID/TIDs are limited to 2^29 ~= 500+ million, see futex.h.] */#define PID_MAX_LIMIT (CONFIG_BASE_SMALL ? PAGE_SIZE * 8 : \\ (sizeof(long) &gt; 4 ? 4 * 1024 * 1024 : PID_MAX_DEFAULT))\n","slug":"LINUX/Linux 下常用命令与技巧汇总","date":"2025-08-05T17:32:52.000Z","categories_index":"bash,LINUX","tags_index":"https,com,redhat","author_index":"dandeliono"},{"id":"f25de4e9e0b08e6b09a0adc003a6760d","title":"ATOP工作原理总结","content":"ATOP工作原理总结ATOP是一款用于观察Linux性能的ASCII全屏交互式工具。类似于top,每隔一段时间报告 CPU，Memory，Disk，Network 等硬件的性能信息，对于严重过载的资源会高亮显示。 除此之外，还包括进程级的相关统计信息。比如进程的CPU、内存、磁盘利用率，用户名，进程状态，启动时间，进程ID等。对于在上一个周期内退出的进程还会显示退出状态码。所有进程信息默认按CPU占用率降序排列。\n运行方式 ​atop 3可以按每3秒刷新一次的频率在ASCII屏幕上显示即时性能信息，结果如下图。 可以实时了解当前系统的负载情况，同时具有很强的交互性。比如按键盘上的c可显示运行进程的完整名称（包括参数）。按键m可以按内存利用率降序排序当前进程列表，称之为内存视图。 \natop -a -w /var/log/atop/atop_&#123;HOSTNAME&#125;_20151123 30则每30秒记录一次数据并持久化到atop_{HOSTNAME}_20151123文件里。这样的命令通常被定时任务拉起。当服务器在特定时间点出现异常或者想要查看近几天内的性能信息时，就可以使用atop -r FileName读取文件并查看里面的性能数据。这里列出文件的一些默认设置\n\n性能收集时间间隔：30s\n文件名：atop_HOSTNAME_CURDAY\n文件保存天数：7days\n文件保存目录：&#x2F;var&#x2F;log&#x2F;atop\n执行ATOP的定时任务脚本：&#x2F;etc&#x2F;cron.d&#x2F;atop\n\n安装atop后主机上每天都会产生对应的atop文件，但我们执行crontab -l却找不到与atop相关的定时任务，其实它在&#x2F;etc&#x2F;cron.d下面。\n\n\n\n\n\n\n\n\n\nAAA:~ # cat &#x2F;etc&#x2F;cron.d&#x2F;atop0 0 * * * root &#x2F;etc&#x2F;atop&#x2F;atop.daily\ncron进程先在/var/spool/cron/tabs目录下搜索以用户名命名的文件，找到就读到内存中，其内容就是crontab -l的输出。接着继续搜索/etc/crontab和 /etc/cron.d目录下的所有文件并读取之。其格式和tabs下的略有不同，主要区别是指定了脚本的执行用户。\n原始信息收集 ​/proc是Linux下一种虚拟文件系统，存储的是当前内核运行状态的一系列特殊文件，用户可以通过查看这些文件了解系统硬件及当前正在运行进程的信息。ATOP正是从/proc下各种文件中读取原始信息，通过采样来计算周期内的即时数据。比如A时间点记录下消耗在用户态的cpu时间和总的CPU时间为M，X。在B时间点记录下此两项对应的值为N，Y。则当前的用户态CPU使用率为(N-M)/(Y-X)。等所有的性能信息都计算加工完毕后,使用libncurses库提供的函数将最终信息打印在字符界面上。\n/proc里记录的统计信息（除内存）都是自设备启动以来或者进程启动以来的累积值。如果没具体说明，则本文所讲到的各字段的值默认都是差值,即当前时间点采样值减去先前时间点的采样值.\n本文所有示例在Suse11环境下通过，atop版本为1.27。示例中数据仅为说明，一些影响阅读且与本文无关的内容会删除。如果想要全面了解&#x2F;proc文件系统里文件含义，可以man 5 proc\nCPU ​读取/proc/stat获取CPU的统计信息，包括每个CPU和总的CPU信息。\n\n\n\n\n\n\n\n\n\nAAA:~ # cat &#x2F;proc&#x2F;statcpu 3870117 23378 3233296 139792496 2051527 159950 29648 0 0cpu0 1903376 11614 1577768 70138672 1000644 81353 12021 0 0cpu1 1966740 11763 1655527 69653824 1050882 78597 17626 0 0….btime 1447557115processes 3543228procs_running 1procs_blocked 0\ncpuN行后面的数值含义从左到右分别是：user，nice，system，idle，iowait，irq，softirq，steal，guest。单位为jiffies，该值等于1/hertz秒。hertz在大部分系统里为100。可以用如下命令查询：\n\n\n\n\n\n\n\n\n\nAAA:~ # getconf -a | grep TCKCLK_TCK 100\n那么当前每个CPU的利用率为：CPU usage = (total - idle - iowait) / total total为cpuN这一行所有值之和.\n顺便介绍下其他几个比较有用的字段含义：\n\nbtime：记录系统开机启动时距1970年1月1号多少秒\nprocesses (total_forks)：自系统启动以来所创建的任务的数目\nprocs_running：当前处于运行队列的进程数\nprocs_blocked：当前被阻塞的进程数\n\nMemory Swap ​读取/proc/meminfo获取内存统计信息，读取/proc/vmstat获取页交换信息。 在高负荷的服务器里当内存不够用时，OS会将本应写入内存的数据写入到Swap空间，等内存充足时再将SWAP内的数据交换到内在里。内存和Swap 的这种交换过程称为页面交换（Paging），单位为页，大小是4K。 在PAGE这行 swout字段显示一秒中有多少页写入Swap。如果这个值超过10，则内存资源会红色高亮显示。只要该值11 &lt;= X &lt; 10，则表明当前物理内存已经不足，有页交换操作。ATOP会以青灰色高亮内存资源，表示已经出现瓶颈但不是特别严重。\n\n计算公式为： swouts / nsecs\nswouts 从 /proc/vmstat 的 pswpout字段获得\nnescs为采样的时间间隔\n\n系统自带的vmstat命令也可以观察到SWAP的交换情况，它正是通过读取/proc/vmstat来获取页交换信息的\n\n\n\n\n\n\n\n\n\nAAA:~ # strace -ftT -e trace&#x3D;open vmstat &gt;&#x2F;dev&#x2F;null09:36:16 open(“&#x2F;etc&#x2F;ld.so.cache”, O_RDONLY) &#x3D; 3 &lt;0.000015&gt;09:36:16 open(“&#x2F;lib64&#x2F;libc.so.6”, O_RDONLY) &#x3D; 3 &lt;0.000014&gt;09:36:16 open(“&#x2F;proc&#x2F;meminfo”, O_RDONLY) &#x3D; 3 &lt;0.000027&gt;09:36:16 open(“&#x2F;proc&#x2F;stat”, O_RDONLY) &#x3D; 4 &lt;0.000018&gt;09:36:16 open(“&#x2F;proc&#x2F;vmstat”, O_RDONLY) &#x3D; 5 &lt;0.000018&gt;\nAAA:~ # cat &#x2F;proc&#x2F;meminfoMemTotal: 7669188 kBMemFree: 1909052 kBBuffers: 424088 kBCached: 3670052 kB….SwapCached: 0 kBSwapTotal: 8393920 kBSwapFree: 8393920 kBMapped: 681904 kB….Shmem: 736624 kBSlab: 178552 kBSReclaimable: 139164 kB\n上述meminfo文件字段解释：\n\nMemTotal：所有可用RAM大小（即物理内存减去一些预留位和内核的二进制代码大小）\nMemFree：被系统留着未使用的内存\nBuffers：用来给文件做缓冲大小\nCached：被高速缓冲存储器（cache memory）用的内存的大小\nSwapTotal: 交换空间的总大小\nSwapFree: 未被使用交换空间的大小\nSlab: 内核数据结构缓存的大小，可以减少申请和释放内存带来的消耗。\nSReclaimable:可收回Slab的大小\nShmem: 共享内存大小\n\n内存利用率的公式为：(MemTotal - MemFree - Cached - Buffers) / MemTotal. Shmem这部分内存是包含在Cache里的，其实它是无法被回收的。 所以从ATOP2.0版本开始，该利用用率公式变为：(MemTotal - MemFree - Cached - Buffers +　Shmem) / MemTotal . 这个结果已经非常准确了。\nSwap利用率公式为：(SwapTotal - SwapFree) / SwapTotal\nDisk ​读取&#x2F;proc&#x2F;diskstats获取磁盘信息。从左至右分别对应主设备号，次设备号和设备名称。后续的11个列解释如下，除了第9个列外所有的列都是从启动时的累积值。\n12345678910AAA:~ # cat /proc/diskstats  8       0 sda 139119 267262 3848795 1357456 3942149 4733328 62031044 62148876 0 36083024 63502552 8       1 sda1 27 415 1388 472 0 0 0 0 0 452 472 8       2 sda2 83999 87253 1992917 588124 883514 1359683 17920106 16570224 0 6728052 17157604 8       3 sda3 4 0 14 84 0 0 0 0 0 84 84 8       5 sda5 49767 174838 1493156 706492 1414495 2617855 32228098 19276908 0 14162008 19981604 8       6 sda6 5257 4121 359986 60828 726761 755790 11882840 13347236 0 6084640 13407536 8       7 sda7 20 194 428 508 0 0 0 0 0 496 508 8       8 sda8 21 405 426 604 0 0 0 0 0 400 604 7       0 loop0 0 0 0 0 0 0 0 0 0 0 0\n\n\n第1列：读磁盘的次数，成功完成读的总次数。\n第2列：合并读次数，为了效率可能会合并相邻的读和写。从而两次4K的读在它最终被处理到磁盘上之前可能会变成一次8K的读，才被计数（和排队），因此只有一次I&#x2F;O操作。这个域使你知道这样的操作有多频繁。\n第3列：读扇区的次数，成功读过的扇区总次数。\n第4列：读花费的毫秒数，这是所有读操作所花费的毫秒数（用__make_request()到end_that_request_last()测量）。\n第5列：写完成的次数，成功写完成的总次数。\n第6列：合并写次数\n第7列：写扇区的次数，成功写扇区总次数。\n第8列：写花费的毫秒数，这是所有写操作所花费的毫秒数（用__make_request()到end_that_request_last()测量）。\n第9列：I&#x2F;O的当前进度，只有这个域应该是0。当请求被交给适当的request_queue_t时增加和请求完成时减小。\n第10列：花在I&#x2F;O操作上的毫秒数，这个域会增长只要field 9不为0。\n第11列：加权， 花在I&#x2F;O操作上的毫秒数，在每次I&#x2F;O开始，I&#x2F;O结束，I&#x2F;O合并时这个域都会增加。这可以给I&#x2F;O完成时间和存储那些可以累积的提供一个便利的测量标准。\n\n下面表格列出常用字段的计算方法，表中的第X列是指/proc/diskstats文件里的对应列的差值（即两次采样点所得值的差值）\n\n\n\nATOP字段\n含义\n计算公式\n单位\n\n\n\nMBr&#x2F;s\n平均每秒读数据量\n第3列 * 2 &#x2F; 1024 &#x2F; nsecs\nMB&#x2F;s\n\n\nMBw&#x2F;s\n平均1秒内写数据量\n第7列* 2 &#x2F; 1024 &#x2F; nsecs\nMB&#x2F;s\n\n\navio\nIO操作的平均操作时长\n第10列 &#x2F; iotot\nms\n\n\navq\n平均阵列深度，即加权后的IO操作时长\n第11列&#x2F;iotot\nms\n\n\nbusy\n磁盘利用率\n第10列 &#x2F; mstot\n百分比\n\n\n\nnsecs：采样时间间隔\niotot：读写次数之和，即第1列+第5列\nmstot：利用CPU数据计算的平均间隔时间，单位是毫秒。公式为cputot * 1000 / hertz / nrcpu\ncputot：两次采样点之间所有cpu的消耗时间之和, 单位是 jiffies\nhertz：100 表示1秒内有100个jiffies\nnrcpu：主机CPU个数\n\n如果avq远大于avio,则说明IO大部分消耗在等待和排队中，而不是数据传输本身。\nNetwork ​读取&#x2F;proc&#x2F;net&#x2F;dev获取所有网卡信息\n\n\n\n\n\n\n\n\n\nAAA:~ # cat &#x2F;proc&#x2F;net&#x2F;devInter-| Receive | Transmitface |bytes packets errs drop fifo frame compressed multicast|bytes packets errs drop fifo colls carrier compressedlo:5993298914 41500939 0 0 0 0 0 0 5993298914 41500939 0 0 0 0 0 0eth0: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0eth4:46036327 544231 0 0 0 0 0 1 31411179 41268 0 0 0 0 0 0eth5:48454032 556137 0 0 0 0 0 30920 3808752 7778 0 0 0 0 0 0eth2: 7766519 106577 0 0 0 0 0 3428 531166 7950 0 0 0 0 0 0eth3:50947490 669306 0 0 0 0 0 30980 680 8 0 0 0 0 0 0bond1:94490359 1100368 0 0 0 0 0 30921 35219931 49046 0 0 0 0 0 0AAA:~ #\n\n最左边的表示接口的名字，Receive表示收包，Transmit表示发包。\nbytes：收发的字节数\npackets：表示收发正确的包量\nerrs：表示收发错误的包量\ndrop：表示收发丢弃的包量\n上面四个值是自网卡启动以来的累积值, 执行ifconfig ethX down;ifconfig ethX up会清零这些值\n\n网卡的带宽和双工模式并不是从/proc读取，而是通过类似下面的代码获取。\nc\n1234567891011121314151617181920212223#include &lt;string.h&gt;#include &lt;stdio.h&gt;#include &lt;sys/ioctl.h&gt;#include &lt;linux/ethtool.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;#include &lt;linux/sockios.h&gt;#include &lt;linux/if.h&gt;int main(int argc, char **argv) &#123; int sockfd; struct ifreq ifreq; struct ethtool_cmd \tethcmd; sockfd = socket(AF_INET, SOCK_DGRAM, 0); memset(&amp;ifreq, 0, sizeof ifreq); memset(&amp;ethcmd, 0, sizeof ethcmd); strncpy((void *)&amp;ifreq.ifr_ifrn.ifrn_name, &quot;eth4&quot;, sizeof ifreq.ifr_ifrn.ifrn_name-1); ifreq.ifr_ifru.ifru_data = (void *)&amp;ethcmd; ethcmd.cmd = ETHTOOL_GSET; ioctl(sockfd, SIOCETHTOOL, &amp;ifreq); printf(&quot;speed is %d Mb, mode is %s duplex\\n&quot;,ethcmd.speed,ethcmd.duplex ? &quot;Full&quot; : &quot;Half&quot;); return 0;&#125;\n\n这篇文章详细地介绍了ETHTOOL 这个操作，只需要配合ioctl就可以获得网卡的全部信息。Linux下的ethtool工具也是通过这种方式查询网卡驱动和配置信息。\n\n\n\n\n\n\n\n\n\nAAA:~ # strace -ftT -e trace&#x3D;ioctl ethtool eth4 &gt;&#x2F;dev&#x2F;null11:31:48 ioctl(1, SNDCTL_TMR_TIMEBASE or TCGETS, 0x7fffca09bb30) &#x3D; -1 ENOTTY (Inappropriate ioctl for device) &lt;0.000012&gt;11:31:48 ioctl(3, SIOCETHTOOL, 0x7fffca09cb10) &#x3D; 0 &lt;0.000017&gt;11:31:48 ioctl(3, SIOCETHTOOL, 0x7fffca09cb10) &#x3D; 0 &lt;0.000012&gt;11:31:48 ioctl(3, SIOCETHTOOL, 0x7fffca09cb10) &#x3D; 0 &lt;0.000025&gt;11:31:48 ioctl(3, SIOCETHTOOL, 0x7fffca09cb10) &#x3D; 0 &lt;0.000027&gt;AAA:~ #\n网卡利用率的计算方法如下： 全双工： 获取rbytes与wbytes中的最大值A, curspeed &#x3D; A * 8 &#x2F; 1000 单双工： curspeed &#x3D; (rbytes + wbytes) * 8 &#x2F; 1000 最终利用率的公式： curspeed &#x2F; （网卡带宽 * 1000）\n\n\n\n\n\n\n\n\n\nrbytes和wbytes是从&#x2F;proc&#x2F;net&#x2F;dev读取 * 8 是把 bytes 转化为 bit, / 1000 单位变为 Kb, 通过SIOCETHTOOL获得的带宽是Mb, 所以 * 1000 转换为Kb\natop预设了针对每个资源（如CPU，Memory)的阈值， 如果当前利用率超过了阈值，则会将该资源红色高亮显示。 当达到阈值的80%时，使用青灰色高亮显示。这些值可以用户自定义。如下是资源及对应的默认阈值：\n\n\n\n资源\n阈值\n\n\n\nCPU\n90%\n\n\n内存\n90%\n\n\nSwap\n80%\n\n\n磁盘\n70%\n\n\n网卡\n90%\n\n\n默认进程列表是按CPU排序的。按A会自动依照当前过载最严重的资源排序当前进程列表。如何检测谁是最严重过载的资源。做法是将每个资源自身的利用率进行加权处理(即除以自身的过载阈值），然后选择最大的那个。举例如下：\n\n\n\n资源\n当前利用率\n加权公式\n加权结果\n\n\n\nCPU\n70%\n70% &#x2F; 90%\n77%\n\n\n内存\n90%\n90% &#x2F; 90%\n100%\n\n\nSwap\n0%\n70% &#x2F; 80%\n0%\n\n\n磁盘\n80%\n80% &#x2F; 70%\n114%\n\n\n网卡\n20%\n20% &#x2F; 90%\n22%\n\n\n这样ATOP判断当前最严重过载的资源是磁盘，则进程按磁盘利用率降序排列。我们经常遇到的都是高负载服务器，使用A能自动判断当前资源瓶颈在哪块，并显示导致相关资源极度紧张的TOP进程。对排查问题很有帮助。这里有一种特殊情况，当最严重过载资源是内存且加权后低于70%， 则仍按CPU排序。\nc\n123456/*** if the system is hardly loaded, still CPU-ordering of** processes is most interesting (instead of memory)*/if (highbadness &lt; 70 &amp;&amp; *highorderp == MSORTMEM) *highorderp = MSORTCPU;\n","slug":"LINUX/ATOP工作原理总结","date":"2025-08-05T17:32:37.000Z","categories_index":"proc,LINUX","tags_index":"CPU,atop,AAA","author_index":"dandeliono"},{"id":"039f48a281ee50d74a45933d415e35b5","title":"Linux 扩容 根分区(LVM+非LVM)","content":"Linux 扩容 &#x2F; 根分区(LVM+非LVM)目录：\n1，概述\n2，CentOS7，LVM根分区扩容步骤\n3，CentOS7，非LVM根分区扩容步骤：\n一、背景，概述\n\nMBR（Master Boot Record）（主引导记录）和GPT（GUID Partition Table）（GUID意为全局唯一标识符）是在磁盘上存储分区信息的两种不同方式\n\n对于传统的MBR分区方式，有很多的限制：\n1：最多4个主分区（3个主分区+1个扩展分区(扩展分区里面可以放多个逻辑分区)），无法创建大于2TB的分区，使用fdisk分区工具，而GPT分区方式不受这样的限制。\n2：GPT分区方式将不会有这种限制，使用的工具是parted；\n\n逻辑卷管理(LVM)，是 Logical Volume Manager（逻辑卷管理）的简写，lvm是卷的一种管理方式，并不是分区工具（也可不采用这种LVM管理方式）。\n\n\nLVM管理导图1\n\nLVM管理导图2\n上图所示： 如果直接扩展&#x2F;home逻辑卷目录，会提示逻辑卷组没有空间。\nLVM扩容思维流程：创建一个物理分区–&gt;将这个物理分区转换为物理卷–&gt;把这个物理卷添加到要扩展的卷组中–&gt;然后才能用extend命令扩展此卷组中的逻辑卷 。。。还是有些乱，根据上图理解。\n问：如何查看本地机器是否使用LVM管理？\npvdisplay #查看物理卷\nvgdisplay #查看卷组\nlvdisplay #查看逻辑卷\n答：执行上面命令，如果没有采用LVM管理的话，是查看不到上面卷组，物理卷，逻辑卷的（有可执行fdisk -l查看）。 逻辑卷即是挂载在目录上的卷。\n\n非LVM\n下面分别介绍LVM 和 非LVM 扩展根分区： 二、CentOS7，LVM根分区扩容步骤：1.查看现有分区大小\ndf -TH\n\nLVM分区，磁盘总大小为20G,根分区总容量为17G\n2.关机增加大小为30G(测试环境使用的Vmware Workstation)\n\n扩展分区到30G\n3.查看扩容后磁盘大小\n\n\n\n\n\n\n\n\n\ndf -THlsblk\n\n磁盘总大小为30G,根分区为17G\n4.创建分区\n\n\n\n\n\n\n\n\n\nfdisk &#x2F;dev&#x2F;sda\n\n将sda剩余空间全部给sda3\n5.刷新分区并创建物理卷\n\n\n\n\n\n\n\n\n\npartprobe &#x2F;dev&#x2F;sdapvcreate &#x2F;dev&#x2F;sda3\n\n\n6.查看卷组名称，以及卷组使用情况\n\n\n\n\n\n\n\n\n\nvgdisplay\n\nVG Name为centos\n7.将物理卷扩展到卷组\n\n\n\n\n\n\n\n\n\nvgextend centos &#x2F;dev&#x2F;sda3\n\n使用sda3扩展VG centos\n8.查看当前逻辑卷的空间状态\n\n\n\n\n\n\n\n\n\nlvdisplay\n\n需要扩展LV &#x2F;dev&#x2F;centos&#x2F;root\n9.将卷组中的空闲空间扩展到根分区逻辑卷\n\n\n\n\n\n\n\n\n\nlvextend -l +100%FREE &#x2F;dev&#x2F;centos&#x2F;root\n\n\n10.刷新根分区\n\n\n\n\n\n\n\n\n\nxfs_growfs &#x2F;dev&#x2F;centos&#x2F;root\n\n\n11.查看磁盘使用情况，扩展之前和之后是不一样的\n\n根分区已经变成27G\n\n三、CentOS7，非LVM根分区扩容步骤： 1.查看现有的分区大小\n\n非LVM分区，目前磁盘大小为20G，根分区总容量为17G\n2.关机增加磁盘大小为30G\n\n\n3.查看磁盘扩容后状态\n\n\n\n\n\n\n\n\n\nlsblkdh -TH\n\n现在磁盘总大小为30G,根分区为17G\n4.进行分区扩展磁盘，记住根分区起始位置和结束位置\n\n\n5.删除根分区，切记不要保存\n\n\n6.创建分区，箭头位置为分区起始位置\n\n\n7.保存退出并刷新分区\n\n\n\n\n\n\n\n\n\npartprobe &#x2F;dev&#x2F;sda\n\n\n8.查看分区状态\n\n这里不知道为啥变成19G了。。\n9.刷新根分区并查看状态\n\n\n\n\n\n\n\n\n\nxfs_growfs &#x2F;dev&#x2F;sda3 (这里先看自己的文件系统是xfs，还是ext4…)\n使用 resize2fs或xfs_growfs 对挂载目录在线扩容 ：\n\nresize2fs 针对文件系统ext2 ext3 ext4 （我在本地用ubuntu18是ext4，我用的是resize2fs &#x2F;dev&#x2F;sda3）\nxfs_growfs 针对文件系统xfs\n\n\n根分区大小已变为27G\n","slug":"LINUX/Linux 扩容 根分区(LVM+非LVM)","date":"2025-08-01T14:54:34.000Z","categories_index":"https,LINUX","tags_index":"com,pic,zhimg","author_index":"dandeliono"},{"id":"9cd0d8d4260de5985eda82c0c9a4fe26","title":"延迟双删如此好用，为何大厂从来不用Cache-aside下数据变更推荐使用删除缓存的策略，为降低数据不一致通常会配合延迟","content":"延迟双删如此好用，为何大厂从来不用Cache-aside下数据变更推荐使用删除缓存的策略，为降低数据不一致通常会配合延迟摘要：  在绝大多数介绍缓存与数据库一致性方案的文章中，随着 Cache-aside 模式的数据变更几乎无例外的推荐使用删除缓存的策略，为进一步降低数据不一致的风险通常会配合延迟双删的策略。但是令人意外的是，在一些互联网大厂中的核心业务却很少使用这种方式。这背后的原因是什么呢？延迟双删策略有什么致命缺陷么？以及这些大厂如何选择缓存与数据库一致性保障的策略呢？如果你对此同样抱有有疑问的话，希望本文能为你答疑解惑。\n\n当数据库（主副本）数据记录变更时，为了降低缓存数据不一致状态的持续时间，通常会选择主动 失效 &#x2F; 更新 缓存数据的方式。绝大多数应用系统的设计方案中会选择通过删除缓存数据的方式使其失效。但同样会出现数据不一致的情况，具体情况参见下图：  所以延迟双删又成为了组合出现的常见模式。延迟双删最复杂的技术实现在于对延迟时间的确定上，间隔时间久的话数据不一致的状态持续时间会变长，如果间隔时间过短可能无法起到一致性保障的作用。所以基于经验会将这个时间设定在秒级，如 1-2 秒后执行第二次删除操作。\n延迟双删的致命缺陷但是延迟时间最大的问题不在于此，而是两次删除缓存数据引起的缓存击穿（Cache Breakdown），短时间对数据库（主副本）造成的流量与负载压力。绝大多数应用系统本身流量与负载并不高，使用缓存通常是为了提升系统性能表现，数据库（主副本）完全可以承载一段时间内的负载压力。对于此类系统延迟双删是一个完全可以接受的高性价比策略。\n现实世界中的系统响应慢所带来的却是流量的加倍上涨。回想一下当你面对 App 响应慢的情况，是如何反应与对待便能明白，几乎所有用户的下意识行为都是如出一辙。\n所以对于那些流量巨大的应用系统而言，短时的访问流量穿透缓存访问数据库（主副本），恐怕很难接受。为了应对这种流量穿透的情况，通常需要增加数据库（主副本）的部署规格或节点。而且这类应用系统的响应变慢的时候，会对其支持系统产生影响，如果其支持系统较多的情况下，会存在影响的增溢。相比延迟双删在技术实现上带来高效便捷而言，其对系统的影响与副作用则变得不可忽视。\nFacebook（今 Meta）解决方案早在 2013 年由 Facebook（今 Meta）发表的论文 “Scaling Memcache at Facebook” 中便提供了其内部的解决方案，通过提供一种类似 “锁” 的 “leases”（本文译为“租约”）机制防止并发带来的数据不一致现象。\n租约机制实现方法大致如下：\n\n\n\n\n\n\n\n\n\n当有多个请求抵达缓存时，缓存中并不存在该值时会返回给客户端一个 64 位的 token ，这个 token 会记录该请求，同时该 token 会和缓存键作为绑定，该 token 即为上文中租约的值，客户端在更新时需要传递这个 token ，缓存验证通过后会进行数据的存储。其他请求需要等待这个租约过期后才可申请新的租约。\n可结合下图辅助理解其作用机制。也可阅读缓存与主副本数据一致性系统设计方案（下篇）一文中的如何解决并发数据不一致，又能避免延迟双删带来的惊群问题章节进一步了解。 \n简易参考实现接下来我们以 Redis 为例，提供一个 Java 版本的简易参考实现。本文中会给出实现所涉及的关键要素与核心代码，你可以访问 Github 项目 来了解整个样例工程，并通过查阅 Issue 与 commits 来了解整个样例工程的演化进程。\n要想实现上述租约机制，需要关注的核心要素有三个：\n\n需要复写 Redis 数据获取操作，当 Redis 中数据不存在时增加对租约的设置；\n需要复写 Redis 数据设置操作，当设置 Redis 中数据时校验租约的有效性；\n最后是当数据库（主副本）数据变更时，删除 Redis 数据同时要连带删除租约信息。\n\n同时为了保障 Redis 操作的原子性，我们需要借助 Lua 脚本来实现上述三点。这里以字符串类型为例，对应脚本分别如下：\nRedis 数据获取操作返回值的第二个属性作为判断是否需要执行数据获取的判断依据。当为 false 时表示 Redis 中无对应数据，需要从数据库中加载，同时保存了当前请求与 key 对应的租约信息。\n123456789101112local key = KEYS[1]local token = ARGV[1]local value = redis.call(&#x27;get&#x27;, key)if not value then    redis.replicate_commands()    local lease_key = &#x27;lease:&#x27;..key    redis.call(&#x27;set&#x27;, lease_key, token)    return &#123;false, false&#125;else    return &#123;value, true&#125;end\n\nRedis 数据设置操作返回值的第二个属性作为判断是否成功执行数据设置操作的依据。该属性为 false 表示租约校验失败，未成功执行数据设置操作。同时意味着有其他进程&#x2F;线程 执行数据查询操作并对该 key 设置了新的租约。\n12345678910111213local key = KEYS[1]local token = ARGV[1]local value = ARGV[2]local lease_key = &#x27;lease:&#x27;..keylocal lease_value = redis.call(&#x27;get&#x27;, lease_key)if lease_value == token then    redis.replicate_commands()    redis.call(&#x27;set&#x27;, key, value)    return &#123;value, true&#125;else    return &#123;false, false&#125;end\n\nRedis 数据删除操作当数据库变更进程&#x2F;线程 完成数据变更操作后，尝试删除缓存需要同时清理对应数据记录的 key 以及其关联租约 key。防止数据变更前的查询操作通过租约校验，将旧数据写入 Redis 。\n12345local key = KEYS[1]local token = ARGV[1]local lease_key = &#x27;lease:&#x27;..keyredis.call(&#x27;del&#x27;, key, leask_key)\n\n该方案主要的影响在应用层实现，主要在集中在三个方面：\n\n应用层不能调用 Redis 数据类型的原始操作命令，而是改为调用 EVAL 命令；\n调用 Redis 返回结果数据结构的变更为数组，需要解析数组；\n应用层对于 Redis 的操作变复杂，需要生成租约用的 token，并根据每个阶段返回结果进行后续处理；\n\n为应对上述三点变化，对应操作 Redis 的 Java 实现如下：\n封装返回结果为便于后续操作，首先是对脚本返回结果的封装。\n123456789101112131415public class EvalResult &#123;      String value;      boolean effect;      public EvalResult(List&lt;?&gt; args) &#123;          value = (String) args.get(0);          if (args.get(1) == null) &#123;              effect = false;          &#125; else &#123;              effect = 1 == (long) args.get(1);          &#125;      &#125;  &#125;\n\n组件设计\n封装 Redis 操作因为在样例工程中独立出了一个 Query Engine 组件，所以需要跨组件传递 token，这里为了实现简单采用了 ThreadLocal 进行 token 的传递，具体系统可查阅样例工程中的用例。\n1234567891011121314151617181920212223242526272829303132333435363738public class LeaseWrapper extends Jedis implements CacheCommands &#123;            private final Jedis jedis;      private final TokenGenerator tokenGenerator;      private final ThreadLocal&lt;String&gt; tokenHolder;            public LeaseWrapper(Jedis jedis) &#123;          this.jedis = jedis;          this.tokenHolder = new ThreadLocal&lt;&gt;();          this.tokenGenerator = () -&gt; UUID.randomUUID().toString();      &#125;   \t@Override    public String get(String key) &#123;          String token = this.tokenGenerator.get();          tokenHolder.set(token);          Object result = this.jedis.eval(LuaScripts.leaseGet(), List.of(key), List.of(token));          EvalResult er = new EvalResult((List&lt;?&gt;) result);          if (er.effect()) &#123;              return er.value();          &#125;          return null;      &#125;  \t@Override    public String set(String key, String value) &#123;          String token = tokenHolder.get();          tokenHolder.remove();          Object result = this.jedis.eval(LuaScripts.leaseSet(), List.of(key), List.of(token, value));          EvalResult er = new EvalResult((List&lt;?&gt;) result);          if (er.effect()) &#123;              return er.value();          &#125;          return null;      &#125;        &#125;\n\n补充在上面的简易参考实现中，我们并没有实现其他请求需要等待这个租约过期后才可申请新的租约。该功能主要是防止惊群问题，进一步降低可能对数据库造成的访问压力。要实现该功能需要在 Redis 数据获取操作中改进脚本：\n1234567891011121314151617local key = KEYS[1]local token = ARGV[1]local value = redis.call(&#x27;get&#x27;, key)if not value then    redis.replicate_commands()    local lease_key = &#x27;lease:&#x27;..key    local current_token = redis.call(&#x27;get&#x27;, lease_key)    if not current_token or token == current_token then\t    redis.call(&#x27;set&#x27;, lease_key, token)\t    return &#123;token, false&#125;\telse\t\treturn &#123;current_token, false&#125;\tendelse    return &#123;value, true&#125;end\n\n同时也可以为租约数据设定一个短时 TTL，并在应用层通过对 EvalResult 的 effect 判断为 false 的情况下等待一段时间后再次执行。\n上述实现的复杂点在于租约过期的时间的选取，以及超过设定时间的逻辑处理。我们可以实现类似自旋锁的机制，在最大等待时间内随时等待一个间隙向 Redis 发起查询请求，超过最大等待时间后直接查询数据库（主副本）获取数据。\nUber 解决方案在 Uber 今年 2 月份发表的一篇技术博客 “How Uber Serves Over 40 Million Reads Per Second from Online Storage Using an Integrated Cache” 中透露了其内部的解决方案，通过比对版本号的方式避免将旧数据写入缓存。\n版本号比对机制实现方法大致如下：\n\n\n\n\n\n\n\n\n\n将数据库中行记录的时间戳作为版本号，通过 Lua 脚本通过 Redis EVAL 命令提供类似 MSET 的更新操作，基于自定义编解码器提取 Redis 记录中的版本号，在执行数据设置操作时进行比对，只写入较新的数据。\n其中 Redis 的数据记录对应的 Key-Value 编码格式如所示： \n简易参考实现接下来我们以 Redis 为例，提供一个 Java 版本的简易参考实现。本文中会给出实现所涉及的关键要素与核心代码，你可以访问 Github 项目 来了解整个样例工程，并通过查阅 Issue 与 commits 来了解整个样例工程的演化进程。\n我们这里不采取定制数据格式，而是通过额外的缓存 Key 存储数据版本，要想实现类似版本号比对机制，需要关注的核心要素有两个：\n\n需要复写 Redis 数据设置操作，当设置 Redis 中数据时校验版本号；\n在版本号比对通过后需要绑定版本号数据，与主数据同步写入 Redis 中。\n\n同时为了保障 Redis 操作的原子性，我们需要借助 Lua 脚本来实现上述两点。这里以字符串类型为例，对应脚本分别如下：\nRedis 数据设置操作返回值的第二个属性作为判断是否成功执行数据设置操作的依据。该属性为 false 表示数据未成功写入 Redis。同时意味当前 进程&#x2F;线程 执行写入的数据为历史数据，在次过程中数据已经发生变更并又其他数据写入。\n123456789101112local key = KEYS[1]  local value = ARGV[1]  local current_version = ARGV[2]  local version_key = &#x27;version:&#x27;..key  local version_value = redis.call(&#x27;get&#x27;, version_key)  if version_value == false or version_value &lt; current_version then      redis.call(&#x27;mset&#x27;, version_key, current_version, key, value)    return &#123;value, true&#125;else    return &#123;false, false&#125;end\n\n该方案主要的影响在应用层实现，需要在调用 Redis 的 EVAL 命令前从数据实体中提取时间戳作为版本号，同时需要保障数据实体中包含时间戳相关属性。\n封装 Redis 操作结合我们的样例工程代码，我们通过实现 VersionWrapper 对 Redis 的操作进行如下封装。\n12345678910111213141516171819public class VersionWrapper extends Jedis implements CacheCommands &#123;            private final Jedis jedis;            public VersionWrapper(Jedis jedis) &#123;          this.jedis = jedis;      &#125;     @Override      public String set(String key, String value, String version) &#123;          Object result = this.jedis.eval(LuaScripts.versionSet(), List.of(key), List.of(value, version));          EvalResult er = new EvalResult((List&lt;?&gt;) result);          if (er.effect()) &#123;              return er.value();          &#125;          return null;      &#125;&#125;\n\n补充透过该方案我们推测 Uber 采取的并非数据变更后删除缓存的策略，很可能是更新缓存的策略（在 Uber 的技术博客中也间接的提到了更新缓存的策略）。\n因为整个版本号比对的方式与删除缓存的逻辑相悖。我们抛开 Uber CacheFront 的整体架构，仅仅将该方案应用在简单架构模型中。采取删除缓存的策略，可能会产生如下图所示的结果，此时应用服务 Server - 2 因为查询缓存未获取到值，而从数据库加载并写入缓存，但是此时缓存中写入的为历史旧值，而在该数据过期前或者下次数据变更前，都不会再触发更新了。  当然对于更新缓存的策略同样面临这个问题，因为当数据变更发生期间，缓存中并没有该数据记录时，通常我们不会采取主动刷新缓存的策略，那么则依然会面对上面的问题。\n而 Uber 的 CacheFront 基于企业内部的 Flux 技术组件实现对缓存的异步处理，通过阅读文章我们也可以发现这个异步延迟在秒级，那么在如此长的时间间隙后，无论采用删除还是更新策略想要产生上图中的不一致现象都比较难，因为对应用系统来说，进程&#x2F;线程阻塞 2-3 秒是很难以忍受的现象，所以通常不会出现如此漫长的阻塞与卡顿。\n如果你想进一步了解如何实现与 Uber 利用 Flux 实现缓存异步处理的内容，也可阅读我们此前缓存与主副本数据一致性系统设计方案（下篇）文章中更新主副本数据后更新缓存并发问题解决方案章节。\n总结本文并非对延迟双删的全盘否定，而是强调在特殊场景下，延迟双删策略的弊端会被放大，进而完全盖过其优势。对于那些业务体量大伴随着流量大的应用系统，必应要从中权衡取舍。\n每一种策略都仅适配应用系统生命周期的一段。只不过部分企业随着业务发展逐步壮大，其研发基础设施的能力也更完善。从而为系统设计带来诸多便捷，从而使得技术决策变得与中小研发团队截然不同。\n","slug":"ALG/延迟双删如此好用，为何大厂从来不用Cache-aside下数据变更推荐使用删除缓存的策略，为降低数据不一致通常会配合延迟","date":"2025-05-23T15:23:11.000Z","categories_index":"https,ALG","tags_index":"com,Redis,design","author_index":"dandeliono"},{"id":"262f61f15e8ffbc737e54526fd907a92","title":"清除信号量队列导致zabbix自动关闭","content":"清除信号量队列导致zabbix自动关闭前几天在海外UCloud机器上部署了一套zabbix proxy和zabbix agentd，可是第二天一大早就收到邮件说zabbix_proxy挂掉了，上去查一下发现两台机器中的一台的proxy和agentd都挂了，而另一台没事，再查一下log日志：\n\nzabbix_agentd [12977]: [file:’cpustat.c’,line:235] lock failed: [22] Invalid argument 12976:20150305:022001.966 One child process died (PID:12977,exitcode&#x2F;signal:255). Exiting … 12976:20150305:022003.967 Zabbix Agent stopped. Zabbix 2.0.13 (revision 48919).\nzabbix_proxy [12970]: [file:’selfmon.c’,line:341] lock failed: [22] Invalid argumentzabbix_proxy [12972]: [file:’selfmon.c’,line:341] lock failed: [22] Invalid argumentzabbix_proxy [12973]: [file:’selfmon.c’,line:341] lock failed: [22] Invalid argument 12951:20150305:022001.362 One child process died (PID:12970,exitcode&#x2F;signal:255). Exiting … 12951:20150305:022003.365 syncing history data…zabbix_proxy [12951]: [file:’dbcache.c’,line:2196] lock failed: [22] Invalid argument\n\n 第一感觉就是crontab跑了一个什么脚本，删除了啥东西导致的，果不其然，的确是删除了信号量导致的（关于信号量的介绍参看大牛博客 ipcs介绍 ），删除脚本如下：\n#!&#x2F;bin&#x2F;shfor semid in `ipcs -s | cut -f2 -d” “` do ipcrm -s $semid done\n这么粗暴的删除，不出事才怪呢，加个删除条件：\n#!&#x2F;bin&#x2F;shfor semid in `ipcs -s | grep -v zabbix | cut -f2 -d” “` do ipcrm -s $semid done\n再跑一下脚本，没问题啦 ^_^\n","slug":"MIDDLEWARE/清除信号量队列导致zabbix自动关闭","date":"2025-04-24T11:26:42.000Z","categories_index":"zabbix,MIDDLEWARE","tags_index":"proxy,file,line","author_index":"dandeliono"},{"id":"0bc98c01f2602afec20cfdc03ed2e502","title":"Kafka常用命令及遇到的问题","content":"Kafka常用命令及遇到的问题Kafka常用命令及可能遇到的问题：\n\n启动Kafka代理服务器\n\n命令：bin&#x2F;kafka-server-start.sh config&#x2F;server.properties\n问题：无法启动，可能是因为配置文件中存在错误或者某些参数未正确指定。\n\n停止Kafka代理服务器\n\n命令：bin&#x2F;kafka-server-stop.sh\n问题：无法停止，可能是因为代理正在执行某些操作或者存在其他未知问题。\n\n查看Kafka代理服务器状态\n\n命令：bin&#x2F;kafka-topics.sh –describe –zookeeper localhost:2181 –topic test\n问题：无法列出主题的详细信息，可能是因为主题不存在，或者ZooKeeper服务未正确启动。\n\n修改Kafka代理服务器的配置文件(config&#x2F;server.properties)\n\n命令：vi config&#x2F;server.properties\n问题：无法修改，可能是因为没有足够的权限或文件不存在。\n\n删除主题(topic)\n\n命令：bin&#x2F;kafka-topics.sh –delete –zookeeper localhost:2181 –topic test\n问题：无法删除主题，可能是因为主题不存在，或者ZooKeeper服务未正确启动。\n\n创建Kafka消费者组(consumer group)\n\n命令：bin&#x2F;kafka-console-consumer.sh –bootstrap-server localhost:9092 –topic test –group my-group\n问题：无法创建消费者组，可能是因为主题不存在，或者Kafka代理不支持在线创建消费者组。\n\n修改消费者组的名称(group)\n\n命令：bin&#x2F;kafka-consumer-groups.sh –bootstrap-server localhost:9092 –group my-group –new-consumer –reset-offsets –to-earliest –all-topics –execute\n问题：无法修改消费者组名称，可能是因为该组不存在或者Kafka代理不支持在线修改消费者组名称。\n\n查看当前Kafka生产者(producer)的配置\n\n命令：bin&#x2F;kafka-console-producer.sh –list-properties\n问题：无法列出生产者的配置信息，可能是因为Kafka代理未正确启动或者连接设置错误。\n\n修改Kafka生产者的轮询时间(interval)\n\n命令：bin&#x2F;kafka-console-producer.sh –broker-list localhost:9092 –topic test –producer-property linger.ms&#x3D;500\n问题：无法修改轮询时间，可能是因为参数设置有误或者Kafka代理不支持在线修改。\n\n回溯消费\n\n命令：bin&#x2F;kafka-consumer-groups.sh –bootstrap-server localhost:9092 –group my-group –reset-offsets –to-earliest –execute –topic test\n问题：无法回溯消费，可能是因为消费者组不存在，或者Kafka代理不支持在线回溯消费。\n\n暂停Kafka的重试机制(retries)\n\n命令：bin&#x2F;kafka-console-producer.sh –broker-list localhost:9092 –topic test –producer-property retries&#x3D;0\n问题：无法暂停重试机制，可能是因为给定的参数有误或者Kafka代理不支持在线修改重试设置。\n\n设置Kafka生产者的超时时间(timeout)\n\n命令：bin&#x2F;kafka-console-producer.sh –broker-list localhost:9092 –topic test –producer-property delivery.timeout.ms&#x3D;3000\n问题：无法设置超时时间，可能是因为参数设置错误或者Kafka代理不支持在线修改超时时间。\n\n查看Kafka消费者组的偏移量(offset)\n\n命令：bin&#x2F;kafka-consumer-groups.sh –bootstrap-server localhost:9092 –group my-group –describe\n问题：无法列出偏移量信息，可能是因为给定的消费者组不存在或者Kafka代理未正确启动。\n\n使用自定义分区(partitioner)发送消息到主题(topic)\n\n命令：bin&#x2F;kafka-console-producer.sh –broker-list localhost:9092 –topic test –property “partitioner.class&#x3D;com.xyz.CustomPartitioner”\n问题：无法使用自定义分区发送消息，可能是因为指定的类文件不存在或者存在语法错误。\n\n更改Kafka生产者的批处理大小(batch size)\n\n命令：bin&#x2F;kafka-console-producer.sh –broker-list localhost:9092 –topic test –producer-property batch.size&#x3D;16384\n问题：无法更改批处理大小，可能是因为参数设置有误或者Kafka代理不支持在线修改。\n\n更改Kafka生产者的缓冲池大小(buffer memory)\n\n命令：bin&#x2F;kafka-console-producer.sh –broker-list localhost:9092 –topic test –producer-property buffer.memory&#x3D;33554432\n问题：无法更改缓冲池大小，可能是因为参数设置错误或者Kafka代理不支持在线修改。\n\n查看Kafka的日志文件(logs)\n\n命令：tail -f logs&#x2F;server.log\n问题：无法查看Kafka服务器的日志文件，可能是因为没有足够的权限或者文件不存在。\n\n修改Kafka的日志级别(log level)\n\n命令：bin&#x2F;kafka-console-consumer.sh –bootstrap-server localhost:9092 –topic test –group my-group –log-level debug\n问题：无法修改日志级别，可能是因为给定的参数有误或者Kafka代理不支持在线修改日志级别。\n\n查看Kafka生产者的发送速度(rate)\n\n命令：bin&#x2F;kafka-producer-perf-test.sh –topic test –num-records 1000000 –record-size 1000 –throughput -1 –producer-props bootstrap.servers&#x3D;localhost:9092 buffer.memory&#x3D;33554432 batch.size&#x3D;16384 –print-metrics\n问题：无法计算发送速度，可能是因为参数设置错误或者Kafka代理未正确启动。\n\n查看Kafka消费者的消费速度(rate)\n\n命令：bin&#x2F;kafka-consumer-perf-test.sh –zookeeper localhost:2181 –messages 100000 –size 1000 –threads 1\n问题：无法计算消费速度，可能是因为参数设置错误或者Kafka代理未正确启动。\n\n将Kafka主题复制到另一个代理(replication)\n\n命令：bin&#x2F;kafka-topics.sh –create –zookeeper localhost:2181 –replica-reassignment ‘{“version”:1,“partitions”:[{“topic”:“test”,“partition”:0,“replicas”:[0,1,2]},{“topic”:“test”,“partition”:1,“replicas”:[0,1,2]}]}’\n问题：无法复制主题到其他代理，可能是因为给定的参数有误或者Kafka代理不支持在线复制主题。\n\n更改Kafka的日志清理策略(log compaction)\n\n命令：bin&#x2F;kafka-topics.sh –zookeeper localhost:2181 –alter –topic test –config cleanup.policy&#x3D;compact\n问题：无法更改清理策略，可能是因为参数设置错误或者Kafka代理不支持在线修改策略。\n\n查看Kafka主题的详细信息(topic)\n\n命令：bin&#x2F;kafka-topics.sh –describe –zookeeper localhost:2181 –topic test\n问题：无法列出主题的详细信息，可能是因为主题不存在或者ZooKeeper服务未正确启动。\n\n更改Kafka主题的最大消息大小(max message size)\n\n命令：bin&#x2F;kafka-topics.sh –alter –zookeeper localhost:2181 –topic test –config max.message.bytes&#x3D;1048576\n问题：无法更改最大消息大小，可能是因为参数设置错误或者Kafka代理不支持在线修改。\n\n修改Kafka生产者的ack机制(acknowledgment)\n\n命令：bin&#x2F;kafka-console-producer.sh –broker-list localhost:9092 –topic test –producer-property acks&#x3D;all\n问题：无法修改ack机制，可能是因为给定的参数有误或者Kafka代理不支持在线修改。\n\n从Kafka主题删除消息(delete topic messages)\n\n命令：bin&#x2F;kafka-delete-records.sh –bootstrap-server localhost:9092 –offset-json-file offsets.json\n问题：无法删除消息，可能是因为给定的参数有误或者Kafka代理不支持在线删除。\n\n更改Kafka主题的最小ISR(minimum in-sync replicas)\n\n命令：bin&#x2F;kafka-topics.sh –alter –zookeeper localhost:2181 –topic test –config min.insync.replicas&#x3D;2\n问题：无法更改最小ISR，可能是因为给定的参数有误或者Kafka代理不支持在线修改。\n\n使用Kafka控制台来发布消息(publish a message using Kafka console)\n\n命令：echo “hello world” | bin&#x2F;kafka-console-producer.sh –broker-list localhost:9092 –topic test\n问题：无法发布消息，可能是因为Kafka代理未正确启动或连接设置错误。\n\n从Kafka主题中消费最新的消息(consume latest messages from Kafka topic)\n\n命令：bin&#x2F;kafka-console-consumer.sh –bootstrap-server localhost:9092 –topic test –from-beginning\n问题：无法消费最新的消息，可能是因为主题不存在或者Kafka代理未正确启动。\n\n更改Kafka消费者组的偏移重置方式(offset reset)\n\n命令：bin&#x2F;kafka-consumer-groups.sh –bootstrap-server localhost:9092 –group my-group –reset-offsets –to-latest –all-topics –execute\n问题：无法更改偏移重置方式，可能是因为消费者组不存在或者Kafka代理不支持在线修改偏移重置方式。\n\n将Kafka主题的分区分配给消费者组(assign partitions to a consumer group)\n\n命令：bin&#x2F;kafka-consumer-groups.sh –bootstrap-server localhost:9092 –group my-group –describe\n问题：无法将主题的分区分配给消费者组，可能是因为消费者组不存在或者Kafka代理未正确启动。\n\n使用Kafka控制台从Kafka主题中消费消息(consume messages from Kafka topic using console)\n\n命令：bin&#x2F;kafka-console-consumer.sh –bootstrap-server localhost:9092 –topic test\n问题：无法消费消息，可能是因为主题不存在或者Kafka代理未正确启动。\n\n修改Kafka性能测试工具的参数(performance tuning for Kafka tool)\n\n命令：bin&#x2F;kafka-producer-perf-test.sh –topic test –num-records 1000000 –record-size 1000 –throughput -1 –producer-props bootstrap.servers&#x3D;localhost:9092 buffer.memory&#x3D;33554432 batch.size&#x3D;16384 –print-metrics\n问题：无法进行性能测试，可能是因为参数设置错误或者Kafka代理未正确启动。\n\n检查Kafka主题的完整性(check integrity of Kafka topic)\n\n命令：bin&#x2F;kafka-check-topic.sh –zookeeper localhost:2181 –topic test\n问题：无法检查主题的完整性，可能是因为主题不存在或者ZooKeeper服务未正确启动。\n\n更改Kafka消费者的轮询时间(poll interval)\n\n命令：bin&#x2F;kafka-console-consumer.sh –bootstrap-server localhost:9092 –topic test –max-poll-interval-ms 5000\n问题：无法更改轮询时间，可能是因为给定的参数有误或者Kafka代理不支持在线修改。\n\n从Kafka主题中消费消息的同时保存到本地文件(consume messages from Kafka topic and save to local file)\n\n命令：bin&#x2F;kafka-console-consumer.sh –bootstrap-server localhost:9092 –topic test –from-beginning &gt; output.txt\n问题：无法将消息保存到本地文件，可能是因为权限不足或者Kafka代理未正确启动。\n\n更改Kafka代理的IP地址和端口号(change IP address and port number of Kafka broker)\n\n命令：vi config&#x2F;server.properties\n问题：无法修改IP地址和端口号，可能是因为参数设置错误或者Kafka代理未正确启动。\n\n将Kafka主题转发到另一个Kafka代理转发(forward Kafka topic to another Kafka broker)\n\n命令：bin&#x2F;kafka-reassign-partitions.sh –zookeeper localhost:2181 –reassignment-json-file reassignment.json –execute\n问题：无法转发主题，可能是因为参数设置错误或者Kafka代理未正确启动。\n\n启用Kafka安全协议(SSL&#x2F;TLS)(enable SSL&#x2F;TLS for Kafka)\n\n命令：vi config&#x2F;server.properties\n问题：无法启用安全协议，可能是因为某些参数设置错误或者证书未正确配置。\n\n启用Kafka授权机制(authorization)(enable authorization for Kafka)\n\n命令：vi config&#x2F;server.properties\n问题：无法启用授权机制，可能是因为某些参数设置错误或者授权配置文件未正确配置。\n\n启用Kafka认证机制(authentication)(enable authentication for Kafka)\n\n命令：vi config&#x2F;server.properties\n问题：无法启用认证机制，可能是因为某些参数设置错误或者身份验证配置不正确。\n\n更改Kafka代理的内存大小(change memory size of Kafka broker)\n\n命令：vi config&#x2F;server.properties\n问题：无法更改内存大小，可能是因为给定的参数有误或者Kafka代理未正确启动。\n\n将Kafka主题的数据备份到其他服务器(backup data of Kafka topic to another server)\n\n命令：bin&#x2F;kafka-replica-verification.sh –zookeeper localhost:2181 –broker-list localhost:9092 –topic test\n问题：无法备份数据，可能是因为参数设置错误或者ZooKeeper服务未正确启动。\n\n使用Kafka控制台将消息发送到特定的分区(send messages to specific partition using Kafka console)\n\n命令：echo “hello world” | bin&#x2F;kafka-console-producer.sh –broker-list localhost:9092 –topic test –property “parse.key&#x3D;true” –property “key.separator&#x3D;:”\n问题：无法发送消息到特定的分区，可能是因为参数设置有误或者Kafka代理未正确启动。\n\n设置Kafka消费者的最大记录数(max records for Kafka consumer)\n\n命令：bin&#x2F;kafka-console-consumer.sh –bootstrap-server localhost:9092 –topic test –max-messages 10\n","slug":"MIDDLEWARE/Kafka常用命令及遇到的问题","date":"2024-12-07T16:14:25.000Z","categories_index":"Kafka,MIDDLEWARE","tags_index":"topic,命令,问题","author_index":"dandeliono"},{"id":"5a79b62bd0cc256cbc198b2e3cc42ba4","title":"在 Kubernetes 集群上安装 KubeVirt","content":"在 Kubernetes 集群上安装 KubeVirt安装 KubeVirt 并创建虚拟机可以按照以下步骤进行。\n安装 KubeVirt\n安装 KubeVirt Operator\n\n首先，需要安装 KubeVirt Operator，它会管理 KubeVirt 的部署和生命周期。运行以下命令来安装 KubeVirt Operator：\n123export VERSION=$(curl -s https://api.github.com/repos/kubevirt/kubevirt/releases/latest | jq -r .tag_name)kubectl create -f https://github.com/kubevirt/kubevirt/releases/download/$&#123;VERSION&#125;/kubevirt-operator.yaml\n\n\n安装 KubeVirt CustomResource (CR)\n\n接下来，创建一个 KubeVirt 自定义资源 (CR)，来启动 KubeVirt：\n12kubectl create -f https://github.com/kubevirt/kubevirt/releases/download/$&#123;VERSION&#125;/kubevirt-cr.yaml\n\n\n验证安装\n\n使用以下命令验证 KubeVirt 是否已正确安装并正在运行：\n123kubectl get pods -n kubevirtkubectl get kubevirt.kubevirt.io/kubevirt -n kubevirt\n\n你应该能看到 KubeVirt 的相关组件已经运行。\n安装 virtctlvirtctl 是一个用于管理 KubeVirt 虚拟机的命令行工具。你可以从 GitHub 上下载适合你系统的二进制文件：\n12345export VIRTCTL_VERSION=$(curl -s https://api.github.com/repos/kubevirt/kubevirt/releases/latest | jq -r .tag_name)curl -L -o virtctl https://github.com/kubevirt/kubevirt/releases/download/$&#123;VIRTCTL_VERSION&#125;/virtctl-$&#123;VIRTCTL_VERSION&#125;-linux-amd64chmod +x virtctlsudo mv virtctl /usr/local/bin/\n\n创建虚拟机的示例\n定义一个虚拟机\n\n创建一个虚拟机 YAML 文件，例如 vm.yaml，内容如下：\n123456789101112131415161718192021222324252627282930313233343536apiVersion: kubevirt.io/v1kind: VirtualMachinemetadata:  name: myvm  namespace: defaultspec:  running: false  template:    metadata:      labels:        kubevirt.io/domain: myvm    spec:      domain:        devices:          disks:          - disk:              bus: virtio            name: containerdisk          - disk:              bus: virtio            name: cloudinitdisk        resources:          requests:            memory: 64M      volumes:      - name: containerdisk        containerDisk:          image: quay.io/kubevirt/cirros-container-disk-demo      - name: cloudinitdisk        cloudInitNoCloud:          userData: |            #cloud-config            password: cirros            chpasswd: &#123; expire: False &#125;            ssh_pwauth: True\n\n这个示例定义了一个简单的虚拟机，使用 CirrOS 镜像和 cloud-init 来配置初始用户。\n\n创建虚拟机\n\n使用以下命令创建虚拟机：\n12kubectl apply -f vm.yaml\n\n\n启动虚拟机\n\n使用 virtctl 启动虚拟机：\n12virtctl start myvm\n\n\n检查虚拟机状态\n\n你可以使用以下命令检查虚拟机的状态：\n123kubectl get vmskubectl get vmi\n\n\n连接到虚拟机\n\n使用 virtctl 连接到虚拟机的控制台：\n12virtctl console myvm\n\n总结通过以上步骤，你可以在 Kubernetes 集群上安装 KubeVirt，并创建和管理虚拟机。这包括安装 KubeVirt Operator 和 CR，配置并启动一个简单的虚拟机，以及使用 virtctl 工具进行管理。根据需要，你可以调整虚拟机的配置和资源请求，以满足不同的应用需求。\n","slug":"OCI/在 Kubernetes 集群上安装 KubeVirt","date":"2024-10-25T15:18:17.000Z","categories_index":"KubeVirt,OCI","tags_index":"安装,virtctl,Operator","author_index":"dandeliono"},{"id":"eb41994bab0b8b1a0fef2450d1035786","title":"mysql GTID主从复制故障后不停机恢复同步流程 - 塔克拉玛攻城狮 - 博客园","content":"mysql GTID主从复制故障后不停机恢复同步流程 - 塔克拉玛攻城狮 - 博客园GTID实现主从复制数据同步\nGTID是一个基于原始mysql服务器生成的一个已经被成功执行的全局事务ID，它由服务器ID以及事务ID组成，这个全局事务ID不仅仅在原始服务器上唯一，在所有主从关系的mysql服务器上也是唯一的。正式因为这样一个特性使得mysql主从复制变得更加简单，以及数据库一致性更可靠。\n介绍\nGTID的概念\n\n 全局事务标识：global transaction identifiers\n GTID是一个事务一一对应，并且全局唯一ID\n 一个GTID在一个服务器上只执行一次，避免重复执行导致数据混乱不一致\n 不再使用传统的MASTER_LOG_FILE+MASTER_LOG_POS开启复制，而是采用MASTER_AUTO_POSTION&#x3D;1的方式开启复制。\n 从MYSQL-5.6.5及后续版本开始支持\n\nGTID的组成\nGTID &#x3D; server_uuid:transaction_id\nserver_uuid： mysql服务器的唯一标识，查看方法mysql客户端内：show variables like ‘%server_uuid%’;\ntransaction_id：此id是当前服务器中提交事务的一个序列号，从1开始自增长，一个数值对应一个事务\nGTID号示例：c9fba9e2-db3b-11eb-81d4-000c298d8da1:1-5\nGTID的优势\n\n 实现主从更简单，不用像以前一样寻找log_file和log_pos\n 比传统的主从更加安全\n GTID是连续没有空洞的，保证数据一致性，零丢失。\n\nGTID工作原理\n\nmaster更新数据时，会在事务前产生GTID，一同记录到binlog日志中\nslave端的I&#x2F;O线程将变更的binlog，写入到本地的relay log中\nSQL线程从relay log中获取GTID，然后对比slave端的binlog是否有记录（所以MySQL5.6 slave端必须开启binlog）\n如果有记录，说明该GTID的事务已经执行，slave会忽略\n如果没有记录，slave就会从relay log中执行该GTID的事务，并记录到binlog\n在解析过程中会判断是否有主键，如果没有就用二级索引，如果没有就用全部扫描\n\n开始配置GTID复制\n主：192.168.152.253   Centos7\n从：192.168.152.252   Centos8\n测试数据库：vfan\n测试表：student\n1、修改mysql服务配置文件，添加以下参数，随后重启：\n\nserver-id=100    #server id log-bin&#x3D;&#x2F;var&#x2F;lib&#x2F;mysql&#x2F;mysql-bin  #开启binlog并指定存储位置expire_logs_days=10 #日志保存时间为10天gtid_mode=on  #gtid模块开关enforce_gtid_consistency=on  #启动GTID强一致性，开启gtid模块必须开启此功能。binlog_format=row  #bin_log日志格式，共有三种STATEMENT、ROW、MIXED；默认为STATEMENTskip_slave_start=1  #防止复制随着mysql启动而自动启动\n\n\n\n\n\n\n\n\n\n\n主服务器和从服务器的配置一致即可，server-id更改一下\n2、在主服务器中创建从服务器连接的用户\nCREATE USER ‘copy‘@’192.168.152.252’ IDENTIFIED BY ‘copy’;GRANT REPLICATION SLAVE ON *.* TO ‘copy‘@’192.168.152.252’;flush privileges;\n\n\n\n\n\n\n\n\n\n创建完毕记得要测试下slave机是否能登录成功\n3、使用mysqldump使两数据库数据同步\n\n主mysql执行：mysqldump -uroot -proot1 vfan &gt; dump2.sql scp dump2.sql 192.168.152.252:&#x2F;data&#x2F; 从mysql执行：mysql&gt; source &#x2F;data&#x2F;dump2.sql\n\n当前主、从服务器数据内容一致，都是以下数据：\n\nmysql&gt; select * from student; +—-+——+—–+| id | name | age |+—-+——+—–+|  1 | Tony |  18 ||  2 | Any  |  17 ||  3 | Goy  |  20 ||  4 | Baly |  18 ||  5 | Heg  |  19 ||  6 | hhh  | 100 ||  7 | lll  |  99 |+—-+——+—–+7 rows in set (0.01 sec)\n\n4、开启主从复制\n\nmysql&gt; CHANGE MASTER TO MASTER_HOST&#x3D;’192.168.152.253’,MASTER_USER&#x3D;’copy’,MASTER_PASSWORD&#x3D;’copy’,MASTER_PORT&#x3D;3306,MASTER_AUTO_POSITION&#x3D;1;Query OK, 0 rows affected, 2 warnings (0.04 sec)\nmysql&gt; start slave;Query OK, 0 rows affected (0.01 sec)\n查看slave状态mysql&gt; show slave status\\G *************************** 1. row *************************** Slave_IO_State: Waiting for master to send event                  Master_Host: 192.168.152.253 Master_User: copy                  Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mysql-bin.000014 Read_Master_Log_Pos: 897 Relay_Log_File: kubenode2-relay-bin.000002 Relay_Log_Pos: 416 Relay_Master_Log_File: mysql-bin.000014 Slave_IO_Running: Yes            Slave_SQL_Running: Yes\n\n5、检查是否同步\n\n主服务器中插入数据：mysql&gt; INSERT INTO student(name,age) VALUES(‘gogoo’,50),(‘zhazha’,25);Query OK, 2 rows affected (0.03 sec)Records: 2  Duplicates: 0  Warnings: 0 从服务器中读取：mysql&gt; select * from student; +—-+——–+—–+| id | name   | age |+—-+——–+—–+|  1 | Tony   |  18 ||  2 | Any    |  17 ||  3 | Goy    |  20 ||  4 | Baly   |  18 ||  5 | Heg    |  19 ||  6 | hhh    | 100 ||  7 | lll    |  99 ||  8 | gogoo  |  50 ||  9 | zhazha |  25 |+—-+——–+—–+9 rows in set (0.00 sec)\n\n\n\n\n\n\n\n\n\n\n数据已经同步，基础的主从复制已经搭建完成\n现在模拟一个主从复制架构中，从服务器中途复制失败，不再同步主服务器的场景，并要求不停业务进行数据同步修复，恢复一致。（或者主服务器宕机，从服务器代替主服务器提供服务，现在要恢复主服务器的数据，使它与从服务器的数据同步）\n1、首先先模拟一个数据插入的场景\nvim insert.sh\n\n#!&#x2F;usr&#x2F;bin&#x2F;env bash\nvalues=(`find &#x2F;usr&#x2F; -type d | awk -F ‘&#x2F;‘ ‘{print $NF}’ | sort -u`) while truedo age=$(( $RANDOM%100 ))name=${values[$(( $RANDOM%6 ))]}\nmysql -h127.1 -P3306 -uroot -proot1 -e “INSERT INTO vfan.student(name,age) VALUES(‘“${name}”‘,${age});” &amp;&gt; &#x2F;dev&#x2F;nullsleep $(( $RANDOM%5 )) done\n\n运行脚本，数据在随机插入（插入时间间隔 &lt; 5s）\n2、数据还在陆续插入，此时模拟slave节点宕机或异常(在此就直接stop slave;)\nmysql&gt; stop slave;Query OK, 0 rows affected (0.01 sec)\n3、此时主库数据还在增加，而从库已经不同步，以下是从库数据：\n\nmysql&gt; select * from student; +—-+———————+—–+| id | name                | age | …… | 82 | 00bash              |  50 || 83 | 00systemd-bootchart |  36 || 84 | 00bash              |  48 || 85 | 00systemd-bootchart |  41 || 86 | 00                  |  72 |+—-+———————+—–+86 rows in set (0.00 sec)\n\n目前主mysql数据：\n\nmysql&gt; select * from student;+—-+———————+—–+| id | name                | age |……|  97 | _                   |   2 ||  98 | 00bash              |  15 ||  99 | 00bash              |  52 || 100 | 00bash              |  43 || 101 | _                   |  65 || 102 | 00                  |  67 |+—–+———————+—–+102 rows in set (0.01 sec)\n\n\n\n\n\n\n\n\n\n\n很明显已经比从库多出很多数据\n4、开始从库恢复数据\n思路：\n 先通过mysqldump全量备份当前的数据，由于不能影响业务，所以在mysqldump数据时不能造成锁表。要保持数据写入\n 由于mysqldump时数据还在写入，所以有一部分数据还是会同步不全，所以导入mysqldump的数据后，跳过dump中包含的GTID事务，再重新建立一次主从配置，开启slave线程，恢复数据并同步。\n（1）mysqldump不锁表备份数据\nmysqldump -uroot -proot1 –single-transaction –master-data&#x3D;2 -R vfan | gzip &gt; dump4.sql\n\n\n\n\n\n\n\n\n\n主要起作用参数：–single-transaction\n（2）查看当前mysqldump导出数据的GTID号\n[root@TestCentos7 data]# grep GLOBAL.GTID_PURGED dump4.sqlSET @@GLOBAL.GTID_PURGED=&#x2F;*!80000 ‘+’*&#x2F; ‘c9fba9e2-db3b-11eb-81d4-000c298d8da1:1-228’;\n\n\n\n\n\n\n\n\n\n以上的 c9fba9e2-db3b-11eb-81d4-000c298d8da1:1-228 表示MASTER机执行到的GTID事务号\n（3）去从数据库导入\n\nscp dump4.sql 192.168.152.252:&#x2F;data\nmysql客户端内：mysql&gt; source &#x2F;data&#x2F;dump4.sql\n此时从库数据：mysql&gt; select * from student; | 230 | 00                  |  53 || 231 | 00bash              |  66 || 232 | _                   |  18 || 233 | 0.33.0              |  98 || 234 | 00bash              |  14 |+—–+———————+—–+234 rows in set (0.00 sec)\n主库数据： | 454 | _                   |  46 || 455 | 03modsign           |  59 || 456 | 00systemd-bootchart |  77 || 457 | 03modsign           |   6 || 458 | 0.33.0              |  88 |+—–+———————+—–+458 rows in set (0.00 sec)\n\n\n\n\n\n\n\n\n\n\n从库数据恢复一部分到234行，主库数据依然在增加，已经是458条\n（4）由于我们mysqldump的数据已经包含了在MASTER执行的 1-228 个事务，所以我们在SLAVE进行同步的时候，要忽略这些事务不再进行同步，不然会出现类似于这种报错：\n\nmysql&gt; show slave status\\G *************************** 1. row *************************** Slave_IO_State: Waiting for master to send event                  Master_Host: 192.168.152.253 Master_User: copy                  Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mysql-bin.000002 Read_Master_Log_Pos: 137827 Relay_Log_File: kubenode2-relay-bin.000002 Relay_Log_Pos: 417 Relay_Master_Log_File: mysql-bin.000002 Slave_IO_Running: Yes            Slave_SQL_Running: No                   Last_Errno: 1062 Last_Error: Could not execute Write_rows event on table vfan.student; Duplicate entry ‘87’ for key ‘student.PRIMARY’, Error_code: 1062; handler error HA_ERR_FOUND_DUPP_KEY; the event’s master log mysql-bin.000002, end_log_pos 10588\n\n要想跳过某些GTID，SLAVE必须保证 gtid_purged 参数为空才能正确跳过，查看当前的gtid_purged：\n\nmysql&gt; show global variables like ‘%gtid%’; +———————————-+————————————————————————————-+| Variable_name                    | Value                                                                               |+———————————-+————————————————————————————-+| binlog_gtid_simple_recovery      | ON                                                                                  || enforce_gtid_consistency         | ON                                                                                  || gtid_executed                    | b30cb2ff-32d4-11eb-a447-000c292826bc:1-2,c9fba9e2-db3b-11eb-81d4-000c298d8da1:1-80 || gtid_executed_compression_period | 1000                                                                                || gtid_mode                        | ON                                                                                  || gtid_owned                       |                                                                                     || gtid_purged                      | c9fba9e2-db3b-11eb-81d4-000c298d8da1:1-70                                           || session_track_gtids              | OFF                                                                                 |+———————————-+————————————————————————————-+8 rows in set (0.02 sec)\n\n当前gtid_purged不为空，所以我们要先设置它为空，执行：\n\nmysql&gt; reset master;Query OK, 0 rows affected (0.05 sec)\nmysql&gt; show global variables like ‘%gtid%’; +———————————-+——-+| Variable_name                    | Value |+———————————-+——-+| binlog_gtid_simple_recovery      | ON    || enforce_gtid_consistency         | ON    || gtid_executed                    |       || gtid_executed_compression_period | 1000  || gtid_mode                        | ON    || gtid_owned                       |       || gtid_purged                      |       || session_track_gtids              | OFF   |+———————————-+——-+8 rows in set (0.00 sec)\n\n（5）gtid_purged为空后，开始重置SLAVE\n\nmysql&gt; stop slave;Query OK, 0 rows affected (0.00 sec)\nmysql&gt; reset slave all;Query OK, 0 rows affected (0.02 sec)\n\n（6）重置后，设置跳过的GTID，并重新同步MASTER\n\nmysql&gt; SET @@GLOBAL.GTID_PURGED&#x3D;’c9fba9e2-db3b-11eb-81d4-000c298d8da1:1-228’;Query OK, 0 rows affected (0.01 sec)\nmysql&gt; CHANGE MASTER TO MASTER_HOST&#x3D;’192.168.152.253’,MASTER_USER&#x3D;’copy’,MASTER_PASSWORD&#x3D;’copy’,MASTER_PORT&#x3D;3306,MASTER_AUTO_POSITION&#x3D;1;Query OK, 0 rows affected, 2 warnings (0.04 sec)\n\n（7）开启SLAVE进程，查看同步状态\n\nmysql&gt; start slave;Query OK, 0 rows affected (0.01 sec)\nmysql&gt; show slave status\\G *************************** 1. row *************************** Slave_IO_State: Waiting for master to send event                  Master_Host: 192.168.152.253 Master_User: copy                  Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mysql-bin.000002 Read_Master_Log_Pos: 137827 Relay_Log_File: kubenode2-relay-bin.000002 Relay_Log_Pos: 84993 Relay_Master_Log_File: mysql-bin.000002 Slave_IO_Running: Yes            Slave_SQL_Running: Yes              Replicate_Do_DB:          Replicate_Ignore_DB:           Replicate_Do_Table:       Replicate_Ignore_Table:      Replicate_Wild_Do_Table:  Replicate_Wild_Ignore_Table:                   Last_Errno: 0 Last_Error:                 Skip_Counter: 0 Exec_Master_Log_Pos: 137827 Relay_Log_Space: 85206 Until_Condition: None               Until_Log_File:                Until_Log_Pos: 0 Master_SSL_Allowed: No           Master_SSL_CA_File:           Master_SSL_CA_Path:              Master_SSL_Cert:            Master_SSL_Cipher:               Master_SSL_Key:        Seconds_Behind_Master: 0 Master_SSL_Verify_Server_Cert: No                Last_IO_Errno: 0 Last_IO_Error:               Last_SQL_Errno: 0 Last_SQL_Error:  Replicate_Ignore_Server_Ids:             Master_Server_Id: 100 Master_UUID: c9fba9e2-db3b-11eb-81d4-000c298d8da1             Master_Info_File: mysql.slave_master_info                    SQL_Delay: 0 SQL_Remaining_Delay: NULL      Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates           Master_Retry_Count: 86400 Master_Bind:      Last_IO_Error_Timestamp:     Last_SQL_Error_Timestamp:               Master_SSL_Crl:           Master_SSL_Crlpath:           Retrieved_Gtid_Set: c9fba9e2-db3b-11eb-81d4-000c298d8da1:229-519 Executed_Gtid_Set: c9fba9e2-db3b-11eb-81d4-000c298d8da1:1-519 Auto_Position: 1 Replicate_Rewrite_DB:                 Channel_Name:           Master_TLS_Version:       Master_public_key_path:        Get_master_public_key: 0 Network_Namespace: 1 row in set (0.00 sec)\n\n\n\n\n\n\n\n\n\n\n可以看到，同步正常！\n（8）最后，查看master与slave数据是否一致\n\nMASTER数据：SELECT * FROM student; | 520 | 00systemd-bootchart |  18 || 521 | 00systemd-bootchart |  44 || 522 | 03modsign           |  98 || 523 | 00systemd-bootchart |  45 || 524 | 00                  |  90 || 525 | 03modsign           |  21 |+—–+———————+—–+525 rows in set (0.00 sec)\nSLAVE数据：SELECT * FROM student; | 519 | 0.33.0              |  99 || 520 | 00systemd-bootchart |  18 || 521 | 00systemd-bootchart |  44 || 522 | 03modsign           |  98 || 523 | 00systemd-bootchart |  45 || 524 | 00                  |  90 || 525 | 03modsign           |  21 |+—–+———————+—–+525 rows in set (0.00 sec)\n\n\n\n\n\n\n\n\n\n\n在我们修过程中插入的数据也已经全部同步。数据完全一致，主从复制修复完成。\n","slug":"MIDDLEWARE/mysql GTID主从复制故障后不停机恢复同步流程 - 塔克拉玛攻城狮 - 博客园","date":"2024-08-19T15:11:26.000Z","categories_index":"mysql,MIDDLEWARE","tags_index":"https,assets,cnblogs","author_index":"dandeliono"},{"id":"1bbad1a585c4bcc2381dfee0a793a3ef","title":"NFQUEUE机制导致DNS请求5秒超时分析","content":"NFQUEUE机制导致DNS请求5秒超时分析在一台CentOS 7.0服务器(内核版本号:3.10.0-123.el7.x86_64)上安装我们的安全防护程序后，会出现curl访问网址超时5秒的情况。现象如下:\n12345[root@localhost ~]real\t0m5.120suser\t0m0.002ssys\t0m0.009s\n\n通过strace分析程序调用的过程:\n1strace -f -tt -o curl.strace curl -s www.baidu.com -o /dev/null\n\n从strace输出可以看到, 第一次curl调用sendmmsg同时发送了两个DNS数据包，分别是A记录和AAAA记录请求，但是只收到了A记录响应包:\n\n然后等待5秒超时后，依次调用sendto和recvfrom串行处理两个DNS请求, 这次两个DNS响应全部收到后，继续向下执行:\n\n而从抓包结果分析，tcpdump只能看到第一次同时发送的两个DNS请求中的A记录请求，AAAA记录请求数据包被内核协议栈丢弃了:\n12345617:04:24.772049 IP 10.10.10.89.57416 &gt; 114.114.114.114.53: 37081+ A? www.baidu.com. (31)17:04:24.773693 IP 114.114.114.114.53 &gt; 10.10.10.89.57416: 37081 3/0/0 CNAME www.a.shifen.com., A 180.101.49.12, A 180.101.49.11 (90)17:04:29.776474 IP 10.10.10.89.57416 &gt; 114.114.114.114.53: 37081+ A? www.baidu.com. (31)17:04:29.778694 IP 114.114.114.114.53 &gt; 10.10.10.89.57416: 37081 3/0/0 CNAME www.a.shifen.com., A 180.101.49.11, A 180.101.49.12 (90)17:04:29.778925 IP 10.10.10.89.57416 &gt; 114.114.114.114.53: 42471+ AAAA? www.baidu.com. (31)17:04:29.780523 IP 114.114.114.114.53 &gt; 10.10.10.89.57416: 42471 1/1/0 CNAME www.a.shifen.com. (115)\n\n在Google上搜索DNS 5秒有非常多关于类似现象的文章介绍，但基本都是在Kubernetes环境中发生。我们的环境只是普通的CentOS环境，为何也会发生呢？\nweave公司的文章Racy conntrack and DNS lookup timeouts介绍了在较旧版本内核的conntrack模块中存在的BUG会导致UDP丢包。其中一种场景是当不同的线程通过相同的socket发送UDP数据包时，存在竞争条件两个数据包都会各自创建一个conntrack条目，但两个条目所包含的tuple信息是一致的，这种情况会导致丢包。\n从现象看，我们的场景丢包根因应该也是由于conntrack模块的BUG导致丢包，但我们的这种场景并不存在多个线程同时使用相同socket进行发送。我们的防护逻辑是内核模块通过NFQUEUE机制将数据包送到用户态，由用户态对数据包进行过滤裁决是否允许放行。因而我们怀疑是由于NFQUEUE机制导致conntrack模块这个已知BUG的触发。\n我们写一个简单的内核模块将DNS请求通过NFQUEUE送到用户态, 用户态程序直接放行, 这样来验证能否复现问题。内核模块代码，nfqdns.c内容:\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475#include &lt;linux/module.h&gt;#include &lt;linux/kernel.h&gt;#include &lt;linux/init.h&gt;#include &lt;linux/skbuff.h&gt;#include &lt;linux/ip.h&gt;#include &lt;linux/netfilter.h&gt;#include &lt;linux/netfilter_ipv4.h&gt;#include &lt;net/udp.h&gt;MODULE_LICENSE(&quot;GPL&quot;);MODULE_DESCRIPTION(&quot;nfqdns&quot;);MODULE_ALIAS(&quot;module nfqdns netfiler&quot;);static int nfqueue_no = 0;MODULE_PARM_DESC(queue, &quot;nfquene number&quot;);module_param(nfqueue_no, int, 0600);static unsigned int nf_hook_out(const struct nf_hook_ops *ops,                   struct sk_buff *sk,                   const struct net_device *in,                   const struct net_device *out,                   const struct nf_hook_state *state)&#123;    struct udphdr *udph = NULL;    struct iphdr *iph = ip_hdr(sk);    u8 proto = iph-&gt;protocol;    if (proto != IPPROTO_UDP) &#123;        return NF_ACCEPT;    &#125;    udph = (struct udphdr *) skb_transport_header(sk);    if (htons(udph-&gt;dest) == 53) &#123;        printk(KERN_INFO &quot;[nfqdns]: %pI4:%d-&gt;%pI4:%d queued in [%d], skb: %p, ct: %p, tid: %d\\n&quot;,                &amp;iph-&gt;saddr, htons(udph-&gt;source), &amp;iph-&gt;daddr, htons(udph-&gt;dest), nfqueue_no,                sk, sk-&gt;nfct, htons(*(unsigned short*)(udph + 1)));        return NF_QUEUE_NR(nfqueue_no);    &#125;    return NF_ACCEPT;&#125;static struct nf_hook_ops nfhooks[] = &#123;&#123;    .hook = nf_hook_out,    .owner = THIS_MODULE,    .pf = NFPROTO_IPV4,    .hooknum = NF_INET_POST_ROUTING,    .priority = NF_IP_PRI_FIRST,&#125;,&#125;;int __init nfqdns_init(void)&#123;    nf_register_hooks(nfhooks, ARRAY_SIZE(nfhooks));    printk(KERN_INFO &quot;nfqdns module init\\n&quot;);    return 0;&#125;void __exit nfqdns_exit(void)&#123;    nf_unregister_hooks(nfhooks, ARRAY_SIZE(nfhooks));    printk(KERN_INFO &quot;nfqdns module exit\\n&quot;);    return;&#125;module_init(nfqdns_init);module_exit(nfqdns_exit);\n\n用户态程序user.c代码:\n12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#include &lt;stdio.h&gt;#include &lt;assert.h&gt;#include &lt;string.h&gt;#include &lt;netinet/in.h&gt;#include &lt;linux/types.h&gt;#include &lt;linux/netfilter.h&gt;#include &lt;libnetfilter_queue/libnetfilter_queue.h&gt;static int cb(struct nfq_q_handle *qh, struct nfgenmsg *nfmsg,        struct nfq_data *nfa, void *data)&#123;    u_int32_t id = 0;    struct nfqnl_msg_packet_hdr *ph;    uint32_t  m = 0;    ph = nfq_get_msg_packet_hdr(nfa);    if (ph) &#123;        id = ntohl(ph-&gt;packet_id);    &#125;    printf(&quot;packet: %u\\n&quot;, id);    return nfq_set_verdict(qh, id, NF_ACCEPT, 0, NULL);&#125;int main(int argc, char **argv)&#123;    struct nfq_handle *h;    struct nfq_q_handle *qh;    struct nfnl_handle *nh;    int    fd;    int rv;    char buf[4096];    assert((h = nfq_open()) != NULL);    assert(nfq_unbind_pf(h, AF_INET) == 0);    assert(nfq_bind_pf(h, AF_INET) == 0);    assert((qh = nfq_create_queue(h, 0, &amp;cb, NULL)) != NULL);    assert(nfq_set_mode(qh, NFQNL_COPY_PACKET, 0xffff) == 0);    fd = nfq_fd(h);    while ((rv = recv(fd, buf, sizeof(buf), 0)) &amp;&amp; rv &gt;= 0) &#123;        nfq_handle_packet(h, buf, rv);    &#125;    nfq_destroy_queue(qh);    nfq_close(h);    return 0;&#125;\n\nMakefile内容如下:\n123456obj-m += nfqdns.oall:    make -C /lib/modules/$(shell uname -r)/build M=$(PWD) modules    gcc user.c  -l netfilter_queue -o user.outclean:    make -C /lib/modules/$(shell uname -r)/build M=$(PWD) clean\n\n因为BUG存在于nf_conntrack模块中，确保加载nf_conntrack和nf_conntrack_ipv4模块，然后加载我们的实验内核模块:\n12modprobe nf_conntrack nf_conntrack_ipv4insmod ./nfqdns.ko\n\n接着运行用户态程序:\n1./user.out\n\n此时执行curl命令依然可以复现DNS超时现象，可以确定和NFQUEUE机制有关系。\n从我们内核模块的日志输出可以看到在执行nf_hook_out时，AAAA记录的请求数据包还没有被丢弃，也可以看到第一次发送的两个DNS请求数据包的conntrack条目确实不同:\n1234[467402.634931] [nfqdns]: 10.10.10.89:56578-&gt;114.114.114.114:53 queued in [0], skb: ffff880067d15d00, ct: ffff8800905ead68, tid: 8725[467402.634958] [nfqdns]: 10.10.10.89:56578-&gt;114.114.114.114:53 queued in [0], skb: ffff880067d15c00, ct: ffff8800905ea750, tid: 5451[467407.643516] [nfqdns]: 10.10.10.89:56578-&gt;114.114.114.114:53 queued in [0], skb: ffff8800b37f9f00, ct: ffff8800905ead68, tid: 8725[467407.645559] [nfqdns]: 10.10.10.89:56578-&gt;114.114.114.114:53 queued in [0], skb: ffff8800b8fe3100, ct: ffff8800905ead68, tid: 5451\n\nconntrack上述BUG丢弃数据包是发生在conntrack条目确认阶段的__nf_conntrack_confirm函数中。conntrack的原理和实现可以参考这篇文章, 本文不详述。\n我们通过systemtap来验证丢包是否发生在这里。systemtap脚本t.stp内容如下:\n1234567891011121314151617181920212223242526272829%&#123;#include &lt;net/udp.h&gt;%&#125;function read_tid: long (udphdr: long) %&#123;    struct udphdr *udph = (struct udphdr *)STAP_ARG_udphdr;    unsigned short tid = htons(*(unsigned short *)(udph + 1));    STAP_RETVALUE = (long) tid;%&#125;probe module(&quot;nf_conntrack&quot;).function(&quot;__nf_conntrack_confirm&quot;) &#123;    iphdr = __get_skb_iphdr($skb)    saddr = format_ipaddr(__ip_skb_saddr(iphdr), %&#123; AF_INET %&#125;)    daddr = format_ipaddr(__ip_skb_daddr(iphdr), %&#123; AF_INET %&#125;)    protocol = __ip_skb_proto(iphdr)    udphdr = __get_skb_udphdr($skb)    if (protocol == %&#123; IPPROTO_UDP %&#125;) &#123;        dport = __tcp_skb_dport(udphdr)        sport = __tcp_skb_sport(udphdr)        if (dport == 53 || sport == 53) &#123;            printf(&quot;__nf_conntrack_confirm: %s:%d-&gt;%s:%d TID: %d, ct: %p, ctinfo: %d\\n&quot;, saddr, sport, daddr, dport, read_tid(udphdr), $skb-&gt;nfct, $ctinfo)        &#125;    &#125;&#125;probe module(&quot;nf_conntrack&quot;).function(&quot;__nf_conntrack_confirm&quot;).return &#123;    printf(&quot;__nf_conntrack_confirm: return %ld\\n&quot;, $return)&#125;\n\n从systemtap执行结果可以确定处理第二个DNS请求数据包时，函数__nf_conntrack_confirm函数返回了0(NF_DROP):\n12345[root@localhost fg]__nf_conntrack_confirm: 10.10.10.89:58305-&gt;114.114.114.114:53 TID: 16998, ct: 0xffff8800905eb860, ctinfo: 2__nf_conntrack_confirm: return 1__nf_conntrack_confirm: 10.10.10.89:58305-&gt;114.114.114.114:53 TID: 568, ct: 0xffff8800905eb248, ctinfo: 2__nf_conntrack_confirm: return 0\n\n而查看3.10.0-123.el7.x86_64版本的__nf_conntrack_confirm(net/netfilter/nf_conntrack_core.c)源码如下:\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108int__nf_conntrack_confirm(struct sk_buff *skb)&#123;        unsigned int hash, repl_hash;        struct nf_conntrack_tuple_hash *h;        struct nf_conn *ct;        struct nf_conn_help *help;        struct nf_conn_tstamp *tstamp;        struct hlist_nulls_node *n;        enum ip_conntrack_info ctinfo;        struct net *net;        u16 zone;        ct = nf_ct_get(skb, &amp;ctinfo);        net = nf_ct_net(ct);                if (CTINFO2DIR(ctinfo) != IP_CT_DIR_ORIGINAL)                return NF_ACCEPT;        zone = nf_ct_zone(ct);                hash = *(unsigned long *)&amp;ct-&gt;tuplehash[IP_CT_DIR_REPLY].hnnode.pprev;        hash = hash_bucket(hash, net);        repl_hash = hash_conntrack(net, zone,                                   &amp;ct-&gt;tuplehash[IP_CT_DIR_REPLY].tuple);                                NF_CT_ASSERT(!nf_ct_is_confirmed(ct));        pr_debug(&quot;Confirming conntrack %p\\n&quot;, ct);        spin_lock_bh(&amp;nf_conntrack_lock);                if (unlikely(nf_ct_is_dying(ct))) &#123;                spin_unlock_bh(&amp;nf_conntrack_lock);                return NF_ACCEPT;        &#125;                hlist_nulls_for_each_entry(h, n, &amp;net-&gt;ct.hash[hash], hnnode)                if (nf_ct_tuple_equal(&amp;ct-&gt;tuplehash[IP_CT_DIR_ORIGINAL].tuple,                                      &amp;h-&gt;tuple) &amp;&amp;                    zone == nf_ct_zone(nf_ct_tuplehash_to_ctrack(h)))                        goto out;        hlist_nulls_for_each_entry(h, n, &amp;net-&gt;ct.hash[repl_hash], hnnode)                if (nf_ct_tuple_equal(&amp;ct-&gt;tuplehash[IP_CT_DIR_REPLY].tuple,                                      &amp;h-&gt;tuple) &amp;&amp;                    zone == nf_ct_zone(nf_ct_tuplehash_to_ctrack(h)))                        goto out;                hlist_nulls_del_rcu(&amp;ct-&gt;tuplehash[IP_CT_DIR_ORIGINAL].hnnode);                ct-&gt;timeout.expires += jiffies;        add_timer(&amp;ct-&gt;timeout);        atomic_inc(&amp;ct-&gt;ct_general.use);        ct-&gt;status |= IPS_CONFIRMED;                tstamp = nf_conn_tstamp_find(ct);        if (tstamp) &#123;                if (skb-&gt;tstamp.tv64 == 0)                        __net_timestamp(skb);                tstamp-&gt;start = ktime_to_ns(skb-&gt;tstamp);        &#125;                __nf_conntrack_hash_insert(ct, hash, repl_hash);        NF_CT_STAT_INC(net, insert);        spin_unlock_bh(&amp;nf_conntrack_lock);        help = nfct_help(ct);        if (help &amp;&amp; help-&gt;helper)                nf_conntrack_event_cache(IPCT_HELPER, ct);        nf_conntrack_event_cache(master_ct(ct) ?                                 IPCT_RELATED : IPCT_NEW, ct);        return NF_ACCEPT;out:        NF_CT_STAT_INC(net, insert_failed);        spin_unlock_bh(&amp;nf_conntrack_lock);        return NF_DROP;&#125;\n\n从代码可以看到，在从conntrack表中找到tuple信息一致的条目时，会跳转到out标签处，返回NF_DROP将数据包丢弃。\n在更高版本的CentOS内核中修复了这个问题, CentOS7.8(内核版本:3.10.0-1127.el7.x86_64)的相同位置增加了冲突处理的逻辑来修复这个问题:\n12345678out:    nf_ct_add_to_dying_list(ct);    ret = nf_ct_resolve_clash(net, skb, ctinfo, h);dying:    nf_conntrack_double_unlock(hash, reply_hash);    NF_CT_STAT_INC(net, insert_failed);    local_bh_enable();    return ret;\n\n从源码中我们也可以看到当出现这个问题时conntrack模块的insert_failed统计值会增加。我们可以通过conntrack -S命令来查看统计值。通过验证可以看到发生该现象时insert_failed统计值确实增加了:\n\n那么为什么NFQUEUE机制会触发该BUG呢？从curl的strace输出可以看到，curl调用sendmmsg同时发送A和AAAA两个DNS请求。系统调用sendmmsg的实现是__sys_sendmmsg(net/socket.c), 它会循环调用___sys_sendmsg来发送两个数据包:\n12345678910111213141516171819202122while (datagrams &lt; vlen) &#123;        if (MSG_CMSG_COMPAT &amp; flags) &#123;                err = ___sys_sendmsg(sock, (struct msghdr __user *)compat_entry,                                     &amp;msg_sys, flags, &amp;used_address);                if (err &lt; 0)                        break;                err = __put_user(err, &amp;compat_entry-&gt;msg_len);                ++compat_entry;        &#125; else &#123;                err = ___sys_sendmsg(sock,                                     (struct msghdr __user *)entry,                                     &amp;msg_sys, flags, &amp;used_address);                if (err &lt; 0)                        break;                err = put_user(err, &amp;entry-&gt;msg_len);                ++entry;        &#125;        if (err)                break;        ++datagrams;&#125;\n\n具体网络发包过程可参考这篇文章。本文不详述。___sys_sendmsg函数会一直调用到协议栈的ip_output(net/ipv4/ip_output.c)函数:\n12345678910111213int ip_output(struct sk_buff *skb)&#123;        struct net_device *dev = skb_dst(skb)-&gt;dev;        IP_UPD_PO_STATS(dev_net(dev), IPSTATS_MIB_OUT, skb-&gt;len);        skb-&gt;dev = dev;        skb-&gt;protocol = htons(ETH_P_IP);        return NF_HOOK_COND(NFPROTO_IPV4, NF_INET_POST_ROUTING, skb, NULL, dev,                            ip_finish_output,                            !(IPCB(skb)-&gt;flags &amp; IPSKB_REROUTED));&#125;\n\nconntrack实现中，外发连接的conntrack条目的创建是在LOCAL_OUT阶段，而条目确认也就是真正插入conntrack表是在POST_ROUTING阶段。这里的NF_HOOK_COND会迭代调用netfilter框架中注册的一系列hook函数。nf_conntrack_ipv4模块在该阶段注册了ipv4_confirm(net/ipv4/netfilter/nf_conntrack_l3proto_ipv4.c)函数，并且优先级为最低，最后才会执行。当没有加载我们的内核模块时，NF_HOOK_COND会一直执行到ipv4_confirm, 最终调用到__nf_conntrack_confirm函数，将conntrack条目插入conntrack表，之后层层返回。处理第二个数据包时，在LOCAL_OUT阶段处理时已经可以查到插入的conntrack条目，因而不会再给该数据包创建conntrack条目。而加载我们的内核模块后，我们在POST_ROUTING阶段注册的nf_hook_out函数优先级高于nf_conntrack_ipv4的ipv4_confirm函数，会先得到执行。我们返回NF_QUEUE之后，NF_HOOK_COND就会直接返回0。后续的函数执行需要内核收到用户态程序的netlink消息后再继续执行。因而__nf_conntrack_confirm这时并未得到执行。层层返回到___sys_send_msg之后，再发送第二个数据包。在LOCAL_OUT阶段由于在conntrack表中不能查找到相应的conntrack条目，所以会给该数据包再创建conntrack条目，最终触发BUG。\n在存在该BUG的系统上可以通过在/etc/resolv.conf中添加options single-request-reopen来规避:\n从man 5 resolv.conf可以看到相关说明:\n1234single-request-reopen (since glibc 2.9)     The resolver uses the same socket for the A and AAAA requests.  Some hardware mistakenly sends back only one reply.  When that happens the client system     will  sit  and  wait for the second reply.  Turning this option on changes this behavior so that if two requests from the same port are not handled cor‐     rectly it will close the  socket and open a new one before sending the second request.\n\n这样就不会同时发送A和AAAA请求包，而是A请求收到回应包再发送AAAA请求，从而能在启用NFQUEUE机制的情况下规避该BUG。\n后续有时间再从源码角度分析一下conntrack机制的实现。\n参考:\n\nhttps://arthurchiao.art/blog/conntrack-design-and-implementation-zh/\nhttp://cxd2014.github.io/2017/08/15/connection-tracking-system/\nhttps://zhuanlan.zhihu.com/p/373060740\nhttps://www.codedump.info/post/20200128-systemtap-by-example/\nhttps://thermalcircle.de/doku.php?id=blog:linux:connection_tracking_1_modules_and_hooks\n\n","slug":"LINUX/NFQUEUE机制导致DNS请求5秒超时分析","date":"2024-08-12T10:01:18.000Z","categories_index":"conntrack,LINUX","tags_index":"https,DNS,BUG","author_index":"dandeliono"},{"id":"a16ed64205294beb02f16921cdcc37a9","title":"Shiro实现session和jwt认证共存【补充篇】","content":"Shiro实现session和jwt认证共存【补充篇】前言前文 Shiro实现session和无状态token认证共存 保姆级代码，但是不够完善，有些难点不清不楚，这里补充一些难点的解决。当时选型 shiro复用session实现前后端分离鉴权 ，纯粹采用有状态的鉴权方案简单强大，而真正意义上的的有状态和无状态共存在shiro上是不好实现的，当然我也给你解决了。\n再次强调无状态如果不了解，赶紧止步，请使用有状态方案，真正了解无状态并坚决落地无状态再来看实现。\n无状态有状态的比喻：前文经典比喻\n\n\n\n\n\n\n\n\n\nsession与jwt的不同：session认证是保险箱在服务器，密码在用户手中，用户把密码送到服务器解开自己的保险箱，而jwt则是保险箱放在用户手中，服务器什么都不放，当用户把保险箱送来，服务器摸一摸保险箱，敲打敲打，认为保险箱是自己家生产的就打开它。这样当服务器开分号时，采用session方式就只能帮用户解锁在自己分号的保险箱，用户如果让a分号打开存在b分号的保险箱，就得顺丰快递从a送到b送过来。而jwt方式每一家分号都能打开任意用户的保险箱。\n\n\n\n\n\n\n\n\n\n补充一下上面比喻，假如保险箱非常多，明显session方案成本在存储保险箱，jwt方案成本在用户得搬运保险箱。redis集群虽然也耗带宽但是可以搭建在内网，相当于分号内部建立的超时空通道。\n运维成本：所谓的有状态无状态关键在于，session存储信息在服务端，jwt存储信息在客户端。抓住要点，这意味着高负荷下有状态耗费服务器内存，无状态耗费外网网络带宽。\n假设权限丰富的token平均大小为5kb，这是非常正常的。200w session存储大概占用10g内存。2000qps jwt传输占用10MB&#x2F;s网速即80M下载带宽。我在这里假定的数据是非常有实际意义的，这个价位的云服务器，10g内存大致对应的就是百M下载带宽，2000qps 非常不错了，这还是因为下载带宽相对上传带宽不值钱，如果你要搞jwt刷新策略返回参数带新jwt就要消耗上传带宽了，这个就是高成本了。很明显200w的session的系统级别远远不是2000qps的级别系统可比的。也就是说成本上来说肯定存储服务器性价比高，所谓的有状态浪费服务器资源不攻自破，无状态还浪费带宽呢。\n网上乱象：在研发权限丰富的系统中，我更倾向使用session的方式。近年来无状态兴起，各种博文动不动就贬低session一无是处，强行给session编了一大堆缺点（要么是根本不懂乱写，要么是没本事解决），其实session才是主流方案，方便控制-权限更新、踢人下线、限制登录等，无状态token不适合小项目趟这个浑水，更适合大型互联网项目的部分功能，它非常适用于只需要认证而鉴权需求少的局部的微服务。我前两篇文章是在2020年1月写的，那时候无状态吹得是牛逼轰轰，动不动就无状态鉴权，现在是2022年3月风向好像倒回去，估计是被坑的人越来越多了。\n不过直到现在很多文章的评论里都混淆无状态、有状态、token、jwt、session、sessionId、cookie、localStorage。随处可见干了好几年的程序员写出session依赖cookie在手机端不能用，必须使用token或者jwt、无状态认证 的评论。\n入门八股文必有一问session和cookie的区别？评论session离不开cookie的绝大多数都背过这个八股文，不然也入不了行，网上的八股文解答很细致，也都能答出什么cookie只能存4k之类的。。。\n概诉作用：这个问题就是个陷阱，不用讲它们的区别，它们压根就是两个东西，只不过历史场景经常绑在一起使用，session代表有状态会话，包含用户登录状态、权限信息，存在服务器的内存之中，比较大，生成一个sessionId的短字符串作为key通过response请求设置cookie并传送给客户端，客户端浏览器就会自动使用cookie存储，这样即使前端不编写传参，访问同域url时会自动带上cookie非常方便。sessionId存在客户端哪都行和cookie无关，只是方便不用前端自己编写传参逻辑，比如安卓小程序之类cookie不能用的场景，sessionId可以前端自己编写保存到存储里，统一拦截请求时带到请求头或者参数里都行，后端也得编写逻辑去拦截获取。这里sessionId通常会被后端改个好听的名字叫做token，token只是个参数名而已，这里的token就是sessionId就是有状态token。而jwt的token才是无状态的，说白了jwt的token就是session的内容加时间之类，你要是存权限之类的话就会非常长，要使用散列还是双向加密都行看你如何落地，不存服务器就得在传输时一直带上，jwt这套必须自己去实现这些传参保存方案，session+cookie设置这套框架自带，所以很多人误解只有token能用在移动端上 ，传来传去变成只有jwt能用在移动端上 。\n歧路方案：真正明白session和cookie和jwt的关系后，就不会走上歧路。比如当年我改造系统有了 shiro复用session实现前后端分离鉴权 这个方案，而在我开发之前的前辈的解决非常奇葩，cookie不能用后，就自己生成一串uuid保存redis叫做token，只能认证不能鉴权相当于shiro的user权限标识符。。。大概是因为不懂shiro框架，不懂如何在后端获取到sessionId，然后实现jwt又被哪篇水文忽悠了搞了个四不像。包括现在网上的文章很多jwt还给存到redis里，都存redis还能叫无状态吗。\n无状态缺点：无状态优点就是服务器不存储，然后为了实现踢下线，限制登录、权限变更，会衍生出各种有状态的方案。比如redis存储有变更user，jwt过来时需要比对user，搞到最后，服务器还是得存东西，所以无状态只是个理想概念，为了丰富功能最后还是得需要有状态来解决。\n我想象中的无状态使用场景：前面讲到有状态耗内存，无状态耗带宽。全部使用无状态不现实，我提倡精简jwt的token，大部分时候只需要认证不需要鉴权的功能使用jwt的token就行，然后用到某个涉及到鉴权的功能开始有状态，才读取权限内容保存进redis。这样可以解决无状态耗带宽的问题，又可以减低有状态对redis集群的过分依赖，防止redis一挂全部功能都得挂。redis集群存储个成千上亿的session虽然不成问题，但是web大集群和redis大集群频繁交互，每个请求都交互一次，耦合度实在太高了。\n所以说完美的无状态不现实，权限信息不可能存入jwt，太大了，耗带宽，小项目更加承受不起这个成本。\n前文已解决1、自定义过滤器2、多realm共存3、重写supports选择realm进行认证4、多realm共存鉴权失败抛出详细异常5、认证失败时 返回json6、授权失败时 返回json7、获取sessionid可以使用其它参数名\n难点解决*禁用session管理这个非常重要，如果关闭，不能实现有状态的token（即session）管理，开启着会影响无状态请求，会导致各种莫名其妙的bug。\n由于系统采用redis作为缓存管理，查找办法就是把redis关闭掉，操作的时候就会触发redis连接导致请求无响应和超时，再一一解决。\n三禁-禁session写入、禁session读取、禁权限读取，都涉及到有状态。\n官方文档\n\n\n\n\n\n\n\n\n\nhttps://shiro.apache.org/session-management.html#SessionManagement-SessionsSubjectState-HybridApproach\n解疑：经过重复试验，在多realm共存的情况下，全部禁用session管理或者开启session管理都没啥问题，但是混用的时候就不好处理了，得想办法处理，不修改源码的前提下，框架只能做到选择性禁止session的缓存写入。\n① 禁创建的session缓存写入官网混用方案只能禁登录时的session缓存写入，也就是说不会禁止缓存读取。\n方法一：重写SessionStorageEvaluator传参有Subject ，可以获取request 的内容，通过token的不同特征来区分是无状态的还是有状态的token重写SessionStorageEvaluator：\n1234567891011121314151617@Servicepublic class CustomeSessionStorge extends DefaultWebSessionStorageEvaluator &#123;    @Override    public boolean isSessionStorageEnabled(Subject subject) &#123;        if(subject instanceof WebSubject)&#123;            HttpServletRequest request = (HttpServletRequest) ((WebSubject) subject).getServletRequest();            String token = request.getParameter(&quot;token&quot;);            if(StringUtils.isBlank(token)) &#123;                token = ((HttpServletRequest) request).getHeader(&quot;token&quot;);            &#125;            if (token == null || token.contains(&quot;.&quot;)) &#123;                return false;            &#125;        &#125;        return super.isSessionStorageEnabled(subject);    &#125;&#125;\n\n配置：\n123DefaultSubjectDAO subjectDAO = new DefaultSubjectDAO();subjectDAO.setSessionStorageEvaluator(customeSessionStorge);securityManager.setSubjectDAO(subjectDAO);\n\n方法二：NoSessionCreationFilter\n\n\n\n\n\n\n\n\n\nhttps://shiro.apache.org/static/1.9.0/apidocs/org/apache/shiro/web/filter/session/NoSessionCreationFilter.html\nsession有提供一个过滤器用于禁止创建session，设置在登录接口上，效果可能比方法一更好。noSessionCreation 过滤器设置到禁止创建session的接口上即可。\n② 禁session的缓存读取禁session的缓存读取，如果不设置，请求带无状态token时认证前会读取session。\n重写 DefaultWebSessionManager getSessionId 方法&#x2F;&#x2F; 自己的token判断 我这里共用一个toekn参数三层判断第一个if判断包含.说明是jwt，sessionId直接返回null，就不会读缓存；第二个if判断有状态token参数，变了传参形式的sessionId；其它则是旧版的cookie传参的sessionId。\n12345678910111213141516171819202122232425262728protected Serializable getSessionId(ServletRequest request, ServletResponse response) &#123;\t\tString sid = request.getParameter(&quot;token&quot;);\t\tif(StringUtils.isBlank(sid)) &#123;\t\t\tsid = ((HttpServletRequest) request).getHeader(&quot;token&quot;);\t\t&#125;\t\t\t\tif (StringUtils.isNotBlank(sid) &amp;&amp; sid.contains(&quot;.&quot;)) &#123;\t\t\treturn null;\t\t&#125; else if (StringUtils.isNotBlank(sid)) &#123;\t\t\t\t\t\tif (WebUtils.isTrue(request, &quot;__cookie&quot;))&#123;\t\t        HttpServletRequest rq = (HttpServletRequest)request;\t\t        HttpServletResponse rs = (HttpServletResponse)response;\t\t\t\tCookie template = getSessionIdCookie();\t\t        Cookie cookie = new SimpleCookie(template);\t\t\t\tcookie.setValue(sid); cookie.saveTo(rq, rs);\t\t\t&#125;\t\t\t            request.setAttribute(ShiroHttpServletRequest.REFERENCED_SESSION_ID_SOURCE,                    ShiroHttpServletRequest.URL_SESSION_ID_SOURCE);             request.setAttribute(ShiroHttpServletRequest.REFERENCED_SESSION_ID, sid);            request.setAttribute(ShiroHttpServletRequest.REFERENCED_SESSION_ID_IS_VALID, Boolean.TRUE);        \treturn sid;\t\t&#125; else &#123;\t\t\treturn super.getSessionId(request, response);\t\t&#125;\t&#125;\n\n③ 禁权限读取禁了前两步，访问带权限标识符的需要鉴权的接口时还会请求redis。调试后发现StatelessAuthorizingRealm的缓存确实禁用了，但是因为鉴权会遍历realm调用了有状态的Realm导致请求了redis。\n解决办法就是重写所有的鉴权方法，判断是否无状态并结束调用。所以前文不齐全，前文拦截的是doGetAuthorizationInfo方法，这个方法是—授权查询回调函数, 进行鉴权但缓存中无用户的授权信息时调用，也就是获取授权。所以要重写的是真正的鉴权方法isPermitted，鉴权方法会读取缓存，在前面判断拦截了，就不会读取。\n加个判断，判断不是当前realm的principals，鉴权false。\n12345678@Overridepublic boolean isPermitted(PrincipalCollection principals, Permission permission) &#123;\tif(!(getAvailablePrincipal(principals) instanceof Principal))&#123;\t\treturn false;\t&#125;\tauthorizationValidate(permission);\treturn super.isPermitted(principals, permission);&#125;\n\njwt方案整合shiro禁缓存才是最大的难点，整合简单多了，前提是得理解一些概念，不然也是会搞得乱七八糟，前文从未提及jwt，这里作补充。\n① 登录接口不重要的全部省略掉，重点在于SecurityUtils.getSubject() ，subject调用login，然后再subject获取Principal得到token。\n12345678910111213141516@RequestMapping(value = &quot;。。。/login2&quot;)   @ResponseBody   public ResultDTO login2(HttpServletRequest request...) &#123;       ...       Subject subject = SecurityUtils.getSubject();       try &#123;           if (subject == null) &#123;               ...           &#125; else &#123;               String username = request.getParameter(&quot;username&quot;);               String password = request.getParameter(&quot;password&quot;);               subject.login(new StatelessToken(username, password));               String jwt = ((StatelessAuthorizingRealm.Principal)SecurityUtils.getSubject().getPrincipal()).getToken();    \t...   &#125;\n\n② 拦截认证拦截认证，token无效返回失败，有效调用login。关键点在于需要login。有状态的时候框架自动根据sessionId获取用户信息，而无状态每次调用都要login，不然鉴权的时候会报错。\n12345678910111213141516171819@Overrideprotected boolean onAccessDenied(ServletRequest request, ServletResponse response) throws Exception &#123;\t\tStatelessToken statelessToken = new StatelessToken();\t\tString token = request.getParameter(&quot;token&quot;);\t\tif(StringUtils.isBlank(token)) &#123;\t\t\ttoken = ((HttpServletRequest) request).getHeader(&quot;token&quot;);\t\t&#125;\t\tif(JwtUtil.verify(token)) &#123;\t\t\t\t\t\tstatelessToken.setToken(token);\t\t\tgetSubject(request, response).login(statelessToken);\t\t\treturn true;\t\t&#125;\t\tResultDTO retDto = null;\t\t...\t\tresponse.getWriter().write(GsonUtils.toJson(retDto));\t\treturn false;\t&#125;\n\n③ 赋予认证信息这里体现了无状态和有状态的最大不同。有状态只需要登录认证就行了，而无状态需要获取是否包含jwt的token，有的话说明登陆过，千万不要再去登录，而是解析token获取用户信息设置到SimpleAuthenticationInfo里，这一步其实相当于session版shiro内置读取session缓存，后面的和有状态实现一样，踢人不行了这得有状态去实现。\n12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758@Override\tprotected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken authcToken) &#123;\t\tStatelessToken token = (StatelessToken) authcToken;\t\tif(StringUtils.isNotBlank(token.getToken())) &#123;\t\t\tUser user = JwtUtil.getUsername(token.getToken());\t\t\tPrincipal principalTmp = new Principal(user);\t\t\tprincipalTmp.setToken(token.getToken());\t\t\treturn  new SimpleAuthenticationInfo(principalTmp, null, null, getName());\t\t&#125;\t\t\t\tUser user = getSystemService().getUserByTLoginName(token.getUsername());\t\tif (user != null) &#123;\t\t\tif (...) &#123;\t\t\t\tthrow new AuthenticationException(&quot;msg:该帐号已禁止登录.&quot;);\t\t\t&#125; else if (...) &#123;\t\t\t\tthrow new AuthenticationException(&quot;msg:该帐号已被加入黑名单.&quot;);\t\t\t&#125;\t\t\tbyte[] salt = Encodes.decodeHex(user.getPassword().substring(。。。));\t\t\tPrincipal principal = new Principal(user);\t\t\tSimpleAuthorizationInfo info = new SimpleAuthorizationInfo();\t\t\tList&lt;Menu&gt; list = UserUtils.get。。。;\t\t\tfor (Menu menu : list) &#123;\t\t\t\tif (StringUtils.isNotBlank(menu.getPermission())) &#123;\t\t\t\t\t\t\t\t\t\tfor (String permission : StringUtils.split(menu.getPermission(), &quot;,&quot;)) &#123;\t\t\t\t\t\tinfo.addStringPermission(permission);\t\t\t\t\t&#125;\t\t\t\t&#125;\t\t\t&#125;\t\t\t\t\t\tinfo.addStringPermission(&quot;user&quot;);\t\t\t\t\t\tfor (Role role : user.getRoleList()) &#123;\t\t\t\tinfo.addRole(role.get...);\t\t\t&#125;\t\t\tString jwt = JwtUtil.createJWT(user, info);\t\t\tprincipal.setToken(jwt);\t\t\t\t\t\tif(token.isTraceless()) &#123;\t\t\t\tprincipal.setTraceless(true);\t\t\t&#125;\t\t\tAuthenticationInfo authenticationInfo = new SimpleAuthenticationInfo(principal, user\t\t\t\t\t.getPassword().substring(...), ByteSource.Util.bytes(salt), getName());\t\t\tif(!token.isTraceless()) &#123;\t\t\t\t\t\t\t\tgetSystemService().update。。。\t\t\t\t\t\t\t\tLogUtils.saveLog(...);\t\t\t&#125;\t\t\t\t\t\treturn authenticationInfo;\t\t&#125; else &#123;\t\t\treturn null;\t\t&#125;\t&#125;\n\n④ 赋予授权信息这是绝对理想化的获取授权信息，有状态版想更新缓存就更新缓存，无状态只能解析jwt获取，实时性就没了。要想实现又得引入redis的一套，比如修改过的user设置进redis。这里面获取授权信息每次都得查询redis看是否存在于名单里是否需要重新读取，这样完美的无状态又不能实现了。\n12345678@Overrideprotected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principals) &#123;\t\tif(!(getAvailablePrincipal(principals) instanceof Principal))&#123;\t\t\treturn null;\t\t&#125;\t\tPrincipal principal = (Principal) getAvailablePrincipal(principals);\t\treturn JwtUtil.getRoles(principal.getToken());&#125;\n\n业务使用善于使用Subject、Principal ，只要搭的好，无状态有状态都可以用的，即使无状态，在一次请求的生命周期里是有状态的，和有状态的用法一模一样，可以通过Principal在业务里传递用户信息。\n123Subject subject = SecurityUtils.getSubject();Principal principal = (Principal) subject.getPrincipal();\n\n最后之前的文章不清不楚，这篇文章算是完美解答了，思路和疑难基本解决。至于使用无状态之后衍生出来的问题那就是后续的头疼了。在中小规模的项目中，我有个应用方案就是把无状态这套当做后备隐藏救急方案，就是当灾难发生时假如redis全挂了，就是用不了，系统可以马上切换到无状态realm。又或者是认证使用jwt减少对redis的耦合，提高系统的高可用，需要鉴权再再加载有状态内容，这也是非常不错的，至于完全的无状态，那是不现实的。\n","slug":"JAVA/Shiro实现session和jwt认证共存【补充篇】","date":"2024-07-11T09:28:24.000Z","categories_index":"session,JAVA","tags_index":"jwt,token,redis","author_index":"dandeliono"},{"id":"86d3016d930075b1294dec7c5e7f9e57","title":"Linux下so动态链接库使用总结","content":"Linux下so动态链接库使用总结本文主要总结在Linux环境下，使用so动态链接库经常遇到的问题，包括使用cp命令覆盖so导致进程coredump之类的问题。主要有以下内容：\n\nLinux下so动态链接库介绍\nldconfig命令的使用\nso动态库使用的常见问题\n\n\n一、Linux下so动态链接库介绍在介绍动态库前，提一个问题：到底什么是库呢？\n库从本质上来说是一种可执行代码的二进制格式，可以被载入内存中执行。库分静态库和动态库两种。\n静态库：这类库的名字一般是libxxx.a，xxx为库的名字。利用静态函数库编译成的文件比较大，因为整个函数库的所有数据都会被整合进目标代码中，他的优点就显而易见了，即编译后的执行程序不需要外部的函数库支持，因为所有使用的函数都已经被编译进去了。当然这也会成为他的缺点，因为如果静态函数库改变了，那么你的程序必须重新编译。\n动态库：这类库的名字一般是libxxx.M.N.so，同样的xxx为库的名字，M是库的主版本号，N是库的副版本号。当然也可以不要版本号，但名字必须有。相对于静态函数库，动态函数库在编译的时候并没有被编译进目标代码中，你的程序执行到相关函数时才调用该函数库里的相应函数，因此动态函数库所产生的可执行文件比较小。由于函数库没有被整合进你的程序，而是程序运行时动态的申请并调用，所以程序的运行环境中必须提供相应的库。动态函数库的改变并不影响你的程序，所以动态函数库的升级比较方便。linux系统有几个重要的目录存放相应的函数库，如&#x2F;lib和&#x2F;usr&#x2F;lib。\n本文主要介绍动态库。\nLinux下动态库文件的文件名形如 libxxx.so，其中so是 Shared Object 的缩写，即可以共享的目标文件。\n共享文件（*.so）也称为动态库文件，它包含了代码和数据（这些数据是在连接时候被连接器ld和运行时动态连接器使用的）。动态连接器可能称为ld.so.1，libc.so.1或者 ld-linux.so.1。我的CentOS6.0系统中该文件为：&#x2F;lib&#x2F;ld-2.12.so\n在链接动态库生成可执行文件时，并不会把动态库的代码复制到执行文件中，而是在执行文件中记录对动态库的引用。程序执行时，再去加载动态库文件。如果动态库已经加载，则不必重复加载，从而能节省内存空间。程序动态链接的优点是\n\n减少依赖相同动态库的多个进程同时运行时的内存的占用（不用每一个进程都加载一份动态库\n可扩展性在程序不用重启的情况下，动态的加载所需要的动态库，可实现对程序的扩展\n程序版本更新与动态链接库的分离\n\nLinux下生成和使用动态库的步骤如下：\n\n编写源文件\n使用命令： gcc -fPIC -shared -o libxxx.so xxx.c，将一个或几个源文件编译链接，生成共享库\n通过 -L -lxxx 的gcc选项链接生成的libxxx.so。\n把libxxx.so放入链接库的标准路径，或指定 LD_LIBRARY_PATH，才能运行链接了libxxx.so的程序。\n\n具体示例请参考：Linux动态库生成与使用指南\n查看程序使用的动态库基本上每一个linux 程序都会使用动态库，查看某个程序使用了那些动态库，可以使用ldd命令：  \n1234567891011# ldd /bin/lslinux-vdso.so.1 =&gt; (0x00007fff597ff000)libselinux.so.1 =&gt; /lib64/libselinux.so.1 (0x00000036c2e00000)librt.so.1 =&gt; /lib64/librt.so.1 (0x00000036c2200000)libcap.so.2 =&gt; /lib64/libcap.so.2 (0x00000036c4a00000)libacl.so.1 =&gt; /lib64/libacl.so.1 (0x00000036d0600000)libc.so.6 =&gt; /lib64/libc.so.6 (0x00000036c1200000)libdl.so.2 =&gt; /lib64/libdl.so.2 (0x00000036c1600000)/lib64/ld-linux-x86-64.so.2 (0x00000036c0e00000)libpthread.so.0 =&gt; /lib64/libpthread.so.0 (0x00000036c1a00000)libattr.so.1 =&gt; /lib64/libattr.so.1 (0x00000036cf600000)\n\n二、ldconfig命令的使用ldconfig命令是在Linux环境下使用so动态链接库时，经常会用到的命令，是Linux下动态链接库的管理命令，该命令位于&#x2F;sbin目录下。\nldconfig命令的用途主要是在默认搜寻目录&#x2F;lib和&#x2F;usr&#x2F;lib以及动态库配置文件&#x2F;etc&#x2F;ld.so.conf内所列的目录下，搜索出可共享的动态链接库（格式如lib_.so_）,进而创建出动态装入程序(ld.so)所需的连接和缓存文件。缓存文件默认为&#x2F;etc&#x2F;ld.so.cache，此文件保存已排好序的动态链接库名字列表，为了让动态链接库为系统所共享，需运行动态链接库的管理命令ldconfig，此执行程序存放在&#x2F;sbin目录下。\n\n\n\n\n\n\n\n\n\nldconfig通常在系统启动时运行，而当用户安装了一个新的动态链接库时，就需要手工运行这个命令。\n使用ldconfig几个需要注意的地方： \n\n往&#x2F;lib和&#x2F;usr&#x2F;lib里面加东西，是不用修改&#x2F;etc&#x2F;ld.so.conf的，但是完了之后要调一下ldconfig，不然这个library会找不到。\n想往上面两个目录以外加东西的时候，一定要修改&#x2F;etc&#x2F;ld.so.conf，然后再调用ldconfig，不然也会找不到。\n比如安装了一个mysql到&#x2F;usr&#x2F;local&#x2F;mysql，mysql有一大堆library在&#x2F;usr&#x2F;local&#x2F;mysql&#x2F;lib下面，这时就需要在&#x2F;etc&#x2F;ld.so.conf下面加一行&#x2F;usr&#x2F;local&#x2F;mysql&#x2F;lib，保存过后ldconfig一下，新的library才能在程序运行时被找到。\n如果想在这两个目录以外放lib，但是又不想在&#x2F;etc&#x2F;ld.so.conf中加东西（或者是没有权限加东西）。那也可以，就是export一个全局变量LD_LIBRARY_PATH，然后运行程序的时候就会去这个目录中找library。一般来讲这只是一种临时的解决方案，在没有权限或临时需要的时候使用,因为这样的export 只对当前shell有效，当另开一个shell时候，又要重新设置，当然可以把export LD_LIBRARY_PATH&#x3D;xxx 语句写到 ~&#x2F;.bashrc中，这样就对当前用户有效了，写到&#x2F;etc&#x2F;bashrc中就对所有用户有效了。\n\n程序执行时的搜索顺序程序执行时按照下列顺序依次装载或者查找共享对象:0）最优先的是，如果在编译时通过-rpath选项指定了路径，便会优先搜索这个路径1）由环境变量 LD_LIBRARY_PATH指定的路径2）由路径缓存文件&#x2F;etc&#x2F;ld.so.cache指定的路径3）默认共享目录 &#x2F;lib和&#x2F;usr&#x2F;lib其中LD_LIBRARY_PATH是一个环境变量，当指定某个程序的LD_LIBRARY_PATH时动态链接器在查找共享库的时候，会首先从指定的路径开始查找\n三、so动态库使用的常见问题介绍完so动态链接库的常见使用之后，下面介绍两个常见的问题：一个是cp命令拷贝一个新的so去覆盖旧的so时，如果有进程或者程序正常使用这个so，那么该操作极有可能导致该进程coredump或者程序崩溃；另一个问题是多个进程或程序都在使用同一个so，但是这个so的路径和版本均不同，那么使用ldconfig命令可能导致另一个进程或者程序出错。\n1. 覆盖so导致coredump问题总结出现问题的场景是升级，在升级流程的脚本中需要升级各个业务进程使用的so，但是有一个so文件是两个业务进程都在同时使用。比如有业务进程A、B、C，升级的过程是:A-&gt;B-&gt;C。其中有so是A和B都依赖，在升级A的过程中，先停掉A进程，升级其需要的so，这个时候升级so，使用的命令是cp，升级完so后，升级A进程使用的二进制，然后拉起A进程。在这一系列的过程中发现B进程coredump了，主要是没有考虑到B进程也在使用那个so。\ncp与mv&#x2F;rm的区别： \ncp from to，则被覆盖文件 to的inode依旧不变（属性也不变），内容变为from的；\nmv from to，则to的inode变为from的，相应的，to的属性也成了from的；rm类似；\n关于为什么会coredump可参考：关于so文件cp覆盖导致调用者core的研究\n解决方法\n\n\n\n\n\n\n\n\n\n方法一：先删除旧的so，然后再把新的so拷贝过去，即：rm oldlib.so 然后 cp newlib.so oldlib.so\n\n\n\n\n\n\n\n\n\n方法二：mv oldlib.so oldlib.so_bak 然后 cp newlib.so oldlib.so\n2. 一次执行ldconfig导致别的模块进程挂掉的经历问题原因是：与我们服务共同部署在同一个Linux服务器的其他服务也使用了zk服务，需要用到zk的动态链接库，我们的业务进程也需要用到zk的动态链接库。本来最初相安无事，但是在执行一个脚本之后，发现他们的服务挂了，经定位发现是因为so使用有问题，用到了我们服务进程的路径下的zk的动态链接库。在那个shell脚本中，直接用了“ldconfig + 路径”的方式搜索指定路径的so，随后导致他们的服务链接到我们的zk动态链接库了，而这动态链接库是有区别的，最终导致他们的服务挂掉。\n解决方法\n方法一：检查使用ldconfig的地方，在多种服务共同使用的服务器上，不能直接用“ldconfig + 路径”的方式随意添加一些常用的so路径（诸如zk这样常用的服务），诸如多种服务共同部署的时候，要注意避免这种情况；如果要调用脚本使用，可以通过export LD_LIBRARY_PATH的方式临时添加。\n方法二（推荐）：在编译的时候通过gcc -rpath 就指定动态库路径，这样就可以避免被其他路径下的不同的版本的so干扰。可以参考：gcc -rpath 指定动态库路径\n\n参考资料有关Linux下库的概念、生成和升级和使用等linux下so动态库一些不为人知的秘密（上）linux下so动态库一些不为人知的秘密（中）Linux共享库.so文件的命名和动态链接Linux动态库生成与使用指南ldconfig命令Linux共享库(so)动态加载和升级gcc -rpath 指定动态库路径Linux系统中“动态库”和“静态库”那点事儿关于so文件cp覆盖导致调用者core的研究\n","slug":"LINUX/Linux下so动态链接库使用总结","date":"2024-07-01T13:35:18.000Z","categories_index":"ldconfig,LINUX","tags_index":"https,Linux,lib","author_index":"dandeliono"},{"id":"2234c4cd1c8f1704146bd28a35b83d4d","title":"ES如何在几十亿数据场景下优化查询性能","content":"ES如何在几十亿数据场景下优化查询性能1. 面试官心里分析es在数据量很大的情况下（数十亿级别）如何提高查询效率啊？\n问这个问题，是肯定的，说白了，就是看你有没有实际干过es，因为啥？es说白了其实性能并没有你想象中那么好的。很多时候数据量大了，特别是有几亿条数据的时候，可能你会懵逼的发现，跑个搜索怎么一下5秒10秒，坑爹了。第一次搜索的时候，是510秒，后面反而就快了，可能就几百毫秒。\n你就很懵，每个用户第一次访问都会比较慢，比较卡么？\n所以你要是没玩儿过es，或者就是自己玩玩儿demo，被问到这个问题容易懵逼，显示出你对es确实玩儿的不怎么样\n2. 面试题剖析说实话，es性能优化是没有什么银弹的，啥意思呢？就是不要期待着随手调一个参数，就可以万能的应对所有的性能慢的场景。也许有的场景是你换个参数，或者调整一下语法，就可以搞定，但是绝对不是所有场景都可以这样。\n一块一块来分析吧\n在这个海量数据的场景下，如何提升es搜索的性能，也是我们之前生产环境实践经验所得\n1 性能优化的杀手锏——filesystem cache01_filesystem cache对es性能的影响\n\nos cache，操作系统的缓存。\n你往es里写的数据，实际上都写到磁盘文件里去了，磁盘文件里的数据操作系统会自动将里面的数据缓存到os cache里面去\nes的搜索引擎严重依赖于底层的filesystem cache，你如果给filesystem cache更多的内存，尽量让内存可以容纳所有的indx segment file索引数据文件，那么你搜索的时候就基本都是走内存的，性能会非常高。\n性能差距可以有大，我们之前很多的测试和压测，如果走磁盘一般肯定上秒，搜索性能绝对是秒级别的，1秒，5秒，10秒。但是如果是走filesystem cache，是走纯内存的，那么一般来说性能比走磁盘要高一个数量级，基本上就是毫秒级的，从几毫秒到几百毫秒不等。\n比如说，你，es节点有3台机器，每台机器，看起来内存很多，64G，总内存，64 * 3 &#x3D; 192g\n每台机器给es jvm heap是32G，那么剩下来留给filesystem cache的就是每台机器才32g，总共集群里给filesystem cache的就是32 * 3 &#x3D; 96g内存\nok，那么就是你往es集群里写入的数据有多少数据量？\n如果你此时，你整个，磁盘上索引数据文件，在3台机器上，一共占用了1T的磁盘容量，你的es数据量是1t，每台机器的数据量是300g\n你觉得你的性能能好吗？filesystem cache的内存才100g，十分之一的数据可以放内存，其他的都在磁盘，然后你执行搜索操作，大部分操作都是走磁盘，性能肯定差\n当时他们的情况就是这样子，es在测试，弄了3台机器，自己觉得还不错，64G内存的物理机。自以为可以容纳1T的数据量。\n归根结底，你要让es性能要好，最佳的情况下，就是你的机器的内存，至少可以容纳你的总数据量的一半。\n比如说，你一共要在es中存储1T的数据，那么你的多台机器留个filesystem cache的内存加起来综合，至少要到512G，至少半数的情况下，搜索是走内存的，性能一般可以到几秒钟，2秒，3秒，5秒\n如果最佳的情况下，我们自己的生产环境实践经验，所以说我们当时的策略，是仅仅在es中就存少量的数据，就是你要用来搜索的那些索引，内存留给filesystem cache的，就100G，那么你就控制在100gb以内，相当于是，你的数据几乎全部走内存来搜索，性能非常之高，一般可以在1秒以内\n比如说你现在有一行数据\nid name age ….30个字段\n但是你现在搜索，只需要根据id name age三个字段来搜索\n如果你傻乎乎的往es里写入一行数据所有的字段，就会导致说70%的数据是不用来搜索的，结果硬是占据了es机器上的filesystem cache的空间，单挑数据的数据量越大，就会导致filesystem cahce能缓存的数据就越少\n仅仅只是写入es中要用来检索的少数几个字段就可以了，比如说，就写入es id name age三个字段就可以了，然后你可以把其他的字段数据存在mysql里面，我们一般是建议用es + hbase的这么一个架构。\nhbase的特点是适用于海量数据的在线存储，就是对hbase可以写入海量数据，不要做复杂的搜索，就是做很简单的一些根据id或者范围进行查询的这么一个操作就可以了\n从es中根据name和age去搜索，拿到的结果可能就20个doc id，然后根据doc id到hbase里去查询每个doc id对应的完整的数据，给查出来，再返回给前端。\n你最好是写入es的数据小于等于，或者是略微大于es的filesystem cache的内存容量\n然后你从es检索可能就花费20ms，然后再根据es返回的id去hbase里查询，查20条数据，可能也就耗费个30ms，可能你原来那么玩儿，1T数据都放es，会每次查询都是5~10秒，现在可能性能就会很高，每次查询就是50ms。\nelastcisearch减少数据量仅仅放要用于搜索的几个关键字段即可，尽量写入es的数据量跟es机器的filesystem cache是差不多的就可以了；其他不用来检索的数据放hbase里，或者mysql。\n所以之前有些学员也是问，我也是跟他们说，尽量在es里，就存储必须用来搜索的数据，比如说你现在有一份数据，有100个字段，其实用来搜索的只有10个字段，建议是将10个字段的数据，存入es，剩下90个字段的数据，可以放mysql，hadoop hbase，都可以\n这样的话，es数据量很少，10个字段的数据，都可以放内存，就用来搜索，搜索出来一些id，通过id去mysql，hbase里面去查询明细的数据\n2 数据预热假如说，哪怕是你就按照上述的方案去做了，es集群中每个机器写入的数据量还是超过了filesystem cache一倍，比如说你写入一台机器60g数据，结果filesystem cache就30g，还是有30g数据留在了磁盘上。\n举个例子，就比如说，微博，你可以把一些大v，平时看的人很多的数据给提前你自己后台搞个系统，每隔一会儿，你自己的后台系统去搜索一下热数据，刷到filesystem cache里去，后面用户实际上来看这个热数据的时候，他们就是直接从内存里搜索了，很快。\n电商，你可以将平时查看最多的一些商品，比如说iphone 8，热数据提前后台搞个程序，每隔1分钟自己主动访问一次，刷到filesystem cache里去。\n对于那些你觉得比较热的，经常会有人访问的数据，最好做一个专门的缓存预热子系统，就是对热数据，每隔一段时间，你就提前访问一下，让数据进入filesystem cache里面去。这样期待下次别人访问的时候，一定性能会好一些。\n3 冷热分离关于es性能优化，数据拆分，我之前说将大量不搜索的字段，拆分到别的存储中去，这个就是类似于后面我最后要讲的mysql分库分表的垂直拆分。\nes可以做类似于mysql的水平拆分，就是说将大量的访问很少，频率很低的数据，单独写一个索引，然后将访问很频繁的热数据单独写一个索引\n你最好是将冷数据写入一个索引中，然后热数据写入另外一个索引中，这样可以确保热数据在被预热之后，尽量都让他们留在filesystem os cache里，别让冷数据给冲刷掉。\n你看，假设你有6台机器，2个索引，一个放冷数据，一个放热数据，每个索引3个shard\n3台机器放热数据index；另外3台机器放冷数据index\n然后这样的话，你大量的时候是在访问热数据index，热数据可能就占总数据量的10%，此时数据量很少，几乎全都保留在filesystem cache里面了，就可以确保热数据的访问性能是很高的。\n但是对于冷数据而言，是在别的index里的，跟热数据index都不再相同的机器上，大家互相之间都没什么联系了。如果有人访问冷数据，可能大量数据是在磁盘上的，此时性能差点，就10%的人去访问冷数据；90%的人在访问热数据。\n4 document模型设计假设场景：mysql，有两张表\n订单表：id order_code total_price\n1 测试订单 5000\n订单条目表：id order_id goods_id purchase_count price\n1 1 1 2 20002 1 2 5 200\n我在mysql里，都是select * from order join order_item on order.id&#x3D;order_item.order_id where order.id&#x3D;1\n1 测试订单 5000 1 1 1 2 20001 测试订单 5000 2 1 2 5 200\n在es里该怎么玩儿，es里面的复杂的关联查询，复杂的查询语法，尽量别用，一旦用了性能一般都不太好\n设计es里的数据模型\n写入es的时候，搞成两个索引，order索引，orderItem索引\norder索引，里面就包含id order_code total_priceorderItem索引，里面写入进去的时候，就完成join操作，id order_code total_price id order_id goods_id purchase_count price\n写入es的java系统里，就完成关联，将关联好的数据直接写入es中，搜索的时候，就不需要利用es的搜索语法去完成join来搜索了\ndocument模型设计是非常重要的，很多操作，不要在搜索的时候才想去执行各种复杂的乱七八糟的操作。es能支持的操作就是那么多，不要考虑用es做一些它不好操作的事情。如果真的有那种操作，尽量在document模型设计的时候，写入的时候就完成。另外对于一些太复杂的操作，比如join，nested，parent-child搜索都要尽量避免，性能都很差的。\n很多同学在问，很多复杂的乱七八糟的一些操作，如何执行？\n两个思路，在搜索&#x2F;查询的时候，要执行一些业务强相关的特别复杂的操作：\n1）在写入数据的时候，就设计好模型，加几个字段，把处理好的数据写入加的字段里面2）自己用java程序封装，es能做的，用es来做，搜索出来的数据，在java程序里面去做，比如说我们，基于es，用java封装一些特别复杂的操作\n5 分页性能优化es的分页是较坑的，为啥呢？举个例子吧，假如你每页是10条数据，你现在要查询第100页，实际上是会把每个shard上存储的前1000条数据都查到一个协调节点上，如果你有个5个shard，那么就有5000条数据，接着协调节点对这5000条数据进行一些合并、处理，再获取到最终第100页的10条数据。\n分布式的，你要查第100页的10条数据，你是不可能说从5个shard，每个shard就查2条数据？最后到协调节点合并成10条数据？你必须得从每个shard都查1000条数据过来，然后根据你的需求进行排序、筛选等等操作，最后再次分页，拿到里面第100页的数据。\n你翻页的时候，翻的越深，每个shard返回的数据就越多，而且协调节点处理的时间越长。非常坑爹。所以用es做分页的时候，你会发现越翻到后面，就越是慢。\n我们之前也是遇到过这个问题，用es作分页，前几页就几十毫秒，翻到10页之后，几十页的时候，基本上就要5~10秒才能查出来一页数据了\n1 不允许深度分页&#x2F;默认深度分页性能很惨你系统不允许他翻那么深的页，pm，默认翻的越深，性能就越差\n2 类似于app里的推荐商品不断下拉出来一页一页的类似于微博中，下拉刷微博，刷出来一页一页的，你可以用scroll api，自己百度\nscroll会一次性给你生成所有数据的一个快照，然后每次翻页就是通过游标移动，获取下一页下一页这样子，性能会比上面说的那种分页性能也高很多很多\n针对这个问题，你可以考虑用scroll来进行处理，scroll的原理实际上是保留一个数据快照，然后在一定时间内，你如果不断的滑动往后翻页的时候，类似于你现在在浏览微博，不断往下刷新翻页。那么就用scroll不断通过游标获取下一页数据，这个性能是很高的，比es实际翻页要好的多的多。\n但是唯一的一点就是，这个适合于那种类似微博下拉翻页的，不能随意跳到任何一页的场景。同时这个scroll是要保留一段时间内的数据快照的，你需要确保用户不会持续不断翻页翻几个小时。\n无论翻多少页，性能基本上都是毫秒级的\n因为scroll api是只能一页一页往后翻的，是不能说，先进入第10页，然后去120页，回到58页，不能随意乱跳页。所以现在很多产品，都是不允许你随意翻页的，app，也有一些网站，做的就是你只能往下拉，一页一页的翻\n","slug":"MIDDLEWARE/ES如何在几十亿数据场景下优化查询性能","date":"2024-06-17T19:45:12.000Z","categories_index":"cache,MIDDLEWARE","tags_index":"filesystem,order,mysql","author_index":"dandeliono"},{"id":"920d4ca514647ffb02b7df5ef178364f","title":"Improving the performance of high-cardinality terms aggregations in Elasticsearch","content":"Improving the performance of high-cardinality terms aggregations in ElasticsearchIntroductionAn Elasticsearch terms aggregation is used to create buckets corresponding to unique values of a given field. For example, a terms aggregation on a field containing country names would create a bucket for USA, a bucket for Canada, a bucket for Spain, and so on. Under normal circumstances, terms aggregations are very fast, however in some exceptional cases they may be slow. One reason for slow terms aggregations may be a mis-configured cluster. Another reason for poor performance may be high-cardinality values on the field which a terms aggregation is executed on.\nIn this blog post, I’ll first give a brief overview of general instructions that should be followed to ensure the best performance of an Elasticsearch cluster. This is then followed by several sections that present background material that will help to understand the underlying mechanics of terms aggregations, including (1) a definition of high cardinality, (2) a description of the refresh interval, and (3) a description of global ordinals. Next, I’ll show how to view the impact of building global ordinals on terms aggregation performance. Finally, I’ll present several techniques to improve the performance of high-cardinality terms aggregations, including (1) time-based indices, (2) eager global ordinals, and (3) techniques to prevent Elasticsearch from building global ordinals.\nIn one instance, the techniques documented in this blog post were able to reduce the execution time of a high-cardinality terms aggregation that was running at a very large retail bank, from 15 seconds to below 15 milliseconds.\nGeneral suggestionsTuning a cluster can have a large impact on overall cluster performance. The following Elasticsearch documentation provides details on configuring and tuning an Elasticsearch cluster, and should be followed:\n\nImportant system configuration\nTune for search speed\nTune for indexing speed\nTune for disk usage\n\nAdditional information on tuning slow queries can be found in this blog about advanced Elasticsearch tuning.\nIn addition to the above, having too many shards is a common cause of performance problems. This blog about sharding gives good rules of thumb to follow.\nThe remainder of this blog focuses specifically on understanding and tuning terms aggregations.  \nCardinalityThe performance of terms aggregations can be greatly impacted by the cardinality of the field that is being aggregated. Cardinality refers to the uniqueness of values stored in a particular field. High cardinality means that a field contains a large percentage of unique values. Low cardinality means that a field contains a lot of repeated values. For example, a field storing country names will be relatively low cardinality since there are less than two hundred countries in the world. Alternatively, a field storing IBAN numbers or email addresses is high cardinality since there may be millions of unique values stored.\nWhen discussing high cardinality in this blog post, we are referring to fields with hundreds of thousands or millions of unique values.\nElasticsearch refresh intervalIn order to understand the remainder of this blog, we must have a general understanding of the refresh interval.\nAs documents are inserted into Elasticsearch, they are written into a buffer and then periodically flushed from that buffer into segments. This flush operation is known as a refresh, and newly inserted documents are only searchable after a refresh. By default refreshes occur every second, however the refresh interval is configurable.\nThe refresh interval is relevant to performance because in the background, Elasticsearch merges small segments into larger segments, and those larger segments are merged into even larger segments, and so on. Therefore, by enabling frequent refreshes, Elasticsearch needs to do more background work merging small segments than it would need to do with less frequent refreshes which would create larger segments.\nWhile frequent refreshes are necessary if near real-time search functionality is required for newly inserted data, such frequent refreshes may not be necessary in other use cases. If an application can wait longer for recent data to appear in its search results, then the refresh interval may be increased in order to improve the efficiency of data ingestion, which in turn should free up resources to help overall cluster performance.\nGlobal OrdinalsTerms aggregations rely on global ordinals to improve efficiency. Global ordinals is a data-structure that maintains an incremental numbering for each unique term for a given field. Global ordinals are computed on each shard, and by default subsequent terms aggregations will rely purely on those global ordinals to efficiently perform the aggregation at the shard level.  Global ordinals are then converted  to the real term for the final reduce phase, which combines results from different shards. If a shard is modified, then new global ordinals will need to be calculated for that shard.\nThe performance of terms aggregations on high-cardinality fields may be slow and unpredictable in-part because (1) the time taken to build global ordinals will increase as the cardinality of the field increases, and (2) by default, global ordinals are lazily built on the first aggregation that occurs since the previous refresh. Furthermore, a combination of frequent document insertions, frequent refreshes, and frequently executed terms aggregations would cause global ordinals to be frequently recomputed. \nAdditional details on global ordinal performance can be found in this GitHub issue.\nHow to view the impact of global ordinals on terms aggregationsElasticsearch log files are very helpful for detecting performance issues. Set log levels to appropriate values, keeping in mind that excessive logging may increase disk IO and could negatively impact performance. Monitor the log file called elasticsearch_index_search_slowlog.log. If slow terms aggregations appear in the slowlog file, then their poor performance may be due to the building of global ordinals, which can be checked as follows:\n\nContinue to insert data into Elasticsearch to ensure that global ordinals will be rebuilt when we execute a terms aggregation. Copy the slow terms aggregation from the slowlog file and manually execute and profile it, to get a feeling for where execution time is spent.\nContinue to insert data into Elasticsearch. Copy the terms aggregation from the slowlog file and manually execute it. While the terms aggregation is executing, simultaneously execute the hot_threads api. If hot threads generally returns results that include references to GlobalOrdinalsBuilder, then the code may be spending significant time building global ordinals.\nTemporarily enable logging of global ordinals information by executing the following command:&#96;&#96;&#96;PUT _cluster&#x2F;settings {  “transient”:  {  “logger.org.elasticsearch.index.fielddata”:  “TRACE”  }  }  123This will write information about time spent building global ordinals to the `elasticsearch.log` file, such as the following:```global-ordinals \\[&lt;field_name&gt;\\]\\[1014089\\] took \\[592.3ms\\]  \n\nBe sure to set the logging back to the default value after completing the above, as performance may be impacted while logging is set to TRACE.\nThe size of the global ordinals data structure for a given field can be seen by viewing memory_size_in_bytes when executing the indices stats command as follows:&#96;&#96;&#96;GET &#x2F;_stats&#x2F;fielddata?fielddata_fields&#x3D;  1234567891011121314151617181920212223242526272829    The above commands should give an idea if building global ordinals is consuming significant resources. The remainder of this blog focuses on steps to mitigate the impact of building global ordinals on terms aggregations.Use time-based indices----------------------Global ordinals only need to be re-created on a shard if that shard has been modified since the last computation of its global ordinals. If a shard is unmodified since the last computation of its global ordinals, then previously calculated global ordinals will continue to be used. For time-series data, implementing time-based indices is a good way to ensure that the majority of indices/shards remain unmodified, which will reduce the size of the global ordinals that need to be recomputed after a refresh operation.For example, if two years of data is stored in monthly indices instead of in one large index, then each monthly index is 1/24th the size that one large index would be. Since we are considering time-series data, we know that only the most recent monthly index will have new documents inserted. This means that only one of the 24 indices is actively written into. Since global ordinals are only rebuilt on shards that have been modified, the shards in 23 of the 24 monthly indices will continue to use previously computed global ordinals. This would reduce the work required to build global ordinals by a factor of up to 24 times when compared to storing two years worth of data in one large index.  Enable eager global ordinals----------------------------The performance of high-cardinality terms aggregations can be improved by [eager building of global ordinals](https://www.elastic.co/guide/en/elasticsearch/reference/6.5/eager-global-ordinals.html). Enabling eager building of global ordinals will create the global ordinals data structure when segments are refreshed, as opposed to the first query after each refresh. However, the trade-off is that eager building of global ordinals will potentially negatively impact ingest performance because new global ordinals will be computed on _every refresh_, even if they might not be used. To minimize the additional workload caused by frequently building global ordinals due to frequent refreshes, the refresh interval should be increased.Do not build global ordinals----------------------------Another approach to improving the performance of high-cardinality terms aggregations is to avoid the building of global ordinals entirely, and instead execute terms aggregations directly on the raw terms. This can be beneficial because computing global ordinals on a high-cardinality field may be slow, and not building global ordinals will eliminate this delay. This comes at the expense of making each subsequent terms aggregation less efficient, as they cannot leverage global ordinals. Additionally, be aware that if global ordinals are not built for a terms aggregation, the terms aggregation will use more memory per request because Elasticsearch will need to keep a map of all of the unique terms that appear in the result set. This could potentially trigger an internal circuit breaker to prevent memory overuse if a large number of unique terms are to be aggregated, which could result in a failed aggregation.  Therefore, this approach should only be applied if a terms aggregation is expected to execute on a relatively small number of documents. For example, this would likely be the case if the aggregation is defined along with a selective [bool query](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-bool-query.html) executing in [filter context](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-filter-context.html). Run an experiment with real data to determine if this approach improves the performance of a specific use case.In Elasticsearch 6.7 or newer this can be accomplished by specifying [&quot;execution_hint&quot;: &quot;map&quot;](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-terms-aggregation.html#search-aggregations-bucket-terms-aggregation-execution-hint) which tells Elasticsearch to aggregate field values directly without leveraging global ordinals, or in older versions this can be accomplished by using the alternate technique of executing a script inside a terms aggregation, which is described below.By default a terms aggregation will return buckets for the top ten terms ordered by the number of documents in each bucket. Below is an example terms aggregation for top IBAN identifiers. By default this terms aggregation will rebuild global ordinals on the first execution after the last refresh.\n“aggregations”:  {  “top-ibans”:  {  “terms”:  {  “field”:  “IBAN_keyword”  }  }  }123This could be re-written to avoid using global ordinals in Elasticsearch version 6.7 or newer as follows.\n“aggregations”:  {  “top-ibans”:  {  “terms”:  {  “field”:  “IBAN_keyword”,  “execution_hint”:  “map”  }  }  }123In versions of Elasticsearch prior to 6.7, there was [a bug](https://github.com/elastic/elasticsearch/issues/37705) that caused global ordinals to be computed even if `&quot;execution_hint&quot;: &quot;map&quot;` was specified. This was fixed in [this github pull request](https://github.com/elastic/elasticsearch/pull/37833). For older versions, the following technique can be applied to prevent global ordinals from being built for the above terms aggregation. \n“aggregations”  :  {  “top-ibans”  :  {  “terms”  :  {  “script”:  {  “source”  :  “doc[‘IBAN_keyword’].value”,  “lang”  :  “painless”  }  }  }  }\n\n\nConclusion\n----------\n\nIn this blog post I first presented a brief overview of documentation that should be followed to ensure the best overall performance of an Elasticsearch cluster. This was followed by a deep dive into terms aggregations including an overview of how they work and several options to improve their performance.\n\nIf you have any questions about terms aggregations, Elasticsearch performance, or any other Elasticsearch-related topics, have a look at our [Discuss forums](https://discuss.elastic.co/) for valuable discussion, insights, and information. Also, don&#39;t forget to try out our [Elasticsearch Service](https://www.elastic.co/cloud/elasticsearch-service), the only hosted Elasticsearch and Kibana offering powered by the creators of Elasticsearch.\n\n","slug":"MIDDLEWARE/Improving the performance of high-cardinality terms aggregations in Elasticsearch","date":"2024-06-17T19:44:51.000Z","categories_index":"the,MIDDLEWARE","tags_index":"ordinals,global,terms","author_index":"dandeliono"},{"id":"ef5ca056fa60b2d5cee6602c81aa277f","title":"如何优雅的升级 Nginx（热部署）版本","content":"如何优雅的升级 Nginx（热部署）版本1. 升级1.1. 下载、编译新版本的 Nginx12345➜  ~ wget https://nginx.org/download/nginx-1.15.6.tar.gz➜  ~ tar zxvf nginx-1.15.6.tar.gz➜  ~ cd nginx-1.15.6➜  ~ ./configure --prefix=/usr/local/nginx➜  ~ make\n\n备份原 Nginx 二进制文件，并用新版本替换\n12➜  ~ cp /usr/local/nginx/sbin/nginx /usr/local/nginx/sbin/nginx.old➜  ~ \\cp -rf objs/nginx /usr/local/nginx/sbin/nginx\n\n1.2. 切换 Nginx 的新老进程12345➜  ~ cd /usr/local/nginx/sbin/➜  ~ ps -ef | grep nginxroot      6680     1  0 12:17 ?        00:00:00 nginx: master process ./nginxnobody    6681  6680  0 12:17 ?        00:00:00 nginx: worker processroot     31579  3601  0 12:28 pts/0    00:00:00 grep --color=auto nginx\n\n向正在运行的老版本的 master 进程发送热部署信号\n12345678910➜  ~ kill -USR2 6680➜  ~ ps -ef | grep nginxroot      6680     1  0 12:17 ?        00:00:00 nginx: master process ./nginxnobody    6681  6680  0 12:17 ?        00:00:00 nginx: worker processroot     31581  6680  0 12:29 ?        00:00:00 nginx: master process ./nginxnobody   31582 31581  0 12:29 ?        00:00:00 nginx: worker processroot     31584  3601  0 12:29 pts/0    00:00:00 grep --color=auto nginx\n\n向老版本的 master 进程发送信号，让其优雅关闭所有 worker 进程\n123456➜  ~ kill -WINCH 6680➜  ~ ps -ef | grep nginxroot      6680     1  0 12:17 ?        00:00:00 nginx: master process ./nginxroot     31581  6680  0 12:29 ?        00:00:00 nginx: master process ./nginxnobody   31582 31581  0 12:29 ?        00:00:00 nginx: worker processroot     31587  3601  0 12:31 pts/0    00:00:00 grep --color=auto nginx\n\n\n\n\n\n\n\n\n\n\n⚠️ 注意：老版本的 master 进程不会自动退出，以方便执行版本回退操作。当新版本稳定运行后，再发送信号退出老版本的 master 进程。\n2. 版本回退\n向老版本的 Nginx 进程发送信号，重启其 worker 进程\n1234567➜  ~ kill -HUP 6680➜  ~ ps -ef | grep nginxroot      6680     1  0 12:17 ?        00:00:00 nginx: master process ./nginxroot     31581  6680  0 12:29 ?        00:00:00 nginx: master process ./nginxnobody   31582 31581  0 12:29 ?        00:00:00 nginx: worker processnobody   31588  6680  0 12:35 ?        00:00:00 nginx: worker processroot     31590  3601  0 12:35 pts/0    00:00:00 grep --color=auto nginx\n\n\n等老版本的 Nginx worker 进程启动后，向新版本的 Nginx master 进程发送信号，让其退出打开的所有 worker 进程\n123456➜  ~ kill -WINCH 31581➜  ~ ps -ef | grep nginxroot      6680     1  0 12:17 ?        00:00:00 nginx: master process ./nginxroot     31581  6680  0 12:29 ?        00:00:00 nginx: master process ./nginxnobody   31588  6680  0 12:35 ?        00:00:00 nginx: worker processroot     31590  3601  0 12:35 pts/0    00:00:00 grep --color=auto nginx\n\n最后退出新版本的 master 进程\n12345➜  ~ kill -QUIT 31581➜  ~ ps -ef | grep nginxroot      6680     1  0 12:17 ?        00:00:00 nginx: master process ./nginxnobody   31588  6680  0 12:35 ?        00:00:00 nginx: worker processroot     31592  3601  0 12:36 pts/0    00:00:00 grep --color=auto nginx\n\n","slug":"MIDDLEWARE/如何优雅的升级 Nginx（热部署）版本","date":"2024-06-17T19:42:10.000Z","categories_index":"Nginx,MIDDLEWARE","tags_index":"master,进程,worker","author_index":"dandeliono"},{"id":"6bea119b93beb5f935a855810bf39c07","title":"平滑重启原理-USR1 USR2信号量使用","content":"平滑重启原理-USR1 USR2信号量使用在POSIX兼容的平台上，SIGUSR1和SIGUSR2是发送给一个进程的信号，它表示了用户定义的情况。nginx常用信号量如下:\n| \nTERM, INT\n | \nQuick shutdown\n || \nQUIT\n | \nGraceful shutdown  优雅的关闭进程,即等请求结束后再关闭\n || \nHUP\n | \nConfiguration reload ,Start the new worker processes with\n a new configuration Gracefully shutdown the old worker processes\n改变配置文件,平滑的重读配置文件\n || \nUSR1\n | \nReopen the log files 重读日志,在日志按月&#x2F;日分割时有用\n || \nUSR2\n | \nUpgrade Executable on the fly 平滑的升级\n || \nWINCH\n | \nGracefully shutdown the worker processes 优雅关闭旧的进程(配合USR2来进行升级)\n |\n二、重启流程重启意味着新旧接替，在交接任务的过程中势必会存在新旧server并存的情形，因此，重启的流程大致为：\n\n启动新的server\n\n新旧server并存，两者共同处理请求，提供服务\n\n旧的server处理完所有的请求之后优雅退出\n\n\n这里，最主要的问题在于如何保证新旧server可以并存，如果重启前后的server端口一致，如何保证两者可以监听同一端口。\n三、nginx实现1.为了验证nginx平滑重启，笔者首先尝试nginx启动的情形下再次开启一个新的server实例，结果如图：\n\n很明显，重新开启server实例是行不通的，原因在于新旧server使用了同一个端口80，在未开始socket reuseport选项复用端口时，bind系统调用会出错。nginx默认bind重试5次，失败后直接退出。而nginx需要监听IPV4地址0.0.0.0和IPV6地址[::]，故图中打印出10条emerg日志。\n2.接下来就开始尝试平滑重启命令了，一共两条命令：\n1kill -USR2 `cat /var/run/nginx.pid`kill -QUIT `cat /var/run/nginx.pid.oldbin`\n\n\n第一条命令是发送信号USR2给旧的master进程，进程的pid存放在&#x2F;var&#x2F;run&#x2F;nginx.pid文件中，其中nginx.pid文件路径由nginx.conf配置。\n\n第二条命令是发送信号QUIT给旧的master进程，进程的pid存放在&#x2F;var&#x2F;run&#x2F;nginx.pid.oldbin文件中，随后旧的master进程退出。\n\n\n那么问题来了，为什么旧的master进程的pid存在于两个pid文件之中？事实上，在发送信号USR2给旧的master进程之后，旧的master进程将pid重命名，原先的nginx.pid文件rename成nginx.pid.oldbin。这样新的master进行就可以使用nginx.pid这个文件名了。\n先执行第一条命令，结果如图： \n\n不错，新旧master和worker进程并存了。 再来第二条命令，结果如图: \n\n如你所见，旧的master进程8527和其worker进程全部退出，只剩下新的master进程12740。\n不由得产生困惑，为什么手动开启一个新的实例行不通，使用信号重启就可以达到。先看下nginx log文件： \n\n除了之前的错误日志，还多了一条notice，意思就是继承了sockets，fd值为6，7。 随着日志翻看nginx源码，定位到nginx.c&#x2F;ngx_exec_new_binary函数之中，\n1ngx_exec_new_binary(ngx_cycle_t *cycle, char *const *argv)    ctx.name = &quot;new binary process&quot;;    env = ngx_set_environment(cycle, &amp;n);    var = ngx_alloc(sizeof(NGINX_VAR)                    + cycle-&gt;listening.nelts * (NGX_INT32_LEN + 1) + 2,    p = ngx_cpymem(var, NGINX_VAR &quot;=&quot;, sizeof(NGINX_VAR));    ls = cycle-&gt;listening.elts;for (i = 0; i &lt; cycle-&gt;listening.nelts; i++) &#123;        p = ngx_sprintf(p, &quot;%ud;&quot;, ls[i].fd);    ctx.envp = (char *const *) env;    ccf = (ngx_core_conf_t *) ngx_get_conf(cycle-&gt;conf_ctx, ngx_core_module);if (ngx_rename_file(ccf-&gt;pid.data, ccf-&gt;oldpid.data) == NGX_FILE_ERROR) &#123;    pid = ngx_execute(cycle, &amp;ctx);if (pid == NGX_INVALID_PID) &#123;if (ngx_rename_file(ccf-&gt;oldpid.data, ccf-&gt;pid.data)\n\n12函数的流程为\n\n\n将旧的master进程监听的所有fd，拷贝至新master进程的env环境变量NGINX_VAR。\n\nrename重命名pid文件\n\nngx_execute函数fork子进程，execve执行命令行启动新的server。\n\n\n在server启动流程之中，涉及到环境变量NGINX_VAR的解析，ngx_connection.c&#x2F;ngx_add_inherited_sockets具体代码为:\n1ngx_add_inherited_sockets(ngx_cycle_t *cycle)    inherited = (u_char *) getenv(NGINX_VAR);if (ngx_array_init(&amp;cycle-&gt;listening, cycle-&gt;pool, 10,for (p = inherited, v = p; *p; p++) &#123;if (*p == &#x27;:&#x27; || *p == &#x27;;&#x27;) &#123;            ls = ngx_array_push(&amp;cycle-&gt;listening);            ngx_memzero(ls, sizeof(ngx_listening_t));            ls-&gt;fd = (ngx_socket_t) s;return ngx_set_inherited_sockets(cycle);\n\n12函数流程为：\n\n\n解析环境变量NGINX_VAR的值，获取fd存入数组\n\nfd对应的socket设为ngx_inherited，保存这些socket的信息。\n\n\n也就是说，新的server压根就没重新bind端口listen，这些fd状态和值都是新的master进程fork时带过来的,新的master进程监听处理继承来的文件描述符即可，这里比较关键的一点在于listen socket文件描述符通过ENV传递。\n四.切割日志在linux中，一个文件对应一个节点，才是在磁盘上的真实位置。nginx在读取&#x2F;写入日志文件时，即便把日志文件改名了，还是指向那个节点(磁盘空间)。所以，要新建一个文件，再告诉nginx，读取&#x2F;写入新的日志文件。 \n\n","slug":"LINUX/平滑重启原理-USR1 USR2信号量使用","date":"2024-06-17T19:32:18.000Z","categories_index":"nginx,LINUX","tags_index":"master,server,pid","author_index":"dandeliono"},{"id":"5df6cb5002e831cbd9dddd37fc2dda58","title":"Shell脚本调试技术","content":"Shell脚本调试技术shell编程在unix&#x2F;linux世界中使用得非常广泛，熟练掌握shell编程也是成为一名优秀的unix&#x2F;linux开发者和系统管理员的必经之路。脚本调试的主要工作就是发现引发脚本错误的原因以及在脚本源代码中定位发生错误的行，常用的手段包括分析输出的错误信息，通过在脚本中加入调试语句，输出调试信息来辅助诊断错误，利用调试工具等。但与其它高级语言相比，shell解释器缺乏相应的调试机制和调试工具的支持，其输出的错误信息又往往很不明确，初学者在调试脚本时，除了知道用echo语句输出一些信息外，别无它法，而仅仅依赖于大量的加入echo语句来诊断错误，确实令人不胜其繁，故常见初学者抱怨shell脚本太难调试了。本文将系统地介绍一些重要的shell脚本调试技术，希望能对shell的初学者有所裨益。\n本篇博客整理自 Shell脚本调试技术\n\n\n\n\n\n\n\n\n\n\nShell脚本调试技术\n\n本文的目标读者是unix&#x2F;linux环境下的开发人员，测试人员和系统管理员，要求读者具有基本的shell编程知识。本文所使用范例在Bash3.1+Redhat Enterprise Server 4.0下测试通过，但所述调试技巧应也同样适用于其它shell。\n通过在程序中加入调试语句把一些关键地方或出错的地方的相关信息显示出来是最常见的调试手段。Shell程序员通常使用echo(ksh程序员常使用print)语句输出信息，但仅仅依赖echo语句的输出跟踪信息很麻烦，调试阶段在脚本中加入的大量的echo语句在产品交付时还得再费力一一删除。针对这个问题，本节主要介绍一些如何方便有效的输出调试信息的方法。\n使用trap命令trap命令用于捕获指定的信号并执行预定义的命令。其基本的语法是:\n1\n\n其中signal是要捕获的信号，command是捕获到指定的信号之后，所要执行的命令。可以用kill –l命令看到系统中全部可用的信号名，捕获信号后所执行的命令可以是任何一条或多条合法的shell语句，也可以是一个函数名。\nshell脚本在执行时，会产生三个所谓的“伪信号”，(之所以称之为“伪信号”是因为这三个信号是由shell产生的，而其它的信号是由操作系统产生的)，通过使用trap命令捕获这三个“伪信号”并输出相关信息对调试非常有帮助。\n表 1. shell伪信号\n\n\n\n信号名\n何时产生\n\n\n\nEXIT\n从一个函数中退出或整个脚本执行完毕\n\n\nERR\n当一条命令返回非零状态时(代表命令执行不成功)\n\n\nDEBUG\n脚本中每一条命令执行之前\n\n\n通过捕获EXIT信号,我们可以在shell脚本中止执行或从函数中退出时，输出某些想要跟踪的变量的值，并由此来判断脚本的执行状态以及出错原因,其使用方法是：\n1\n\n通过捕获ERR信号,我们可以方便的追踪执行不成功的命令或函数，并输出相关的调试信息，以下是一个捕获ERR信号的示例程序，其中的$LINENO是一个shell的内置变量，代表shell脚本的当前行号。\n1\n\n其输出结果如下：\n1\n\n在调试过程中，为了跟踪某些变量的值，我们常常需要在shell脚本的许多地方插入相同的echo语句来打印相关变量的值，这种做法显得烦琐而笨拙。而通过捕获DEBUG信号，我们只需要一条trap语句就可以完成对相关变量的全程跟踪。\n以下是一个通过捕获DEBUG信号来跟踪变量的示例程序:\n1\n\n其输出结果如下：\n1\n\n从运行结果中可以清晰的看到每执行一条命令之后，相关变量的值的变化。同时，从运行结果中打印出来的行号来分析，可以看到整个脚本的执行轨迹，能够判断出哪些条件分支执行了，哪些条件分支没有执行。\n使用tee命令在shell脚本中管道以及输入输出重定向使用得非常多，在管道的作用下，一些命令的执行结果直接成为了下一条命令的输入。如果我们发现由管道连接起来的一批命令的执行结果并非如预期的那样，就需要逐步检查各条命令的执行结果来判断问题出在哪儿，但因为使用了管道，这些中间结果并不会显示在屏幕上，给调试带来了困难，此时我们就可以借助于tee命令了。\ntee命令会从标准输入读取数据，将其内容输出到标准输出设备,同时又可将内容保存成文件。例如有如下的脚本片段，其作用是获取本机的ip地址：\n1\n\n#注意&#x3D;号后面的整句是用反引号(数字1键的左边那个键)括起来的。\n运行这个脚本，实际输出的却不是本机的ip地址，而是广播地址,这时我们可以借助tee命令，输出某些中间结果，将上述脚本片段修改为：\n1\n\n之后，将这段脚本再执行一遍，然后查看temp.txt文件的内容：\n1\n\n我们可以发现中间结果的第二列(列之间以:号分隔)才包含了IP地址，而在上面的脚本中使用cut命令截取了第三列，故我们只需将脚本中的cut -d : -f3改为cut -d : -f2即可得到正确的结果。\n具体到上述的script例子，我们也许并不需要tee命令的帮助，比如我们可以分段执行由管道连接起来的各条命令并查看各命令的输出结果来诊断错误，但在一些复杂的shell脚本中，这些由管道连接起来的命令可能又依赖于脚本中定义的一些其它变量，这时我们想要在提示符下来分段运行各条命令就会非常麻烦了，简单地在管道之间插入一条tee命令来查看中间结果会更方便一些。\n使用”调试钩子”在C语言程序中，我们经常使用DEBUG宏来控制是否要输出调试信息，在shell脚本中我们同样可以使用这样的机制，如下列代码所示：\n1\n\n这样的代码块通常称之为“调试钩子”或“调试块”。在调试钩子内部可以输出任何您想输出的调试信息，使用调试钩子的好处是它是可以通过DEBUG变量来控制的，在脚本的开发调试阶段，可以先执行export DEBUG&#x3D;true命令打开调试钩子，使其输出调试信息，而在把脚本交付使用时，也无需再费事把脚本中的调试语句一一删除。\n如果在每一处需要输出调试信息的地方均使用if语句来判断DEBUG变量的值，还是显得比较繁琐，通过定义一个DEBUG函数可以使植入调试钩子的过程更简洁方便，如下面代码所示:\n1\n\n在上面所示的DEBUG函数中，会执行任何传给它的命令，并且这个执行过程是可以通过DEBUG变量的值来控制的，我们可以把所有跟调试有关的命令都作为DEBUG函数的参数来调用，非常的方便。回页首\n使用shell的执行选项上一节所述的调试手段是通过修改shell脚本的源代码，令其输出相关的调试信息来定位错误的，那有没有不修改源代码来调试shell脚本的方法呢？答案就是使用shell的执行选项，本节将介绍一些常用选项的用法：\n\n-n 只读取shell脚本，但不实际执行\n-x 进入跟踪方式，显示所执行的每一条命令\n-c “string” 从strings中读取命令\n\n“-n”可用于测试shell脚本是否存在语法错误，但不会实际执行命令。在shell脚本编写完成之后，实际执行之前，首先使用“-n”选项来测试脚本是否存在语法错误是一个很好的习惯。因为某些shell脚本在执行时会对系统环境产生影响，比如生成或移动文件等，如果在实际执行才发现语法错误，您不得不手工做一些系统环境的恢复工作才能继续测试这个脚本。\n“-c”选项使shell解释器从一个字符串中而不是从一个文件中读取并执行shell命令。当需要临时测试一小段脚本的执行结果时，可以使用这个选项，如下所示：  \n1\n\n“-x”选项可用来跟踪脚本的执行，是调试shell脚本的强有力工具。“-x”选项使shell在执行脚本的过程中把它实际执行的每一个命令行显示出来，并且在行首显示一个”+”号。 “+”号后面显示的是经过了变量替换之后的命令行的内容，有助于分析实际执行的是什么命令。 “-x”选项使用起来简单方便，可以轻松对付大多数的shell调试任务,应把其当作首选的调试手段。\n如果把本文前面所述的trap ‘command’ DEBUG机制与“-x”选项结合起来，我们 就可以既输出实际执行的每一条命令，又逐行跟踪相关变量的值，对调试相当有帮助。\n仍以前面所述的exp2.sh为例，现在加上“-x”选项来执行它：\n1\n\n在上面的结果中，前面有“+”号的行是shell脚本实际执行的命令，前面有“++”号的行是执行trap机制中指定的命令，其它的行则是输出信息。\nshell的执行选项除了可以在启动shell时指定外，亦可在脚本中用set命令来指定。 “set -参数”表示启用某选项，”set +参数”表示关闭某选项。有时候我们并不需要在启动时用”-x”选项来跟踪所有的命令行，这时我们可以在脚本中使用set命令，如以下脚本片段所示：\n1\n\nset命令同样可以使用上一节中介绍的调试钩子—DEBUG函数来调用，这样可以避免脚本交付使用时删除这些调试语句的麻烦，如以下脚本片段所示：\n1\n\n对”-x”选项的增强“-x”执行选项是目前最常用的跟踪和调试shell脚本的手段，但其输出的调试信息仅限于进行变量替换之后的每一条实际执行的命令以及行首的一个”+”号提示符，居然连行号这样的重要信息都没有，对于复杂的shell脚本的调试来说，还是非常的不方便。幸运的是，我们可以巧妙地利用shell内置的一些环境变量来增强”-x”选项的输出信息，下面先介绍几个shell内置的环境变量：\n$LINENO代表shell脚本的当前行号，类似于C语言中的内置宏LINE\n$FUNCNAME函数的名字，类似于C语言中的内置宏func,但宏func只能代表当前所在的函数名，而$FUNCNAME的功能更强大，它是一个数组变量，其中包含了整个调用链上所有的函数的名字，故变量${FUNCNAME[0]}代表shell脚本当前正在执行的函数的名字，而变量${FUNCNAME[1]}则代表调用函数${FUNCNAME[0]}的函数的名字，余者可以依此类推。\n$PS4主提示符变量$PS1和第二级提示符变量$PS2比较常见，但很少有人注意到第四级提示符变量$PS4的作用。我们知道使用“-x”执行选项将会显示shell脚本中每一条实际执行过的命令，而$PS4的值将被显示在“-x”选项输出的每一条命令的前面。在Bash Shell中，缺省的$PS4的值是”+”号。(现在知道为什么使用”-x”选项时，输出的命令前面有一个”+”号了吧？)。\n利用$PS4这一特性，通过使用一些内置变量来重定义$PS4的值，我们就可以增强”-x”选项的输出信息。例如先执行export PS4&#x3D;’+{$LINENO:${FUNCNAME[0]}} ‘, 然后再使用“-x”选项来执行脚本，就能在每一条实际执行的命令前面显示其行号以及所属的函数名。\n以下是一个存在bug的shell脚本的示例，本文将用此脚本来示范如何用“-n”以及增强的“-x”执行选项来调试shell脚本。这个脚本中定义了一个函数isRoot(),用于判断当前用户是不是root用户，如果不是，则中止脚本的执行\n1\n\n首先执行sh –n exp4.sh来进行语法检查，输出如下：\n1\n\n发现了一个语法错误，通过仔细检查第6行前后的命令，我们发现是第4行的if语句缺少then关键字引起的(写惯了C程序的人很容易犯这个错误)。我们可以把第4行修改为if [ “$UID” -ne 0 ]; then来修正这个错误。再次运行sh –n exp4.sh来进行语法检查，没有再报告错误。接下来就可以实际执行这个脚本了，执行结果如下：\n1\n\n尽管脚本没有语法错误了，在执行时却又报告了错误。错误信息还非常奇怪“[1: command not found”。现在我们可以试试定制$PS4的值，并使用“-x”选项来跟踪：\n1\n\n从输出结果中，我们可以看到脚本实际被执行的语句，该语句的行号以及所属的函数名也被打印出来，从中可以清楚的分析出脚本的执行轨迹以及所调用的函数的内部执行情况。由于执行时是第11行报错，这是一个if语句，我们对比分析一下同为if语句的第4行的跟踪结果：\n1\n\n可知由于第11行的[号后面缺少了一个空格，导致[号与紧挨它的变量$?的值1被shell解释器看作了一个整体，并试着把这个整体视为一个命令来执行，故有“[1: command not found”这样的错误提示。只需在[号后面插入一个空格就一切正常了。\nshell中还有其它一些对调试有帮助的内置变量，比如在Bash Shell中还有BASH_SOURCE, BASH_SUBSHELL等一批对调试有帮助的内置变量，您可以通过man sh或man bash来查看，然后根据您的调试目的,使用这些内置变量来定制$PS4，从而达到增强“-x”选项的输出信息的目的。\n现在让我们来总结一下调试shell脚本的过程：\n首先使用“-n”选项检查语法错误，然后使用“-x”选项跟踪脚本的执行，使用“-x”选项之前，别忘了先定制PS4变量的值来增强“-x”选项的输出信息，至少应该令其输出行号信息(先执行export PS4&#x3D;’+[$LINENO]’，更一劳永逸的办法是将这条语句加到您用户主目录的.bash_profile文件中去)，这将使你的调试之旅更轻松。也可以利用trap,调试钩子等手段输出关键调试信息，快速缩小排查错误的范围，并在脚本中使用“set -x”及“set +x”对某些代码块进行重点跟踪。这样多种手段齐下，相信您已经可以比较轻松地抓出您的shell脚本中的臭虫了。如果您的脚本足够复杂，还需要更强的调试能力，可以使用shell调试器bashdb，这是一个类似于GDB的调试工具，可以完成对shell脚本的断点设置，单步执行，变量观察等许多功能，使用bashdb对阅读和理解复杂的shell脚本也会大有裨益。关于bashdb的安装和使用，不属于本文范围，您可参阅http://bashdb.sourceforge.net/上的文档并下载试用。\n","slug":"LINUX/Shell脚本调试技术","date":"2024-06-12T15:15:07.000Z","categories_index":"shell,LINUX","tags_index":"使用,DEBUG,trap","author_index":"dandeliono"},{"id":"28da5e689dcfe147ac9028067fa50b4a","title":"Longhorn云原生存储","content":"Longhorn云原生存储官方github：https://github.com/longhorn/longhorn\n官方网站：https://longhorn.io\nLonghorn是一个轻量级、可靠且功能强大的分布式块存储系统，适用于 Kubernetes。使用容器和微服务实现分布式块存储。Longhorn 为每个块储存设备卷创建一个专用的存储控制器，并在存储在多个节点上的多个副本之间同步复制该卷。存储控制器和副本本身是使用 Kubernetes 编排的。Longhorn 是免费的开源软件。它最初由Rancher Labs开发，现在作为云原生计算基金会的孵化项目进行开发。\nLonghorn 支持以下架构：\n\nAMD64\nARM64（实验性）\n\n使用Longhorn，您可以：\n\n使用 Longhorn 卷作为 Kubernetes 集群中分布式有状态应用程序的持久存储\n将您的块存储分区为 Longhorn 卷，以便您可以在有或没有云提供商的情况下使用 Kubernetes 卷。\n跨多个节点和数据中心复制块存储以提高可用性\n将备份数据存储在外部存储（如 NFS 或 AWS S3）中\n创建跨集群灾难恢复卷，以便从第二个 Kubernetes 集群中的备份中快速恢复主 Kubernetes 集群中的数据\n计划卷的定期快照，并计划定期备份到 NFS 或与 S3 兼容的辅助存储\n从备份还原卷\n在不中断持久卷的情况下升级 Longhorn\n\nLonghorn带有一个独立的UI，可以使用Helm，kubectl或Rancher应用程序目录进行安装。\n使用微服务简化分布式块存储\n由于现代云环境需要数以万计到数百万个分布式块存储卷，因此一些存储控制器已成为高度复杂的分布式系统。相比之下，Longhorn 可以通过将大型块存储控制器分区为多个较小的存储控制器来简化存储系统，只要这些卷仍然可以从公共磁盘池构建。通过为每个卷使用一个存储控制器，Longhorn 将每个卷转换为微服务。控制器被称为Longhorn Engine。\nLonghorn Manager 组件编排 Longhorn 引擎，因此它们可以连贯地协同工作。\n在 Kubernetes 中使用持久性存储，而无需依赖云提供商\nPod 可以直接引用存储，但不建议这样做，因为它不允许 Pod 或容器可移植。相反，工作负载的存储要求应该在 Kubernetes 持久卷 （PV） 和持久卷声明 （PVC） 中定义。使用 Longhorn，您可以指定卷的大小、IOPS 要求以及跨为卷提供存储资源的主机所需的同步副本数。然后，您的 Kubernetes 资源可以为每个 Longhorn 卷使用 PVC 和相应的 PV，或者使用 Longhorn 存储类自动为工作负载创建 PV。\n副本在基础磁盘或网络存储上进行精简置备。\n跨多个计算或存储主机计划多个副本\n为了提高可用性，Longhorn 会创建每个卷的副本。副本包含卷的快照链，每个快照都存储与上一个快照相比的更改。卷的每个副本也在容器中运行，因此具有三个副本的卷会产生四个容器。\n每个卷的副本数可在 Longhorn 中配置，以及将调度副本的节点数。Longhorn 会监控每个复制副本的运行状况并执行修复，并在必要时重建复制副本。\n为每个卷分配多个存储前端\n常见的前端包括一个 Linux 内核设备（映射在 &#x2F;dev&#x2F;longhorn 下）和一个 iSCSI 目标。\n为定期快照和备份操作指定计划\n指定这些操作的频率（每小时、每天、每周、每月和每年）、执行这些操作的确切时间（例如，每个星期日的凌晨 3：00）以及保留的定期快照和备份集数。\n1.1 架构介绍Longhorn设计有两层：数据平面和控制平面。Longhorn Engine 是对应于数据平面的存储控制器，而 Longhorn Manager 对应于控制平面。\nLonghorn Manager Pod 作为 Kubernetes DaemonSet 在 Longhorn 集群中的每个节点上运行。它负责在 Kubernetes 集群中创建和管理卷，并处理来自 UI 或 Kubernetes 的卷插件的 API 调用。它遵循 Kubernetes 控制器模式，有时称为运算符模式。Longhorn Manager 与 Kubernetes API 服务器通信，以创建新的 Longhorn 卷 CRD。然后，Longhorn Manager 会观察 API 服务器的响应，当它看到 Kubernetes API 服务器创建了一个新的 Longhorn 卷 CRD 时，Longhorn Manager 会创建一个新卷。当要求 Longhorn Manager 创建卷时，它会在卷附加到的节点上创建一个 Longhorn Engine 实例，并在将放置副本的每个节点上创建一个副本。副本应放置在单独的主机上，以确保最大的可用性。副本的多个数据路径确保了 Longhorn 卷的高可用性。即使某个副本或引擎出现问题，该问题也不会影响所有副本或 Pod 对卷的访问。Pod 仍将正常运行。长角引擎始终与使用 Longhorn 卷的 Pod 在同一节点中运行。它跨存储在多个节点上的多个副本同步复制卷。引擎和副本是使用 Kubernetes 编排的。\n\n\n有三个实例与 Longhorn 卷。\n每个卷都有一个专用的控制器，称为Longhorn Engine，并作为Linux进程运行。\n每个 Longhorn 卷都有两个副本，每个副本都是一个 Linux 进程。\n图中的箭头表示卷、控制器实例、副本实例和磁盘之间的读&#x2F;写数据流。\n通过为每个卷创建单独的 Longhorn 引擎，如果一个控制器出现故障，其他卷的功能将不受影响。\n\n1.2 微服务设置的优势在 Longhorn 中，每个引擎只需要为一个卷提供服务，从而简化了存储控制器的设计。由于控制器软件的故障域被隔离到各个卷，因此控制器崩溃只会影响一个卷。\nLonghorn Engine简单而轻巧，因此我们可以创建多达100，000个单独的引擎。Kubernetes 调度这些单独的引擎，从一组共享磁盘中提取资源，并与 Longhorn 合作以形成一个有弹性的分布式块存储系统。\n由于每个卷都有自己的控制器，因此还可以升级每个卷的控制器和副本实例，而不会对 IO 操作造成明显中断。\nLonghorn 可以创建一个长时间运行的作业来协调所有实时卷的升级，而不会中断系统的持续运行。为确保升级不会导致不可预见的问题，Longhorn 可以选择升级一小部分卷，并在升级过程中出现问题时回滚到旧版本。\n1.3 精简置备和卷大小、Longhorn 是一个精简配置的存储系统。这意味着Longhorn卷将仅占用当前所需的空间。例如，如果分配了 20 GB 的卷，但仅使用其中的 1GB，则磁盘上的实际数据大小将为 1 GB。您可以在 UI 的卷详细信息中查看实际数据大小。\n如果从卷中删除了内容，则 Longhorn 卷本身的大小无法缩小。例如，如果创建一个 20 GB 的卷，使用了 10 GB，然后删除了 9 GB 的内容，则磁盘上的实际大小仍将是 10 GB 而不是 1 GB。发生这种情况是因为 Longhorn 在块级别而不是文件系统级别运行，因此 Longhorn 不知道用户是否删除了内容。该信息主要保存在文件系统级别。\n1.4 副本每个副本都包含 Longhorn 卷的快照链。快照类似于图像的图层，最旧的快照用作基础图层，较新的快照位于顶部。仅当数据覆盖旧快照中的数据时，数据才会包含在新快照中。快照链一起显示数据的当前状态。\n对于每个 Longhorn 卷，该卷的多个副本应在 Kubernetes 集群中运行，每个副本都在一个单独的节点上。所有副本都被视为相同，并且 Longhorn 引擎始终在与 pod 相同的节点上运行，Pod 也是卷的使用者。通过这种方式，我们确保即使 Pod 已关闭，引擎也可以移动到另一个 Pod，并且您的服务将继续不受中断。\n可以在设置中更改默认副本计数。附加卷后，可以在 UI 中更改卷的副本计数。\n如果当前正常运行的副本计数小于指定的副本计数，Longhorn 将开始重建新副本。\n如果当前正常运行的副本计数大于指定的副本计数，则 Longhorn 将不执行任何操作。在此情况下，如果复制副本发生故障或删除，Longhorn 将不会开始重建新副本，除非正常的副本计数低于指定的副本计数。\n1.副本的工作原理从卷的副本读取数据时，如果可以在实时数据中找到该数据，则使用该数据。否则，将读取最新的快照。如果在最新快照中找不到数据，则读取下一个最旧的快照，依此类推，直到读取最旧的快照。拍摄快照时，将创建一个差异磁盘。随着快照数量的增加，差异磁盘链（也称为快照链）可能会变得很长。因此，为了提高读取性能，Longhorn 维护了一个读取索引，用于记录哪个差异磁盘为每个 4K 存储块保存有效数据。\n在下图中，该卷有八个块。读取索引有八个条目，并且在执行读取操作时会懒惰地填满。\n写入操作会重置读取索引，使其指向实时数据。实时数据由某些索引中的数据和其他索引中的空白空间组成。\n除了读取索引之外，我们目前不维护其他元数据来指示使用了哪些块。\n\n上图采用颜色编码，根据读取索引显示哪些块包含最新数据，下表中还列出了最新数据的来源\n\n\n\n读取索引\n最新数据来源\n\n\n\n0\n最新快照\n\n\n1\n实时数据\n\n\n2\n最早的快照\n\n\n3\n最早的快照\n\n\n4\n最早的快照\n\n\n5\n实时数据\n\n\n6\n实时数据\n\n\n7\n实时数据\n\n\n请注意，如上图中的绿色箭头所示，读取索引的索引 5 之前指向第二个最旧的快照作为最新数据的来源，然后当索引 5 处的 4K 存储块被实时数据覆盖时，它更改为指向实时数据。读取索引保存在内存中，每个 4K 块消耗一个字节。字节大小的读取索引意味着您可以为每个卷拍摄多达 254 个快照。读取索引为每个副本消耗一定量的内存中数据结构。例如，1 TB 卷占用 256 MB 的内存中读取索引。\n1.5 快照快照功能使卷能够恢复到历史记录中的某个点。辅助存储中的备份也可以从快照构建。从快照还原卷时，它反映创建快照时卷的状态。快照功能也是 Longhorn 重建过程的一部分。每当 Longhorn 检测到副本出现故障时，它都会自动拍摄（系统）快照，并开始在另一个节点上重建它。\n1.快照工作原理快照类似于图像的图层，最旧的快照用作基础图层，较新的快照位于顶部。仅当数据覆盖旧快照中的数据时，数据才会包含在新快照中。快照在创建后无法更改，除非删除快照，在这种情况下，其更改将与下一个最新的快照合并。新数据始终写入实时版本。新快照始终是从实时数据创建的。要创建新快照，实时数据将成为最新的快照。然后创建一个新的空白版本的实时数据，以取代旧的实时数据。\n重复快照\n为了减少快照占用的空间，用户可以计划定期快照或备份，其中包含要保留的多个快照，这将按计划自动创建新的快照&#x2F;备份，然后清理任何过多的快照&#x2F;备份。\n删除快照\n可以通过 UI 手动删除不需要的快照。如果触发了任何快照的删除，则系统生成的任何快照都将自动标记为删除。在 Longhorn 中，无法删除最新的快照。这是因为每当删除快照时，Longhorn 都会将其内容与下一个快照混淆，以便下一个和以后的快照保留正确的内容。但是 Longhorn 无法对最新快照执行此操作，因为没有更新的快照可与已删除的快照混为一谈。最新快照的下一个“快照”是实时卷（卷头），用户当前正在读取&#x2F;写入该卷，因此无法进行合并过程。相反，最新的快照将被标记为已删除，并且将在下次可能的情况下将其清理。要清理最新快照，可以创建新快照，然后删除以前的“最新”快照。\n存储快照\n快照存储在本地，作为卷的每个副本的一部分。它们存储在 Kubernetes 集群中节点的磁盘上。快照存储在与主机物理磁盘上的卷数据相同的位置。\n崩溃一致性\nLonghorn 是一种崩溃一致性块存储解决方案。操作系统在写入块层之前将内容保留在缓存中是正常的。这意味着，如果所有副本都已关闭，则 Longhorn 可能不包含关机前立即发生的更改，因为内容保存在操作系统级缓存中，尚未传输到 Longhorn 系统。此问题类似于台式计算机因断电而关闭时可能发生的问题。恢复电源后，您可能会在硬盘驱动器中找到一些损坏的文件。要在任何给定时刻强制将数据写入块层，可以在节点上手动运行 sync 命令，也可以卸载磁盘。在这两种情况下，操作系统都会将缓存中的内容写入块层。Longhorn 在创建快照之前会自动运行同步命令。\n1.6 备份和辅助存储备份是备份存储中的对象，备份存储是 Kubernetes 集群外部的 NFS 或 S3 兼容对象存储。备份提供了一种辅助存储形式，因此即使您的 Kubernetes 集群变得不可用，仍然可以检索您的数据。由于卷复制是同步的，并且由于网络延迟，因此很难执行跨区域复制。备份存储还用作解决此问题的介质。在 Longhorn 设置中配置备份目标后，Longhorn 可以连接到备份存储，并在 Longhorn UI 中显示现有备份的列表。如果 Longhorn 在第二个 Kubernetes 集群中运行，它还可以将灾难恢复卷同步到辅助存储中的备份，以便您的数据可以在第二个 Kubernetes 集群中更快地恢复。\n1.备份原理备份是使用一个快照作为源创建的，以便它反映创建快照时卷数据的状态。备份远程存储在群集外部。与快照相比，可以将备份视为快照链的扁平化版本。与将分层图像转换为平面图像时丢失信息的方式类似，将快照链转换为备份时数据也会丢失。在这两次转化中，所有被覆盖的数据都将丢失。由于备份不包含快照，因此它们不包含卷数据更改的历史记录。从备份还原卷后，该卷最初包含一个快照。此快照是原始链中所有快照的合并版本，它反映了创建备份时卷的实时数据。虽然快照可能为数百 GB，但备份由 2 MB 文件组成。同一原始卷的每个新备份都是增量备份，用于检测和传输快照之间的更改块。这是一项相对简单的任务，因为每个快照都是一个差异文件，并且仅存储上次快照中的更改。此设计还意味着，如果没有更改任何块并进行了备份，则 backupstore 中的备份将显示为 0 字节。但是，如果要从该备份还原，它仍将包含完整的卷数据，因为它将还原备份存储上已存在的必要块，这些块是备份所必需的。为了避免存储大量小块存储，Longhorn 使用 2 MB 块执行备份操作。这意味着，如果 2MB 边界中的任何 4K 块发生更改，Longhorn 将备份整个 2MB 块。这在可管理性和效率之间提供了适当的平衡。\n\n上图描述了如何从 Longhorn 中的快照创建备份：\n\n图的主存储端显示了 Kubernetes 集群中 Longhorn 卷的一个副本。副本由四个快照链组成。按从最新到最旧的顺序，快照为“实时数据”、“snap3”、“snap2”和“snap1”。\n图的辅助存储端显示了外部对象存储服务（如 S3）中的两个备份。\n在辅助存储中，从 snap2 备份的颜色编码显示它包括来自 snap1 的蓝色变化和来自 snap2 的绿色变化。snap2 中没有任何更改覆盖 snap1 中的数据，因此 snap1 和 snap2 的更改都包含在从 snap2 备份中。\n名为“从快照备份 3”的备份反映了创建 snap3 时卷数据的状态。颜色编码和箭头表示从 snap3 备份包含来自 snap3 的所有深红色更改，但仅包含来自 snap2 的一个绿色更改。这是因为 snap3 中的一个红色变化覆盖了 snap2 中的一个绿色变化。这说明了备份如何不包括完整的更改历史记录，因为它们将快照与之前的快照混淆。\n每个备份都维护自己的一组 2 MB 块。每个 2 MB 块仅备份一次。两个备份共享一个绿色块和一个蓝色块。\n\n从辅助存储中删除备份时，Longhorn 不会删除它使用的所有块。相反，它会定期执行垃圾回收，以清理辅助存储中未使用的块。属于同一卷的所有备份的 2 MB 块存储在一个公共目录下，因此可以在多个备份之间共享。为了节省空间，可以在备份之间未更改的 2 MB 块重用于在辅助存储中共享同一备份卷的多个备份。由于校验和用于寻址 2 MB 块，因此我们对同一卷中的 2 MB 块实现了一定程度的重复数据删除。卷级元数据存储在卷.cfg中。每个备份（例如，snap2.cfg）的元数据文件相对较小，因为它们仅包含备份中所有 2 MB 块的偏移量和校验和。每个 2 MB 块（.blk 文件）都会被压缩。\n我这里部署的版本为v1.2.4最新版本，在部署 Longhorn v1.2.4 之前，请确保您的 Kubernetes 集群至少为 v1.18，因为支持的 Kubernetes 版本已在 v1.2.4 中更新 （&gt;&#x3D; v1.18）。如果是低版本请部署选择相应的版本\n2.1 安装前准备在安装了 Longhorn 的 Kubernetes 集群中，每个节点都必须满足以下要求\n\n与 Kubernetes 兼容的容器运行时（Docker v1.13+、containerd v1.3.7+ 等）\nKubernetes v1.18+，v1.2.4要求。\nopen-iscsi已安装，并且守护程序正在所有节点上运行。\n\n12345yum install iscsi-initiator-utilssystemctl enable iscsidsystemctl start iscsid\n\n\nRWX 支持要求每个节点都安装了 NFSv4 客户端。\n\n123yum install nfs-utils\n\n\n主机文件系统支持存储数据的功能。\n\next4\nXFS\n\n\nbash必须安装curl findmnt grep awk blkid lsblk\n\n\n123yum install curl util-linux grep gawk\n\n\n必须启用装载传播。\n\n官方提供了一个测试脚本，需要在kube-master节点执行：https://raw.githubusercontent.com/longhorn/longhorn/v1.2.4/scripts/environment_check.sh\n123456789101112131415[16:04:12 root@node-1 ~][16:02:09 root@node-1 ~]daemonset.apps/longhorn-environment-check createdwaiting for pods to become ready (0/3)waiting for pods to become ready (0/3)all pods ready (3/3)  MountPropagation is enabled!cleaning up...daemonset.apps &quot;longhorn-environment-check&quot; deletedclean up complete\n\n2.2 使用Helm安装Longhorn官方提供了俩种安装方式，我们在这里使用helm进行安装\n首先在kube-master安装Helm v2.0+。\n1.部署123[16:05:21 root@node-1 ~]version.BuildInfo&#123;Version:&quot;v3.7.2&quot;, GitCommit:&quot;663a896f4a815053445eec4153677ddc24a0a361&quot;, GitTreeState:&quot;clean&quot;, GoVersion:&quot;go1.16.10&quot;&#125;\n\n添加 Longhorn Helm 存储库\n12helm repo add longhorn https://charts.longhorn.io\n\n从存储库中获取最新图表\n12helm repo update\n\n下载chart包\n12helm  pull longhorn/longhorn\n\n解压\n12tar xf longhorn-1.2.4.tgz\n\n进入目录执行安装\n123cd longhornhelm install longhorn  --namespace longhorn-system --create-namespace .\n\n验证\n1234567kubectl get pod -n longhorn-system [16:51:32 root@node-1 longhorn]NAME                 PROVISIONER          RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGElonghorn (default)   driver.longhorn.io   Delete          Immediate           true                   8m32s\n\n创建pvc并且挂载验证\n12345678910111213141516171819202122232425262728293031323334353637383940apiVersion: v1  apiVersion: apps/v1 kind: Deployment    metadata:             name: nginx         namespace: defaultspec:                 replicas: 3        selector:              matchLabels:           app: nginx     template:              metadata:      creationTimestamp: null      labels:        app: nginx    spec:      containers:      - name: test        image: 192.168.10.254:5000/bash/nginx:v1        volumeMounts:         - mountPath: &quot;/data&quot;          name: data      volumes:                      - name: data                    persistentVolumeClaim:             claimName: claim---kind: PersistentVolumeClaimapiVersion: v1metadata:  name: claimspec:  accessModes:    - ReadWriteMany  storageClassName: longhorn  resources:    requests:      storage: 1Gi\n\n创建后验证\n123456789$kubectl get pvcNAME    STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGEclaim   Bound    pvc-30ec630f-4b4d-4fad-a26f-36f7036919a7   1Gi        RWX            longhorn       2m23s$kubectl get podNAME                    READY   STATUS    RESTARTS   AGEnginx-996746966-g7fsw   1/1     Running   0          2m26snginx-996746966-lw67n   1/1     Running   0          2m26snginx-996746966-v7dst   1/1     Running   0          2m26s\n\n2.修改配置存储类配置\n1234567891011121314151617181920persistence:  defaultClass: true  defaultFsType: xfs  defaultClassReplicaCount: 1  reclaimPolicy: Delete  recurringJobSelector:    enable: false    jobList: []  backingImage:    enable: false    name: ~    dataSourceType: ~    dataSourceParameters: ~    expectedChecksum: ~\n\n全局配置\n12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061defaultSettings:  backupTarget: ~  backupTargetCredentialSecret: ~  allowRecurringJobWhileVolumeDetached: ~  createDefaultDiskLabeledNodes: ~  defaultDataPath: /var/lib/longhorn  defaultDataLocality: ~  replicaSoftAntiAffinity: ~  replicaAutoBalance: ~  storageOverProvisioningPercentage: ~  storageMinimalAvailablePercentage: ~  upgradeChecker: ~  defaultReplicaCount: ~  defaultLonghornStaticStorageClass: ~  backupstorePollInterval: ~  priorityClass: ~  autoSalvage: ~  disableSchedulingOnCordonedNode: ~  replicaZoneSoftAntiAffinity: ~  nodeDownPodDeletionPolicy: ~  allowNodeDrainWithLastHealthyReplica: ~  mkfsExt4Parameters: ~  replicaReplenishmentWaitInterval: ~  concurrentReplicaRebuildPerNodeLimit: ~  disableRevisionCounter: ~  systemManagedPodsImagePullPolicy: ~  allowVolumeCreationWithDegradedAvailability: ~  autoCleanupSystemGeneratedSnapshot: ~  concurrentAutomaticEngineUpgradePerNodeLimit: ~  backingImageCleanupWaitInterval: ~  backingImageRecoveryWaitInterval: ~  guaranteedEngineManagerCPU: ~  guaranteedReplicaManagerCPU: ~\n\ncsi配置\n12345678csi:  kubeletRootDir: /data1/kubelet/root                                                                          attacherReplicaCount: ~  provisionerReplicaCount: ~  resizerReplicaCount: ~  snapshotterReplicaCount: ~\n\n3.1 为UI控制台设置认证UI控制台自己并没有认证功能，依赖与ingress提供认证功能，这里使用ingress-nginx演示：\n123456789101112131415161718192021222324252627USER=admin; PASSWORD=123456; echo &quot;$&#123;USER&#125;:$(openssl passwd -stdin -apr1 &lt;&lt;&lt; $&#123;PASSWORD&#125;)&quot; &gt;&gt; authkubectl -n longhorn-system create secret generic basic-auth --from-file=authapiVersion: networking.k8s.io/v1beta1kind: Ingressmetadata:  name: longhorn-ingress  namespace: longhorn-system  annotations:    nginx.ingress.kubernetes.io/auth-type: basic    nginx.ingress.kubernetes.io/ssl-redirect: &#x27;false&#x27;    nginx.ingress.kubernetes.io/auth-secret: basic-auth    nginx.ingress.kubernetes.io/auth-realm: &#x27;Authentication Required &#x27;    nginx.ingress.kubernetes.io/proxy-body-size: 10000mspec:  rules:     - host: www.longhornio.org    http:       paths:      - pathType: Prefix        path: &quot;/&quot;        backend:          serviceName: longhorn-frontend          servicePort: 80\n\n3.2 k8s中创建存储类使用 kubectl 创建 Longhorn Volumes时，首先您将创建一个 Longhorn StorageClass。接下来，创建一个引用存储类的 PersistentVolumeClaim。最后，PersistentVolumeClaim 作为 Pod 中的卷挂载。部署 Pod 后，Kubernetes 主节点将检查 PersistentVolumeClaim，以确保资源请求可以得到满足。如果存储可用，Kubernetes 主节点将创建 Longhorn 卷并将其绑定到 Pod。\n官方示例文件：\n1234567891011121314kind: StorageClassapiVersion: storage.k8s.io/v1metadata:  name: longhorn-test           provisioner: driver.longhorn.ioallowVolumeExpansion: true      reclaimPolicy: Delete           volumeBindingMode: Immediate    parameters:         numberOfReplicas: &quot;2&quot;     staleReplicaTimeout: &quot;2880&quot;    fromBackup: &quot;&quot;  fsType: &quot;ext4&quot;   \n\n3.3 卷扩展使用StorageClass创建的卷，可以直接修改pvc的spec.resources.requests.storage字段进行扩容,如果卷是被挂载状态修改完成后并不会立马进行扩容操作，必须停止挂载pvc的Pod使pvc处于detached状态后才会进行扩容操作。\n在扩展期间不允许重建和添加复制副本，在重建或添加复制副本时不允许扩展。\n示例：\n1234567891011121314$ kubectl get pvcNAME    STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGEclaim   Bound    pvc-5119939d-ff12-42c4-b6f8-5d2fcc393bc2   1Gi        RWX            longhorn       11s$ kubectl edit pvc claimspec:  accessModes:  - ReadWriteMany  resources:    requests:      storage: 2Gi  $kubectl get pvcNAME    STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGEclaim   Bound    pvc-5119939d-ff12-42c4-b6f8-5d2fcc393bc2   2Gi        RWX            longhorn       63s\n\n3.4 单节点多磁盘实现Longhorn 支持在节点上使用多个磁盘来存储卷数据，默认情况下会初始k8s所有的node节点作为Longhorn的数据节点，默认的数据目录为/var/lib/longhorn。但是可以通过安装时设置createDefaultDiskLabeledNodes为true，改变默认操作表示只有node.longhorn.io/create-default-disk=true标签的node才会初始化数据目录，可以不给节点打这个标签，之后手动在UI页面配置各个节点的磁盘，配置方式如下。\n\n修改部署helm的valuse文件，重新部署\n\n123456defaultSettings:  createDefaultDiskLabeledNodes: true  #修改这个值#重新部署$ helm install longhorn -n longhorn-system .\n\n\nUI验证\n\n可以看到所有node都是禁止调度以及没有数据目录的状态\n\n\nnode添加磁盘\n\n选择一个node节点,点击Edit Node and Disks进入编辑状态。\n点击Add Disk进行数据目录的添加，数据目录需事先进行格式化硬盘并且挂载到相应目录\n可以重复添加多个数据目录，每个数据目录对应一个物理磁盘\n\n填写的内容有\n\nName: 磁盘名称\nPath：数据目录路径\nStorage Reserved: 保留空间\nScheduling: 是否允许调度\nEviction Requested: 是否驱逐副本\n\n3.5 驱逐节点上的副本与磁盘驱逐节点上的副本需要在UI页面进行操作，在Node选项卡找到对应的节点，需要先设置禁用节点相应磁盘后在设置驱逐副本。\n驱逐节点磁盘副本\n\n到选项卡，选择其中一个节点，然后在下拉菜单中进行选择。Node Edit Node and Disks\n确保已禁用磁盘以进行调度并设置为 。Scheduling Disable\n设置为并保存。Eviction Requested true\n之后等待磁盘中的副本变为0，即可进行删除磁盘操作\n\n\n驱逐节点所有副本\n\n转到选项卡，选择一个或多个节点，然后单击 。Node Edit Node\n确保已禁用节点以进行调度并设置为 。Scheduling Disable\n设置为 ，然后保存。Eviction Requested true\n\n\n3.6 移除node节点步骤\n禁用磁盘调度。\n逐出节点上的所有副本。\n驱逐节点所有Pod\n\n12kubectl drain &lt;node-name&gt;\n\n\n使用选项卡中的 从 Longhorn 中删除节点。Delete Node\n或者，从 Kubernetes 中删除该节点，使用：\n12kubectl delete node &lt;node-name&gt;\n\nLonghorn 将自动从群集中删除该节点。\n\n\n3.7 卷访问模式Longhorn部署在k8s中时创建PVC时可指定俩种访问模式分别为ReadWriteMany,ReadWriteOnce,俩种模式在Pod中的挂载方式有所不同。\nReadWriteMany：可以被多个Pod以读写的方式进行挂载\n当pvc被定义为ReadWriteMany访问模式时，当挂载这个pvc的Pod启动时Longhorn会创建一个共享挂载的容器以iSCSI的方式先把这个pvc挂载到这个pod，之后所有挂载这个Pod的容器都以nfs的方式通过这个容器挂载这个Pod，从而实现一个PVC可以被多个Pod挂载\n1234567891011[15:49:26 root@nexus test]NAME                                                     READY   STATUS    RESTARTS   AGEshare-manager-pvc-e8dc9c09-f46e-46bf-8873-4aa6b1cc2a4e   1/1     Running   0          8m9s  [15:50:15 root@nexus test]NAME                                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)     AGEpvc-e8dc9c09-f46e-46bf-8873-4aa6b1cc2a4e   ClusterIP   172.10.53.247    &lt;none&gt;        2049/TCP    8m56s  root@nginx-996746966-cvr94:/Filesystem                                               Size  Used Avail Use% Mounted on172.10.53.247:/pvc-e8dc9c09-f46e-46bf-8873-4aa6b1cc2a4e 1014M   39M  975M   4% /data    \n\nReadWriteOnce: 只允许一个Pod挂载\n当pvc被定义为ReadWriteOnce访问模式时，挂载Pvc的Pod会直接以iSCSI方式挂载这个pvc\n12345678[15:50:27 root@nexus test]NAME                        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGEnginx-nginx-statefulset-0   Bound    pvc-24a2da15-d1fb-4e33-a99a-ed32081b52a2   1Gi        RWO            longhorn       28mroot@nginx-statefulset-3:/Filesystem                                              Size  Used Avail Use% Mounted on/dev/longhorn/pvc-5785b32c-fc5e-4122-939a-57f8eb306d78 1014M   40M  975M   4% /data\n\n3.8 高可用相关1.自动平衡副本当副本在节点或区域上的调度不均匀时，Longhorn 设置允许副本在新节点可用于群集时进行自动平衡。\n设置参数为eplicaAutoBalance，全局配置，可以安装时在valuse中设置\n\ndisabled.这是默认选项，不会执行副本自动平衡。\nleast-effort.此选项指示 Longhorn 平衡副本以实现最小冗余。例如，添加节点 2 后，具有 4 个不平衡副本的卷将仅重新平衡 1 个副本。\nbest-effort.此选项指示 Longhorn 尝试平衡副本以实现均匀冗余。例如，添加节点 2 后，具有 4 个不平衡副本的卷将重新平衡 2 个副本。\n\n在存储类中设置\n123456789101112kind: StorageClassapiVersion: storage.k8s.io/v1metadata:  name: hyper-convergedprovisioner: driver.longhorn.ioallowVolumeExpansion: trueparameters:  numberOfReplicas: &quot;3&quot;  replicaAutoBalance: &quot;least-effort&quot;    staleReplicaTimeout: &quot;2880&quot;   fromBackup: &quot;&quot;\n\n官方文档：https://longhorn.io/docs/1.2.4/monitoring\n这里采用kube-Prometheus项目进行监控。\nLonghorn 在 REST 端点上以 Prometheus 文本格式原生公开指标。http://LONGHORN_MANAGER_IP:PORT/metrics\n官方示例。监控系统使用Prometheus来收集数据和警报，Grafana用于可视化&#x2F;仪表板收集的数据。从高级概述来看，监控系统包含：\n\nPrometheus服务器，从Longhorn指标端点抓取和存储时间序列数据。Prometheus还负责根据配置的规则和收集的数据生成警报。然后，Prometheus服务器向警报管理器发送警报。\n然后，AlertManager 管理这些警报，包括静音、抑制、聚合以及通过电子邮件、待命通知系统和聊天平台等方法发送通知。\nGrafana查询Prometheus服务器以获取数据并绘制用于可视化的仪表板。\n\n\n4.1 使用ServiceMonitor获取指标数据示例文件\n1234567891011121314151617apiVersion: monitoring.coreos.com/v1kind: ServiceMonitormetadata:  name: longhorn  namespace: monitoring  labels:    name: longhornspec:  endpoints:  - port: manager  selector:    matchLabels:      app: longhorn-manager  namespaceSelector:    matchNames:    - longhorn-system\n\n4.2 创建ruleg告警规则示例：\n1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192apiVersion: monitoring.coreos.com/v1kind: PrometheusRulemetadata:  labels:    prometheus: longhorn    role: alert-rules  name: prometheus-longhorn-rules  namespace: monitoringspec:  groups:  - name: longhorn.rules    rules:    - alert: LonghornVolumeActualSpaceUsedWarning      annotations:        description: The actual space used by Longhorn volume &#123;&#123;$labels.volume&#125;&#125; on &#123;&#123;$labels.node&#125;&#125; is at &#123;&#123;$value&#125;&#125;% capacity for          more than 5 minutes.        summary: The actual used space of Longhorn volume is over 90% of the capacity.      expr: (longhorn_volume_actual_size_bytes / longhorn_volume_capacity_bytes) * 100 &gt; 90      for: 5m      labels:        issue: The actual used space of Longhorn volume &#123;&#123;$labels.volume&#125;&#125; on &#123;&#123;$labels.node&#125;&#125; is high.        severity: warning    - alert: LonghornVolumeStatusCritical      annotations:        description: Longhorn volume &#123;&#123;$labels.volume&#125;&#125; on &#123;&#123;$labels.node&#125;&#125; is Fault for          more than 2 minutes.        summary: Longhorn volume &#123;&#123;$labels.volume&#125;&#125; is Fault      expr: longhorn_volume_robustness == 3      for: 5m      labels:        issue: Longhorn volume &#123;&#123;$labels.volume&#125;&#125; is Fault.        severity: critical    - alert: LonghornVolumeStatusWarning      annotations:        description: Longhorn volume &#123;&#123;$labels.volume&#125;&#125; on &#123;&#123;$labels.node&#125;&#125; is Degraded for          more than 5 minutes.        summary: Longhorn volume &#123;&#123;$labels.volume&#125;&#125; is Degraded      expr: longhorn_volume_robustness == 2      for: 5m      labels:        issue: Longhorn volume &#123;&#123;$labels.volume&#125;&#125; is Degraded.        severity: warning    - alert: LonghornNodeStorageWarning      annotations:        description: The used storage of node &#123;&#123;$labels.node&#125;&#125; is at &#123;&#123;$value&#125;&#125;% capacity for          more than 5 minutes.        summary:  The used storage of node is over 70% of the capacity.      expr: (longhorn_node_storage_usage_bytes / longhorn_node_storage_capacity_bytes) * 100 &gt; 70      for: 5m      labels:        issue: The used storage of node &#123;&#123;$labels.node&#125;&#125; is high.        severity: warning    - alert: LonghornDiskStorageWarning      annotations:        description: The used storage of disk &#123;&#123;$labels.disk&#125;&#125; on node &#123;&#123;$labels.node&#125;&#125; is at &#123;&#123;$value&#125;&#125;% capacity for          more than 5 minutes.        summary:  The used storage of disk is over 70% of the capacity.      expr: (longhorn_disk_usage_bytes / longhorn_disk_capacity_bytes) * 100 &gt; 70      for: 5m      labels:        issue: The used storage of disk &#123;&#123;$labels.disk&#125;&#125; on node &#123;&#123;$labels.node&#125;&#125; is high.        severity: warning    - alert: LonghornNodeDown      annotations:        description: There are &#123;&#123;$value&#125;&#125; Longhorn nodes which have been offline for more than 5 minutes.        summary: Longhorn nodes is offline      expr: (avg(longhorn_node_count_total) or on() vector(0)) - (count(longhorn_node_status&#123;condition=&quot;ready&quot;&#125; == 1) or on() vector(0)) &gt; 0      for: 5m      labels:        issue: There are &#123;&#123;$value&#125;&#125; Longhorn nodes are offline        severity: critical    - alert: LonghornIntanceManagerCPUUsageWarning      annotations:        description: Longhorn instance manager &#123;&#123;$labels.instance_manager&#125;&#125; on &#123;&#123;$labels.node&#125;&#125; has CPU Usage / CPU request is &#123;&#123;$value&#125;&#125;% for          more than 5 minutes.        summary: Longhorn instance manager &#123;&#123;$labels.instance_manager&#125;&#125; on &#123;&#123;$labels.node&#125;&#125; has CPU Usage / CPU request is over 300%.      expr: (longhorn_instance_manager_cpu_usage_millicpu/longhorn_instance_manager_cpu_requests_millicpu) * 100 &gt; 300      for: 5m      labels:        issue: Longhorn instance manager &#123;&#123;$labels.instance_manager&#125;&#125; on &#123;&#123;$labels.node&#125;&#125; consumes 3 times the CPU request.        severity: warning    - alert: LonghornNodeCPUUsageWarning      annotations:        description: Longhorn node &#123;&#123;$labels.node&#125;&#125; has CPU Usage / CPU capacity is &#123;&#123;$value&#125;&#125;% for          more than 5 minutes.        summary: Longhorn node &#123;&#123;$labels.node&#125;&#125; experiences high CPU pressure for more than 5m.      expr: (longhorn_node_cpu_usage_millicpu / longhorn_node_cpu_capacity_millicpu) * 100 &gt; 90      for: 5m      labels:        issue: Longhorn node &#123;&#123;$labels.node&#125;&#125; experiences high CPU pressure.        severity: warning\n\n4.3 导入grafana模板模板：https://grafana.com/grafana/dashboards/13032\n5.1 快照功能快照是 Kubernetes 卷在任何给定时间点的状态。\n要创建现有集群的快照，\n\n在 Longhorn UI 的顶部导航栏中，单击Volume。\n单击要为其拍摄快照的卷的名称。这将进入卷详细页面。\n单击Take Snapshot按钮\n\n创建快照后，您将在卷头之前的卷快照列表中看到它\n5.2 卷克隆Longhorn 支持 CSI 卷克隆。官方说明https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/\n示例：\n123456789101112kind: PersistentVolumeClaimapiVersion: v1metadata:  name: claimspec:  accessModes:    - ReadWriteMany  storageClassName: longhorn  resources:    requests:      storage: 1Gi\n\n您可以通过应用以下 yaml 文件来创建一个内容与 完全相同的新 PVC，除了官方的一些要求之外还必须满足cloned-claim和claim的resources.requests.storage相同。\n123456789101112131415apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: cloned-claimspec:  storageClassName: longhorn  dataSource:    name: claim    kind: PersistentVolumeClaim  accessModes:    - ReadWriteMany  resources:    requests:      storage: 1Gi\n\n创建后验证：\n12345[16:15:14 root@nexus test]NAME           STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGEclaim          Bound    pvc-e8dc9c09-f46e-46bf-8873-4aa6b1cc2a4e   1Gi        RWX            longhorn       50mcloned-claim   Bound    pvc-911789f9-fe73-44a8-bc12-7cd53674bf04   1Gi        RWO            longhorn       2m49s\n\n5.3 备份详解Longhorn可以备份数据到s3对象存储或者nfs当中，并且可以设置定期备份。\n1.备份目标设置备份目标是用于访问 Longhorn 中的备份存储的端点。备份存储是 NFS 服务器或 S3 兼容服务器，用于存储 Longhorn 卷的备份。\n我这里使用minio与nfs进行演示，如果使用其他s3对象存储请查看官方文档：https://longhorn.io/docs/1.2.4/snapshots-and-backups/backup-and-restore/set-backup-target/\n备份目标的配置有俩项分别是\nbackupTarget：备份目标设置\n123456789s3://对象存储桶名称@区域/目录名称/s3://backupbucket@us-east-1/backup/nfs://nfs服务器地址:nfs服务器共享目录位置nfs://longhorn-test-nfs-svc.default:/opt/backupstore\n\nbackupTargetCredentialSecret：备份目标的其他设置是secret资源名称，资源配置如下,只有使用s3对象存储才会有这个配置\n123456789101112apiVersion: v1kind: Secretmetadata:  name: minio-secret  namespace: longhorn-systemtype: Opaquedata:    AWS_ACCESS_KEY_ID: xxx     AWS_SECRET_ACCESS_KEY: xxxx   AWS_ENDPOINTS: xxxx   AWS_CERT: xxxx     \n\nMInio配置示例：\n需先部署minio服务，之后进行配置：\n安装时设置，需修改values文件：\n1234567891011defaultSettings:  backupTarget: s3://backupbucket@us-east-1/  backupTargetCredentialSecret: minio-secrethelm install longhorn -n longhorn-system . kubectl create secret generic minio-secret -n longhorn-system \\--from-literal=AWS_ACCESS_KEY_ID=minio \\--from-literal=AWS_SECRET_ACCESS_KEY=minio123 \\--from-literal=AWS_ENDPOINTS=http://192.168.10.254:9000\n\n配置完成后登录UI页面点击Backup选项卡，没有任何报错信息表示配置没有问题。\nNFS配置示例：\n1234defaultSettings:  backupTarget: nfs://192.168.254:/nfs/backup  backupTargetCredentialSecret: ~  \n\n2.手动创建备份必须设置备份目标。确定备份目标配置没有问题，并且备份的volume必须处于挂载状态才可以进行备份。\n\n导航到Volume菜单。\n选择要备份的卷。\n单击Create backup\n添加任何适当的标签，然后单击确定。\n\n创建完成后可以在Backup导航菜单看到所创建的备份。\n3.定期备份可以在UI中配置，也可以在k8s中进行配置，他是一个CRD资源RecurringJob。\n1234567891011121314151617apiVersion: longhorn.io/v1beta1kind: RecurringJobmetadata:  name: backup  namespace: longhorn-systemspec:  cron: &quot;*/5 * * * *&quot;  task: &quot;backup&quot;  groups:  - default  - group1  retain: 1  concurrency: 2  labels:    label/1: a    label/2: b\n\n\nname：定期作业的名称。不要使用重复的名称。并且的长度不应超过40个字符。\ntask：作业类型。它支持（定期创建快照）或（定期创建快照然后进行备份）。选项有snapshot，backup\ncron：克伦表达式。它告诉作业的执行时间。\nretain：Longhorn 将为每个卷作业保留多少个快照&#x2F;备份。它应该不小于1。\nconcurrency：要并发运行的作业数。它应该不小于1。\n\n可以指定可选参数：\n\ngroups：作业应属于的组，默认volues创建后都属于default组，所以如果对所有卷进行备份请设置default，如果不同的卷有不同的备份请配置不同组\nlabels：创建快照与备份后添加相应的标签。\n\n4.允许在分离状态的卷进行备份作业Longhorn 提供的设置允许您执行定期备份，即使卷已分离也是如此。\n您可以在 Longhorn UI 中找到该设置。allow-recurring-job-while-volume-detached，安装时设置项在values配置文件中。\n启用该设置后，Longhorn 将自动附加卷，并在需要执行定期快照&#x2F;备份时拍摄快照&#x2F;备份。\n请注意，在自动附加卷期间，卷尚未准备好执行工作负载。工作负荷必须等到定期作业完成。\n5.4 重备份还原pvc还原操作必须去UI页面进行操作，需要先删除要还原的卷。\n\n导航到Backup\n选择要还原的备份，然后单击Restore Backup backup\n在name字段中，选择要还原的卷。\n单击OK\n此时卷已经恢复但是PVC是不会自己创建的需要进行Volume中找到恢复的卷点击编辑选择Create PV/PVC配置完成后点击OK即可。\n\n","slug":"OCI/Longhorn云原生存储","date":"2024-06-11T17:02:24.000Z","categories_index":"Longhorn,OCI","tags_index":"https,Kubernetes,Pod","author_index":"dandeliono"},{"id":"b7a0ddae62c7a26d383904662692ec9c","title":"高校项目汇总","content":"高校项目汇总【清华大学】 \n1.学堂在线：https://www.xuetangx.com\n2.深言达意：https://www.shenyandayi.com\n3.i校对：https://ijiaodui.com\n4.九歌：https://jiuge.thunlp.org\n5.Vidu：https://www.shengshu-ai.com/home?ref=maginative.com\n6.Aminer：https://www.aminer.cn\n【北京大学】 \n1.chatlaw：https://www.chatlaw.cloud\n2.中国历代人物：https://renwuzhi.wiki\n3.chatexcel：https://chatexcel.com\n4.华文慕课：https://www.chinesemooc.org\n5.北大出版社电子书架：https://pup6.yunzhan365.com/bookcase/jmyr/index.html\n6.识典古籍：https://www.shidianguji.com\n【复旦大学】 \n1.MOSS：https://github.com/OpenMOSS/MOSS?tab=readme-ov-file\n2.听见世界：http://mousi.org\n3.LEGO：https://github.com/lzw-lzw/GroundingGPT\n【华东师范大学】\n1.小花狮：https://elion.ecnu.edu.cn\n2.EmoGPT：学习强国内置\n【北京师范大学】 \n1.北京师范大学AI工具平台：https://aitools.bnu.edu.cn\n2.AI好老师：https://aic-fe.bnu.edu.cn/ai_teacher/\n【香港大学】 \n1.anydoor：https://damo-vilab.github.io/AnyDoor-Page/\n2.app listener：https://www.usenix.org/conference/usenixsecurity23/presentation/ni\n","slug":"OTHER/高校项目汇总","date":"2024-06-06T23:53:57.000Z","categories_index":"https,OTHER","tags_index":"com,www,org","author_index":"dandeliono"},{"id":"7c7426f177489c87044a1c12209cf448","title":"java进程cpu占用高如何排查_java web应用服务器cpu90","content":"java进程cpu占用高如何排查_java web应用服务器cpu90问题：\n\n\n\n\n\n\n\n\n公司参加HW期间，项目两台双活的jboss服务器频繁触发cpu利用率过高告警，cpu利用率长时间在90%以上。 \n排查思路：第一步： 在两台Linux服务器上，执行top命令，并按大写P以cpu利用率排序，确定cpu占用最高的进程为 java进程\n那么，java进程cpu占用过高该如何排查呢，我们从两个角度出发： \n\n\n\n\n\n\n\n\n\n（1）执行任务的java线程本身存在bug，死循环或者操作本身耗cpu，导致cpu占用过高\n（2）jvm发生频繁gc，导致cpu过高\n第二步： 先排查java任务线程本身，确定什么线程cpu占用过高\n（1）方法一：ps -mp [java进程id] -o THREAD,tid,time | sort -n   找到cpu占用最大的线程id\n\n注意，该方法在生产环境测试，发现无法找到cpu占用高的线程，显示所有线程cpu占用均为0，因此实际排查采用方法二\n\n（2）使用top -H -p [java进程id]，找到cpu占用较高的线程id，如下图所示，左边红框标注的PID列为线程id\n\n第三步： 计算java线程id的16进制值，因为后续用jstack看到的线程快照中，线程id为小写十六进制值\n（1）可百度在线进制转换\n（2）可使用windows自带的计算器，程序员模式，可转换十六进制\n（3）Linux可使用命令：printf “%x\\n” [线程_id]\n\n第四步：使用命令 jstack [java进程pid] | grep [线程id十六进制值] -A 30（-A 30表示向下打印30行）\n\n分析上图可知，线程在做正则匹配，查看完整堆栈可定位到具体代码位置，分析后，确定在做url的正则匹配。继续分析其他线程，发现基本都在做url正则匹配。 \n后续分析，则需要结合业务代码分析，正则匹配是否耗时。 \n正则分析参考博客：https://blog.csdn.net/weixin_33023873&#x2F;article&#x2F;details&#x2F;114740786\n第五步： 从gc角度出发，是否存在大量gc，首先确定当前内存消耗情况，使用top命令或者查看设备监控管理系统，确定内存利用率达97%：\n\n第六步： 确认gc次数，使用命令 jstat -gc [java进程ID]：\n\nYGC，表示 Young GC，也就是Minor GC，发生在新生代中的GC\nFGC，表示 Full GC，发生在老年代中的GC\n\n\n\n\n\n\n\n\n\n_S0C：第一个幸存区的大小S1C：第二个幸存区的大小S0U：第一个幸存区的使用大小S1U：第二个幸存区的使用大小EC：伊甸园区的大小EU：伊甸园区的使用大小OC：老年代大小OU：老年代使用大小MC：方法区大小MU：方法区使用大小CCSC:压缩类空间大小CCSU:压缩类空间使用大小YGCT：年轻代垃圾回收消耗时间FGCT：老年代垃圾回收消耗时间GCT：垃圾回收消耗总时间 _\n如何计算gc频率，参考：https://blog.csdn.net/qq_18671415&#x2F;article&#x2F;details&#x2F;104446568\n结合上图可知，程序运行以来共发生7436次YGC，63次FGC，gc次数较多\n基本可以说明存在频繁GC导致cpu占用高的问题\n第七步： 使用命令dump 内存堆存储快照：jmap -dump:format&#x3D;b,file&#x3D;&#x2F;tmp&#x2F;my.hprof [java进程id]\n第八步： 使用内存分析工具，如Eclipse Memory Analyzer等分析my.hprof文件，分析内存那块占用大，存在内存泄露，导致空间无法释放。\n总结：cpu利用率过高排查，需要从两个角度排查，一是自身任务线程是否存在bug，二是是否内存泄露导致触发频繁gc；然后利用top、jstack、jmap等工具，定位出问题的代码位置，然后针对性分析修改。\n","slug":"JAVA/java进程cpu占用高如何排查_java web应用服务器cpu90","date":"2024-05-29T09:25:15.000Z","categories_index":"cpu,JAVA","tags_index":"https,java,blog","author_index":"dandeliono"},{"id":"0a458e361f4a92244bc682099842a8db","title":"我在 Elasticsearch 集群内应该设置多少个分片？","content":"我在 Elasticsearch 集群内应该设置多少个分片？我在 Elasticsearch 集群内应该设置多少个分片？ | Elastic Blog \n Elasticsearch 是一个功能十分丰富的平台，支持各种用例，能够在数据整理和复制策略方面提供很大的灵活性。然而，这一灵活性有时也会带来困扰，让您在前期难以确定如何最好地将数据整理为索引和分片，如果您刚上手使用 Elastic Stack，这一点可能更明显。如果未能做出最佳选择，尽管这在开始的时候可能不会造成问题，但随着数据量越来越大，便有可能会引发性能问题。集群中的数据越多，要纠正这一问题就越难，这是因为有时必须对大量数据进行重新索引。\n据我们了解，当用户遇到性能问题时，原因通常都可回溯至数据的索引方式以及集群中的分片数量。对于涉及多租户和&#x2F;或用到时序型索引的用例，这一点尤为突出。与用户讨论这一问题时，无论是在活动或聚会中面对面讨论，还是在论坛上讨论，我们遇到的一些最常见问题就是“我应该设置多少个分片？”以及“我应该设置多大的分片？”\n这篇博文旨在帮您集中解答这些问题，并为涉及时序型索引（例如日志或安全分析）的用例提供实用指南。\n什么是分片？开始之前，我们需要明确一下基本知识以及在后面部分会用到的术语。\nElasticsearch 中的数据会整理为索引。每个索引又由一个或多个分片组成。每个分片都是一个 Lucene 索引实例，您可以将其视作一个独立的搜索引擎，它能够对 Elasticsearch 集群中的数据子集进行索引并处理相关查询。\n数据写到分片上之后，会定期发布到磁盘上不可更改的新 Lucene 段中，此时，数据便可用于查询了。这称为刷新。相关原理的详细介绍，请参见 Elasticsearch：权威指南。\n随着段数越来越多，这些段会定期合并为更大的段。这一过程称为合并。由于所有段都是不可更改的，这意味着在索引期间所用磁盘空间通常会上下浮动，这是因为只有合并后的新段创建完毕之后，它们所替换的那些段才能删掉。合并是一项极其耗费资源的任务，尤其耗费磁盘 I&#x2F;O。\n分片是 Elasticsearch 在集群内分发数据的单位。Elasticsearch 在对数据进行再平衡（例如发生故障后）时移动分片的速度取决于分片的大小和数量，以及网络和磁盘性能。\n提示：__避免分片过大，因为这样会对集群从故障中恢复造成不利影响。尽管并没有关于分片大小的固定限值，但是人们通常将 50GB 作为分片上限，而且这一限值在各种用例中都已得到验证。\n按照保留期限进行索引由于段是不可更改的，所以更新文档时必须要求 Elasticsearch 首先找到既有文档，然后将其标为已删除，并添加更新后版本。删除文档时同样也要求先找到文档，再将其标为已删除。有鉴于此，已删除文档仍将继续占用磁盘空间和系统资源，直至将它们合并，而合并过程也会消耗大量系统资源。\n通过 Elasticsearch，用户可以十分高效地从文件系统中直接删除整个索引，而无需单独删除所有记录。这是迄今为止从 Elasticsearch 中删除数据的最高效方法。\n\n_提示： _但凡可能，尽量使用时序型索引来管理数据保留期。根据保留期限对数据分组，将它们存储到索引中。通过时序型索引，用户还能随着时间推移轻松调整主分片和副本分片的数量，这是因为用户可针对要生成的下个索引进行这方面的更改。这样便能简化对不断变化的数据量和数据要求的适应过程。\n\n索引分片不是免费的吗？对于每个 Elasticsearch 索引，映射和状态的相关信息都存储在集群状态中。这些信息存储在内存中，以便快速访问。因此，如果集群中的索引和分片数量过多，这会导致集群状态过大，如果映射较大的话，尤为如此。这会导致更新变慢，因为所有更新都需要通过单线程完成，从而在将变更分发到整个集群之前确保一致性。\n\n_提示： _为了减少索引数量并避免造成过大且无序的映射，可以考虑在同一索引中存储类似结构的数据，而不要基于数据来源将数据分到不同的索引中。很重要的一点是在索引&#x2F;分片的数量和每个单独索引的映射大小之间实现良好平衡。由于集群状态会加载到每个节点（包括主节点）上的堆内存中，而且堆内存大小与索引数量以及单个索引和分片中的字段数成正比关系，所以还需要同时监测主节点上的堆内存使用量并确保其大小适宜，这一点很重要。\n\n每个分片都有一部分数据需要保存在内存中，这部分数据也会占用堆内存空间。这包括存储分片级别以及段级别信息的数据结构，因为只有这样才能确定数据在磁盘上的存储位置。这些数据结构的大小并不固定，不同用例之间会有很大的差别。\n段相关开销有一个重要特征，那就是其并不与段的大小呈严格正比关系。这意味着，与较小的段相比，对于较大的段而言，其单位数据量所需的开销要小一些。二者之间的差异可能会十分巨大。\n为了能够在单个节点上存储尽可能多的数据，下面两点至关重要：管理堆内存使用量；尽可能减少开销。节点的堆内存空间越多，其能处理的数据和分片就越多。\n从集群角度来说，索引和分片都不是免费的，因为每个索引和分片都会产生一定的资源开销。\n\n_提示： _分片过小会导致段过小，进而致使开销增加。您要尽量将分片的平均大小控制在至少几 GB 到几十 GB 之间。对时序型数据用例而言，分片大小通常介于 20GB 至 40GB 之间。\n_提示： _由于单个分片的开销取决于段数量和段大小，所以通过 forcemerge 操作强制将较小的段合并为较大的段能够减少开销并改善查询性能。理想状况下，应当在索引内再无数据写入时完成此操作。请注意：这是一个极其耗费资源的操作，所以应该在非高峰时段进行。\n提示： 每个节点上可以存储的分片数量与可用的堆内存大小成正比关系，但是 Elasticsearch 并未强制规定固定限值。这里有一个很好的经验法则：确保对于节点上已配置的每个 GB，将分片数量保持在 20 以下。如果某个节点拥有 30GB 的堆内存，那其最多可有 600 个分片，但是在此限值范围内，您设置的分片数量越少，效果就越好。一般而言，这可以帮助集群保持良好的运行状态。（编者按：从 8.3 版开始，我们大幅减小了每个分片的堆使用量，因此对本博文中的经验法则也进行了相应更新。请按照以下提示了解 8.3+ 版本的 Elasticsearch。）\n_新提示： __为数据节点上每个索引的每个字段留出 1kB 的堆空间，还要为开销留出额外的空间_每个映射字段的具体资源使用情况取决于其类型，但经验法则是，每个数据节点持有的每个索引的每个映射字段允许大约 1kB 的堆开销。您还必须为 Elasticsearch 的基线使用以及工作负载（例如索引、搜索和聚合）留出足够的堆空间。额外留出 0.5GB 的堆足以满足许多合理的工作负载，如果您的工作负载非常轻，可能需要更少的空间，而工作负载繁重的话可能需要更多空间。\n例如，如果一个数据节点持有来自 1000 个索引的分片，每个索引包含 4000 个映射字段，那么您应该为字段留出大约 1000 × 4000 × 1kB &#x3D; 4GB 的堆空间，并为其分配另外 0.5GB 的堆空间用于工作负载和其他开销，因此该节点需要至少 4.5GB 的堆大小。\n\n分片大小对性能有何影响？在 Elasticsearch 中，每个查询都是在单个分片上以单线程方式执行的。然而，可以同时对多个分片进行处理，正如可以针对同一分片进行多次查询和聚合一样。\n这意味着，最低查询延时（假设没有缓存）将取决于数据、查询类型，以及分片大小。尽管查询很多个小分片会加快单个分片的处理速度，但是由于有很多任务需要进入队列并按顺序加以处理，所以与查询较少的大分片相比，这种方法并不一定会加快查询速度。如果有多个并发查询，拥有很多小分片还会降低查询吞吐量。\n\n_提示： _从查询性能的角度来看，确定最大分片大小的最佳方法是使用具有实际意义的数据和查询进行基准测试。进行基准测试时，务必确保所使用的查询和索引负载能够代表节点在生产环境中需要处理的内容，因为只针对单一查询进行优化可能会得出错误结果。\n\n我应该如何管理分片大小呢？使用时序型索引时，按照传统方法，每个索引都关联至固定时间段。按天索引是一种十分常见的方法，通常用来存储保留期较短的数据或者用来存储较大的每日数据量。此类索引允许用户在很细的粒度层面管理保留期，也方便用户根据每天不断变化的数据量轻松进行调整。对于拥有较长保留期的数据，尤其如果每日数据量并不能保证用完每日索引，通常可按周索引或按月索引，以便提高分片大小。长期来看，这有助于减少存储在集群中的索引和分片数量。\n\n提示：如果使用时序型索引来存储固定期限内的数据，用户应根据保留期和预计数据量对每个索引覆盖的期限进行调整，从而确保达到目标分片大小。\n\n如果能够很好地预估数据量并且数据量变化缓慢，则固定期限式时序型索引的效果很好。如果索引速度变化很快，则很难保持统一的目标分片大小。\n为了能够更好地应对此类情形，可以采用所推出的 Rollover（汇总）和 Shrink（压缩）API。这些 API 能够让用户更加灵活地管理索引和分片，尤其是时序型索引。\n通过 rollover index API（汇总索引 API），用户能够指定每个索引应该存储的文档数量，以及&#x2F;或者最长可在多长时间内向索引内写入文档。一旦超出这些条件，Elasticsearch 可触发新索引创建操作以继续写入数据，而不会造成中断。通过这种方法，每个索引不再覆盖特定的时间段，数据可在索引达到具体大小后转到新索引，这样用户可以更加轻松地确保所有索引都达到均等的分片大小。\n由于使用此 API 时事件时间戳与事件所在的索引之间并无明显联系，所以如需更新数据，需要先搜索才能完成每次更新，而这会大大降低更新效率。\n\n_提示： _如果您拥有不可更改的时序型数据，并且数据量在一段期间内会巨幅变化，可以考虑使用 rollover index API（汇总索引 API）通过动态调整每个索引所覆盖的时间段来实现最佳的目标分片大小。这为用户提供了很大的灵活性，并且在数据量难以预测时，还有助于避免分片过大或过小。  \n\n通过 shrink index API（压缩索引 API），您能够将现有索引压缩到拥有较少主分片的新索引中。如果在索引时希望不同节点上的分片覆盖均等的时间段，但是这会导致分片过小，则您可以考虑在索引内再无数据写入时使用此 API 来降低主分片数量。这会形成较大的分片，能够更好地满足长期存储数据的需求。  \n\n_提示： _如果您希望每个索引既要对应至特定时间段，同时还想将索引过程分散到大量节点上，可以考虑使用 Shrink（压缩）API 来在索引内再无数据写入时减少主分片数量。如果开始时配置了过多的分片，您也可以使用此 API 来减少分片数量。  \n\n结论本篇博文针对在 Elasticsearch 中管理数据的最佳方法提供了一些建议和使用指南。如想深入了解，可以参阅“Elasticsearch：权威指南”中的扩容设计一节，尽管此部分内容已经发布一段时间了，但却仍然值得一读。\n虽讲了这么多，关于如何最好地在索引和分片之间分配数据，很多决策仍取决于用例的具体情况，有时的确很难确定如何最好地应用现有建议。如需更多深入的个人建议，您可通过我们的商用订阅服务与我们进行交流，我们的支持和咨询团队能够帮助您加快项目进度。如果想公开讨论自己的用例，您也可以向我们的社区和公共论坛寻求帮助。\n这篇博文最初于 2017 年 9 月 18 日发布。于 2022 年 7 月 6 日更新。\n","slug":"MIDDLEWARE/我在 Elasticsearch 集群内应该设置多少个分片？","date":"2024-05-29T09:22:31.000Z","categories_index":"Elasticsearch,MIDDLEWARE","tags_index":"https,elastic,guide","author_index":"dandeliono"},{"id":"90ecd18311a9fc88b1f7131b4ce334a3","title":"问题排查利器：Linux 原生跟踪工具 Ftrace 必知必会","content":"问题排查利器：Linux 原生跟踪工具 Ftrace 必知必会本文地址：https://www.ebpf.top/post/ftrace_tools\nTLDR，建议收藏，需要时查阅。\n\n\n\n\n\n\n\n\n\n如果你只是需要快速使用工具来进行问题排查，包括但不限于函数调用栈跟踪、函数调用子函数流程、函数返回结果，那么推荐你直接使用 BCC trace 或 Brendan Gregg 封装的 perf-tools 工具即可，本文尝试从手工操作 Ftrace 跟踪工具的方式展示在底层是如何通过 tracefs 实现这些能力的。如果你对某个跟踪主题感兴趣，建议直接跳转到相关的主题查看。\n快速说明：\n\nkprobe 为内核中提供的动态跟踪机制，/proc/kallsyms 中的函数几乎都可以用于跟踪，但是内核函数可能随着版本演进而发生变化，为非稳定的跟踪机制，数量比较多。\nuprobe 为用户空间提供的动态机制；\ntracepoint 是内核提供的静态跟踪点，为稳定的跟踪点，需要研发人员代码编写，数量有限；\nusdt 为用户空间提供的静态跟踪点 【本次暂不涉及】\n\nFtrace 是 Linux 官方提供的跟踪工具，在 Linux 2.6.27 版本中引入。Ftrace 可在不引入任何前端工具的情况下使用，让其可以适合在任何系统环境中使用。\nFtrace 可用来快速排查以下相关问题：\n\n特定内核函数调用的频次 （function）\n内核函数在被调用的过程中流程（调用栈） （function + stack）\n内核函数调用的子函数流程（子调用栈）（function graph）\n由于抢占导致的高延时路径等\n\nFtrace 跟踪工具由性能分析器（profiler）和跟踪器（tracer）两部分组成：\n\n性能分析器，用来提供统计和直方图数据（需要 CONFIG_ FUNCTION_PROFILER&#x3D;y）\n函数性能分析\n直方图\n\n\n跟踪器，提供跟踪事件的详情：\n函数跟踪（function）\n跟踪点（tracepoint）\nkprobe\nuprobe\n函数调用关系（function_graph）\nhwlat 等\n\n\n\n除了操作原始的文件接口外，也有一些基于 Ftrace 的前端工具，比如 perf-tools 和 trace-cmd （界面 KernelShark）等。整体跟踪及前端工具架构图如下：\n\n图片来自于 《Systems Performance Enterprise and the Cloud 2nd Edition》 14.1 P706\nFtrace 的使用的接口为 tracefs 文件系统，需要保证该文件系统进行加载：\n| \n12345678910123456789\n\n | \n12345678910$ sysctl -q kernel.ftrace_enabled=1$ mount -t tracefs tracefs /sys/kernel/tracing$ mount -t debugfs,tracefstracefs on /sys/kernel/tracing type tracefs (rw,nosuid,nodev,noexec,relatime)debugfs on /sys/kernel/debug type debugfs (rw,nosuid,nodev,noexec,relatime)tracefs on /sys/kernel/debug/tracing type tracefs (rw,nosuid,nodev,noexec,relatime)$ ls -F /sys/kernel/debug/tracing  # 完整目录如下图\n\n |\n\ntracing 目录下核心文件介绍如下表格，当前可仅关注黑体加粗的项，其他项可在需要的时候再进行回顾：\n\n\n\n文件\n描述\n\n\n\navailable_tracers\n可用跟踪器，hwlat blk function_graph wakeup_dl wakeup_rt wakeup function nop，nop 表示不使用跟踪器\n\n\ncurrent_tracer\n当前使用的跟踪器\n\n\nfunction_profile_enabled\n启用函数性能分析器\n\n\navailable_filter_functions\n可跟踪的完整函数列表\n\n\nset_ftrace_filter\n选择跟踪函数的列表，支持批量设置，例如 *tcp、tcp* 和 *tcp* 等\n\n\nset_ftrace_notrace\n设置不跟踪的函数列表\n\n\nset_event_pid\n设置跟踪的 PID，表示仅跟踪 PID 程序的函数或者其他跟踪\n\n\ntracing_on\n是否启用跟踪，1 启用跟踪 0 关闭跟踪\n\n\ntrace_options\n设置跟踪的选项\n\n\ntrace_stat（目录）\n函数性能分析的输出目录\n\n\nkprobe_events\n启用 kprobe 的配置\n\n\nuprobe_events\n启用 uprobe 的配置\n\n\nevents ( 目录 )\n事件（Event）跟踪器的控制文件： tracepoint、kprobe、uprobe\n\n\ntrace\n跟踪的输出 （Ring Buffer）\n\n\ntrace_pipe\n跟踪的输出；提供持续不断的数据流，适用于程序进行读取\n\n\n\n\n\n\n\n\n\n\n\nperf_tools 包含了一个复位所有 ftrace 选型的工具脚本，在跟踪不符合预期的情况下，建议先使用 reset-ftrace 进行复位，然后再进行测试。\n1. 内核函数调用跟踪基于 Ftrace 的内核函数调用跟踪整体架构如下所示：\n\n图片来自于 《Systems Performance Enterprise and the Cloud 2nd Edition》 14.4 P713\n这里我们尝试对于内核中的系统调用函数 __arm64_sys_openat 进行跟踪（前面两个下划线），需要注意的是 __arm64_sys_openat 是在 arm64 结构体系下 sys_openat 系统调用的包装，如果在 x86_64 架构下则为 __x64_sys_openat() ，由于我们本地的电脑是 M1 芯片，所以演示的样例以 arm64 为主。\n在不同的体系结构下，可以在 /proc/kallsym 文件中搜索确认。\n\n\n\n\n\n\n\n\n\n后续的目录，如无特殊说明，都默认位于 /sys/kernel/debug/tracing/ 根目录。\n| \n1234567891011121314151617181920212223242526272829303132333435 1 2 3 4 5 6 7 8 910111213141516171819202122232425262728293031323334\n\n | \n1234567891011121314151617181920212223242526272829303132333435# 使用 function 跟踪器，并将其设置到 current_tracer$ sudo echo function &gt; current_tracer# 将跟踪函数 __arm64_sys_openat 设置到 set_ftrace_filter 文件中$ sudo echo __arm64_sys_openat &gt; set_ftrace_filter# 开启全局的跟踪使能$ sudo echo 1 &gt; tracing_on# 运行 ls 命令触发 sys_openat 系统调用，新的内核版本中直接调用 sys_openat$ ls -hl # 关闭$ sudo echo 0 &gt; tracing_on$ sudo echo nop &gt; current_tracer# 需要主要这里的 echo 后面有一个空格，即 “echo+ 空格&gt;&quot; $ sudo echo  &gt; set_ftrace_filter # 通过 cat trace 文件进行查看$ sudo cat trace# tracer: function## entries-in-buffer/entries-written: 224/224   #P:4##                                _-----=&gt; irqs-off#                               / _----=&gt; need-resched#                              | / _---=&gt; hardirq/softirq#                              || / _--=&gt; preempt-depth#                              ||| /     delay#           TASK-PID     CPU#  ||||   TIMESTAMP  FUNCTION#              | |         |   ||||      |         |            sudo-15099   [002] .... 29469.444400: __arm64_sys_openat &lt;-invoke_syscall            sudo-15099   [002] .... 29469.444594: __arm64_sys_openat &lt;-invoke_syscall\n\n |\n我们可以看到上述的结果表明了函数调用的任务名称、PID、CPU、标记位、时间戳及函数名字。\n在 perf_tools 工具集中的前端封装工具为 functrace ，需要注意的是该工具默认不会设置 tracing_on 为 1， 需要在启动前进行设置，即 ”echo 1 &gt; tracing_on“。\nperf_tools 工具集中 kprobe 也可以实现类似的效果，底层基于 kprobe 机制实现，ftrace 机制中的 kprobe 在后续章节会详细介绍。\n2. 函数被调用流程（栈）在第 1 部分我们获得了内核函数的调用，但是有些场景我们更可能希望获取调用该内核函数的流程（即该函数是在何处被调用），这需要通过设置 options/func_stack_trace 选项实现。\n| \n123456789101112131415161718192021222324252627282930313233343536 1 2 3 4 5 6 7 8 91011121314151617181920212223242526272829303132333435\n\n | \n123456789101112131415161718192021222324252627282930313233343536$ sudo echo function &gt; current_tracer$ sudo echo __arm64_sys_openat &gt; set_ftrace_filter$ sudo echo 1 &gt; options/func_stack_trace # 设置调用栈选项$ sudo echo 1 &gt; tracing_on$ ls -hl $ sudo echo 0 &gt; tracing_on$ sudo cat trace# tracer: function## entries-in-buffer/entries-written: 292/448   #P:4##                                _-----=&gt; irqs-off#                               / _----=&gt; need-resched#                              | / _---=&gt; hardirq/softirq#                              || / _--=&gt; preempt-depth#                              ||| /     delay#           TASK-PID     CPU#  ||||   TIMESTAMP  FUNCTION#              | |         |   ||||      |         |            sudo-15134   [000] .... 29626.670430: __arm64_sys_openat &lt;-invoke_syscall            sudo-15134   [000] .... 29626.670431: &lt;stack trace&gt; =&gt; __arm64_sys_openat =&gt; invoke_syscall =&gt; el0_svc_common.constprop.0 =&gt; do_el0_svc =&gt; el0_svc =&gt; el0_sync_handler =&gt; el0_sync# 关闭$ sudo echo nop &gt; current_tracer$ sudo echo  &gt; set_ftrace_filter $ sudo echo 0 &gt; options/func_stack_trace\n\n |\n通过上述跟踪记录，我们可以发现记录同时展示了函数调用的记录和被调用的函数流程，__arm64_sys_openat 的被调用栈如下：\n| \n123456781234567\n\n | \n12345678=&gt; __arm64_sys_openat =&gt; invoke_syscall =&gt; el0_svc_common.constprop.0 =&gt; do_el0_svc =&gt; el0_svc =&gt; el0_sync_handler =&gt; el0_sync\n\n |\nperf_tools 工具集中 kprobe 通过添加 ”-s“ 参数实现同样的功能，运行的命令如下：\n| \n121\n\n | \n12$ ./kprobe -s &#x27;p:__arm64_sys_openat&#x27;\n\n |\n3. 函数调用子流程跟踪（栈）如果想要分析内核函数调用的子流程（即本函数调用了哪些子函数，处理的流程如何），这时需要用到 function_graph 跟踪器，从字面意思就可看出这是函数调用关系跟踪。\n基于 __arm64_sys_openat 子流程调用关系的跟踪的完整设置过程如下：\n| \n123456789101112131415161718192021222324252627 1 2 3 4 5 6 7 8 91011121314151617181920212223242526\n\n | \n123456789101112131415161718192021222324252627# 将当前 current_tracer 设置为 function_graph$ sudo echo function_graph &gt; current_tracer$ sudo echo __arm64_sys_openat &gt; set_graph_function# 设置跟踪子函数的最大层级数$ sudo echo 3 &gt; max_graph_depth  # 设置最大层级$ sudo echo 1 &gt; tracing_on$ ls -hl$ sudo echo 0 &gt; tracing_on#$ echo nop &gt; set_graph_function$ sudo cat trace# tracer: function_graph## CPU  DURATION                  FUNCTION CALLS# |     |   |                     |   |   |   | 1)               |  __arm64_sys_openat() &#123; 1)               |    do_sys_openat2() &#123; 1)   0.875 us    |      getname(); 1)   0.125 us    |      get_unused_fd_flags(); 1)   2.375 us    |      do_filp_open(); 1)   0.084 us    |      put_unused_fd(); 1)   0.125 us    |      putname(); 1)   4.083 us    |    &#125; 1)   4.250 us    |  &#125;\n\n |\n在本样例中 __arm64_sys_openat 函数的调用子流程仅包括 do_sys_openat2() 子函数，而 do_sys_openat2() 函数又调用了 getname()/get_unused_fd_flags() 等子函数。\n这种完整的子函数调用关系，对于我们学习内核源码和分析线上的问题都提供了便利，排查问题时则可以顺藤摸瓜逐步缩小需要分析的范围。\n在 perf_tools 工具集的前端工具为 funcgraph ，使用 funcgraph 启动命令如下所示：\n| \n121\n\n | \n12$./funcgraph -m 3 __arm64_sys_openat\n\n |\n如果函数调用栈比较多，直接查看跟踪记录则非常不方便，基于此社区补丁 [PATCH] ftrace: Add vim script to enable folding for function_graph traces 提供了一个基于 vim 的配置，可通过树状关系来折叠和展开函数调用的最终记录，vim 设置完整如下：\n| \n12345678910111213141516171819202122232425262728293031323334353637383940414243 1 2 3 4 5 6 7 8 9101112131415161718192021222324252627282930313233343536373839404142\n\n | \n12345678910111213141516171819202122232425262728293031323334353637383940414243&quot; Enable folding for ftrace function_graph traces.&quot;&quot; To use, :source this file while viewing a function_graph trace, or use vim&#x27;s&quot; -S option to load from the command-line together with a trace.  You can then&quot; use the usual vim fold commands, such as &quot;za&quot;, to open and close nested&quot; functions.  While closed, a fold will show the total time taken for a call,&quot; as would normally appear on the line with the closing brace.  Folded&quot; functions will not include finish_task_switch(), so folding should remain&quot; relatively sane even through a context switch.&quot;&quot; Note that this will almost certainly only work well with a&quot; single-CPU trace (e.g. trace-cmd report --cpu 1).function! FunctionGraphFoldExpr(lnum)  let line = getline(a:lnum)  if line[-1:] == &#x27;&#123;&#x27;    if line =~ &#x27;finish_task_switch() &#123;$&#x27;      return &#x27;&gt;1&#x27;    endif    return &#x27;a1&#x27;  elseif line[-1:] == &#x27;&#125;&#x27;    return &#x27;s1&#x27;  else    return &#x27;=&#x27;  endifendfunctionfunction! FunctionGraphFoldText()  let s = split(getline(v:foldstart), &#x27;|&#x27;, 1)  if getline(v:foldend+1) =~ &#x27;finish_task_switch() &#123;$&#x27;    let s[2] = &#x27; task switch  &#x27;  else    let e = split(getline(v:foldend), &#x27;|&#x27;, 1)    let s[2] = e[2]  endif  return join(s, &#x27;|&#x27;)endfunctionsetlocal foldexpr=FunctionGraphFoldExpr(v:lnum)setlocal foldtext=FunctionGraphFoldText()setlocal foldcolumn=12setlocal foldmethod=expr\n\n |\n将上述指令保存为 function-graph-fold.vim 文件，在 vim 使用时通过 -S 参数指定上述配置，就可实现按照层级展示跟踪记录。在 vim 中，可通过 za 展开，zc 折叠跟踪记录。（通过文件分析，我们需要在 cat trace 文件时候重定向到文件）。\n| \n121\n\n | \n12$ vim -S function-graph-fold.vim trace.log\n\n |\n4. 内核跟踪点（tracepoint）跟踪可基于 ftrace 跟踪内核静态跟踪点，可跟踪的完整列表可通过 available_events 查看。events 目录下查看到各分类的子目录，详见下图：\n\n| \n123456789101112131415161718192021222324252627282930313233343536 1 2 3 4 5 6 7 8 91011121314151617181920212223242526272829303132333435\n\n | \n123456789101112131415161718192021222324252627282930313233343536# available_events 文件中包括全部可用于跟踪的静态跟踪点$ sudo grep openat available_eventssyscalls:sys_exit_openat2syscalls:sys_enter_openat2syscalls:sys_exit_openatsyscalls:sys_enter_openat# 我们可以在 events/syscalls/sys_enter_openat 中查看该跟踪点相关的选项$ sudo ls -hl events/syscalls/sys_enter_openattotal 0-rw-r----- 1 root root 0 Jan  1  1970 enable  # 是否启用跟踪 1 启用-rw-r----- 1 root root 0 Jan  1  1970 filter  # 跟踪过滤-r--r----- 1 root root 0 Jan  1  1970 format  # 跟踪点格式-r--r----- 1 root root 0 Jan  1  1970 hist-r--r----- 1 root root 0 Jan  1  1970 id--w------- 1 root root 0 Jan  1  1970 inject-rw-r----- 1 root root 0 Jan  1  1970 trigger$ sudo cat events/syscalls/sys_enter_openat/formatname: sys_enter_openatID: 555format:\tfield:unsigned short common_type;\toffset:0;\tsize:2;\tsigned:0;\tfield:unsigned char common_flags;\toffset:2;\tsize:1;\tsigned:0;\tfield:unsigned char common_preempt_count;\toffset:3;\tsize:1;\tsigned:0;\tfield:int common_pid;\toffset:4;\tsize:4;\tsigned:1;\tfield:int __syscall_nr;\toffset:8;\tsize:4;\tsigned:1;\tfield:int dfd;\toffset:16;\tsize:8;\tsigned:0;\tfield:const char * filename;\toffset:24;\tsize:8;\tsigned:0;\tfield:int flags;\toffset:32;\tsize:8;\tsigned:0;\tfield:umode_t mode;\toffset:40;\tsize:8;\tsigned:0;print fmt: &quot;dfd: 0x%08lx, filename: 0x%08lx, flags: 0x%08lx, mode: 0x%08lx&quot;, ((unsigned long)(REC-&gt;dfd)), ((unsigned long)(REC-&gt;filename)), ((unsigned long)(REC-&gt;flags)), ((unsigned long)(REC-&gt;mode))\n\n |\n这里直接使用 tracepoint 跟踪 sys_openat 系统调用，设置如下：\n| \n123456789101112131415161718192021 1 2 3 4 5 6 7 8 91011121314151617181920\n\n | \n123456789101112131415161718192021$ sudo echo 1 &gt; events/syscalls/sys_enter_openat/enable$ sudo echo 1 &gt; tracing_on$ sudo cat trace# tracer: nop## entries-in-buffer/entries-written: 19/19   #P:4##                                _-----=&gt; irqs-off#                               / _----=&gt; need-resched#                              | / _---=&gt; hardirq/softirq#                              || / _--=&gt; preempt-depth#                              ||| /     delay#           TASK-PID     CPU#  ||||   TIMESTAMP  FUNCTION#              | |         |   ||||      |         |             cat-16961   [003] .... 47683.934082: sys_openat(dfd: ffffffffffffff9c, filename: ffff9abf20f0, flags: 80000, mode: 0)             cat-16961   [003] .... 47683.934326: sys_openat(dfd: ffffffffffffff9c, filename: ffff9ac09f20, flags: 80000, mode: 0)             cat-16961   [003] .... 47683.935468: sys_openat(dfd: ffffffffffffff9c, filename: ffff9ab75150, flags: 80000, mode: 0)# 关闭$ sudo echo 0 &gt; events/syscalls/sys_enter_openat/enable\n\n |\n我们通过设置 sys_enter_openat/enable 开启对于 sys_enter_openat 的跟踪，trace 文件中的跟踪记录格式与 sys_enter_openat/format 中的 print 章节的格式一致。\n| \n121\n\n | \n12print fmt: &quot;dfd: 0x%08lx, filename: 0x%08lx, flags: 0x%08lx, mode: 0x%08lx&quot; ...\n\n |\nFilter 跟踪记录条件过滤\n关于 sys_enter_openat/filter 文件为跟踪记录的过滤条件设置，格式如下：\n其中：\n\nfield 为 sys_enter_openat/format 中的字段。\noperator 为比较符\n整数支持：&#x3D;&#x3D;，!&#x3D;，&lt;&#x2F;、，&lt;&#x3D;，&gt;&#x3D; 和 &amp; ，\n字符串支持 &#x3D;&#x3D;，!&#x3D;，~ 等，其中 ~ 支持 shell 脚本中通配符 *，？，[] 等操作。\n不同的条件也支持 &amp;&amp; 和 || 进行组合。\n\n\n\n如需要通过 format 格式中的 mode 字段过滤：\n| \n121\n\n | \n12field:umode_t mode;\toffset:40;\tsize:8;\tsigned:0;\n\n |\n只需要将进行如下设置即可：\n| \n121\n\n | \n12$ sudo echo &#x27;mode != 0&#x27; &gt;  events/syscalls/sys_enter_openat/filter\n\n |\n如果需要清除 filter，直接设置为 0 即可：\n| \n121\n\n | \n12$ sudo echo 0 &gt; events/syscalls/sys_enter_openat/filter\n\n |\n5. kprobe 跟踪kprobe 为内核提供的动态跟踪机制。与第 1 节介绍的函数跟踪类似，但是 kprobe 机制允许我们跟踪函数任意位置，还可用于获取函数参数与结果返回值。使用 kprobe 机制跟踪函数须是 available_filter_functions 列表中的子集。\nkprobe 设置文件和相关文件如下所示，其中部分文件为设置 kprobe 跟踪函数后，Ftrace 自动创建：\n\nkprobe_events\n设置 kprobe 跟踪的事件属性；\n完整的设置格式如下，其中 GRP 用户可以直接定义，如果不设定默认为 kprobes：\n| \n1234123\n\n | \n1234p[:[GRP/]EVENT] [MOD:]SYM[+offs]|MEMADDR [FETCHARGS] # 设置 probe 探测点r[:[GRP/]EVENT] [MOD:]SYM[+0] [FETCHARGS] # 函数地址的返回跟踪-:[GRP/]EVENT # 删除跟踪\n\n |\n\nkprobes/&lt;GRP&gt;/&lt;EVENT&gt;/enabled\n设置后动态生成，用于控制是否启用该内核函数的跟踪；\n\nkprobes/&lt;GRP&gt;/&lt;EVENT&gt;/filter\n设置后动态生成，kprobe 函数跟踪过滤器，与上述的跟踪点 fliter 类似；\n\nkprobes/&lt;GRP&gt;/&lt;EVENT&gt;/format\n设置后动态生成，kprobe 事件显示格式；\n\nkprobe_profile\nkprobe 事件统计性能数据；\n\n\nKprobe 跟踪过程可以指定函数参数的显示格式，这里我们先给出 sys_openat 函数原型：\n| \n12312\n\n | \n123SYSCALL_DEFINE4(openat, int, dfd, const char __user *, filename, int, flags,\t\tumode_t, mode);\n\n |\n跟踪函数入口参数\n这里仍然以 __arm64_sys_openat 函数为例，演示使用 kpboe 机制进行跟踪：\n| \n123456789101112131415161718192021222324252627282930313233343536373839404142 1 2 3 4 5 6 7 8 91011121314151617181920212223242526272829303132333435363738394041\n\n | \n123456789101112131415161718192021222324252627282930313233343536373839404142# p[:[GRP/]EVENT] [MOD:]SYM[+offs]|MEMADDR [FETCHARGS]# GRP=my_grp EVENT=arm64_sys_openat  # SYM=__arm64_sys_openat# FETCHARGS = dfd=$arg1 flags=$arg3 mode=$arg4$ sudo echo &#x27;p:my_grp/arm64_sys_openat __arm64_sys_openat dfd=$arg1 flags=$arg3 mode=$arg4&#x27; &gt;&gt; kprobe_events$ sudo cat events/my_grp/arm64_sys_openat/formatname: __arm64_sys_openatID: 1475format:\tfield:unsigned short common_type;\toffset:0;\tsize:2;\tsigned:0;\tfield:unsigned char common_flags;\toffset:2;\tsize:1;\tsigned:0;\tfield:unsigned char common_preempt_count;\toffset:3;\tsize:1;\tsigned:0;\tfield:int common_pid;\toffset:4;\tsize:4;\tsigned:1;\tfield:unsigned long __probe_ip;\toffset:8;\tsize:8;\tsigned:0;print fmt: &quot;(%lx)&quot;, REC-&gt;__probe_ipevents/my_grp/arm64_sys_openat/format$ sudo echo 1 &gt; events/my_grp/arm64_sys_openat/enable# $ sudo echo 1 &gt; options/stacktrace # 启用栈$ cat trace# tracer: nop## entries-in-buffer/entries-written: 38/38   #P:4##                                _-----=&gt; irqs-off#                               / _----=&gt; need-resched#                              | / _---=&gt; hardirq/softirq#                              || / _--=&gt; preempt-depth#                              ||| /     delay#           TASK-PID     CPU#  ||||   TIMESTAMP  FUNCTION#              | |         |   ||||      |         |             cat-17025   [002] d... 52539.651096: arm64_sys_openat: (__arm64_sys_openat+0x0/0xb4) dfd=0xffff8000141cbeb0 flags=0x1bf mode=0xffff800011141778# 关闭，注意需要先 echo 0 &gt; enable 停止跟踪# 然后再使用 &quot;-:my_grp/arm64_sys_openat&quot; 停止，否则会正在使用或者忙的错误$ sudo echo 0 &gt; events/my_grp/arm64_sys_openat/enable$ sudo echo &#x27;-:my_grp/arm64_sys_openat&#x27; &gt;&gt; kprobe_events\n\n |\n跟踪函数返回值\nkprobe 可用于跟踪函数返回值，格式如下：\n| \n121\n\n | \n12r[:[GRP/]EVENT] [MOD:]SYM[+offs]|MEMADDR [FETCHARGS]\n\n |\n例如：\n| \n121\n\n | \n12$ sudo echo &#x27;r:my_grp/arm64_sys_openat __arm64_sys_openat ret=$retval&#x27; &gt;&gt; kprobe_events\n\n |\n变量 $retval 参数表示函数返回值，其他的使用格式与 kprobe 类似。\n6. uprobe 跟踪uprobe 为用户空间的动态跟踪机制，格式和使用方式与 kprobe 的方式类似，但是由于是用户态程序跟踪需要指定跟踪的二进制文件和偏移量。\n| \n1234123\n\n | \n1234p[:[GRP/]EVENT]] PATH:OFFSET [FETCHARGS]  # 跟踪函数入口r[:[GRP/]EVENT]] PATH:OFFSET [FETCHARGS]  # 跟踪函数返回值-:[GRP/]EVENT]                            # 删除跟踪点\n\n |\n这里以跟踪 /bin/bash 二进制文件中的 readline() 函数为例：\n| \n1234567891011121314151617181920212223 1 2 3 4 5 6 7 8 910111213141516171819202122\n\n | \n1234567891011121314151617181920212223$ readelf -s /bin/bash | grep -w readline   920: 00000000000d6070   208 FUNC    GLOBAL DEFAULT   13 readline$ echo &#x27;p:my_grp/readline /bin/bash:0xd6070&#x27; &gt;&gt; uprobe_events$ echo 1 &gt; events/my_grp/readline/enable  $ cat trace# tracer: nop## entries-in-buffer/entries-written: 1/1   #P:4##                                _-----=&gt; irqs-off#                               / _----=&gt; need-resched#                              | / _---=&gt; hardirq/softirq#                              || / _--=&gt; preempt-depth#                              ||| /     delay#           TASK-PID     CPU#  ||||   TIMESTAMP  FUNCTION#              | |         |   ||||      |         |            bash-14951   [003] .... 54570.055093: readline: (0xaaaab3ce6070)$ echo 0 &gt; events/my_grp/readline/enable$ echo &#x27;-:my_grp/readline&#x27; &gt;&gt; uprobe_events\n\n |\nuprobe 跟踪是跟踪用户态的函数，因此需要指定二进制文件+符号偏移量才能进行跟踪。不同系统中的二进制版本或者编译方式不同，会导致函数符号表的位置不同，因此需要跟踪前进行确认。\n7. 总结至此，我们完整介绍 Ftrace 的整体应用场景，也通过具体的设置，学习了使用的完整流程。\n实际问题排查中，考虑到效率和易用性，推荐大家这样选择：\n\n如果排查问题机器上支持 eBPF技术，首选 BCC trace 及相关工具；\n否则推荐使用 perf-tools ；\n最后的招数就是使用本文 Ftrace 的完整流程了。\n\n\n\n\n\n\n\n\n\n\n但目前基于 eBPF 的工具还未支持 function_graph 跟踪器，特定场景下还需要 ftrace 的 function_graph 跟踪器的配合。\nFtrace 与 eBPF 并非是相互替代，而是相互补充协同关系，在后续的问题排查案例中我们将看到这一点。\n参考\n高效分析 Linux 内核源码 ， 相关代码参见这里 。\nLinux kprobe 调试技术使用\nftrace 在实际问题中的应用\n《Systems Performance Enterprise and the Cloud 2nd Edition》\n\n","slug":"LINUX/问题排查利器：Linux 原生跟踪工具 Ftrace 必知必会","date":"2024-05-29T09:19:10.000Z","categories_index":"tools,LINUX","tags_index":"https,kprobe,ftrace","author_index":"dandeliono"},{"id":"c8fc4014b00b09e3a48ea3095cec49d3","title":"zhcon安装过程记录","content":"zhcon安装过程记录参考资料：\n　　1. http://www.linuxdiyf.com/viewarticle.php?id=81796\n需要下载的文件有两个：zhcon-0.2.5.tar.gz和zhcon-0.2.5-to-0.2.6.diff.gz。\n下载地址：http://sourceforge.net/project/showfiles.php?group_id&#x3D;27400将两个文件下载到&#x2F;root目录下（也可以将文件下载到喜欢的别的目录）。\n1.解压并解包zhcon-0.2.5.tar.gz文件：\n1234567apt-get update &amp;&amp; apt-get install -y build-essentialtar zxvf zhcon-0.2.5.tar.gzcd zhcon-0.2.5   zcat ../zhcon-0.2.5-to-0.2.6.diff.gz | patch -p1   ./configure make  make install\n\n　在这个文档里有几点没有说清楚：\n\n在使用make编译源代码的时候出现了很多的错误，主要原因有：一些源文件没有引入必要的头文件、缺少常量的宏定义、以及在Makefile文件中的编译依赖中缺少必要的链接包，于是又参考了下面两份资料：\n\nhttp://blog.sina.com.cn/s/blog_637fe9d80101561k.html\nLinux编程 报错 找不到 term.h和curses.h\n源程序badterm.c如下：\n12345678910111213141516171819#include &lt;stdio.h&gt;#include &lt;term.h&gt;#include &lt;curses.h&gt;#include &lt;stdlib.h&gt;int main()&#123;    setupterm(&quot;unlisted&quot;, fileno(stdout), (int *)0);    printf(&quot;Done.\\\\n&quot;);    exit(0);&#125;\n执行编译gcc -o badterm badterm.c -lcurses后报错情报如下：\n12term.h: 没有那个文件或目录  curses.h: 没有那个文件或目录\n很明显，程序找不到term.h和curses.h，那么为了安装这两个头文件，就在shell中执行sudo apt-get install libncurses5-dev，之后再执行上述编译命令即可通过。ENJOY！～\nhttp://www.linuxidc.com/Linux/2010-05/26183.htm\n\nfblinear4.cpp:75: 错误：‘memset’在此作用域中尚未声明\n\n如果SUSE 下安装zhcon出问题，这里是第一个报错的地方。我也在网上搜了一下，有人说是gcc4.3开始头文件的变更，让我想起去看看这个版本的时 间，2006-05-04，即便是那个2.6的patch也已经过时了。但毕竟用zhcon的人已经越来越少了，所以我只要把思路给出来，也不去做 patch了。在zhcon的make过程中，有很多次报错，说缺少memset，memcpy等函数的定义，是缺少#include &lt;string.h&gt;；有几次缺少abs，atoi，free等的定义，是缺少#include &lt;stdlib.h&gt;；还有报fd_set结构的定义，是缺少#include &lt;sys&#x2F;select.h&gt;；另外，缺少几个常量定义，我从以前版本的内核头文件中找到，然后写入了src&#x2F;lrmi.c中（好像就是这个 文件缺），共用到如下几个量：  \n1234#define VIF_MASK        0x00080000      /* virtual interrupt flag */  #define IF_MASK         0x00000200  #define IOPL_MASK       0x00003000  #define TF_MASK         0x00000100  \n如果有人 遇到同样的问题，希望对你有帮助。\n所有增加的内容，写入到 config.h 中 :  \n12345678include &lt;memory.h&gt;  #include &lt;stdlib.h&gt;  #include &lt;sys/select.h&gt;#define VIF_MASK        0x00080000      /* virtual interrupt flag */  #define IF_MASK         0x00000200  #define IOPL_MASK       0x00003000  #define TF_MASK         0x00000100  \n另外这些以后还会有一个文件报错。到时候添加一个 #include &lt;stdlib.h&gt; 即可。\n环境：kernel 2.6.27gcc-4.3.2\n这里讲的将这些缺少的东西写在 configure文件中，我是在每个缺少的文件中都添加上了。\n\n在参考上面的资料后出现了 undefined reference to tgetent的错误，参考http://blog.csdn.net/azhang00000/article/details/2936551 LINUX readline 库的使用,,编译代码时必须链接libreadline 与libtermcap基只有前者,,将会有如下的链接失败123456789 /usr/lib/libreadline.so: undefined reference to `tgetnum&#x27;  : /usr/lib/libreadline.so: undefined reference to `tgoto&#x27;  : /usr/lib/libreadline.so: undefined reference to `tgetflag&#x27;  : /usr/lib/libreadline.so: undefined reference to `BC&#x27;  : /usr/lib/libreadline.so: undefined reference to `tputs&#x27;  : /usr/lib/libreadline.so: undefined reference to `PC&#x27;  : /usr/lib/libreadline.so: undefined reference to `tgetent&#x27;  : /usr/lib/libreadline.so: undefined reference to `UP&#x27;  : /usr/lib/libreadline.so: undefined reference to `tgetstr&#x27;\nMakeifle (eg):12345read:   read.o          gcc -o read read.o -I/usr/include -lreadline -ltermcapread.o: read.c          gcc -c read.c -lreadline\n\n 在Makefile里增加上面的内容。 从上面的文档中可以知道这个错误是Makefile文件中没有引入 -ltermcap包导致\n参考了上面的文档，然后枯燥的添了一下午的包和头文件终于编译通过，出现了等待已久的运行界面:\n\n","slug":"LINUX/zhcon安装过程记录","date":"2024-05-20T17:35:44.000Z","categories_index":"zhcon,LINUX","tags_index":"com,http,include","author_index":"dandeliono"},{"id":"c699c53537948b0233bf4a3c00fbe8ed","title":"docker容器内执行linux的dmidecode命令","content":"docker容器内执行linux的dmidecode命令报错：[Errno 2] No such file or directory: ‘dmidecode’: ‘dmidecode’\ndocker容器内执行dmidecode命令出现\n\n原因：容器内没有dmidecode工具\n解决：将宿主机的如下两个目录挂载到容器中\n1. &#x2F;sbin&#x2F;dmidecode – 这个目录是dmidecode程序的目录，如果不挂载那么容器中识别不了dmidecode命令\n2. &#x2F;dev&#x2F;mem – dmidecode调用时会使用到mem这个文件，如果不挂载会找不到文件\n3. 还需要在启动时增加 –privileged 这个参数，让容器获得近似于宿主机root的权限\n \n 启动：\ndocker run -itd –privileged -v &#x2F;sbin&#x2F;dmidecode:&#x2F;sbin&#x2F;dmidecode -v &#x2F;dev&#x2F;mem:&#x2F;dev&#x2F;mem  -p 端口:端口 –name 容器名 镜像名:版本号 &#x2F;bin&#x2F;bash\n","slug":"OCI/docker容器内执行linux的dmidecode命令","date":"2024-05-15T11:45:44.000Z","categories_index":"dmidecode,OCI","tags_index":"docker,mem,sbin","author_index":"dandeliono"},{"id":"36b3336ca3e4df2e2f8cc5b82199f426","title":"Docker 镜像一定要瘦身成功！","content":"Docker 镜像一定要瘦身成功！# 简介容器镜像类似于虚拟机镜像，封装了程序的运行环境，保证了运行环境的一致性，使得我们可以一次创建任意场景部署运行。镜像构建的方式有两种，一种是通过 docker build 执行 Dockerfile 里的指令来构建镜像，另一种是通过 docker commit 将存在的容器打包成镜像，通常我们都是使用第一种方式来构建容器镜像。\n在构建 docker 容器时，我们一般希望尽量减小镜像，以便加快镜像的分发；但是不恰当的镜像构建方式，很容易导致镜像过大，造成带宽和磁盘资源浪费，尤其是遇到 daemonset 这种需要在每台机器上拉取镜像的服务，会造成大量资源浪费；而且镜像过大还会影响服务的启动速度，尤其是处理紧急线上镜像变更时，直接影响变更的速度。如果不是刻意控制镜像大小、注意镜像瘦身，一般的业务系统中可能 90% 以上的大镜像都存在镜像空间浪费的现象（不信可以尝试检测看看）。因此我们非常有必要了解镜像瘦身方法，减小容器镜像。\n# 如何判断镜像是否需要瘦身通常，我们可能都是在容器镜像过大，明显影响到镜像上传&#x2F;拉取速度时，才会考虑到分析镜像，尝试镜像瘦身。此时采用的多是 docker image history 等 docker 自带的镜像分析命令，以查看镜像构建历史、镜像大小在各层的分布等。然后根据经验判断是否存在空间浪费，但是这种判断方式起点较高、没有量化，不方便自动化判断。当前，社区中也有很多镜像分析工具，其中比较流行的 dive (opens new window) 分析工具，就可以量化给出_容器镜像有效率_、_镜像空间浪费率_等指标，如下图： \n采用 dive 对一个 mysql 镜像进行效率分析，发现镜像有效率只有 41%，镜像空间浪费率高达 59%，显然需要瘦身。\n# 如何进行镜像瘦身当判断一个镜像需要瘦身后，我们就需要知道如何进行镜像瘦身，下面将结合具体案例讲解一些典型的镜像瘦身方法。\n# 多阶段构建所谓多阶段构建，实际上是允许在一个 Dockerfile 中出现多个 FROM 指令。最后生成的镜像，以最后一条 FROM 构建阶段为准，之前的 FROM 构建阶段会被抛弃。通过多阶段构建，后一个阶段的构建过程可以直接利用前一阶段的构建缓存，有效降低镜像大小。一个典型的场景是将编译环境和运行环境分离，以一个 go 项目镜像构建过程为例：\n这种传统的构建方式有以下缺点：\n\n基础镜像为支持编译环境，包含大量go语言的工具&#x2F;库，而运行时并不需要\nCOPY 源码，增加了镜像分层，同时有源码泄漏风险\n\n采用多阶段构建方式，可以将上述传统的构建方式修改如下：\n可以看到，使用多阶段构建，可以获取如下好处：\n\n最终镜像只关心运行时，采用了更小的基础镜像。\n直接拷贝上一个编译阶段的编译结果，减少了镜像分层，还避免了源码泄漏。\n\n# 减少镜像分层镜像的层就像 Git 的提交（commit）一样，用于保存镜像的当前版本与上一版本之间的差异，但是镜像层会占用空间，拥有的层越多，最终的镜像就越大。在构建镜像时，RUN, ADD, COPY 指令对应的层会增加镜像大小，其他命令并不会增加最终的镜像大小。下面以实际工作中的一个案例讲解如何减少镜像分层，以减小镜像大小。\n# 背景测试项目 mysql 镜像时，遇到了容器创建比较慢的情况，我们发现主要是因为容器镜像较大，拉取镜像时间较长，所以就打算看看 mysql 镜像为什么这么大，是否可以减小容器镜像。\n# 镜像大小分析通过 docker image history 查看镜像构建历史及各层大小。\n镜像大小：2.9GB \n其相应 Dockerfile 如下：\n可以发现：Dockerfile 中存在过多分散的 RUN&#x2F;COPY 指令，而且还是大文件相关操作，导致了过多的镜像分层，使得镜像过大，可以尝试合并相关指令，以减小镜像分层。\n# 合并 RUN 指令该 Dockerfile 中 RUN 指令较多，可以将 RUN 指令合并到同一层：\n编译后镜像大小显著下降：\n镜像大小：1.92GB \n# COPY 指令转换合并到 RUN 指令从上图中可以看到，一个较大的镜像层是 COPY 指令导致的，拷贝的文件较大，所以我们考虑将 COPY 指令转换合并到 RUN 指令；具体做法是将文件上传到 oss，在 RUN 指令中下载。当然也可以发现之前还有一个 RUN 指令漏掉没有合并，需要继续合并到已有 RUN 指令中。\nRUN curl -o &#x2F;tmp&#x2F;mysql-5.7.29-1.el7.x86_64.rpm-bundle.tar https://xxx.oss.aliyuncs.com/addon-pkgs/mysql-5.7.29-1.el7.x86\\_64.rpm-bundle.tar &amp;&amp;tar -vxf &#x2F;tmp&#x2F;mysql-5.7.29-1.el7.x86_64.rpm-bundle.tar &amp;&amp;…\n编译后镜像大小显著下降：\n镜像大小： 1.27GB\n注意：此处主要是因为 COPY 指令操作的相关文件较大，对应层占用空间较多，才会将 COPY 指令转换合并到RUN 指令；如果其对应层占用空间较小，则只需分别合并 COPY 指令、RUN 指令，会更加清晰，而没必要将两者转换合并到一层。\n# 减少容器中不必要的包还是以上述 mysql 镜像为例，我们发现下载的包 mysql-5.7.29-1.el7.x86_64.rpm-bundle.tar 包含如下 rpm 包：\n\n而安装所需的 rmp 包只有：\n\n删除不必要的包，用最新的最小 rpm 压缩包替换 mysql-5.7.29-1.el7.x86_64.rpm-bundle.tar 后重新编译镜像：\n镜像大小： 1.19GB\n# 镜像分析工具前面我们通过 docker 自带的 docker image history 命令分析镜像，本节主要讲解镜像分析工具 dive (opens new window) 的使用，其主要特征如下：\n\n按层显示 Docker 镜像内容\n指出每一层的变化\n评估 “镜像的效率”，浪费的空间\n快速的构建&#x2F;分析周期\n和 CI 集成，方便自动化检测镜像效率是否合格\n\n# 镜像效率分析之前是通过 docker image history 分析镜像体积分布，并进行镜像瘦身，此处将采用 dive 分析镜像有效率。\n使用方法：dive \n# 优化前原始镜像有效率: 41%，大部分镜像体积都是浪费的。\n如下： \n# 优化后优化后镜像有效率：97%\n注意：优化后，镜像分层明显减少，镜像有效率显著提高；但是此时的镜像效率提升主要是依靠减少浪费空间获取的，如果要继续优化镜像体积，需要结合镜像体积瓶颈点评估下一步优化方向。一个通常的继续优化点是：减小基础镜像体积和不必要的包。\n如下所示： \n# 番外篇：如何通过镜像恢复 Dockerfile前面主要通过镜像分析工具分析镜像体积分布，发现浪费空间，优化镜像大小。镜像分析工具的另一个典型应用场景是：当只有容器镜像时如何通过镜像恢复 Dockerfile？\n# 镜像构建历史查看一般，我们可以通过 docker image history 查看镜像构建历史、镜像层及对应的构建指令，从而还原出对应Dockerfile。\n注意：docker image history 查看对应的构建命令可能显示不全，需要带上 –no-trunc 选项。\n这种方法有如下缺陷：\n\n一些指令信息提取不完整、不易读，如 COPY&#x2F;ADD 指令，对应的操作文件用 id 表示，如下图所示。 \n\n对于一些镜像层，不是通过 Dockerfile 指令构建出来的，而是直接通过修改容器内容，然后 docker commit 生成，不方便查看该层变更的文件。\n\n\n# 借助 dive 分析工具还原借助 dive 分析工具还原 Dockerfile，主要是因为 dive 可以指出每一层的变化，如下：\n\n可以根据 COPY 层变化内容（右侧），直观判断拷贝的文件。\n因为可以查看每一层的变化，所以对于 docker commit 也更容易分析相关操作对应的变动范围。\n\n\n# 思考# 镜像变胖的原因镜像变胖的原因很多，如：\n\n无用文件，比如编译过程中的依赖文件对编译或运行无关的指令被引入到镜像\n系统镜像冗余文件多\n各种日志文件，缓存文件\n重复编译中间文件\n重复拷贝资源文件\n运行无依赖文件\n\n但是一般情况是，用户可能对少量的镜像空间浪费不那么敏感；但是在操作大文件时，一些不当的指令（RUN&#x2F;COPY&#x2F;ADD）使用方式却很容易造成大量的空间浪费，此时尤其要注意镜像分析与镜像瘦身。\n# 镜像瘦身难吗对于基础镜像的减小、系统包的减小，将镜像体积从 200M 减小到 190M 等可能相对难些，此时需要对程序镜像非常熟悉，并结合专门的分析工具具体分析。但是一般场景下，镜像的浪费很可能仅仅是因为镜像构建命令的使用姿势不佳。此时结合本文的镜像瘦身方法，和 Dockerfile 最佳实践，一般都能实现镜像瘦身。\n# 如何评价瘦身效果（镜像效率）如果可以评价镜像的空间使用效率，一方面可以比较直观的判断哪些镜像空降浪费严重，需要瘦身；另一方面也可以对瘦身的效果进行评价。上文介绍的，镜像分析工具 dive (opens new window) 即可满足要求。\n# CI 集成如果需要对大量镜像的体积使用效率进行把关，就必须将效率检测作为自动化流程的一环，而 dive (opens new window) 就比较容易集成到 CI 中，只需执行如下指令：\nCI&#x3D;true dive &lt;image-name&gt;\n优化前 mysql 镜像执行结果：由上文可知，优化前实际效率值为 41%，由于默认效率阈值为 90%，所以执行失败。\n\n优化后镜像执行结果：效率值为 97%，由于默认效率阈值为 90%，所以执行通过。\n\n同时项目也可以根据其对镜像大小的敏感度，将镜像大小最为一个检测条件，如只有镜像大小超过 1G 时，才进行镜像效率检测，这就可以避免大量小镜像的检测，加快 CI 流程。\n# 如何自动化的检测 Dockerfile 并给出优化建议呢结合上文，ADD&#x2F;COPY&#x2F;RUN 指令对应层会增加最终镜像大小，而一般镜像的构建过程包含：文件准备、文件操作等。文件准备阶段在 ADD&#x2F;COPY&#x2F;RUN 指令中都有可能出现；文件操作阶段主要由 RUN 指令实现，如果指令过于分散，文件操作阶段会根据 写时复制 原则，拷贝一份到当前镜像层，造成空间浪费，尤其是在涉及大文件操作时。更严重的情况是，假如对文件的操作分散在不同的 RUN 指令中，不就造成了多次文件拷贝浪费了。试想一下，如果拷贝和操作在同一层进行，不就可以避免这些文件跨层拷贝了吗。\n所以有以下一些通用的优化检测方法和建议：\n\n检测 RUN 指令是否过于分散，建议合并。\n检测 COPY&#x2F;ADD 指令是否有拷贝大文件，且在 RUN 指令中有对文件进行操作，则建议将 COPY&#x2F;ADD 指令转换合并到 RUN 指令中。当然此种检测方法，仅仅只有 Dockerfile 还是不够的，还需要有上下文，才能检测相关文件的大小。\n\n","slug":"OCI/Docker 镜像一定要瘦身成功！","date":"2024-04-28T14:19:56.000Z","categories_index":"oss,OCI","tags_index":"paas,RUN,image","author_index":"dandeliono"},{"id":"382d6c325a8d429096616c76d2784a58","title":"nginx平滑重启原理","content":"nginx平滑重启原理背景对于一个高并发的网络服务来说，平滑重启功能是必不可少的。试想，如果我们直接kill然后再start一个服务进程，则会导致：\n12341. 正在处理的连接直接中断，客户端返回错误，即便通过客户端重试，仍然会对业务造成影响；2. kill后start前，中间短时间内新的请求无法建立连接；3. 对于设置了factcgi_bind或者proxy_bind的nginx，在kill后会有socket处于FIN_WAIT或者TIME_WAIT阶段，立即启动后相关接口会返回500。\n\nnginx平滑重启原理1. nginx的主要架构    nginx是一种master-worker的架构。程序启动后会解析配置文件、打开监听端口、初始化共享内存，然后根据worker_processes的配置，fork出几个woker进程，worker进程处理所有的网络请求，master进程会一直阻塞，只处理各种信号。启动流程如下：\n\n注：\n​ 1）上图的启动流程省略了很多细节，只保留跟平滑重启有关的一些函数；\n​ 2）如果在配置文件中将master_process置为0（默认为1），nginx会只有一个master进程，进程解析配置、打开端口后会进行网络处理。\n2. fork后两个进程的关系    fork是glibc提供的函数，它内部调用clone或者fork系统调用，clone或者fork系统调用会复制出一个新的进程（其实根据clone的参数，也可能复制出来的是一个线程，即对于一般用户来说的进程或者线程，都是使用同一个系统调用生成的，对于操作系统来说，进程或者线程都对应一个task_struck），新的woker进程跟master有同样的代码，并且继承了所有的文件描述符。因为在master中打开了一个socket并且bind、listen，那么在几个worker中都继承了这个listening socket，都可以进行accept。如果来了一个请求，哪个woker会accept成功呢？其实操作系统会随机选择一个进程，accept成功，其他不会，类似操作系统在listen的端口上进行了负载均衡。\n3. nginx收到reload信号后的处理逻辑    使用nginx -s reload进行平滑重启。nginx启动时会通过参数-s 发现目前要进行信号处理而不是启动nginx服务，然后他会查看nginx的pid文件，pid文件中保存有master的进程号，然后向master进行发送相应的信号，reload对应的是HUP信号，所以nginx –s reload 跟kill -1 masterpid 一样。Master收到HUP信号后的处理流程如下：\n​ 1）master解析新的配置文件。\n​ 2）master fork出新的worker进程，此时新的worker会和旧的worker共存。\n​ 3）master向旧的worker发送QUIT命令。\n​ 4）旧的worker会关闭监听端口，不再接受新的网络请求，并等待所有正在处理的请求完成后，退出。\n​ 5）此时只有新的worker存在，nginx完成了重启。\n    假设nginx监听80端口，nginx启动后，master会进行socket()，bind()，listen()三个函数，此时只有master监听80端口，然后master fork出来n个worker，此时有n+1个进程监听80端口，n个woker都会调用accept处理新来的连接。然后用户修改了nginx配置文件，并进行重启，此时master会首先fork出n个新的worker，此时有2*n+1个worker监听80端口；然后旧的n个worker会关闭listening socket，此时有n+1个监听80端口，新的请求都会落在新的worker上。从头至尾，80端口一直都处于打开状态，客户端一直都能连接。\n    在master启动新的worker后，会sleep一小段时间，然后向旧的worker发送quit消息，那sleep的功能是什么？假设，master启动新的worker后，直接向旧的worker发送quit，因为系统调度的不确定性，可能旧的worker已经关闭了listening socket，而新的worker还没有运行，这样一个很小的时间间隔中，将没有进程对listening socket 进行accept，客户端的连接请求将放在listening socket的半连接队列中，当半连接个数大于backlog时，客户端将无法连接（如果间隔t*流量qps &gt; listen的backlog时），关于listen的backlog可以查阅相关资料。\n4. 总结：nginx实现的平滑重启主要逻辑\nmaster解析配置文件，打开监听端口；worker继承监听端口，处理网络请求，master等待信号。\nmaster 处理reload信号，重新解析配置，建立新的listening socket，fork新的worker，并向旧的worker发送quit消息。\n旧的worker关闭listening socket，处理完所有连接后，退出。\n\n","slug":"MIDDLEWARE/nginx平滑重启原理","date":"2024-04-26T15:57:42.000Z","categories_index":"worker,MIDDLEWARE","tags_index":"master,nginx,fOxm","author_index":"dandeliono"},{"id":"d1455563815fa1914c63f89b230d2e22","title":"一次完整的JVM堆外内存泄漏故障排查记录","content":"一次完整的JVM堆外内存泄漏故障排查记录前言记录一次线上JVM堆外内存泄漏问题的排查过程与思路，其中夹带一些JVM内存分配机制以及常用的JVM问题排查指令和工具分享，希望对大家有所帮助。\n在整个排查过程中，我也走了不少弯路，但是在文章中我仍然会把完整的思路和想法写出来，当做一次经验教训，给后人参考，文章最后也总结了下内存泄漏问题快速排查的几个原则。\n本文的主要内容： \n\n故障描述和排查过程\n故障原因和解决方案分析\nJVM堆内内存和堆外内存分配原理\n常用的进程内存泄漏排查指令和工具介绍和使用\n\n\n\n\n\n\n\n\n\n\n文章撰写不易，请大家多多支持我的原创技术公众号：后端技术漫谈\n故障描述8月12日中午午休时间，我们商业服务收到告警，服务进程占用容器的物理内存（16G）超过了80%的阈值，并且还在不断上升。\n\n监控系统调出图表查看：\n\n像是Java进程发生了内存泄漏，而我们堆内存的限制是4G，这种大于4G快要吃满内存应该是JVM堆外内存泄漏。\n确认了下当时服务进程的启动配置：\n12-Xms4g -Xmx4g -Xmn2g -Xss1024K -XX:PermSize=256m -XX:MaxPermSize=512m -XX:ParallelGCThreads=20 -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+UseCMSCompactAtFullCollection -XX:CMSInitiatingOccupancyFraction=80\n\n虽然当天没有上线新代码，但是当天上午我们正在使用消息队列推送历史数据的修复脚本，该任务会大量调用我们服务其中的某一个接口，所以初步怀疑和该接口有关。\n下图是该调用接口当天的访问量变化：\n\n可以看到案发当时调用量相比正常情况（每分钟200+次）提高了很多（每分钟5000+次）。\n我们暂时让脚本停止发送消息，该接口调用量下降到每分钟200+次，容器内存不再以极高斜率上升，一切似乎恢复了正常。 \n接下来排查这个接口是不是发生了内存泄漏。\n排查过程首先我们先回顾下Java进程的内存分配，方便我们下面排查思路的阐述。\n以我们线上使用的JDK1.8版本为例。JVM内存分配网上有许多总结，我就不再进行二次创作。\nJVM内存区域的划分为两块：堆区和非堆区。\n\n堆区：就是我们熟知的新生代老年代。\n非堆区：非堆区如图中所示，有元数据区和直接内存。\n\n\n这里需要额外注意的是：永久代（JDK8的原生去）存放JVM运行时使用的类，永久代的对象在full GC时进行垃圾收集。 \n复习完了JVM的内存分配，让我们回到故障上来。\n堆内存分析虽说一开始就基本确认与堆内存无关，因为泄露的内存占用超过了堆内存限制4G，但是我们为了保险起见先看下堆内存有什么线索。\n我们观察了新生代和老年代内存占用曲线以及回收次数统计，和往常一样没有大问题，我们接着在事故现场的容器上dump了一份JVM堆内存的日志。\n堆内存Dump堆内存快照dump命令：\n12jmap -dump:live,format=b,file=xxxx.hprof pid\n\n\n\n\n\n\n\n\n\n\n画外音：你也可以使用jmap -histo:live pid直接查看堆内存存活的对象。\n导出后，将Dump文件下载回本地，然后可以使用Eclipse的MAT（Memory Analyzer）或者JDK自带的JVisualVM打开日志文件。\n使用MAT打开文件如图所示：\n\n可以看到堆内存中，有一些nio有关的大对象，比如正在接收消息队列消息的nioChannel，还有nio.HeapByteBuffer，但是数量不多，不能作为判断的依据，先放着观察下。 \n下一步，我开始浏览该接口代码，接口内部主要逻辑是调用集团的WCS客户端，将数据库表中数据查表后写入WCS，没有其他额外逻辑\n发觉没有什么特殊逻辑后，我开始怀疑WCS客户端封装是否存在内存泄漏，这样怀疑的理由是，WCS客户端底层是由SCF客户端封装的，作为RPC框架，其底层通讯传输协议有可能会申请直接内存。\n是不是我的代码出发了WCS客户端的Bug，导致不断地申请直接内存的调用，最终吃满内存。 \n我联系上了WCS的值班人，将我们遇到的问题和他们描述了一下，他们回复我们，会在他们本地执行下写入操作的压测，看看能不能复现我们的问题。\n既然等待他们的反馈还需要时间，我们就准备先自己琢磨下原因。\n我将怀疑的目光停留在了直接内存上，怀疑是由于接口调用量过大，客户端对nio使用不当，导致使用ByteBuffer申请过多的直接内存。 \n\n\n\n\n\n\n\n\n\n画外音：最终的结果证明，这一个先入为主的思路导致排查过程走了弯路。在问题的排查过程中，用合理的猜测来缩小排查范围是可以的，但最好先把每种可能性都列清楚，在发现自己深入某个可能性无果时，要及时回头仔细审视其他可能性。 \n沙箱环境复现为了能还原当时的故障场景，我在沙箱环境申请了一台压测机器，来确保和线上环境一致。\n首先我们先模拟内存溢出的情况（大量调用接口）： \n我们让脚本继续推送数据，调用我们的接口，我们持续观察内存占用。\n当开始调用后，内存便开始持续增长，并且看起来没有被限制住（没有因为限制触发Full GC）。\n\n接着我们来模拟下平时正常调用量的情况（正常量调用接口）： \n我们将该接口平时正常的调用量（比较小，且每10分钟进行一次批量调用）切到该压测机器上，得到了下图这样的老生代内存和物理内存趋势：\n\n\n问题来了：为何内存会不断往上走吃满内存呢？\n当时猜测是由于JVM进程并没有对于直接内存大小进行限制（-XX:MaxDirectMemorySize），所以堆外内存不断上涨，并不会触发FullGC操作。\n上图能够得出两个结论： \n\n在内存泄露的接口调用量很大的时候，如果恰好堆内老生代等其他情况一直不满足FullGC条件，就一直不会FullGC，直接内存一路上涨。\n而在平时低调用量的情况下， 内存泄漏的比较慢，FullGC总会到来，回收掉泄露的那部分，这也是平时没有出问题，正常运行了很久的原因。\n\n由于上面提到，我们进程的启动参数中并没有限制直接内存，于是我们将-XX:MaxDirectMemorySize配置加上，再次在沙箱环境进行了测验。 \n结果发现，进程占用的物理内存依然会不断上涨，超出了我们设置的限制，“看上去”配置似乎没起作用。\n这让我很讶异，难道JVM对内存的限制出现了问题？\n到了这里，能够看出我排查过程中思路执着于直接内存的泄露，一去不复返了。 \n\n\n\n\n\n\n\n\n\n画外音：我们应该相信JVM对内存的掌握，如果发现参数失效，多从自己身上找原因，看看是不是自己使用参数有误。 \n直接内存分析为了更进一步的调查清楚直接内存里有什么，我开始对直接内存下手。由于直接内存并不能像堆内存一样，很容易的看出所有占用的对象，我们需要一些命令来对直接内存进行排查，我有用了几种办法，来查看直接内存里到底出现了什么问题。\n查看进程内存信息 pmappmap - report memory map of a process(查看进程的内存映像信息)\npmap命令用于报告进程的内存映射关系，是Linux调试及运维一个很好的工具。\n12pmap -x pid 如果需要排序  | sort -n -k3**\n\n执行后我得到了下面的输出，删减输出如下:\n123456789101112131415161718192021222324252627282930..00007fa2d4000000    8660    8660    8660 rw00007fa65f12a000    8664    8664    8664 rw00007fa610000000    9840    9832    9832 rw00007fa5f75ff000   10244   10244   10244 rw00007fa6005fe000   59400   10276   10276 rw00007fa3f8000000   10468   10468   10468 rw00007fa60c000000   10480   10480   10480 rw00007fa614000000   10724   10696   10696 rw00007fa6e1c59000   13048   11228       0 r-x00007fa604000000   12140   12016   12016 rw00007fa654000000   13316   13096   13096 rw00007fa618000000   16888   16748   16748 rw00007fa624000000   37504   18756   18756 rw00007fa62c000000   53220   22368   22368 rw00007fa630000000   25128   23648   23648 rw00007fa63c000000   28044   24300   24300 rw00007fa61c000000   42376   27348   27348 rw00007fa628000000   29692   27388   27388 rw00007fa640000000   28016   28016   28016 rw00007fa620000000   28228   28216   28216 rw00007fa634000000   36096   30024   30024 rw00007fa638000000   65516   40128   40128 rw00007fa478000000   46280   46240   46240 rw0000000000f7e000   47980   47856   47856 rw00007fa67ccf0000   52288   51264   51264 rw00007fa6dc000000   65512   63264   63264 rw00007fa6cd000000   71296   68916   68916 rwx00000006c0000000 4359360 2735484 2735484 rw\n\n可以看出，最下面一行是堆内存的映射，占用4G，其他上面有非常多小的内存占用，不过通过这些信息我们依然看不出问题。\n堆外内存跟踪 NativeMemoryTracking\n\n\n\n\n\n\n\n\nNative Memory Tracking (NMT) 是Hotspot VM用来分析VM内部内存使用情况的一个功能。我们可以利用jcmd（jdk自带）这个工具来访问NMT的数据。\nNMT必须先通过VM启动参数中打开，不过要注意的是，打开NMT会带来5%-10%的性能损耗。\n12345-XX:NativeMemoryTracking=[off | summary | detail]\n\n然后运行进程，可以使用下面的命令查看直接内存：\n123456789jcmd &lt;pid&gt; VM.native_memory [summary | detail | baseline | summary.diff | detail.diff | shutdown] [scale= KB | MB | GB] \n\n我们使用：\n12jcmd pid VM.native_memory detail scale=MB &gt; temp.txt\n\n得到如图结果：\n\n\n上图中给我们的信息，都不能很明显的看出问题，至少我当时依然不能通过这几次信息看出问题。\n排查似乎陷入了僵局。\n山重水复疑无路在排查陷入停滞的时候，我们得到了来自WCS和SCF方面的回复，两方都确定了他们的封装没有内存泄漏的存在，WCS方面没有使用直接内存，而SCF虽然作为底层RPC协议，但是也不会遗留这么明显的内存bug，否则应该线上有很多反馈。\n查看JVM内存信息 jmap此时，找不到问题的我再次新开了一个沙箱容器，运行服务进程，然后运行jmap命令，看一看JVM内存的实际配置：\n12jmap -heap pid\n\n得到结果：\n12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152Attaching to process ID 1474, please wait...Debugger attached successfully.Server compiler detected.JVM version is 25.66-b17using parallel threads in the new generation.using thread-local object allocation.Concurrent Mark-Sweep GCHeap Configuration:   MinHeapFreeRatio         = 40   MaxHeapFreeRatio         = 70   MaxHeapSize              = 4294967296 (4096.0MB)   NewSize                  = 2147483648 (2048.0MB)   MaxNewSize               = 2147483648 (2048.0MB)   OldSize                  = 2147483648 (2048.0MB)   NewRatio                 = 2   SurvivorRatio            = 8   MetaspaceSize            = 21807104 (20.796875MB)   CompressedClassSpaceSize = 1073741824 (1024.0MB)   MaxMetaspaceSize         = 17592186044415 MB   G1HeapRegionSize         = 0 (0.0MB)Heap Usage:New Generation (Eden + 1 Survivor Space):   capacity = 1932787712 (1843.25MB)   used     = 1698208480 (1619.5378112792969MB)   free     = 234579232 (223.71218872070312MB)   87.86316621615607% usedEden Space:   capacity = 1718091776 (1638.5MB)   used     = 1690833680 (1612.504653930664MB)   free     = 27258096 (25.995346069335938MB)   98.41346682518548% usedFrom Space:   capacity = 214695936 (204.75MB)   used     = 7374800 (7.0331573486328125MB)   free     = 207321136 (197.7168426513672MB)   3.4349974840697497% usedTo Space:   capacity = 214695936 (204.75MB)   used     = 0 (0.0MB)   free     = 214695936 (204.75MB)   0.0% usedconcurrent mark-sweep generation:   capacity = 2147483648 (2048.0MB)   used     = 322602776 (307.6579818725586MB)   free     = 1824880872 (1740.3420181274414MB)   15.022362396121025% used29425 interned Strings occupying 3202824 bytes\n\n输出的信息中，看得出老年代和新生代都蛮正常的，元空间也只占用了20M，直接内存看起来也是2g…\n嗯？为什么MaxMetaspaceSize = 17592186044415 MB？看起来就和没限制一样。\n再仔细看看我们的启动参数：\n12-Xms4g -Xmx4g -Xmn2g -Xss1024K -XX:PermSize=256m -XX:MaxPermSize=512m -XX:ParallelGCThreads=20 -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+UseCMSCompactAtFullCollection -XX:CMSInitiatingOccupancyFraction=80\n\n配置的是-XX:PermSize=256m -XX:MaxPermSize=512m，也就是永久代的内存空间。而1.8后，Hotspot虚拟机已经移除了永久代，使用了元空间代替。  由于我们线上使用的是JDK1.8，所以我们对于元空间的最大容量根本就没有做限制，-XX:PermSize=256m -XX:MaxPermSize=512m 这两个参数对于1.8就是过期的参数。\n下面的图描述了从1.7到1.8，永久代的变更：\n\n那会不会是元空间内存泄露了呢？\n我选择了在本地进行测试，方便更改参数，也方便使用JVisualVM工具直观的看出内存变化。\n使用JVisualVM观察进程运行首先限制住元空间，使用参数-XX:MetaspaceSize=64m -XX:MaxMetaspaceSize=128m，然后在本地循环调用出问题的接口。\n得到如图：\n\n可以看出，在元空间耗尽时，系统出发了Full GC，元空间内存得到回收，并且卸载了很多类。 \n然后我们将元空间限制去掉，也就是使用之前出问题的参数：\n12-Xms4g -Xmx4g -Xmn2g -Xss1024K -XX:ParallelGCThreads=20 -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+UseCMSCompactAtFullCollection -XX:CMSInitiatingOccupancyFraction=80 -XX:MaxDirectMemorySize=2g -XX:+UnlockDiagnosticVMOptions\n\n得到如图：\n\n可以看出，元空间在不断上涨，并且已装入的类随着调用量的增加也在不断上涨，呈现正相关趋势。 \n柳暗花明又一村问题一下子明朗了起来，随着每次接口的调用，极有可能是某个类都在不断的被创建，占用了元空间的内存。\n观察JVM类加载情况 -verbose\n\n\n\n\n\n\n\n\n在调试程序时，有时需要查看程序加载的类、内存回收情况、调用的本地接口等。这时候就需要-verbose命令。在myeclipse可以通过右键设置（如下），也可以在命令行输入java -verbose来查看。\n1234-verbose:class 查看类加载情况-verbose:gc 查看虚拟机中内存回收情况-verbose:jni 查看本地方法调用的情况\n\n我们在本地环境，添加启动参数-verbose:class循环调用接口。\n可以看到生成了无数com.alibaba.fastjson.serializer.ASMSerializer_1_WlkCustomerDto:\n1234567[Loaded com.alibaba.fastjson.serializer.ASMSerializer_1_WlkCustomerDto from file:/C:/Users/yangzhendong01/.m2/repository/com/alibaba/fastjson/1.2.71/fastjson-1.2.71.jar][Loaded com.alibaba.fastjson.serializer.ASMSerializer_1_WlkCustomerDto from file:/C:/Users/yangzhendong01/.m2/repository/com/alibaba/fastjson/1.2.71/fastjson-1.2.71.jar][Loaded com.alibaba.fastjson.serializer.ASMSerializer_1_WlkCustomerDto from file:/C:/Users/yangzhendong01/.m2/repository/com/alibaba/fastjson/1.2.71/fastjson-1.2.71.jar][Loaded com.alibaba.fastjson.serializer.ASMSerializer_1_WlkCustomerDto from file:/C:/Users/yangzhendong01/.m2/repository/com/alibaba/fastjson/1.2.71/fastjson-1.2.71.jar][Loaded com.alibaba.fastjson.serializer.ASMSerializer_1_WlkCustomerDto from file:/C:/Users/yangzhendong01/.m2/repository/com/alibaba/fastjson/1.2.71/fastjson-1.2.71.jar][Loaded com.alibaba.fastjson.serializer.ASMSerializer_1_WlkCustomerDto from file:/C:/Users/yangzhendong01/.m2/repository/com/alibaba/fastjson/1.2.71/fastjson-1.2.71.jar]\n\n当调用了很多次，积攒了一定的类时，我们手动执行Full GC，进行类加载器的回收，我们发现大量的fastjson相关类被回收。\n\n如果在回收前，使用jmap查看类加载情况，同样也可以发现大量的fastjson相关类： \n12jmap -clstats 7984\n\n\n这下有了方向，这次仔细排查代码，查看代码逻辑里哪里用到了fastjson，发现了如下代码：\n1234567891011public static String buildData(Object bean) &#123;    try &#123;        SerializeConfig CONFIG = new SerializeConfig();        CONFIG.propertyNamingStrategy = PropertyNamingStrategy.SnakeCase;        return jsonString = JSON.toJSONString(bean, CONFIG);    &#125; catch (Exception e) &#123;        return null;    &#125;&#125;\n\n问题根因我们在调用wcs前将驼峰字段的实体类序列化成下划线字段，**这需要使用fastjson的SerializeConfig，而我们在静态方法中对其进行了实例化。SerializeConfig创建时默认会创建一个ASM代理类用来实现对目标对象的序列化。也就是上面被频繁创建的类com.alibaba.fastjson.serializer.ASMSerializer_1_WlkCustomerDto,如果我们复用SerializeConfig，fastjson会去寻找已经创建的代理类，从而复用。但是如果new SerializeConfig()，则找不到原来生成的代理类，就会一直去生成新的WlkCustomerDto代理类。\n下面两张图时问题定位的源码：\n\n\n我们将SerializeConfig作为类的静态变量，问题得到了解决。\n123456private static final SerializeConfig CONFIG = new SerializeConfig();static &#123;    CONFIG.propertyNamingStrategy = PropertyNamingStrategy.SnakeCase;&#125;\n\nfastjson SerializeConfig 做了什么SerializeConfig介绍：\n\n\n\n\n\n\n\n\n\nSerializeConfig的主要功能是配置并记录每种Java类型对应的序列化类（ObjectSerializer接口的实现类），比如Boolean.class使用BooleanCodec（看命名就知道该类将序列化和反序列化实现写到一起了）作为序列化实现类，float[].class使用FloatArraySerializer作为序列化实现类。这些序列化实现类，有的是FastJSON中默认实现的（比如Java基本类），有的是通过ASM框架生成的（比如用户自定义类），有的甚至是用户自定义的序列化类（比如Date类型框架默认实现是转为毫秒，应用需要转为秒）。当然，这就涉及到是使用ASM生成序列化类还是使用JavaBean的序列化类类序列化的问题，这里判断根据就是是否Android环境（环境变量”java.vm.name”为”dalvik”或”lemur”就是Android环境），但判断不仅这里一处，后续还有更具体的判断。\n理论上来说，每个SerializeConfig实例若序列化相同的类，都会找到之前生成的该类的代理类，来进行序列化。们的服务在每次接口被调用时，都实例化一个ParseConfig对象来配置Fastjson反序列的设置，而未禁用ASM代理的情况下，由于每次调用ParseConfig都是一个新的实例，因此永远也检查不到已经创建的代理类，所以Fastjson便不断的创建新的代理类，并加载到metaspace中，最终导致metaspace不断扩张，将机器的内存耗尽。\n升级JDK1.8才会出现问题导致问题发生的原因还是值得重视。为什么在升级之前不会出现这个问题？这就要分析jdk1.8和1.7自带的hotspot虚拟机的差异了。\n\n\n\n\n\n\n\n\n\n从jdk1.8开始，自带的hostspot虚拟机取消了过去的永久区，而新增了metaspace区，从功能上看，metaspace可以认为和永久区类似，其最主要的功用也是存放类元数据，但实际的机制则有较大的不同。\n首先，metaspace默认的最大值是整个机器的物理内存大小，所以metaspace不断扩张会导致java程序侵占系统可用内存，最终系统没有可用的内存；而永久区则有固定的默认大小，不会扩张到整个机器的可用内存。当分配的内存耗尽时，两者均会触发full gc，但不同的是永久区在full gc时，以堆内存回收时类似的机制去回收永久区中的类元数据（Class对象），只要是根引用无法到达的对象就可以回收掉，而metaspace判断类元数据是否可以回收，是根据加载这些类元数据的Classloader是否可以回收来判断的，只要Classloader不能回收，通过其加载的类元数据就不会被回收。这也就解释了我们这两个服务为什么在升级到1.8之后才出现问题，因为在之前的jdk版本中，虽然每次调用fastjson都创建了很多代理类，在永久区中加载类很多代理类的Class实例，但这些Class实例都是在方法调用是创建的，调用完成之后就不可达了，因此永久区内存满了触发full gc时，都会被回收掉。\n而使用1.8时，因为这些代理类都是通过主线程的Classloader加载的，这个Classloader在程序运行的过程中永远也不会被回收，因此通过其加载的这些代理类也永远不会被回收，这就导致metaspace不断扩张，最终耗尽机器的内存了。\n这个问题并不局限于fastjson，只要是需要通过程序加载创建类的地方，就有可能出现这种问题。尤其是在框架中，往往大量采用类似ASM、javassist等工具进行字节码增强，而根据上面的分析，在jdk1.8之前，因为大多数情况下动态加载的Class都能够在full gc时得到回收，因此不容易出现问题，也因此很多框架、工具包并没有针对这个问题做一些处理，一旦升级到1.8之后，这些问题就可能会暴露出来。\n总结问题解决了，接下来复盘下整个排查问题的流程，整个流程暴露了我很多问题，最主要的就是对于JVM不同版本的内存分配还不够熟悉，导致了对于老生代和元空间判断失误，走了很多弯路，在直接内存中排查了很久，浪费了很多时间。\n其次，排查需要的一是仔细，二是全面，，最好将所有可能性先行整理好，不然很容易陷入自己设定好的排查范围内，走进死胡同不出来。\n最后，总结一下这次的问题带来的收获：\n\nJDK1.8开始，自带的hostspot虚拟机取消了过去的永久区，而新增了metaspace区，从功能上看，metaspace可以认为和永久区类似，其最主要的功用也是存放类元数据，但实际的机制则有较大的不同。\n对于JVM里面的内存需要在启动时进行限制，包括我们熟悉的堆内存，也要包括直接内存和元生区，这是保证线上服务正常运行最后的兜底。\n使用类库，请多注意代码的写法，尽量不要出现明显的内存泄漏。\n对于使用了ASM等字节码增强工具的类库，在使用他们时请多加小心（尤其是JDK1.8以后）。\n\n\n\n\n\n\n\n\n\n\n文章撰写不易，请大家多多支持我的原创技术公众号：后端技术漫谈\n参考观察程序运行时类加载的过程\nblog.csdn.net&#x2F;tenderhearted&#x2F;article&#x2F;details&#x2F;39642275\nMetaspace整体介绍（永久代被替换原因、元空间特点、元空间内存查看分析方法）\nhttps://www.cnblogs.com/duanxz/p/3520829.html\njava内存占用异常问题常见排查流程（含堆外内存异常）\nhttps://my.oschina.net/haitaohu/blog/3024843\nJVM源码分析之堆外内存完全解读\nhttp://lovestblog.cn/blog/2015/05/12/direct-buffer/\nJVM 类的卸载\nhttps://www.cnblogs.com/caoxb/p/12735525.html\nfastjson在jdk1.8上面开启asm\nhttps://github.com/alibaba/fastjson/issues/385\nfastjson：PropertyNamingStrategy_cn\nhttps://github.com/alibaba/fastjson/wiki/PropertyNamingStrategy_cn\n警惕动态代理导致的Metaspace内存泄漏问题\nhttps://blog.csdn.net/xyghehehehe/article/details/78820135\n","slug":"JAVA/一次完整的JVM堆外内存泄漏故障排查记录","date":"2024-04-03T11:02:02.000Z","categories_index":"https,JAVA","tags_index":"com,JVM,gitee","author_index":"dandeliono"},{"id":"ba2c98fe76e98638ad19f6c9f02b695c","title":"linux - 深入浅出TCP中的SYN-Cookies","content":"linux - 深入浅出TCP中的SYN-Cookies\n\n\n\n\n\n\n\n\n\n本文渐进地介绍TCP中的syn-cookie技术，包括其由来、原理、实例测试。\nSYN Flood 攻击TCP连接建立时，客户端通过发送SYN报文发起向处于监听状态的服务器发起连接，服务器为该连接分配一定的资源，并发送SYN+ACK报文。对服务器来说，此时该连接的状态称为半连接(Half-Open)，而当其之后收到客户端回复的ACK报文后，连接才算建立完成。在这个过程中，如果服务器一直没有收到ACK报文(比如在链路中丢失了)，服务器会在超时后重传SYN+ACK。\n如果经过多次超时重传后，还没有收到, 那么服务器会回收资源并关闭半连接，仿佛之前最初的SYN报文从来没到过一样！\n\n这看上一切正常，但是如果有坏人故意大量不断发送伪造的SYN报文，那么服务器就会分配大量注定无用的资源，并且从backlog的意义 中可知，服务器能保存的半连接的数量是有限的！所以当服务器受到大量攻击报文时，它就不能再接收正常的连接了。换句话说，它的服务不再可用了！这就是SYN Flood攻击的原理，它是一种典型的DDoS攻击。\n连接请求的关键信息Syn-Flood攻击成立的关键在于服务器资源是有限的，而服务器收到请求会分配资源。通常来说，服务器用这些资源保存此次请求的关键信息，包括请求的来源和目(五元组)，以及TCP选项，如最大报文段长度MSS、时间戳timestamp、选择应答使能Sack、窗口缩放因子Wscale等等。当后续的ACK报文到达，三次握手完成，新的连接创建，这些信息可以会被复制到连接结构中，用来指导后续的报文收发。\n那么现在的问题就是服务器如何在不分配资源的情况下\n\n验证之后可能到达的ACK的有效性，保证这是一次完整的握手\n获得SYN报文中携带的TCP选项信息\n\nSYN cookies 算法SYN Cookies算法wiki可以解决上面的第1个问题以及第2个问题的一部分\n我们知道，TCP连接建立时，双方的起始报文序号是可以任意的。SYN cookies利用这一点，按照以下规则构造初始序列号：\n\n设t为一个缓慢增长的时间戳(典型实现是每64s递增一次)\n设m为客户端发送的SYN报文中的MSS选项值\n设s是连接的元组信息(源IP,目的IP,源端口，目的端口)和t经过密码学运算后的Hash值，即s = hash(sip,dip,sport,dport,t)，s的结果取低 24 位\n\n则初始序列号n为：\n\n高 5 位为t mod 32\n接下来3位为m的编码值\n低 24 位为s\n\n当客户端收到此SYN+ACK报文后，根据TCP标准，它会回复ACK报文，且报文中ack = n + 1，那么在服务器收到它时，将ack - 1就可以拿回当初发送的SYN+ACK报文中的序号了！服务器巧妙地通过这种方式间接保存了一部分SYN报文的信息。\n接下来，服务器需要对ack - 1这个序号进行检查：\n\n将高 5 位表示的t与当前之间比较，看其到达地时间是否能接受。\n根据t和连接元组重新计算s，看是否和低 24 一致，若不一致，说明这个报文是被伪造的。\n解码序号中隐藏的mss信息\n\n到此，连接就可以顺利建立了。\nSYN Cookies 缺点既然SYN Cookies可以减小资源分配环节，那为什么没有被纳入TCP标准呢？原因是SYN Cookies也是有代价的：\n\nMSS的编码只有3位，因此最多只能使用 8 种MSS值\n服务器必须拒绝客户端SYN报文中的其他只在SYN和SYN+ACK中协商的选项，原因是服务器没有地方可以保存这些选项，比如Wscale和SACK\n增加了密码学运算\n\nLinux 中的 SYN CookiesLinux上的SYN Cookies实现与wiki中描述的算法在序号生成上有一些区别，其SYN+ACK的序号通过下面的公式进行计算：\n\n\n\n\n\n\n\n\n\n内核编译需要打开 CONFIG_SYN_COOKIES\n1seq = hash(saddr, daddr, sport, dport, 0, 0) \\+ req.th.seq + t &lt;&lt; 24 \\+ (hash(saddr, daddr, sport, dport, t, 1) \\+ mss_ind) &amp; 0x00FFFFFF\n\n其中，req.th.seq表示客户端的SYN报文中的序号，mss_ind是客户端通告的MSS值得编码，它的取值在比较新的内核中有 4 种(老的内核有 8 种), 分别对应以下 4 种值\n123456static __u16 const msstab\\[\\] = &#123; 536, 1300, 1440,     1460,&#125;\n\n感兴趣的可以顺着以下轨迹浏览调用顺序\n12345tcp\\_conn\\_request  |\\-\\- cookie\\_init\\_sequence     |\\-\\- cookie\\_v4\\_init_sequence        |\\-\\- \\_\\_cookie\\_v4\\_init\\_sequence           |\\-\\- secure\\_tcp\\_syn_cookie\n\nSYN Cookies 与时间戳如果服务器和客户端都打开了时间戳选项，那么服务器可以将客户端在SYN报文中携带了TCP选项的使能情况暂时保存在时间戳中。当前使用了低 6 位，分别保存Wscale、SACK和ECN。\n\n客户端会在ACK的TSecr字段，把这些值带回来。\n实验\n\n\n\n\n\n\n\n\nLinux中的/proc/sys/net/ipv4/tcp_syncookies是内核中的SYN Cookies开关,0表示关闭SYN Cookies；1表示在新连接压力比较大时启用SYN Cookies,2表示始终使用SYN Cookies。\n本实验是在4.4.0内核运行的，服务端监听50001端口，backlog参数为3(该参数意义)。同时，模拟不同的客户端注入SYN报文。\n测试代码\n不开启 SYN Cookies1echo 0 \\&gt; /proc/sys/net/ipv4/tcp_syncookies\n\n可以看到，在收到3个SYN报文后，服务器不再响应新的连接请求了，这也就是SYN-Flood的攻击方式。\n有条件使用 SYN Cookies1echo 1 \\&gt; /proc/sys/net/ipv4/tcp_syncookies\n\n\n由于服务器的backlog参数为3，因此图中的从第4个SYN+ACK(#8报文)开始使用SYN Cookies。\n从时间戳可以看出，**#8**报文(44167748)比 #6号报文(44167796)还要小。\n144167748 = 0x2A1F244 ,最后低6位是 0b000100 ,与SYN报文中 wscale = 4 是相符的\n\n小结SYN Cookie技术可以让服务器在收到客户端的SYN报文时，不分配资源保存客户端信息，而是将这些信息保存在SYN+ACK的初始序号和时间戳中。对正常的连接，这些信息会随着ACK报文被带回来。\n","slug":"LINUX/linux - 深入浅出TCP中的SYN-Cookies","date":"2024-04-03T10:36:59.000Z","categories_index":"SYN,LINUX","tags_index":"https,Cookies,ACK","author_index":"dandeliono"},{"id":"5d97a4bdf6449b8cb573c2f758d846ee","title":"深入理解堆外内存 Metaspace","content":"深入理解堆外内存 Metaspace在之前介绍的分代垃圾回收算法中，我们一直有一个永久代存在，叫 PermGen，内存上它是挨着堆的。为了垃圾回收方便，HotSpot 在永久代上一直是使用老年代的垃圾回收算法。\n永久代主要存放以下数据：\n\nJVM internal representation of classes and their metadata\nClass statics\nInterned strings\n\n从 JDK7 开始，JDK 开发者们就有消灭永久代的打算了。有部分数据移到永久代之外了：\n\nSymbols &#x3D;&gt; native memory\nInterned strings &#x3D;&gt; Java Heap\nClass statics &#x3D;&gt; Java Heap\n\n到了 JDK8，这个工作终于完成了，彻底废弃了 PermGen，Metaspace 取而代之。\n本文的内容主要是翻译 Thomas Stüfe 的 Metaspace 系列文章，他是 OpenJDK Committer&#x2F;Reviewer. JVM developer at SAP，一看 Title 就很靠谱，因为他是 JVM 开发者，当然主要是内容也写得非常棒。\n当然了，我不是一字一句翻译，文中会删掉部分累赘的内容，讲清楚就可以了。同时，原文第五篇是介绍使用 jcmd 工具观察 Metaspace 的空间使用情况，这一节我觉得没有必要介绍，所以没有加进来。\n1、什么是 MetaspaceMetaspace 区域位于堆外，所以它的最大内存大小取决于系统内存，而不是堆大小，我们可以指定 MaxMetaspaceSize 参数来限定它的最大内存。\nMetaspace 是用来存放 class metadata 的，class metadata 用于记录一个 Java 类在 JVM 中的信息，包括但不限于 JVM class file format 的运行时数据：\n1、Klass 结构，这个非常重要，把它理解为一个 Java 类在虚拟机内部的表示吧；\n2、method metadata，包括方法的字节码、局部变量表、异常表、参数信息等；\n3、常量池；\n4、注解；\n5、方法计数器，记录方法被执行的次数，用来辅助 JIT 决策；\n6、 其他\n虽然每个 Java 类都关联了一个 java.lang.Class 的实例，而且它是一个贮存在堆中的 Java 对象。但是类的 class metadata 不是一个 Java 对象，它不在堆中，而是在 Metaspace 中。\n什么时候分配 Metaspace 空间当一个类被加载时，它的类加载器会负责在 Metaspace 中分配空间用于存放这个类的元数据。\n\n上面这个示意图非常简单，可以看到在 Id 这个类加载器第一次加载类 X 和 Y 的时候，在 Metaspace 中为它们开辟空间存放元信息。\n什么时候回收 Metaspace 空间分配给一个类的空间，是归属于这个类的类加载器的，只有当这个类加载器卸载的时候，这个空间才会被释放。\n所以，只有当这个类加载器加载的所有类都没有存活的对象，并且没有到达这些类和类加载器的引用时，相应的 Metaspace 空间才会被 GC 释放。看下图：\n\n所以，一个 Java 类在 Metaspace 中占用的空间，它是否释放，取决于这个类的类加载器是否被卸载。\n内存通常会被保留释放 Metaspace 的空间，并不意味着将这部分空间还给系统内存，这部分空间通常会被 JVM 保留下来。\n这部分被保留的空间有多大，取决于 Metaspace 的碎片化程度。另外，Metaspace 中有一部分区域 Compressed Class Space 是一定不会还给操作系统的。\n\n\n\n\n\n\n\n\n\n这里先了解概念，后面都会展开来说。\n配置 Metaspace 空间我们只需要关心两个配置参数：\n\n-XX:MaxMetaspaceSize：Metaspace 总空间的最大允许使用内存，默认是不限制。\n\n-XX:CompressedClassSpaceSize：Metaspace 中的 Compressed Class Space 的最大允许内存，默认值是 1G，这部分会在 JVM 启动的时候向操作系统申请 1G 的虚拟地址映射，但不是真的就用了操作系统的 1G 内存。\n\n\nMetaspace 和 GCMetaspace 只在 GC 运行并且卸载类加载器的时候才会释放空间。当然，在某些时候，需要主动触发 GC 来回收一些没用的 class metadata，即使这个时候对于堆空间来说，还达不到 GC 的条件。\nMetaspace 可能在两种情况下触发 GC：\n1、分配空间时：虚拟机维护了一个阈值，如果 Metaspace 的空间大小超过了这个阈值，那么在新的空间分配申请时，虚拟机首先会通过收集可以卸载的类加载器来达到复用空间的目的，而不是扩大 Metaspace 的空间，这个时候会触发 GC。这个阈值会上下调整，和 Metaspace 已经占用的操作系统内存保持一个距离。\n2、碰到 Metaspace OOM：Metaspace 的总使用空间达到了 MaxMetaspaceSize 设置的阈值，或者 Compressed Class Space 被使用光了，如果这次 GC 真的通过卸载类加载器腾出了很多的空间，这很好，否则的话，我们会进入一个糟糕的 GC 周期，即使我们有足够的堆内存。\n\n\n\n\n\n\n\n\n\n所以大家千万不要把 MaxMetaspaceSize 设置得太小。\n2、Metaspace 的架构这一节将深入到 Metaspace 的架构实现，将描述它的每一层和每一个组件，以及它们是怎么工作的。\n对于开发者来说，这一定是非常有趣的一件事情，我们大部分开发者都不可能去开发 JDK，但是了解这些总是充满着乐趣。\nMetaspace 在实现上分为多层。最底层，负责向操作系统申请大块的内存；中间的一层，负责分出一小块一小块给每个类加载器；最顶层，类加载器负责把这些申请到的内存块用来存放 class metadata。\n最底层：the space list在最底层，JVM 通过 mmap(3) 接口向操作系统申请内存映射，在 64 位平台上，每次申请 2MB 空间。\n\n\n\n\n\n\n\n\n\n当然，这里的 2MB 不是真的就消耗了主存的 2MB，只有之后在使用的时候才会真的消耗内存。这里是虚拟内存映射。\n每次申请过来的内存区域，放到一个链表中 VirtualSpaceList，作为其中的一个 Node。看下图。\n一个 Node 是 2MB 的空间，前面说了在使用的时候再向操作系统申请实际的内存，但是频繁的系统调用会降低性能，所以 Node 内部需要维护一个水位线，当 Node 内已使用内存快达到水位线的时候，向操作系统要新的内存页。并且相应地提高水位线。\n直到一个 Node 被完全用完，会分配一个新的 Node，并且将其加入到链表中，老的 Node 就 “退休” 了。下图中，前面的三个 Node 就是退休状态了。\n从一个 Node 中分配内存，每一块称为 MetaChunk，chunk 有三种规格，在 64 位系统中分别为 1K、4K、64K。\n\n链表 VirtualSpaceList 和每个节点 Node 是全局的，而 Node 内部的一个个 MetaChunk 是分配给每个类加载器的。所以一个 Node 通常由分配给多个类加载器的 chunks 组成。\n\n当一个类加载器和它加载的所有的类都卸载的时候，它占用的 chunks 就会加入到一个全局的空闲列表中：ChunkManager，看下图：\n\n这些 chunks 会被复用：如果其他的类加载器加载新的类，它可能就会得到一个空闲列表中的 chunk，而不是去 Node 中申请一个新的 chunk。\n\n\n\n\n\n\n\n\n\n\n后面会说到，如果刚好把整个 Node 都清空了，那么这整个 Node 的内存会直接还给操作系统。\n当然，由这个 Node 进入到空闲列表的节点也要删除。\n中间层：Metachunk通常一个类加载器在申请 Metaspace 空间用来存放 metadata 的时候，也就需要几十到几百个字节，但是它会得到一个 Metachunk，一个比要求的内存大得多的内存块。\n为什么？因为前面说了，要从全局的 VirtualSpaceList 链表的 Node 中分配内存是昂贵的操作，需要加锁。我们不希望这个操作太频繁，所以一次性给一个大的 MetaChunk，以便于这个类加载器之后加载其他的类，这样就可以做到多个类加载器并发分配了。只有当这个 chunk 用完了，类加载器才需要又去 VirtualSpaceList 申请新的 chunk。\n前面说了，chunk 有三种规格，那 Metaspace 的分配器怎么知道一个类加载器每次要多大的 chunk 呢？这当然是基于猜测的：\n\n通常，一个标准的类加载器在第一次申请空间时，会得到一个 4K 的 chunk，直到它达到了一个随意设置的阈值（4），此时分配器失去了耐心，之后会一次性给它一个 64K 的大 chunk。\nbootstrap classloader 是一个公认的会加载大量的类的加载器，所以分配器会给它一个巨大的 chunk，一开始就会给它 4M。可以通过 InitialBootClassLoaderMetaspaceSize 进行调优。\n反射类类加载器 (jdk.internal.reflect.DelegatingClassLoader) 和匿名类类加载器只会加载一个类，所以一开始只会给它们一个非常小的 chunk（1K），因为给它们太多就是一种浪费。\n\n类加载器申请空间的时候，每次都给类加载器一个 chunk，这种优化，是建立在假设它们立马就会需要新的空间的基础上的。这种假设可能正确也可能错误，可能在拿到一个很大的 chunk 后，这个类加载器恰巧就不再需要加载新的类了。\n\n\n\n\n\n\n\n\n\n对于这部分可能的空间浪费，可以在后面介绍的系统工具中观察到。\n最顶层：Metablock在 Metachunk 上，我们有一个二级分配器（class-loader-local allocator），它将一个 Metachunk 分割成一个个小的单元，这些小的单元称为 Metablock，它们是实际分配给每个调用者的。\n这个二级分配器非常原始，它的速度也非常快：\n前面说过，class metadata 的生命周期是和类加载器绑定的，所以在类加载器卸载的时候，JVM 可以大块大块地释放这些空间。\n下面展示一个 Metachunk 的结构：\n\n这个 chunk 诞生的时候，它只有一个 header，之后的分配都只要在顶部进行分配就行。\n由于这个 chunk 是归属于一个类加载器的，所以如果它不再加载新的类，那么 unused 空间就将真的浪费掉。\nClassloaderData and ClassLoaderMetaspace在 JVM 内部，一个类加载器以一个 ClassLoaderData 结构标识，这个结构引用了一个 ClassLoaderMetaspace 结构，它维护了该加载器使用的所有的 Metachunk。\n当这个类加载器被卸载的时候，这个 ClassLoaderData 和 ClassLoaderMetaspace 会被删除。并且会将所有的这个加载器用到的 chunks 归还到空闲列表中。这部分内存是否可以直接归还给操作系统取决于是否满足其他条件，后面会介绍。\n\n\n\n\n\n\n\n\n\n就是前面提过的，如果恰好把整个 Node 都清空了，那么这个 Node 的内存直接还给操作系统\n匿名类ClassloaderData != ClassLoaderMetaspace\n注意，我们前面说，“Metaspace 内存是属于类加载器的”，但是，这里其实撒了一个小谎，如果将匿名类考虑进去，那就更加复杂了：\n当类加载器加载一个匿名类时，这个类有自己独立的 ClassLoaderData，它的生命周期是跟随着这个匿名类的，而不是这个类加载器（所以，和它相关的空间可以在类加载器卸载前得到释放）。所以，一个类加载器有一个主要的 ClassLoaderData 结构用来服务所有的正常的类，对于每一个匿名类，还有一个二级的 ClassLoaderData 结构来维护。\n这样做的目的之一，其实就是没有必要扩大大量的 Lambdas 和 method handlers 在 Metaspace 中的空间的生命周期。\n\n内存什么时候会还给操作系统当一个 VirtualSpaceListNode 中的所有 chunk 都是空闲的时候，这个 Node 就会从链表 VirtualSpaceList 中移除，它的 chunks 也会从空闲列表中移除，这个 Node 就没有被使用了，会将其内存归还给操作系统。\n\n对于一个空闲的 Node 来说，拥有其上面的 chunks 的所有的类加载器必然都是被卸载了的。\n至于这个情况是否可能发生，主要就是取决于碎片化：\n一个 Node 是 2M，chunks 的大小为 1K, 4K 或 64K，所以通常一个 Node 上有约 150-200 个 chunks，如果这些 chunks 全部由同一个类加载器拥有，回收这个类加载器就可以一次性回收这个 Node，并且把它的空间还给操作系统。\n但是，如果这些 chunks 分配给不同的类加载器，每个类加载器都有不同的生命周期，那么什么都不会被释放。这也许就是在告诉我们，要小心对待大量的小的类加载器，如那些负责加载匿名类或反射类的加载器。\n同时也要清楚，Metaspace 中的 Compressed Class Space 是永远不会将内存还给操作系统的。我们马上就要介绍这部分内容了。\n本节小结\n每次向操作系统申请 2M 的虚拟空间映射，放置到全局链表中，待需要使用的时候申请内存。\n一个 Node 会分割为一个个的 chunks，分配给类加载器，一个 chunk 属于一个类加载器。\nchunk 再细分为一个个 Metablock，这是分配给调用者的最小单元。\n当一个类加载器被卸载，它占有的 chunks 会进入到空闲列表，以便复用，如果运气好的话，有可能会直接把内存归还给操作系统。\n\n3、什么是 Compressed Class Space在 64 位平台上，HotSpot 使用了两个压缩优化技术，Compressed Object Pointers (“CompressedOops”) 和 Compressed Class Pointers。\n压缩指针，指的是在 64 位的机器上，使用 32 位的指针来访问数据（堆中的对象或 Metaspace 中的元数据）的一种方式。\n这样有很多的好处，比如 32 位的指针占用更小的内存，可以更好地使用缓存，在有些平台，还可以使用到更多的寄存器。\n当然，在 64 位的机器中，最终还是需要一个 64 位的地址来访问数据的，所以这个 32 位的值是相对于一个基准地址的值。\n\n\n\n\n\n\n\n\n\nCompressedOops 说的是对象引用的压缩，它不在本文的讨论范围内。\n在 64 位平台上，本质上还是需要使用 64 位地址来引用每一个对象的，但是这项技术使得可以只使用 32 位地址来实现引用。大家可以参考一下评论区的讨论，这里就不展开了。\n由于本文在描述的是 Metaspace，所以我们这里不关心 Compressed Object Pointers，下面将描述 Compressed Class Pointers：\n每个 Java 对象，在它的头部，有一个引用指向 Metaspace 中的 Klass 结构。\n\n当使用了 compressed class pointers，这个引用是 32 位的值，为了找到真正的 64 位地址，需要加上一个 base 值：\n\n上面的内容应该很好理解，这项技术对 Klass 的分配带来的问题是：由于 32 位地址只能访问到 4G 的空间，所以最大只允许 4G 的 Klass 地址。这项限制也意味着，JVM 需要向 Metaspace 分配一个连续的地址空间。\n当从系统申请内存时，通过调用系统接口 malloc(3) 或 mmap(3)，操作系统可能返回任意一个地址值，所以在 64位系统中，它并不能保证在 4G 的范围内。\n所以，我们只能用一个 mmap() 来申请一个区域单独用来存放 Klass 对象。我们需要提前知道这个区域的大小，而且不能超过 4G。显然，这种方式是不能扩展的，因为这个地址后面的内存可能是被占用的。\n只有 Klass 结构有这个限制，对于其他的 class metadata 没有这个必要: 因为只有 Klass 实例是通过 Java 对象 header 中的压缩指针访问的。其他的 metadata 都是通过 64 位的地址进行访问的，所以它们可以被放到任意的地址上。\n所以，我们决定将 Metaspace 分为两个区域：non-class part 和 class part。\n\nclass part：存放 Klass 对象，需要一个连续的不超过 4G 的内存\nnon-class part：包含其他的所有 metadata\n\n\n\n\n\n\n\n\n\n\nclass part 被称作 Compressed Class Space，这个名字会有点怪，因为 Klass 本身其实没有使用压缩技术，而是引用它们的指针被压缩了。\ncompressed class space 空间的大小，是通过 -XX:CompressedClassSpaceSize 指定的。\n我们需要提前知道自己需要多少内存，它的默认值是 1G。当然这个 1G 并不是真的使用了操作系统的 1G，而是虚拟地址映射。\n实现为了复用已有的 Metaspace 空间，使用了一个小技巧：\n在 Class Space 和 Non-Class Space 中，分别都有 VirtualSpaceList 和 ChunkManager 两个结构。\n但是对于 Class Space，既然我们需要一个连续的空间我们不能使用一个链表来存放所有的 Node，所以这个链表退化为只有一个节点，并且不能扩展。这个 Node 就是 compressed class space，和 Non-Class Space 中的 Node 相比，它可是巨大无比。\n\nClassLoaderMetaspace（记录当前类加载器持有哪些 chunks）需要两个链表，一个用于记录 Class Space 中的 chunks，一个用于记录 Non-Class Space 中的 chunks。\n\n\n\n\n\n\n\n\n\n到这里应该也很好理解，就是对于一个类加载器来说，它需要知道自己使用了 non-class part 中的哪些 chunks 和 class part 中的哪些 chunks。\n开关: UseCompressedClassPointers, UseCompressedOops-XX:+UseCompressedOops 允许对象指针压缩。\n-XX:+UseCompressedClassPointers 允许类指针压缩。\n它们默认都是开启的，可以手动关闭它们。\n如果不允许类指针压缩，那么将没有 compressed class space 这个空间，并且-XX:CompressedClassSpaceSize 这个参数无效。\n-XX:-UseCompressedClassPointers 需要搭配 -XX:+UseCompressedOops，但是反过来不是: 我们可以只压缩对象指针，不压缩类指针。\n\n\n\n\n\n\n\n\n\n这里面为什么这么规定我也不懂，但是从直觉上来说，压缩对象指针显然是比较重要的，能获得较大的收益。也许就是基于这种考量吧：你连对象指针都不压缩，类指针压缩不压缩又有什么关系呢？\n注意，对象指针压缩要求堆小于 32G，所以如果堆大于等于 32G，那么对象指针压缩和类指针压缩都会被关闭。\n\n\n\n\n\n\n\n\n\n32G 可不是一个掐指一算随便指定的数字，看下评论区就知道原因了。\n4、度量 Metaspace前面我们介绍过，MaxMetaspaceSize 和 CompressedClassSpaceSize 是控制 Metaspace 的两个配置。\n回顾一下：\n\nMaxMetaspaceSize\n最大允许 Metaspace 使用的内存，包括 Class Space 和 Non-Class Space，默认是不限制。\n\nCompressedClassSpaceSize\n在启动的时候就限制 Class Space 的大小，默认值是 1G，启动后不可以修改。再说一遍，它是 reserved 不是 committed 的内存。\n\n\n下图展示了它们是怎么工作的：\n\n红色部分是 Metaspace 中已使用的系统内存，包括 Non-Class Space 链表中的红色部分和 Class Space 中大 Node 的红色部分。这个总和受到 -XX:MaxMetaspaceSize 的限制，超出将抛出 **OutOfMemoryError(“Metaspace”)**。\n-XX:CompressedClassSpaceSize 限制了下方的 Class Space 中，这个大 Node 的大小，包括了红色已使用的内存和蓝色未使用的内存。如果这个 Node 被用完了，会抛出 **OutOfMemoryError(“Compressed Class Space”)**。\n所以这意味着什么？当一个 Java 类被加载后，它需要 Non-Class Space 和 Class Space 的空间，而且后者通常都是被限制的(默认 1G)，所以我们总是有那么一个上限存在，即使 -XX:MaxMetaspaceSize 没有配置。\n所以，是否会触及到这个上限，取决于 Non-Class Space 和 Class Space 的使用比例。\n对于每个类，我们假设这个比例是 1: 5 （class:non-class） 。\n这意味着，对于 -XX:CompressedClassSpaceSize 的 1G 的默认值，我们的上限约 6G，1G 的 Class Space 再加约 5G 的 Non-Class Space。\n一个类大概需要多大的 Metaspace 空间对于一个被加载到虚拟机中的类，Metaspace 需要分配 class 和 non-class 空间，那么这些空间花在哪里了呢？看下图：\n\n深入 Class Space：最大的一部分是 Klass 结构，它是固定大小的。\n然后紧跟着两个可变大小的 vtable 和 itable，前者由类中方法的数量决定，后者由这个类所实现接口的方法数量决定。\n随后是一个 map，记录了类中引用的 Java 对象的地址，尽管该结构一般都很小，不过也是可变的。\nvtable 和 itable 通常也很小，但是对于一些巨大的类，它们也可以很大，一个有 30000 个方法的类，vtable 的大小会达到 240k，如果类派生自一个拥有 30000 个方法的接口，也是同理。但是这些都是测试案例，除了自动生成代码，你从来不会看到这样的类。\n深入 Non-Class Space这个区域有很多的东西，下面这些占用了最多的空间：\n\n常量池，可变大小；\n\n每个成员方法的 metadata：ConstMethod 结构，包含了好几个可变大小的内部结构，如方法字节码、局部变量表、异常表、参数信息、方法签名等；\n\n运行时数据，用来控制 JIT 的行为；\n\n注解\n\n\n\n\n\n\n\n\n\n\n\nMetaspace 中的结构都继承自 MetaspaceObj，所以查看它的类继承结构能了解更详细的信息。\nClass space 和 Non-Class Space 比例下面看一下在一些典型的应用中，它们之间的大小比例数据。\n下面是 WildFly 应用服务器，16.0.0，运行在 SAPMachine 11 平台上，没有加载任何应用。我们检查下总共需要多少 Metaspace 空间，然后计算平均每个类所需要的空间。我们使用 jcmd VM.metaspace 进行度量。\n\n\n\nloader\n#classes\nnon-class space (avg per class)\nclass space (&#x2F;avg per class)\nratio non-class&#x2F;class\n\n\n\nall\n11503\n60381k (5.25k)\n9957k (0.86k)\n6.0 : 1\n\n\nbootstrap\n2819\n16720k (5.93k)\n1768k (0.62k)\n9.5 : 1\n\n\napp\n185\n1320k (7.13k)\n136k (0.74k)\n9.7 : 1\n\n\nanonymous\n869\n1013k (1.16k)\n475k (0.55k)\n2.1 : 1\n\n\n这个表告诉我们：\n\n对于正常的类（我们假设通过 bootstrap 和 app 加载的类是正常的），我可以得到平均每个类需要约 5-7k 的 Non-Class Space 和 600-900 bytes 的 Class Space。\n匿名类要小得多，但是也有一个有趣的事情，Class 和 Non-Class Space 之间的比例，相对的，我们需要更多的 Class Space。这也不奇怪，因为诸如 Lambda 类都是很小的，但是它的 Klass 结构不可能小于 sizeof(Klass)。所以，我们得到 1k Non-Class Space 和 0.5k Class Space。\n\n注意，在我们的案例中，匿名类的数据可能没有代表性，需要收集更多的匿名类，才能得到更准确的数据。\nMetaspace 默认大小如果我们完全不设置限制 Metaspace 的大小，那么 Metaspace 可以容纳多少类呢？\nMaxMetaspaceSize 默认是没有限制的，CompressedClassSpaceSize 默认是 1G，所以我们唯一会触碰到的是 Class Space 空间的上限。\n使用上面的数据，每个类约 5-7k 的 Non-Class Space 和 600-900 bytes 的 Class Space，我们可以估算出大约 1-1.5 百万的类（假设没有碎片、没有浪费）以后会触碰到 Class Space 的 OOM。这是一个很大的数值了。\n限制 Metaspace 空间大小免责声明：不要盲目使用你在网络上找到的规则，尤其是这些数据并非来自生产数据。 \n其实我们没有什么选择，你确实可以限制 Metaspace 的空间增长，但是如果你的程序需要更多的空间用来存放 class metadata，那么你就会碰到 OOM，除了让你的代码加载更少的类，否则，你几乎是无能为力。\n和堆进行比较：你可以增加和减少堆的大小，而不必影响代码功能，所以堆的配置是比较灵活的，而 Metaspace 不具备这个特性。\n那么你为什么要限制 Metaspace 的大小呢？\n\n告警系统需要知道，为什么 Metaspace 空间以一个异常的速度在消耗，需要有人去看一下发生了什么。\n有时候需要限制虚拟内存地址的大小。通常我们感兴趣的是实际消耗内存，但是虚拟内存大小可能会导致虚拟机进程达到系统限制。\n\n\n\n\n\n\n\n\n\n\n注意：JDK 版本依赖：与 JDK 11或更高版本相比，JDK 8 中的元空间受到碎片的影响更大。所以在 JDK 8 环境下分配的时候，需要设置更多的缓冲。\n如果要限制 Metaspace 大小使得系统更容易被监控，同时不用在乎虚拟地址空间的大小，那么最好只设置 MaxMetaspaceSize 而不用设置 CompressedClassSpaceSize。如果要单独设置，那么最好设置 CompressedClassSpaceSize 为 MaxMetaspaceSize 的 80% 左右。\n除了 MaxMetaspaceSize 之外，减小 CompressedClassSpaceSize 的唯一原因是减小虚拟机进程的虚拟内存大小。 但是，如果将 CompressedClassSpaceSize 设置得太低，则可能在用完 MaxMetaspaceSize 之前先用完了 Compressed Class Space。 在大多数情况下，比率为1：2（CompressedClassSpaceSize &#x3D; MaxMetaspaceSize &#x2F; 2）应该是安全的。\n那么，你应该将 MaxMetaspaceSize 设置为多大呢？ 首先应该是计算预期的 Metaspace 使用量。你可以使用上面给出的数字，然后给每个类约 1K 的 Class Space 和 3~8K 的 Non-Class Space 作为缓冲。\n因此，如果你的应用程序计划加载10000个类，那么从理论上讲，你只需要 10M 的 Class Space 和 80M Non-Class Space。\n然后，你需要考虑安全系数。在大多数情况下，因子 2 是比较安全的。你当然也可以碰运气，设置低一点，但是要做好在碰到 OOM 后调大 Metaspace 空间的准备。\n如果设置安全因子为 2，那么需要 20M 的 Class Space 和 160M 的 Non-Class Space，也就是总大小为 180M。因此，在这里 -XX:MaxMetaspaceSize=180M 是一个很好的选择。\n小结这是一篇快速写完的文章，不过我仔细检查过了几遍，应该不会有太多的出入。\n如果你发现有些地方看得不是很懂，希望你可以先对比一下 原文。\n","slug":"JAVA/深入理解堆外内存 Metaspace","date":"2024-04-03T10:36:07.000Z","categories_index":"Metaspace,JAVA","tags_index":"Class,Space,metaspace","author_index":"dandeliono"},{"id":"127dff9b19567a7dce7fda63128b9f65","title":"JVM内存与Kubernetes中pod的内存、容器的内存不一致所引发的OOMKilled问题总结","content":"JVM内存与Kubernetes中pod的内存、容器的内存不一致所引发的OOMKilled问题总结承接上文在整个JVM进程体系而言，不仅仅只包含了Heap堆内存，其实还有其他相关的内存存储空间是需要我们考虑的，一边防止这些内存空间会造成我们的容器内存溢出的场景，正如下图所示。\n接下来了我们需要进行分析出heap之外的一部分就是对外内存就是Off Heap Space，也就是Direct buffer memory堆外内存。主要通过的方式就是采用Unsafe方式进行申请内存，大多数场景也会通过Direct ByteBuffer方式进行获取。好废话不多说进入正题。\nJVM参数MaxDirectMemorySize我们先研究一下jvm的-XX:MaxDirectMemorySize，该参数指定了DirectByteBuffer能分配的空间的限额，如果没有显示指定这个参数启动jvm，默认值是xmx对应的值（低版本是减去幸存区的大小）。\nDirectByteBuffer对象是一种典型的”冰山对象”，在堆中存在少量的泄露的对象，但其下面连接用堆外内存，这种情况容易造成内存的大量使用而得不到释放\nhighlighter- code-theme-dark\n12-XX:MaxDirectMemorySize\n\n-XX:MaxDirectMemorySize&#x3D;size 用于设置 New I&#x2F;O (java.nio) direct-buffer allocations 的最大大小，size 的单位可以使用 k&#x2F;K、m&#x2F;M、g&#x2F;G；如果没有设置该参数则默认值为 0，意味着JVM自己自动给NIO direct-buffer allocations选择最大大小。\n-XX:MaxDirectMemorySize的默认值是什么？在sun.misc.VM中，它是Runtime.getRuntime.maxMemory()，这就是使用-Xmx配置的内容。而对应的JVM参数如何传递给JVM底层的呢？主要通过的是hotspot&#x2F;share&#x2F;prims&#x2F;jvm.cpp。我们来看一下jvm.cpp的JVM源码来分一下。\nc\n123456789101112131415if (!FLAG\\_IS\\_DEFAULT(MaxDirectMemorySize)) &#123;  char as_chars\\[256\\];  jio\\_snprintf(as\\_chars, sizeof(as\\_chars), JULONG\\_FORMAT, MaxDirectMemorySize);  Handle key\\_str = java\\_lang\\_String::create\\_from\\_platform\\_dependent_str(&quot;sun.nio.MaxDirectMemorySize&quot;, CHECK_NULL);  Handle value\\_str  = java\\_lang\\_String::create\\_from\\_platform\\_dependent\\_str(as\\_chars, CHECK_NULL);  result\\_h-&gt;obj\\_at_put(ndx * 2,  key_str());  result\\_h-&gt;obj\\_at_put(ndx * 2 \\+ 1, value_str());  ndx++;&#125;\n\njvm.cpp 里头有一段代码用于把 - XX:MaxDirectMemorySize 命令参数转换为 key 为 sun.nio.MaxDirectMemorySize的属性。我们可以看出来他转换为了该属性之后，进行设置和初始化直接内存的配置。针对于直接内存的核心类就在http://www.docjar.com/html/api/sun/misc/VM.java.html。大家有兴趣可以看一下对应的视线。在JVM源码里面的目录是：java.base/jdk/internal/misc/VM.java，我们看一下该类关于直接内存的重点部分。\njava\n1234567891011121314151617181920212223242526public class VM &#123;        private static final int JAVA\\_LANG\\_SYSTEM_INITED     = 1;    private static final int MODULE\\_SYSTEM\\_INITED        = 2;    private static final int SYSTEM\\_LOADER\\_INITIALIZING  = 3;    private static final int SYSTEM_BOOTED               = 4;    private static final int SYSTEM_SHUTDOWN             = 5;        private static volatile int initLevel;    private static final Object lock = new Object();                                        private static long directMemory = 64 \\* 1024 \\* 1024;\n\n上面可以看出来64MB最初是任意设置的。在-XX:MaxDirectMemorySize 是用来配置NIO direct memory上限用的VM参数。可以看一下JVM的这行代码。\nhighlighter- code-theme-dark Bash\n123product(intx, MaxDirectMemorySize, -1,        &quot;Maximum total size of NIO direct-buffer allocations&quot;)\n\n但如果不配置它的话，direct memory默认最多能申请多少内存呢？这个参数默认值是-1，显然不是一个“有效值”。所以真正的默认值肯定是从别的地方来的。\njava\n1234567891011121314151617181920212223242526272829303132333435363738394041424344454647                    public static long maxDirectMemory() &#123;        return directMemory;    &#125;                        public static void saveProperties(Map&lt;String, String&gt; props) &#123;        if (initLevel() != 0)            throw new IllegalStateException(&quot;Wrong init level&quot;);                        if (savedProps == null) &#123;            savedProps = props;        &#125;                                                        String s = props.get(&quot;sun.nio.MaxDirectMemorySize&quot;);        if (s == null || s.isEmpty() || s.equals(&quot;-1&quot;)) &#123;                        directMemory = Runtime.getRuntime().maxMemory();        &#125; else &#123;            long l = Long.parseLong(s);            if (l &gt; -1)                directMemory = l;        &#125;                s = props.get(&quot;sun.nio.PageAlignDirectMemory&quot;);        if (&quot;true&quot;.equals(s))            pageAlignDirectMemory = true;    &#125;    &#125;\n\n从上面的源码可以读取 sun.nio.MaxDirectMemorySize 属性，如果为 null 或者是空或者是 - 1，那么则设置为 Runtime.getRuntime ().maxMemory ()；如果有设置 MaxDirectMemorySize 且值大于 - 1，那么使用该值作为 directMemory 的值；而 VM 的 maxDirectMemory 方法则返回的是 directMemory 的值。\n因为当MaxDirectMemorySize参数没被显式设置时它的值就是-1，在Java类库初始化时maxDirectMemory()被java.lang.System的静态构造器调用，走的路径就是这条：\njava\n12345if (s.equals(&quot;-1&quot;)) &#123;          directMemory = Runtime.getRuntime().maxMemory();  &#125;\n\n而Runtime.maxMemory()在HotSpot VM里的实现是：\nhighlighter- code-theme-dark Bash\n123456JVM\\_ENTRY\\_NO\\_ENV(jlong, JVM\\_MaxMemory(void))    JVMWrapper(&quot;JVM_MaxMemory&quot;);    size\\_t n = Universe::heap()-&gt;max\\_capacity();    return convert\\_size\\_t\\_to\\_jlong(n);  JVM_END  \n\n这个max_capacity()实际返回的是 -Xmx减去一个survivor space的预留大小。\n结论分析说明MaxDirectMemorySize没显式配置的时候，NIO direct memory可申请的空间的上限就是-Xmx减去一个survivor space的预留大小。例如如果您不配置-XX:MaxDirectMemorySize并配置-Xmx5g，则”默认” MaxDirectMemorySize也将是5GB-survivor space区，并且应用程序的总堆+直接内存使用量可能会增长到5 + 5 &#x3D; 10 Gb 。\n其他获取 maxDirectMemory 的值的API方法BufferPoolMXBean 及 JavaNioAccess.BufferPool (通过SharedSecrets获取) 的 getMemoryUsed 可以获取 direct memory 的大小；其中 java9 模块化之后，SharedSecrets 从原来的 sun.misc.SharedSecrets 变更到 java.base 模块下的 jdk.internal.access.SharedSecrets；要使用 –add-exports java.base&#x2F;jdk.internal.access&#x3D;ALL-UNNAMED 将其导出到 UNNAMED，这样才可以运行\njava\n1234567891011public BufferPoolMXBean getDirectBufferPoolMBean()&#123;        return ManagementFactory.getPlatformMXBeans(BufferPoolMXBean.class)                .stream()                .filter(e -&gt; e.getName().equals(&quot;direct&quot;))                .findFirst()                .orElseThrow();&#125;public JavaNioAccess.BufferPool getNioBufferPool()&#123;     return SharedSecrets.getJavaNioAccess().getDirectBufferPool();&#125;\n\n内存分析问题-XX:+DisableExplicitGC 与 NIO的direct memory\n用了-XX:+DisableExplicitGC参数后，System.gc()的调用就会变成一个空调用，完全不会触发任何GC（但是“函数调用”本身的开销还是存在的哦～）。\n\n做ygc的时候会将新生代里的不可达的DirectByteBuffer对象及其堆外内存回收了，但是无法对old里的DirectByteBuffer对象及其堆外内存进行回收，这也是我们通常碰到的最大的问题，如果有大量的DirectByteBuffer对象移到了old，但是又一直没有做cms gc或者full gc，而只进行ygc，那么我们的物理内存可能被慢慢耗光，但是我们还不知道发生了什么，因为heap明明剩余的内存还很多(前提是我们禁用了System.gc)。\n\n\n","slug":"JAVA/JVM内存与Kubernetes中pod的内存、容器的内存不一致所引发的OOMKilled问题总结","date":"2024-04-03T09:43:02.000Z","categories_index":"MaxDirectMemorySize,JAVA","tags_index":"JVM,java,direct","author_index":"dandeliono"},{"id":"81d9f9cbb43ffb5d74b9291c7102db6e","title":"ConditionalOnMissingBean失效问题追踪","content":"ConditionalOnMissingBean失效问题追踪遇到一个@ConditionalOnMissingBean失效的问题，今天花点时间来分析一下。\n现场回放services首先介绍下代码结构：有RunService,以及它的两个实现类：TrainRunServiceImpl和CarRunServiceImpl\nRunService\n1234`public interface RunService &#123;    void run();&#125;` \n\nTrainRunServiceImpl\n12345678`public class TrainRunServiceImpl implements RunService &#123;    @Override    public void run() &#123;        System.out.println(&quot;开火车,wuwuwuwuwu&quot;);    &#125;&#125;` \n\nCarRunServiceImpl\n123456789`public class CarRunServiceImpl implements RunService &#123;    @Override    public void run() &#123;        System.out.println(&quot;汽车,dididi&quot;);    &#125;&#125;` \n\n操作类操作类MyInitBean中，注入了RunService – byType\n123456789101112`@Componentpublic class MyInitBean implements InitializingBean &#123;    @Autowired    private RunService runService;    @Override    public void afterPropertiesSet() throws Exception &#123;        runService.run();    &#125;&#125;` \n\nconfiguration我们在配置类中，注入RunService的实现bean,并通过@ConditionalOnMissingBean来判断是否注入。\n12345678910111213141516`@Configurationpublic class MyConfiguration &#123;    @Bean    @ConditionalOnMissingBean    public RunService carRunServiceImpl() &#123;        return new CarRunServiceImpl();    &#125;        @Bean    public RunService trainRunServiceImpl() &#123;        return new TrainRunServiceImpl();    &#125;&#125;` \n\n抛出异常按照上述的代码，执行后，本以为会成功执行，但是却抛出了异常，异常信息如下：\n在spring容器中存在了两个RunService实现类。这导致了MyInitBean无法决定它到底该使用这两个中的哪一个。(默认是byType注入的)\n按照上述的异常信息，它给出了两种解决方案：\n@Qualifier在注入bean时，指定bean的名称.\n123456789`@Controllerpublic class MyInitBean implements InitializingBean &#123;    @Autowired    @Qualifier(&quot;carRunServiceImpl&quot;)    private RunService runService;&#125;` \n\n\n\n\n\n\n\n\n\n\n\n通过@Configuration配置类注入的bean,默认名称为方法名称\n\n123456`@Bean  public RunService trainRunServiceImpl() &#123;    return new TrainRunServiceImpl();&#125;` \n\n\n直接在类头部申明注入的bean,默认名称为类名称\n\n1234`@Service  public class TrainRunServiceImpl implements RunService &#123;&#125;` \n@Primary@Primary的作用是，在bean存在多个候选者且无法决定使用哪一个时，优先使用带有该注解的bean.\n\n在配置类中Configuration添加\n\n1234567`@Bean @Primary public RunService trainRunServiceImpl() &#123;     return new TrainRunServiceImpl(); &#125;` \n\n\n在类申明中添加\n\n1234`@Primarypublic class TrainRunServiceImpl implements RunService &#123;&#125;` \n\n\n注意在上述给出的两种方法中，无论是使用@Primary还是这里容器中仍然存在多个实现类,这并不是我们想要的结果。这里为什么@ConditionalOnMissingBean会失效呢?\n问题定位在进行问题定位前，我们先来回顾一下@ConditionalOnMissingBean的工作原理\n工作原理@ConditionalOnMissingBeanConditionalOnMissingBean的注解定义如下：\n123456789101112`@Target(&#123; ElementType.TYPE, ElementType.METHOD &#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Conditional(OnBeanCondition.class)public @interface ConditionalOnMissingBean &#123;\tClass&lt;?&gt;[] value() default &#123;&#125;;\tString[] type() default &#123;&#125;;\t\t&#125;` \n\n@ConditionalOnMissingBean通常可以有如下三种使用方式：\n12345678`@Bean   @ConditionalOnMissingBean    public RunService carRunServiceImpl() &#123;       return new CarRunServiceImpl();   &#125;` \n\n在注解上看到了一个OnBeanCondition类，在@ConditionalOnBean,ConditionalOnSingleCandidate和ConditionalOnMissingBean都看到了它的身影。\nOnBeanCondition\n1234567891011121314151617181920212223242526272829303132333435363738`@Order(Ordered.LOWEST_PRECEDENCE)class OnBeanCondition extends SpringBootCondition implements ConfigurationCondition &#123;\t@Override\tpublic ConditionOutcome getMatchOutcome(ConditionContext context,\t\t\tAnnotatedTypeMetadata metadata) &#123;\t\t\t\t\t\t\t\tif (metadata.isAnnotated(ConditionalOnMissingBean.class.getName())) &#123;\t\t\t\t\t\tBeanSearchSpec spec = new BeanSearchSpec(context, metadata,ConditionalOnMissingBean.class);\t\t\t\t\t\tMatchResult matchResult = getMatchingBeans(context, spec);\t\t\tif (matchResult.isAnyMatched()) &#123;\t\t\t\t\t\t\t\t\t\t\t\tString reason = createOnMissingBeanNoMatchReason(matchResult);\t\t\t\t\t\t\t\treturn ConditionOutcome.noMatch(ConditionMessage\t\t\t\t\t\t.forCondition(ConditionalOnMissingBean.class, spec)\t\t\t\t\t\t.because(reason));\t\t\t&#125;\t\t\t\t\t\tmatchMessage = matchMessage.andCondition(ConditionalOnMissingBean.class, spec)\t\t\t\t\t.didNotFind(&quot;any beans&quot;).atAll();\t\t&#125;\t\t\t\treturn ConditionOutcome.match(matchMessage);\t&#125;&#125;` \n\n\n\n\n\n\n\n\n\n\nConditionOutcome 的用法：当match= true时，才注入容器.若@ConditionalOnMissingBean找到了匹配项，则返回ConditionOutcome.notMatch，则不注入容器。\n问题出在哪?有了上面的一系列原理支撑,但是为什么没有执行到我们想要的结果呢？debug执行后，发现问题出现在OnBeanCondition .getMatchingBeans(context, spec)这个方法中。首先再次回顾下配置类：\n在注入carRunServiceImpl时，执行OnBeanCondition .getMatchingBeans(context, spec)并没有找到下面定义的trainRunServiceImpl.\n真相只有一个：@Configuration 在初始化bean的时候，顺序出现了问题，那么如何控制初始化bean的顺序呢？\n解决问题一顿分析之后，我们发现只要控制了bean的加载顺序之后，上述的问题就可以解决了。接下来我们来尝试控制bean初始化顺序：\nConfiguration中bean使用@Order —————– failure\n123456789101112131415161718`@Configurationpublic class MyConfiguration &#123;    @Order(2)    @Bean    @ConditionalOnMissingBean    public RunService carRunServiceImpl() &#123;        return new CarRunServiceImpl();    &#125;    @Order(1)    @Bean    public RunService trainRunServiceImpl() &#123;        return new TrainRunServiceImpl();    &#125;&#125;` \n\nConfiguration 调整bean申明顺序—————– success将带有@ConditionalOnMissingBean注解的bean,申明在代码的末尾位置，操作成功：\n123456789101112131415`@Configurationpublic class MyConfiguration &#123;\t@Bean    public RunService trainRunServiceImpl() &#123;        return new TrainRunServiceImpl();    &#125;    @Bean    @ConditionalOnMissingBean    public RunService carRunServiceImpl() &#123;        return new CarRunServiceImpl();    &#125;&#125;` \n\n配置多个Configuration类，并通过@Order指定顺序—————- failure\n1234567891011121314151617181920`@Configuration@Order(Ordered.LOWEST_PRECEDENCE) public class MyConfiguration &#123;    @Bean    @ConditionalOnMissingBean    public RunService carRunServiceImpl() &#123;        return new CarRunServiceImpl();    &#125;&#125;@Configuration@Order(Ordered.HIGHEST_PRECEDENCE) public class MyConfiguration2 &#123;    @Bean    public RunService trainRunServiceImpl() &#123;        return new TrainRunServiceImpl();    &#125;&#125;` \n\n@Configuration并不能通过@Order指定顺序。大胆猜测下： @Configuration通过配置类名的自然顺序来加载的。\n@Configuration配置类加载顺序通过类名顺序来加载 ——- 验证success将MyConfiguration2重命名为Configuration2，而它的加载顺序在MyConfiguration之前，执行程序成功。\n这里貌似所有的问题似乎都解决了， 只需要我们自定义的配置类名称保证最优先加载就可以了。我们只需要注意配置类的命名规则即可.但是，这种解决方案，似乎并不是那么令人信服。\n@AutoConfigureBefore，@AutoConfigureAfter经查文档，终于找到了需要的东西：我们可以通过@AutoConfigureBefore,@AutoConfigureAfter来控制配置类的加载顺序。\n1234567891011121314151617`@Configurationpublic class MyConfiguration &#123;    @Bean    @ConditionalOnMissingBean    public RunService carRunServiceImpl() &#123;        return new CarRunServiceImpl();    &#125;&#125;@Configuration@AutoConfigureBefore(MyConfiguration.class)public class MyConfiguration2 &#123;    @Bean    public RunService trainRunServiceImpl() &#123;        return new TrainRunServiceImpl();    &#125;&#125;` \n\n\n注意:如果要开启@EnableAutoConfiguration需要在META-INF/spring.factories文件中添加如下内容：\n1234`org.springframework.boot.autoconfigure.EnableAutoConfiguration=\\xxx.configuration.MyConfiguration2,\\xxx.configuration.MyConfiguration` \n\n结论我们需要控制目标bean的加载顺序即可。但是我们在实际的使用一些通用plugin过程中（如redis），并没有刻意的指定bean的加载顺序，这是为什么呢?因为：在实际的应用过程中，我们使用第三方插件，他们的默认配置都会存在于插件的jar包中，而我们的个性化配置则存在于自身的应用中。而容器会优先执行classes/，然后才执行jars/classes.\n","slug":"JAVA/ConditionalOnMissingBean失效问题追踪","date":"2024-04-01T18:25:44.000Z","categories_index":"bean,JAVA","tags_index":"ConditionalOnMissingBean,Configuration,RunService","author_index":"dandeliono"},{"id":"a159c983ebe30de9eaec5108a827bcdd","title":"Netty ChannelInactive 断链场景分析","content":"Netty ChannelInactive 断链场景分析本文档主要列举离会、关闭进程、断网、重连等会导致sdk与服务端断开连接的场景的设计与实现，并试图解释其原理\n1.Netty断链场景分析1. Netty对断链的处理简单来说Netty在检测到断开连接的情况下会抛出channelInactive事件（其实准确的说应该是de-register事件），这个事件会在pipeline的Handler中被传递和被处理，当然也可以选择不往下传递，即不调用fireChannelInactive()，对pipeline和handler处理事件等原理有兴趣可以看一下《netty in action》第16章，这里我们就重点关注哪些场景会触发channelInactive  \n2. ChannelInactive触发场景\n客户端发送close帧（FIN包）\n客户端关闭进程（RST包）\n服务端或客户端主动调用channel.close()\n\n以上为使用Netty官方示例测试得出的场景，需要注意的是直接断网并不会触发channelInactive，原因大概是由于直接断开网络并没有发送fin包，netty无法感知当前连接的存活状态，当然我们可以通过心跳超时来处理这种情况\n另外心跳超时的情况也需要额外说明一下，在pipeline中添加IdleHandler可以配置一个自定义的心跳超时策略，例如我的服务中配置的是35s无新消息，当无新消息写入时，抛出一个心跳超时时间；但是心跳超时事件本身如果不去捕获netty是不会去做额外处理的，所以我的服务会在pipeline后面的heartbeatHandler中捕获心跳超时事件并主动关闭channel（对应上述第三种断链场景）\n3. EventLoopGroup顺带提一下Netty的线程处理模型，Netty主要分为两个线程的group，bossGroup和workerGroup，可以当成两个线程池，其中bossGroup一般只有一个线程，用来处理新连接的请求，而连接具体的IO和业务操作则放在workerGroup中完成。一般workerGroup中包含多个EventLoop（一个EventLoop可以理解成一个线程或者nio中的selector），多个channel可以注册到同一个EventLoop上，而对一个channel的处理从头至尾只会由同一个EventLoop来完成，想了解更多关于EventLoopGroup的细节也可以看下《netty in action》前几章的内容。这里提线程处理模型的原因主要是想说明，即使当客户端突然断开连接，netty也不会终止对前几个消息的处理，而是会等待前面的消息处理完再处理关闭事件，因为虽然netty是一个异步的框架，但是同一个channel的操作都是在同一个线程上顺序执行的\n2.直接关闭客户端的场景当客户端直接关闭&#x2F;结束进程时，抓包信息如下\n110.164.184.68    100.94.8.71       TCP    54    53435 → 80 [FIN, ACK] Seq=1 Ack=1 Win=1022 Len=0100.94.8.71       10.164.184.68    TCP    60    80 → 53435 [FIN, ACK] Seq=1 Ack=2 Win=103 Len=010.164.184.68    100.94.8.71       TCP    54    53435 → 80 [ACK] Seq=2 Ack=2 Win=1022 Len=0\n\n可以看到客户端主动发送了fin包，服务端会调用WsSubscribeHandler的channelInactive，触发一个自定义的channelInactiveEvent事件，之后主要执行bye和evict方法，其中bye主要是向mcs发送leaveRoom请求，evict主要是删除acs的相关缓存，这是断链场景下最简单的一个流程\n","slug":"JAVA/Netty ChannelInactive 断链场景分析","date":"2024-03-25T16:04:40.000Z","categories_index":"Netty,JAVA","tags_index":"netty,channel,channelInactive","author_index":"dandeliono"},{"id":"6097fb5f130a2c4c51fe513482d1af88","title":"直观感受 TLS 握手流程(下)","content":"直观感受 TLS 握手流程(下)在 HTTPS 开篇的文章中，笔者分析了 HTTPS 之所以安全的原因是因为 TLS 协议的存在。TLS 能保证信息安全和完整性的协议是记录层协议。(记录层协议在上一篇文章中详细分析了)。看完上篇文章的读者可能会感到疑惑，TLS 协议层加密的密钥是哪里来的呢？客户端和服务端究竟是如何协商 Security Parameters 加密参数的？这篇文章就来详细的分析一下 TLS 1.2 和 TLS 1.3 在 TLS 握手层上的异同点。\nTLS 1.3 在 TLS 1.2 的基础上，针对 TLS 握手协议最大的改进在于提升速度和安全性。本篇文章会重点分析这两块。\n先简述一下 TLS 1.3 的一些优化和改进:\n\n减少握手等待时间，将握手时间从 2-RTT 降低到 1-RTT，并且增加 0-RTT 模式。\n\n删除 RSA 密钥协商方式，静态的 Diffie-Hellman 密码套件也被删除了。因为 RSA 不支持前向加密性。TLS 1.3 只支持 (EC)DHE 的密钥协商算法。删除了 RSA 的方式以后，能有效预防心脏出血的攻击。所有基于公钥的密钥交换算法现在都能提供前向安全。TLS 1.3 规范中只支持 5 种密钥套件，TLS13-AES-256-GCM-SHA384、TLS13-CHACHA20-POLY1305-SHA256、TLS13-AES-128-GCM-SHA256、TLS13-AES-128-CCM-8-SHA256、TLS13-AES-128-CCM-SHA256，隐藏了非对称加密密钥协商算法，因为默认都是椭圆曲线密钥协商。\n\n删除对称加密中，分组加密和 MAC 导致的一些隐患。在 TLS1.3 之前的版本中，选择的是 MAC-then-Encrypt 方式。但是这种方式带来了一些漏洞，例如 BEAST，一系列填充 oracle 漏洞(Lucky 13 和 Lucky Microseconds)。CBC 模式和填充之间的交互也是 SSLv3 和一些 TLS 实现中广泛宣传的 POODLE 漏洞原因。在 TLS 1.3 中，已移除所有有安全隐患的密码和密码模式。你不能再使用 CBC 模式密码或不安全的流式密码，如 RC4 。TLS 1.3 中允许的唯一类型的对称加密是一种称为 AEAD（authenticated encryption with additional data）的新结构，它将加密性和完整性整合到一个无缝操作中。\n\n在 TLS 1.3中，删除了 PKCS＃1 v1.5 的支持，而选择更新的设计 RSA-PSS，提高了安全性。认证方面通过非对称算法，例如，RSA, 椭圆曲线数字签名算法(ECDSA)，或 Edwards 曲线数字签名算法(EdDSA)完成，或通过一个对称的预共享密钥（PSK)。\n\n在 TLS 1.2 的握手流程中，只有 ChangeCipherSpec 之后的消息会被加密，如 Finished 消息和 NewSessionTicket，其他的握手子消息不会加密。TLS 1.3 针对这个问题，对握手中大部分子消息全部进行加密处理。这样可以有效的预防 FREAK，LogJam 和 CurveSwap 这些降级攻击(降级攻击是中间人利用协商，强制使通信双方使用能被支持的最低强度的加密算法，从而暴力攻击计算出密钥，允许攻击者在握手时伪造 MAC)。在TLS 1.3中，这种类型的降级攻击是不可能的，因为服务器现在签署了整个握手，包括密码协商。\n\n\n\n\n\nTLS 1.3 完全禁止重协商。\n密钥导出函数被重新设计，由 TLS 1.2 的 PRF 算法改为更加安全的 HKDF 算法。\n废除 Session ID 和 Session Ticket 会话恢复方式，统一通过 PSK 的方式进行会话恢复，并在 NewSessionTicket 消息中添加过期时间和用于混淆时间的偏移值。\n\n更多重要的变更，见笔者之前的文章 《TLS 1.3 Introduction》\n七. TLS 1.3 首次握手流程\n\n\n\n\n\n\n\n\n由于笔者在之前的某篇文章中已经将 TLS 1.3 握手流程的细节分析过了，所以这篇文章中不会像上篇分析 TLS 1.2 中那么详细，如果想了解 TLS 1.3 中细节，请阅读这篇文章《TLS 1.3 Handshake Protocol》。本篇文章主要从 wireshark 角度带读者直观感受 TLS 1.3 的握手流程。\n在 TLS 1.3 中，存在 4 种密钥协商的方法:\n\nClient 支持的加密套件列表。密码套件里面中能体现出 Client 支持的 AEAD 算法或者 HKDF 哈希对。\n“supported_groups” 的扩展 和 “key_share” 扩展。“supported_groups” 这个扩展表明了 Client 支持的 (EC)DHE groups，”key_share” 扩展表明了 Client 是否包含了一些或者全部的（EC）DHE共享。\n“signature_algorithms” 签名算法和 “signature_algorithms_cert” 签名证书算法的扩展。”signature_algorithms” 这个扩展展示了 Client 可以支持了签名算法有哪些。”signature_algorithms_cert” 这个扩展展示了具体证书的签名算法。\n“pre_shared_key” 预共享密钥和 “psk_key_exchange_modes” 扩展。预共享密钥扩展包含了 Client 可以识别的对称密钥标识。”psk_key_exchange_modes” 扩展表明了可能可以和 psk 一起使用的密钥交换模式。\n\n第一种方法是 TLS 1.2 中已经存在的，通过 ClientHello 中的 Cipher Suites 进行协商。第二种方法是 TLS 1.3 新增的，在 TLS 1.3 中完整握手就是通过这种方法实现的。第三种方法也是 TLS 1.3 新增的。这种方法没有第二种方法用的多。第四种方法也是 TLS 1.3 新增的，它将 TLS 1.2 中 Session ID 和 Session Ticket 废除以后，统一通过 PSK 的方式进行会话恢复。TLS 1.3 中的 0-RTT 模式也是通过 PSK 进行的。\nTLS 1.3 完整握手的流程如下：\n1234567891011121314151617Client                                               Server         ClientHello         + key_share               --------&gt;                                                         ServerHello                                                         + key_share                                               &#123;EncryptedExtensions&#125;                                               &#123;CertificateRequest*&#125;                                                      &#123;Certificate*&#125;                                                &#123;CertificateVerify*&#125;                                                          &#123;Finished&#125;                                   &lt;--------     [Application Data*]         &#123;Certificate*&#125;         &#123;CertificateVerify*&#125;         &#123;Finished&#125;                --------&gt;                                   &lt;--------      [NewSessionTicket]         [Application Data]        &lt;-------&gt;      [Application Data] \n\n在 TLS 1.3 握手中，主要能分为 3 个阶段:\n\n密钥交换：建立共享密钥数据并选择密码参数。在这个阶段之后所有的数据都会被加密。\nServer 参数：建立其它的握手参数（Client 是否被认证，应用层协议支持等）。\n认证：认证 Server（并且选择性认证 Client），提供密钥确认和握手完整性。\n\n密钥交换是 ClientHello 和 ServerHello，Server 参数是 EncryptedExtensions 和 CertificateRequest 消息。认证是 Certificate、CertificateVerify、Finished。\nClient 发起完整握手流程从 ClientHello 开始：\n12345678910111213uint16 ProtocolVersion;     opaque Random[32];     uint8 CipherSuite[2];         struct &#123;         ProtocolVersion legacy_version = 0x0303;             Random random;         opaque legacy_session_id&lt;0..32&gt;;         CipherSuite cipher_suites&lt;2..2^16-2&gt;;         opaque legacy_compression_methods&lt;1..2^8-1&gt;;         Extension extensions&lt;8..2^16-1&gt;;     &#125; ClientHello; \n\n在 ClientHello 结构体重，legacy_version &#x3D; 0x0303，0x0303 是 TLS 1.2 的版本号，这个字段规定必须设置成这个值。其他字段和 TLS 1.2 含义相同，不再赘述了。\n在 TLS 1.3 的 ClientHello 的 Extension 中，一定会有 supported_versions 这个字段，如果这个字段，ClientHello 会被解读成 TLS 1.2 的 ClientHello 消息。在 TLS 1.3 中 Server 根据 supported_versions 这个字段来决定是否协商 TLS 1.3 。\nTLS 1.3 之所以能比 TLS 1.2 完整握手减少 1-RTT 的原因就在 ClientHello 中就已经包含了 (EC)DHE 所需要的密钥参数，不需要像 TLS 1.2 中额外用第二次 RTT 来进行 DH 协商参数。在 TLS 1.3 的 ClientHello 的 Extension 中，带有 key_share 扩展，这个扩展中包含了 Client 所能支持的 (EC)DHE 算法的密钥参数。并且 Extension 中还会有 supported_groups 扩展，这个扩展表明了 Client 支持的用于密钥交换的命名组。按照优先级从高到低。\nServer 收到 ClientHello 以后，回应一条 ServerHello 消息：\n12345678struct &#123;         ProtocolVersion legacy_version = 0x0303;             Random random;         opaque legacy_session_id_echo&lt;0..32&gt;;         CipherSuite cipher_suite;         uint8 legacy_compression_method = 0;         Extension extensions&lt;6..2^16-1&gt;;     &#125; ServerHello; \n\n在 ServerHello 消息中，legacy_version &#x3D; 0x0303，这个也是 TLS 1.3 规范的规定，这个值必须固定填 0x0303(TLS 1.2)。Server 会读取 ClientHello 扩展中 “supported_versions” 扩展字段，如果 Client 能支持 TLS 1.3，那么 Server 在 ServerHello 扩展中的 “supported_versions” 扩展字段标识可以进行 TLS 1.3 的握手。\nServer 在协商 TLS 1.3 之前的版本，必须要设置 ServerHello.version，不能发送 “supported_versions” 扩展。Server 在协商 TLS 1.3 版本时候，必须发送 “supported_versions” 扩展作为响应，并且扩展中要包含选择的 TLS 1.3 版本号(0x0304)。还要设置 ServerHello.legacy_version 为 0x0303(TLS 1.2)。Client 必须在处理 ServerHello 之前检查此扩展(尽管需要先解析 ServerHello 以便读取扩展名)。如果 “supported_versions” 扩展存在，Client 必须忽略 ServerHello.legacy_version 的值，只使用 “supported_versions” 中的值确定选择的版本。如果 ServerHello 中的 “supported_versions” 扩展包含了 Client 没有提供的版本，或者是包含了 TLS 1.3 之前的版本(本来是协商 TLS 1.3 的，却又包含了 TLS 1.3 之前的版本)，Client 必须立即发送 “illegal_parameter” alert 消息中止握手。\n在 ServerHello 的 Extension 中必须要有的这 2 个扩展，supported_versions、key_share(如果是 PSK 会话恢复方式，还必须包含 pre_shared_key)。key_share 扩展标识了 Server 选择了 Client 支持的哪一个椭圆曲线，以及它对应的密钥协商所需参数。这里有两种情况，一种是协商 Diffie-Hellman 参数，具体分析见这一章节，另外一种协商是 ECDHE 参数，具体分析见这一章节。\nkey_share 传输过程中并没有使用私钥加密，整个过程的不可抵赖和防篡改是通过 CertificateVerify 验证 Server 持有私钥，以及 Finished 消息使用 HMAC 验证历史消息来确定的。\n发完 ServerHello 消息以后，Server 会继续发送 EncryptedExtensions 和 CertificateRequest 消息，如果对 Client 不进行认证，就不需要发送 CertificateRequest 消息。上面这 2 条消息都是加密的，通过 server_handshake_traffic_secret 中派生的密钥加密的。\nearly secret 和 ecdhe secret 导出 server_handshake_traffic_secret。再从 server_handshake_traffic_secret中导出 key 和 iv，使用该 key 和 iv 对 Server hello 之后的握手消息加密。同样的计算 client_handshake_traffic_secret，使用对应的 key 和 iv 进行解密后续的握手消息。\n1234567891011Early Secret = HKDF-Extract(salt, IKM) = HKDF-Extract(0, PSK) = HKDF-Extract(0, 0)      Handshake Secret = HKDF-Extract(salt, IKM) = HKDF-Extract(Derive-Secret(Early Secret, &quot;derived&quot;, &quot;&quot;), (EC)DHE)      client_handshake_traffic_secret = Derive-Secret(Handshake Secret, &quot;c hs traffic&quot;, ClientHello...ServerHello)      server_handshake_traffic_secret = Derive-Secret(Handshake Secret, &quot;s hs traffic&quot;, ClientHello...ServerHello)      client_write_key = HKDF-Expand-Label(client_handshake_traffic_secret, &quot;key&quot;, &quot;&quot;, key_length)      client_write_iv  = HKDF-Expand-Label(client_handshake_traffic_secret, &quot;iv&quot;, &quot;&quot;, iv_length)      server_write_key = HKDF-Expand-Label(server_handshake_traffic_secret, &quot;key&quot;, &quot;&quot;, key_length)      server_write_iv  = HKDF-Expand-Label(server_handshake_traffic_secret, &quot;iv&quot;, &quot;&quot;, iv_length) \n\nEncryptedExtensions 消息包含应该被保护的扩展。即，任何不需要建立加密上下文但不与各个证书相互关联的扩展。比如 ALPN 扩展。Client 必须检查 EncryptedExtensions 消息中是否存在任何禁止的扩展，如果有发现禁止的扩展，必须立即用 “illegal_parameter” alert 消息中止握手。\n12345Structure of this message:     struct &#123;         Extension extensions&lt;0..2^16-1&gt;;     &#125; EncryptedExtensions; \n\n\nextensions:扩展列表。\n\nCertificateRequest 消息细节，见这一章节\n接下来 Server 还要继续发送 Certificate、CertificateVerify、Finished 消息。这 3 条消息是握手消息的最后 3 条消息。这 3 条消息使用从 sender_handshake_traffic_secret 派生出来的密钥进行加密。\nServer 发送自己的 Certificate 给 Client，在 Certificate 消息中，有 4 种情况，第一种包含了 OCSP Status and SCT Extensions，细节请看这一章节，第二种包含了 Server Certificate Selection，细节请看这一章节，第三种包含了 Client Certificate Selection，细节请看这一章节，最后一种包含了 Receiving a Certificate Message，细节请看这一章节。\nServer 发送完 Certificate 消息以后，紧接着是 CertificateVerify 消息。Server 将当前所有的握手消息进行签名，具体验证过程见这一章节。\n最后一条消息是 Finished 消息。它对提供握手和计算密钥的身份验证起了至关重要的作用。\n用于计算 Finished 消息的密钥是使用 HKDF，特别的:\n12finished_key =      HKDF-Expand-Label(BaseKey, &quot;finished&quot;, &quot;&quot;, Hash.length) \n\nBaseKey 是 handshake_traffic_secret。\n这条消息的数据结构是:\n123struct &#123;         opaque verify_data[Hash.length];     &#125; Finished; \n\nverify_data 按照如下方法计算:\n123456verify_data =         HMAC(finished_key,              Transcript-Hash(Handshake Context,                              Certificate*, CertificateVerify*))     * Only included if present. \n\nHMAC [RFC2104] 使用哈希算法进行握手。如上所述，HMAC 输入通常是通过动态的哈希实现的，即，此时仅是握手的哈希。\n在以前版本的 TLS 中，verify_data 的长度总是 12 个八位字节。在 TLS 1.3 中，它是用来表示握手的哈希的 HMAC 输出的大小。\n注意：警报和任何其他非握手记录类型不是握手消息，并且不包含在哈希计算中。\nFinished 消息之后的任何记录都必须在适当的应用程序流量密钥下加密。特别是，这包括 Server 为了响应 Client 的 Certificate 消息和 CertificateVerify 消息而发送的任何 alert。\nFinish 消息发送完后，再导出最终对称加密的密钥。从 Handshake Secret 中导出 master secret，再从 master secret 导出两个方向的对称密钥 key 和 iv。\n123Master Secret = HKDF-Extract(salt, IKM) = HKDF-Extract(Derive-Secret(Handshake Secret, &quot;derived&quot;, &quot;&quot;), 0)      client_application_traffic_secret_0 = Derive-Secret(Master Secret, &quot;c ap traffic&quot;, ClientHello...server Finished)      server_application_traffic_secret_0 = Derive-Secret(Master Secret, &quot;s ap traffic&quot;, ClientHello...server Finished) \n\nFinished 消息发送以后，在完整握手的流程中，Server 收到 Client 的 Finished 消息后，验证完后，还需要发送 NewSessionTicket 消息。通过 master secret 和整个握手的摘要，计算最后的 resumption secret。\nNewSessionTicket 使用 server_application_traffic_secret 加密。在加密的 ticket过程中，TLS 1.3 相比 TLS 1.2，还包含了当前的创建时间，因此可以方便的配置和验证 ticket 的过期时间。\n注意：虽然恢复主密钥取决于 Client 的第二次 flight，但是不请求 Client 身份验证的 Server 可以独立计算转录哈希的剩余部分，然后在发送 Finished 消息后立即发送 NewSessionTicket 而不是等待 Client 的 Finished 消息。这可能适用于 Client 需要并行打开多个 TLS 连接并且可以从减少恢复握手的开销中受益的情况。\n1234567struct &#123;         uint32 ticket_lifetime;         uint32 ticket_age_add;         opaque ticket_nonce&lt;0..255&gt;;         opaque ticket&lt;1..2^16-1&gt;;         Extension extensions&lt;0..2^16-2&gt;;     &#125; NewSessionTicket; \n\n\nticket_lifetime：这个字段表示 ticket 的生存时间，这个时间是以 ticket 发布时间为网络字节顺序的 32 位无符号整数表示以秒为单位的时间。Server 禁止使用任何大于 604800 秒(7 天)的值。值为零表示应立即丢弃 ticket。无论 ticket_lifetime 如何，Client 都不得缓存超过 7 天的 ticket，并且可以根据本地策略提前删除 ticket。Server 可以将 ticket 视为有效的时间段短于 ticket_lifetime 中所述的时间段。**这是 TLS 1.2 和 TLS 1.3 的区别，TLS 1.2 中并不包含 ticket 有效时间段(即生存时间)**。\n\nticket_age_add:安全的生成的随机 32 位值，用于模糊 Client 在 “pre_shared_key” 扩展中包含的 ticket 的时间。Client 的 ticket age 以模 2 ^ 32 的形式添加此值，以计算出 Client 要传输的值。Server 必须为它发出的每个 ticket 生成一个新值。\n\nticket_nonce:每一个 ticket 的值，在本次连接中发出的所有的 ticket 中是唯一的。\n\nticket:这个值是被用作 PSK 标识的值。ticket 本身是一个不透明的标签。它可以是数据库查找键，也可以是自加密和自我验证的值。\n\nextensions：ticket 的一组扩展值。Client 必须忽略无法识别的扩展。\n\n\n当前为 NewSessionTicket 定义的唯一扩展名是 “early_data”，表示该 ticket 可用于发送 0-RTT 数据。 它包含以下值：\n\nmax_early_data_size:这个字段表示使用 ticket 时允许 Client 发送的最大 0-RTT 数据量(以字节为单位)。数据量仅计算应用数据有效载荷(即，明文但不填充或内部内容类型字节)。Server 如果接收的数据大小超过了 max_early_data_size 字节的 0-RTT 数据，应该立即使用 “unexpected_message” alert 消息终止连接。请注意，由于缺少加密材料而拒绝 early data 的 Server 将无法区分内容中的填充部分，因此 Client 不应该依赖于能够在 early data 记录中发送大量填充内容。\n\nPSK 关联的 ticket 计算方法如下：\n12HKDF-Expand-Label(resumption_master_secret,                       &quot;resumption&quot;, ticket_nonce, Hash.length) \n\n因为 ticket_nonce 值对于每个 NewSessionTicket 消息都是不同的，所以每个 ticket 会派生出不同的 PSK。\n请注意，原则上可以继续发布新 ticket，该 ticket 无限期地延长生命周期，这个生命周期是最初从初始非 PSK 握手中(最可能与对等证书相关联)派生得到的密钥材料的生命周期。\n建议实现方对密钥材料这些加上总寿命时间的限制。这些限制应考虑到对等方证书的生命周期，干预撤销的可能性以及自从对等方在线 CertificateVerify 签名到当前时间的这段时间。\n完整握手的流程图如下：\n\n握手完成以后，还可能受到 KeyUpdate 的子消息。这个子消息是负责更新密钥以保证 AEAD 安全性的 Key Update(KU) 消息。\nTLS 协议的最终目的是协商出会话过程使用的对称密钥和加密算法，双方最终使用该密钥和对称加密算法对消息进行加密。AEAD（Authenticated_Encrypted_with_associated_data）是 TLS 1.3 中唯一保留和支持的加密方式。AEAD 将完整性校验和数据加密两种功能集成在同一算法中完成。TLS 1.2 还支持流加密和 CBC 分组模式的块加密方法，使用 MAC 来进行完整性校验数据，这两种方式均被证明有一定的安全缺陷。\n但是即使是 AEAD 仍然有研究表明它有一定局限性：使用同一密钥加密的明文达到一定长度后，就不能再保证密文的安全性。因此，TLS 1.3 中引入了密钥更新机制，一方可以（通常是服务器）向另一方发送 Key Update（KU）消息，对方收到消息后对当前会话密钥再使用一次 HKDF，计算出新的会话密钥，使用该密钥完成后续的通信。\n\n\n\n\n\n\n\n\n\n如果想了解更多关于 Key Update 消息的，可以看笔者之前的这篇文章 《Key and Initialization Vector Update》\n八. 直观感受 TLS 1.3 首次握手流程这一章，笔者用 wireshark 抓取 TLS 1.3 握手流程中的数据包，让读者直观感受一下 TLS 1.3 握手流程。\n\n上图是 TLS 1.3 中的 ClientHello 消息。在这个消息的结构中，与 TLS 1.2 差别主要在扩展上。TLS 1.2 中有的扩展，TLS 1.3 也有，但是 TLS 1.3 中多了一些重要的扩展。\n\n上图是 TLS 1.3 中 ClientHello 首次完整握手中所有的扩展。\n\n\n展开这些扩展，可以看到，TLS 1.2 中有的扩展，TLS 1.3 都包含。并且数据结构也都没有发生变化。\n\n这是 TLS 1.3 新增的 key_share 扩展。在这个扩展中，包含了 Client 所能支持的椭圆曲线类型和对应的 (EC)DHE 密钥协商参数。\n\npsk_key_exchange_modes 也是 TLS 1.3 中新增的扩展，这个扩展语意是 Client 仅支持使用具有这些模式的 PSK。这就限制了在这个 ClientHello 中提供的 PSK 的使用，也限制了 Server 通过 NewSessionTicket 提供的 PSK 的使用。\npsk_ke: 代表仅 PSK 密钥建立。在这种模式下，Server 不能提供 “key_share” 值。\npsk_dhe_ke: PSK 和 (EC)DHE 建立。在这种模式下，Client 和 Server 必须提供 “key_share” 值。\n\nsupported_versions 是 TLS 1.3 中必带的扩展，如果没有这个扩展，Server 会认为 Client 只能支持 TLS 1.2，于是接下来的握手会进行 TLS 1.2 的握手流程。\n\n\n在 ServerHello 中回应 Client，supported_versions 扩展中包含了协商以后的协议版本。\n\n在 ServerHello 中也会带上 Server 的密钥协商参数，放在 key_share 扩展中。\n\n\nEncryptedExtensions 子消息中会带和任何不需要建立加密上下文但不与各个证书相互关联的扩展，比如这里的 server_name 和 ALPN 扩展。\n\nCertificate 消息中会带上 OCSP Response 扩展。\n\nChangeCipherSpec 和 Finished 消息与 TLS 1.2 中没有区别。\n\n首次完整握手完成以后，还会发送 NewSessionTicket 消息。在这个消息中会带 early_data 的扩展。如果有这个扩展，就表明 Server 可以支持 0-RTT。如果没有带这个扩展，如下图:\n\n如果没有带这个扩展，表明 Server 不支持 0-RTT，Client 在下次会话恢复的时候不要发送 early_data 扩展。\n九. TLS 1.3 会话恢复这里网上很多文章对 TLS 1.3 第二次握手有误解。经过自己实践以后发现了“真理”。\nTLS 1.3 在宣传的时候就以 0-RTT 为主，大家都会认为 TLS 1.3 再第二次握手的时候都是 0-RTT 的，包括网上一些分析的文章里面提到的最新的 PSK 密钥协商，PSK 密钥协商并非是 0-RTT 的。\nTLS 1.3 再次握手其实是分两种：会话恢复模式、0-RTT 模式。非 0-RTT 的会话恢复模式和 TLS 1.2 在耗时上没有提升，都是 1-RTT，只不过比 TLS 1.2 更加安全了。只有在 0-RTT 的会话恢复模式下，TLS 1.3 才比 TLS 1.2 有提升。具体提升对比见下表:\n\n\n\n\nHTTP&#x2F;2 + TLS 1.2 首次连接\nHTTP&#x2F;2 + TLS 1.2 会话恢复\nHTTP&#x2F;2 + TLS 1.3 首次连接\nHTTP&#x2F;2 + TLS 1.3 会话恢复\nHTTP&#x2F;2 + TLS 1.3 0-RTT\n\n\n\nDNS 解析\n1-RTT\n0-RTT\n1-RTT\n0-RTT\n0-RTT\n\n\nTCP 握手\n1-RTT\n1-RTT\n1-RTT\n1-RTT\n1-RTT\n\n\nTLS 握手\n2-RTT\n1-RTT\n1-RTT\n1-RTT\n0-RTT\n\n\nHTTP 请求\n1-RTT\n1-RTT\n1-RTT\n1-RTT\n1-RTT\n\n\n总计\n5-RTT\n3-RTT\n4-RTT\n3-RTT\n2-RTT\n\n\n如果开启 TCP 的 TFO，收到第一个 HTTPS 响应包的时间，可以在上表的基础上再减少一个 RTT。\n在完整握手中，Client 在收到 Finished 消息以后，还会收到 NewSessionTicket 消息。\n1234567struct &#123;         uint32 ticket_lifetime;         uint32 ticket_age_add;         opaque ticket_nonce&lt;0..255&gt;;         opaque ticket&lt;1..2^16-1&gt;;         Extension extensions&lt;0..2^16-2&gt;;     &#125; NewSessionTicket; \n\nServer 将 ticket_nonce 和发送 Finished 子消息后计算的 resumption_master_secret 一起作为 HKDF-Expand-Label 的入参，计算 NewSessionTicket 中的 ticket 字段：\n12PskIdentity.identity = ticket     \t\t\t\t\t  = HKDF-Expand-Label(resumption_master_secret, &quot;resumption&quot;, ticket_nonce, Hash.length) \n\nTLS 1.2 和 TLS 1.3 的区别，TLS 1.2 中 NewSessionTicket 是主密钥，而 TLS 1.3 中 ticket 只是一个 PSK。Client 收到 NewSessionTicket 以后就可以生成 PskIdentity 了，如果有多个 PskIdentity，就都放在 identities 数组中。binders 数组中是与 identities 顺序一一对应的 HMAC 值 PskBinderEntry。\n123456789101112131415161718struct &#123;         opaque identity&lt;1..2^16-1&gt;;         uint32 obfuscated_ticket_age;     &#125; PskIdentity;     opaque PskBinderEntry&lt;32..255&gt;;     struct &#123;         PskIdentity identities&lt;7..2^16-1&gt;;         PskBinderEntry binders&lt;33..2^16-1&gt;;     &#125; OfferedPsks;     struct &#123;         select (Handshake.msg_type) &#123;             case client_hello: OfferedPsks;             case server_hello: uint16 selected_identity;         &#125;;     &#125; PreSharedKeyExtension; \n\nPskBinderEntry 的计算方法：\n1234 PskBinderEntry = HMAC(binder_key, Transcript-Hash(Truncate(ClientHello1)))\t\t\t\t   = HMAC(Derive-Secret(HKDF-Extract(0, PSK), &quot;ext binder&quot; | &quot;res binder&quot;, &quot;&quot;), Transcript-Hash(Truncate(ClientHello1)))\t\t\t\t   其中     binder_key = Derive-Secret(HKDF-Extract(0, PSK), &quot;ext binder&quot; | &quot;res binder&quot;, &quot;&quot;) \n\nHMAC 会包含 PreSharedKeyExtension.identities 字段。也就是说，HMAC 包含所有的 ClientHello，但是不包含 binder list(否则就出现鸡生蛋，蛋生鸡的死循环问题了)。Truncate() 函数的作用是把 ClientHello 中的 binders list 移除。\nClient 可以把 PSK 保存到本地 cache 中，serverName 作为 cache 的 key。\n1. 会话恢复模式TLS 1.3 中更改了会话恢复机制，废除了原有的 Session ID 和 Session TIcket 的方式，使用 PSK 的机制，同时 New Session Ticket 中添加了过期时间。TLS 1.2 中 的 ticket 不包含过期时间，可以通过 ticket key 的更新让之前所有发送的 ticket 都失效，或者在生成 ticket 的时候加入自定义可以判断过期时间的策略。\n在经历了一次完整握手以后，生成了 PSK，下次握手就会进入会话恢复模式，在 Client hello 中，先在本地 cache 中查找 servername 对应的 PSK，找到后在 Client hello 的 pre_shared_key 扩展中带上两部分\n\nIdentity: NewSessionTicket 中加密的 ticket\nBinder: 由 PSK 导出 binder_key，使用 binder_key 对不包含 binder list 部分的 ClientHello 作 HMAC 计算。\n\n1234Early Secret = HKDF-Extract(0, PSK)      binder_key = Derive-Secret(Early Secret, &quot;ext binder&quot; | &quot;res binder&quot;, &quot;&quot;)      client_early_traffic_secret = Derive-Secret(Early Secret, &quot;c e traffic&quot;, ClientHello)      early_exporter_master_secret = Derive-Secret(Early Secret, &quot;e exp master&quot;, ClientHello) \n\n注意：当存在多种不同类型的扩展的时候，除了 “pre_shared_key” 必须是 ClientHello 的最后一个扩展，其他的扩展间的顺序可以是任意的。(“pre_shared_key” 可以出现在 ServerHello 中扩展块中的任何位置)。不能存在多个同一个类型的扩展。\n通过 resumption secret 导出 PSK。PSK 会最终导出 earlyData 的加密密钥，以及 pre_shared_key 扩展中 binder 的 HMAC 密钥。发送 ClientHello 后，使用 resumption secret 导出的 PskIdentity.identity 生成 PSK，进而导出 client_early_traffic_secret 密钥，再生成 Key 和 IV，对 early data 加密后发送。\nServer 收到带有 PSK 的 ClientHello 以后，生成协商之后的 keyshare，并检查 Client hello 中的 pre_shared_key 扩展，解密 PskIdentity.identity(即 ticket)，查看该 ticket 是否过期，各项检查通过以后，由 PSK 导出 binder_key 并计算 client hello 的 HMAC，检查 binder 是否正确。验证完 ticket 和 binder 之后，在 ServerHello 扩展中带上 pre_shared_key 扩展，标识使用哪个 PSK 进行会话恢复。和 Client 一样，从 resumtion secret 开始导出 PSK，最终导出 earlyData 使用的密钥。后续的密钥导出规则和完整握手是一样的，唯一的区别就是会话恢复多了 PSK，它是作为 early secret 的输入密钥材料 IKM。\nTLS 1.3 和 TLS 1.2 在会话恢复的密钥导出上有很大不同，TLS 1.2 会话恢复会直接使用之前的 master secret，然后生成会话密钥(密钥块)。TLS 1.3 只会利用 resumption secret 导出 early data 密钥的输入密钥材料 IKM —— PSK，之后的密钥导出规则和 TLS 1.3 完整握手是一样的。\n发送完 ServerHello 以后，还需要继续发送 EncryptedExtensions 和 Finished 消息。不过会话恢复模式就不需要再发送 Certificate 和 CerficateVerify 消息了。只要证明了双方都持有相同的 PSK，就不再需要证书认证来证明双方的身份，这样看来，PSK 也算是一种身份认证机制。\n流程图如下：\n12345678910111213141516171819202122232425262728293031Client                                               Server  Initial Handshake:         ClientHello         + key_share               --------&gt;                                                         ServerHello                                                         + key_share                                               &#123;EncryptedExtensions&#125;                                               &#123;CertificateRequest*&#125;                                                      &#123;Certificate*&#125;                                                &#123;CertificateVerify*&#125;                                                          &#123;Finished&#125;                                   &lt;--------     [Application Data*]         &#123;Certificate*&#125;         &#123;CertificateVerify*&#125;         &#123;Finished&#125;                --------&gt;                                   &lt;--------      [NewSessionTicket]         [Application Data]        &lt;-------&gt;      [Application Data]  Subsequent Handshake:         ClientHello         + key_share*         + pre_shared_key          --------&gt;                                                         ServerHello                                                    + pre_shared_key                                                        + key_share*                                               &#123;EncryptedExtensions&#125;                                                          &#123;Finished&#125;                                   &lt;--------     [Application Data*]         &#123;Finished&#125;                --------&gt;         [Application Data]        &lt;-------&gt;      [Application Data] \n\n笔者之前写过一篇关于 PSK 细节分析的文章，如果读者感兴趣，可以看《Pre-Shared Key Extension》。这里简单描述一下 PSK 扩展。\n“pre_shared_key” 扩展用来协商标识的，这个标识是与 PSK 密钥相关联的给定握手所使用的预共享密钥的标识。\n这个扩展中的 “extension_data” 字段包含一个 PreSharedKeyExtension 值:\n123456789101112131415161718struct &#123;         opaque identity&lt;1..2^16-1&gt;;         uint32 obfuscated_ticket_age;     &#125; PskIdentity;     opaque PskBinderEntry&lt;32..255&gt;;     struct &#123;         PskIdentity identities&lt;7..2^16-1&gt;;         PskBinderEntry binders&lt;33..2^16-1&gt;;     &#125; OfferedPsks;     struct &#123;         select (Handshake.msg_type) &#123;             case client_hello: OfferedPsks;             case server_hello: uint16 selected_identity;         &#125;;     &#125; PreSharedKeyExtension; \n\n\nidentity:key 的标签。例如，一个 ticket 或者是一个外部建立的预共享密钥的标签。\n\nobfuscated_ticket_age:age of the key 的混淆版本。这一章节描述了通过 NewSessionTicket 消息建立，如何为标识(identities)生成这个值。对于外部建立的标识(identities)，应该使用 0 的 obfuscated_ticket_age，并且 Server 也必须忽略这个值。\n\nidentities:Client 愿意和 Server 协商的 identities 列表。如果和 “early_data” 一起发送，第一个标识被用来标识 0-RTT 的。\n\nbinders:一系列的 HMAC 值。和 identities 列表中的每一个值都一一对应，并且顺序一致。\n\nselected_identity:Server 选择的标识，这个标识是以 Client 列表中标识表示为基于 0 的索引。\n\n\n每一个 PSK 都和单个哈希算法相关联。对于通过 ticket 建立的 PSK，当 ticket 在连接中被建立，这时候用的哈希算法是 KDF 哈希算法。对于外部建立的 PSK，当 PSK 建立的时候，哈希算法必须设置，如果没有设置，默认算法是 SHA-256。Server 必须确保它选择的是兼容的 PSK (如果有的话) 和密钥套件。\n在接受PSK密钥建立之前，Server 必须先验证相应的 binder 值。如果这个值不存在或者未验证，则 Server 必须立即中止握手。Server 不应该尝试去验证多个 binder，而应该选择单个 PSK 并且仅验证对应于该 PSK 的 binder。为了接受 PSK 密钥建立连接，Server 发送 “pre_shared_key” 扩展，标明它所选择的 identity。\nClient 必须验证 Server 的 selected_identity 是否在 Client 提供的范围之内。Server 选择的加密套件标明了与 PSK 关联的哈希算法，如果 ClientHello “psk_key_exchange_modes” 有需要，Server 还应该发送 “key_share” 扩展。如果这些值不一致，Client 必须立即用 “illegal_parameter” alert 消息中止握手。\n如果 Server 提供了 “early_data” 扩展，Client 必须验证 Server 的 selected_identity 是否为 0。如果返回任何其他值，Client 必须使用 “illegal_parameter” alert 消息中止握手。\n“pre_shared_key” 扩展必须是 ClientHello 中的最后一个扩展(这有利于下面的描述的实现)。Server 必须检查它是最后一个扩展，否则用 “illegal_parameter” alert 消息中止握手。\n(1) Ticket Age从 Client 的角度来看，ticket 的时间指的是，收到 NewSessionTicket 消息开始到当前时刻的这段时间。Client 决不能使用时间大于 ticket 自己标明的 “ticket_lifetime” 这个时间的 ticket。每个 PskIdentity 中的 “obfuscated_ticket_age” 字段都必须包含 ticket 时间的混淆版本，混淆方法是用 ticket 时间(毫秒为单位)加上 “ticket_age_add” 字段，最后对 2^32 取模。除非这个 ticket 被重用了，否则这个混淆就可以防止一些相关联连接的被动观察者。注意，NewSessionTicket 消息中的 “ticket_lifetime” 字段是秒为单位，但是 “obfuscated_ticket_age” 是毫秒为单位。因为 ticke lifetime 限制为一周，32 位就足够去表示任何合理的时间，即使是以毫秒为单位也可以表示。\n(2) PSK BinderPSK binder 的值形成了 2 种绑定关系，一种是 PSK 和当前握手的绑定，另外一种是 PSK 产生以后(如果是通过 NewSessionTicket 消息)的握手和当前握手的绑定。每一个在 binder 列表中的条目都会根据有一部分 ClientHello 的哈希副本计算 HMAC，最终 HMAC 会包含 PreSharedKeyExtension.identities 字段。也就是说，HMAC 包含所有的 ClientHello，但是不包含 binder list 。如果存在正确长度的 binders，消息的长度字段（包括总长度，扩展块的长度和 “pre_shared_key” 扩展的长度）都被设置。\nPskBinderEntry 的计算方法和 Finished 消息一样。但是 BaseKey 是派生的 binder_key，派生方式是通过提供的相应的 PSK 的密钥派生出来的。\n如果握手包括 HelloRetryRequest 消息，则初始的 ClientHello 和 HelloRetryRequest 随着新的 ClientHello 一起被包含在副本中。例如，如果 Client 发送 ClientHello，则其 binder 将通过以下方式计算：\n1Transcript-Hash(Truncate(ClientHello1)) \n\nTruncate() 函数的作用是把 ClientHello 中的 binders list 移除。\n如果 Server 响应了 HelloRetryRequest，那么 Client 会发送 ClientHello2，它的 binder 会通过以下方式计算：\n123Transcript-Hash(ClientHello1,                     HelloRetryRequest,                     Truncate(ClientHello2)) \n\n完整的 ClientHello1&#x2F;ClientHello2 都会包含在其他的握手哈希计算中。请注意，在第一次发送中，Truncate(ClientHello1) 是直接计算哈希的，但是在第二次发送中，ClientHello1 计算哈希，并且还会再注入一条 “message_hash” 消息。\n关于会话恢复密钥的一些计算流程表示出来如下：\n12345678910111213141516170            |            v  PSK -&gt;  HKDF-Extract = Early Secret            |            +-----&gt; Derive-Secret(., &quot;ext binder&quot; | &quot;res binder&quot;, &quot;&quot;)            |                     = binder_key            |            +-----&gt; Derive-Secret(., &quot;c e traffic&quot;, ClientHello)            |                     = client_early_traffic_secret            |            +-----&gt; Derive-Secret(., &quot;e exp master&quot;, ClientHello)            |                     = early_exporter_master_secret            v      Derive-Secret(., &quot;derived&quot;, &quot;&quot;)            |            v \n\nPSK 会话恢复的流程图如下：\n\n2. 0-RTT 模式先来看看 0-RTT 在整个草案里面的变更历史。\n\n\n\n草案\n变更\n\n\n\ndraft-07\n0-RTT 最早是在 draft-07 中加入了基础的支持\n\n\ndraft-11\n1. 在 draft-11 中删除了early_handshake内容类型\n\n\n2. 使用一个 alert 终止 0-RTT 数据\n\n\n\ndraft-13\n1. 删除 0-RTT 客户端身份验证\n\n\n2. 删除 (EC)DHE 0-RTT\n\n\n\n3. 充实 0-RTT PSK 模式并 shrink EarlyDataIndication\n\n\n\ndraft-14\n1. 移除了 0-RTT EncryptedExtensions\n\n\n2. 降低使用 0-RTT 的门槛\n\n\n\n3. 阐明 0-RTT 向后兼容性\n\n\n\n4. 说明 0-RTT 和 PSK 密钥协商的相互关系\n\n\n\ndraft-15\n讨论 0-RTT 时间窗口\n\n\ndraft-16\n1. 禁止使用 0-RTT 和 PSK 的 CertificateRequest\n\n\n2. 放宽要求检查 SNI 的 0-RTT\n\n\n\ndraft-17\n1. 删除 0-RTT Finished 和 resumption_context，并替换为 PSK 本身的 psk_binder 字段\n\n\n2. 协调密码套件匹配的要求：会话恢复只需要匹配 KDF 但是对于 0-RTT 需要匹配整个密码套件。允许 PSK 实际去协商密码套件\n\n\n\n3. 阐明允许使用 PSK 进行 0-RTT 的条件\n\n\n\ndraft-21\n关于 0-RTT 和重放的讨论，建议实现一些反重放机制\n\n\n从历史来看，人们从功能问题讨论到性能问题，最后讨论到安全问题。\n据 Google 统计，全网有 60% 的网站访问流量是来自于新访问的网站和过去曾经访问过但是隔了一段时间再次访问。这部分流量在 TLS 1.3 的优化下，已经从 2-RTT 降低到 1-RTT 了。剩下 40% 的网站访问流量是来自于会话恢复，TLS 1.3 废除了之前的 Session ID 和 Session Ticket 的会话恢复的方式，统一成了 PSK 方式，使得原有会话恢复变的更加安全。但是 TLS 1.3 的会话恢复并没有降低 RTT，依旧停留在了 1-RTT。为了进一步降低延迟，于是提出了 0-RTT 的概念。0-RTT 能让用户有更快更顺滑更好的用户体验，在移动网络上更加明显。\nTLS 1.3 的里程碑标志就是添加了 0-RTT 会话恢复模式。也就是说，当 client 和 server 共享一个 PSK（从外部获得或通过一个以前的握手获得）时，TLS 1.3 允许 client 在第一个发送出去的消息中携带数据（”early data”）。Client 使用这个 PSK 生成 client_early_traffic_secret 并用它加密 early data。Server 收到这个 ClientHello 之后，用 ClientHello 扩展中的 PSK 导出 client_early_traffic_secret 并用它解密 early data。\n0-RTT 会话恢复模式如下：\n123456789101112131415161718Client                                               Server        ClientHello        + early_data        + key_share*        + psk_key_exchange_modes        + pre_shared_key        (Application Data*)     --------&gt;                                                        ServerHello                                                   + pre_shared_key                                                       + key_share*                                              &#123;EncryptedExtensions&#125;                                                      + early_data*                                                         &#123;Finished&#125;                                &lt;--------       [Application Data*]        (EndOfEarlyData)        &#123;Finished&#125;              --------&gt;        [Application Data]      &lt;-------&gt;        [Application Data] \n\n想实现 0-RTT 也是有一些条件的，条件比较苛刻，如果条件有一条不满足，会话恢复都只能是 1-RTT 的 PSK 会话恢复模式。\n0-RTT 的开启条件是：\n\n\nServer 在前一次完整握手中，发送了 NewSessionTicket，并且 Session ticket 中存在max_early_data_size 扩展表示愿意接受 early data。如果没有这个扩展，0-RTT 无法开启。\n\n\n\n在 PSK 会话恢复的过程中，ClientHello 的扩展中配置了 early data 扩展，表示 Client 想要开启 0-RTT 模式。\n\n\n\nServer 在 Encrypted Extensions 消息中携带了 early data 扩展表示同意读取 early data。0-RTT 模式开启成功。\n\n\n\n只有同时满足了上面 3 个条件，才能开启 0-RTT 会话恢复模式。否则握手会是 1-RTT 的会话恢复模式。\n\n\n\n\n\n\n\n\n\n目前不少浏览器虽然支持 TLS 1.3 协议，但是还不支持发送 early data，所以它们也没法启用 0-RTT 模式的会话恢复。\n从 0-RTT 的开启条件中就能看出它和上面 1-RTT 会话恢复的区别。ClientHello 中需要带 early_data 的扩展，Server 要在 Encrypted Extensions 消息中携带了 early_data 扩展，Client 发送完 early_data 数据以后，还需要回一个 EndOfEarlyData 的子消息。\n1struct &#123;&#125; EndOfEarlyData; \n\nClient 在发送 early_data 之后，可以一直发 early_data 数据。如果 Server 在 EncryptedExtensions 中发送了 “early_data” 扩展，则 Client 必须在收到 Server 的 Finished 消息后发送 EndOfEarlyData 消息。 如果 Server 没有在 EncryptedExtensions中发送 “early_data” 扩展，那么 Client 禁止发送 EndOfEarlyData 子消息。此消息表示已传输完了所有 0-RTT application_data消息(如果有)，并且接下来的记录受到握手流量密钥的保护。Server 不能发送此消息，Client 如果收到了这条消息，那么必须使用 “unexpected_message” alert 消息终止连接。这条消息使用从 client_early_traffic_secret 中派生出来的密钥进行加密保护。\n注意: early data 并不参与最后的 Finished 校验计算，其次，EndOfEarlyData 子消息也不参与最后 application traffic secret 的计算。\nServer 在接收到 ClientHello 以后，应立即发送 ServerHello、ChangeCipherSpec、EncryptedExtensions、Finished 子消息。\nServer 想拒绝 Client 的 0-RTT 会话恢复，只要打破 3 个开启条件即可：\n\n拒绝 PSK。Server 在 ServerHello 中不加入 pre_shared_key 扩展，那么握手就会回退到完整握手，自然拒绝了 0-RTT。\n只拒绝 early_data，接受 PSK。在 ServerHello 中，加入 pre_shared_key 扩展，但是EncryptedExtension 子消息中不加入 early_data 扩展。\n\nClient 即使发送握手消息还是带有 early_data 扩展，但是 Server 导出的密钥已经是 server&#x2F;client_handshake_traffic_secret 而不是 client_early_traffic_secret 了，也无法解密 early_data 内容。解密失败出现错误就丢弃这个扩展，忽略它。于是 0-RTT 就会降级到 1-RTT。\n0-RTT 握手的流程图如下：\n\n虽然 TLS 1.3 革命性的提出了 0-RTT 会话恢复模式，但是 0-RTT 存在安全性风险。0-RTT 数据安全性比其他类型的 TLS 数据要弱一些，特别是：\n\n0-RTT 的数据是没有前向安全性的，它使用的是被提供的 PSK 中导出的密钥进行加密的。\n在多个连接之间不能保证不存在重放攻击。普通的 TLS 1.3 1-RTT 数据为了防止重放攻击的保护方法是使用 server 下发的随机数，现在 0-RTT 不依赖于 ServerHello 消息，因此保护措施更差。如果数据与 TLS client 认证或与应用协议里一起验证，这一点安全性的考虑尤其重要。这个警告适用于任何使用 early_exporter_master_secret 的情况。\n\nTLS 1.3 0-RTT 中要预防重放攻击。预防 0-RTT 有 4 种措施：\n\n第一个措施检查 PSK 中的过期时间，如果过期了，就不处理 early_data 中的请求，并且将握手降级到 1-RTT。\n\n第二个措施是不允许非幂等性的请求出现在 0-RTT 中，如果出现了非幂等性的请求，Server 将会忽略不处理，GET 请求是幂等性的，但是也不能允许后面带参数，不带参数的 GET 请求才能允许。\n\n第三个措施是，在请求头中记录 PSK binder 的值或者一个随机值，这个值能保证 0-RTT 的 early_data 全局唯一，这样就可以防止重放攻击。当收到 ClientHello 时，Server 首先验证 PSK binder。然后它会计算 expected_arrival_time，如果它在记录窗口之外，则拒绝 0-RTT，然后回到 1-RTT 握手。如果 expected_arrival_time 在窗口中，则 Server 检查它是否记录了匹配的 ClientHello。如果找到一个，它将使用 “illegal_parameter” alert 消息中止握手或接受 PSK 但拒绝 0-RTT。如果找不到匹配的 ClientHello，则它接受 0-RTT，然后只要 expected_arrival_time 在窗口内，就存储 ClientHello。Server 也可以实现具有误报的数据存储，例如布隆过滤器，在这种情况下，它们必须通过拒绝 0-RTT 来响应明显的重放，但绝不能中止握手。关于这一个措施，还可能存在多个 binder 的情况，如果是分布式系统，还会存在多个 zone 的问题，具体分析见笔者这篇文章 《TLS 1.3 0-RTT and Anti-Replay》。\n\n第四个措施是，在数据库里面记录所有未完成有效的 ticket，使用一次就删除掉，如果产生重放攻击，那么这个 ticket 必然是数据库里面查不到的，那么就回退到完整握手。\n\n\n\n\n\n\n\n\n\n\n\n关于 0-RTT 安全性的问题，笔者专门写了一篇文章探讨这个问题，见《TLS 1.3 0-RTT and Anti-Replay》\n十. 直观感受 TLS 1.3 会话恢复这一章，笔者用 wireshark 抓取 TLS 1.3 会话恢复中的数据包，让读者直观感受一下 TLS 1.3 会话恢复流程。\n1. PSK 会话恢复\n这是 TLS 1.3 会话恢复的完整流程。\n\n\n\n\n上面这 4 个扩展是 TLS 1.3 PSK 会话恢复中 ClientHello 必须配置的。psk_key_exchange_modes、pre_shared_key、key_share、supported_versions。\n\n\n\n上面这 3 个扩展是 TLS 1.3 PSK 会话恢复中 ServerHello 必须配置的。pre_shared_key、key_share、supported_versions。\n\n一旦 PSK 校验完成，Server 就不需要再次发送证书了，直接回应 ChangeCipherSpec、Encrypted Extensions、Finished 即可完成会话恢复。\n2. 0-RTT截止到笔者写这篇文章为止，当前主流浏览器对 TLS 1.3 的支持度如下图。\n\nGoogle Chrome Canary 最新 74.0.3702.0 还不能支持 0-RTT 模式，Firefox Nightly 最新 67.0a1 可以支持 0-RTT 模式(在 about:config 中 security.tls.enable_0rtt_data 设置为 true)，Safari 最新的 12.0.3 (14606.4.5) 还不能支持 0-RTT 模式。所以笔者只能用 Firefox Nightly 抓取 0-RTT 的包。\n当然 OpenSSL 最新版 1.1.1a 的 Client 是支持发送 early_data 的，也就是支持 0-RTT 的，用它来调试 TLS 1.3 0-RTT 也更加方便。\n先来看看支持 0-RTT 的 Firefox Nightly 抓到的包是怎么样的。\n\n\n\n\n\n\n\n\n可以发现整个会话恢复过程满足了 0-RTT 的条件，所以 0-RTT 开启成功。\n在用 OpenSSL 的 Client 来测试测试 0-RTT。\n先将必要参数导出来，比如协商的密钥和 session 信息。\n1$ openssl s_client -connect halfrost.com:443 -tls1_3 -keylogfile=/Users/ydz/Documents/sslkeylog.log -sess_out=/Users/ydz/Documents/tls13.sess \n\n输出如下:\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134CONNECTED(00000006)depth=1 C = US, O = Let&#x27;s Encrypt, CN = Let&#x27;s Encrypt Authority X3verify error:num=20:unable to get local issuer certificate---Certificate chain 0 s:CN = halfrost.com   i:C = US, O = Let&#x27;s Encrypt, CN = Let&#x27;s Encrypt Authority X3 1 s:C = US, O = Let&#x27;s Encrypt, CN = Let&#x27;s Encrypt Authority X3   i:O = Digital Signature Trust Co., CN = DST Root CA X3 2 s:C = US, O = Let&#x27;s Encrypt, CN = Let&#x27;s Encrypt Authority X3   i:O = Digital Signature Trust Co., CN = DST Root CA X3---Server certificate-----BEGIN CERTIFICATE-----MIIEljCCA36gAwIBAgISA9VdA6rPN6mIzBxEPL/3iAICMA0GCSqGSIb3DQEBCwUAMEoxCzAJBgNVBAYTAlVTMRYwFAYDVQQKEw1MZXQncyBFbmNyeXB0MSMwIQYDVQQDExpMZXQncyBFbmNyeXB0IEF1dGhvcml0eSBYMzAeFw0xOTAyMTAwMTQxMjJaFw0xOTA1MTEwMTQxMjJaMBcxFTATBgNVBAMTDGhhbGZyb3N0LmNvbTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABA7sYzIwq29BkT1mQ2TSZRPe34BlnuqN65xoLY+A87M8PpblV0IvNyj4ZdcgiSmSZffocVF6wzck6TmsQ/j2/sujggJyMIICbjAOBgNVHQ8BAf8EBAMCB4AwHQYDVR0lBBYwFAYIKwYBBQUHAwEGCCsGAQUFBwMCMAwGA1UdEwEB/wQCMAAwHQYDVR0OBBYEFOD4YIpf+PkD1Jvy+eayPn0csEi/MB8GA1UdIwQYMBaAFKhKamMEfd265tE5t6ZFZe/zqOyhMG8GCCsGAQUFBwEBBGMwYTAuBggrBgEFBQcwAYYiaHR0cDovL29jc3AuaW50LXgzLmxldHNlbmNyeXB0Lm9yZzAvBggrBgEFBQcwAoYjaHR0cDovL2NlcnQuaW50LXgzLmxldHNlbmNyeXB0Lm9yZy8wKQYDVR0RBCIwIIIMaGFsZnJv\\ghfhjghjjbmd3cuaGFsZnJvc3QuY29tMEwGA1UdIARFMEMwCAYGZ4EMAQIBMDcGCysGAQQBgt8TAQEBMCgwJgYIKwYBBQUHAgEWGmh0dHA6Ly9jcHMubGV0c2VuY3J5cHQub3JnMIIBAwYKKwYBBAHWeQIEAgSB9ASB8QDvAHUA4mlLribo6UAJ6IYbtjuD1D7n/nSI+6SPKJMBnd3x2/4AAAFo1UfZTgAABAMARjBEAiAsXJLCA5uO2R926Dba3fZpV/zvzG9tCPVtTKAeso5bAwIgMXoLRtLqhG5bEcXIpGXJcrd06S8tbUdS9YRAIWpMX1oAdgApPFGWVMg5ZbqqUPxYB9S3b79Yeily3KTDDPTlRUf0eAAAAWjVR9lQAAAEAwBHMEUCIHv6NJ9MWMiL+AHxU8ilL3APMmPkUcc03SjBiDaWVm6JAiEA5YF/XHKuYH0S0+mqfB+YdT0FIey9wFQObkR4/Qvzla4wDQYJKoZIhvcNAQELBQADggEBAHU7a+EgzdhrsyD+2ch7AGD1n1TjDfdxkEjmoitN0Tjh4q3jP/IK7FPs0LBsDRusmtJVK3gZQc9cTEy/om86VQtcnV0LhK83GnFUIuLTEzeTZmnz6Qbs3KznprZH0DRUbfpmZsDNIfBEOUOXiBR4DpLd3tPVfRkQowmO6o39vM4UOGlB0zIAg977q97IT6wS9BCEiGmuF0HSjpLfiPhTy9bpl2VGcJVpIy2TS+d4+JWRI7K5BFSzncGDzHJ+zGsx4wS+dxuiwaS9hw4c0FG2V4kMFnA+orAa/oTnfwFlRIehTbDBO+rNTNtjm4yh63M9gInoQEI1REl2EkGcWug6Ijs=-----END CERTIFICATE-----subject=CN = halfrost.comissuer=C = US, O = Let&#x27;s Encrypt, CN = Let&#x27;s Encrypt Authority X3---No client certificate CA names sentPeer signing digest: SHA256Peer signature type: ECDSAServer Temp Key: X25519, 253 bits---SSL handshake has read 3912 bytes and written 316 bytesVerification error: unable to get local issuer certificate---New, TLSv1.3, Cipher is TLS_AES_256_GCM_SHA384Server public key is 256 bitSecure Renegotiation IS NOT supportedCompression: NONEExpansion: NONENo ALPN negotiatedEarly data was not sentVerify return code: 20 (unable to get local issuer certificate)------Post-Handshake New Session Ticket arrived:SSL-Session:    Protocol  : TLSv1.3    Cipher    : TLS_AES_256_GCM_SHA384    Session-ID: DECE5063ABC2D1162A5E767C55083FDFFA6A86B64082FE3AD990A213AE    Session-ID-ctx:    Resumption PSK: EACCC93ACB3DC420DF5027BEC576EE130D11BF546463034C1BB92B54806057E0C9F5C3DB557AD10D425E    PSK identity: None    PSK identity hint: None    SRP username: None    TLS session ticket lifetime hint: 86400 (seconds)    TLS session ticket:    0000 - 0b 8d e5 44 b2 62 71 9d-f9 0a ec da f0 d0 6a 0b   ...D.bq.......j.    0010 - 97 5d 63 21 ea 1e 8a 69-01 52 a9 0a 19 bf 5c a3   .]c!...i.R....\\.    0020 - 67 45 a3 a0 28 65 ea 9c-c8 d4 cf df 5d c5 5a be   gE..(e......].Z.    0030 - 32 45 0d 1e af f7 32 67-4a d8 66 cb b6 cb c8 0e   2E....QgJ.f.....    0040 - 6b b8 53 a8 d2 d4 4b 7b-cc a6 cb 52 39 61 20 6d   k.S...K&#123;...R9a m    0050 - 75 f8 cb 43 11 1d 58 a2-de 2b 74 b0 ca 70 a2 9c   u..C..X..+t..p..    0060 - 85 6b 1a 00 9a f1 bd 9b-8c b4 5a 41 aa 4b 64 5d   .k........ZA.Kd]    0070 - 5a 48 23 a6 10 49 4f 61-c9 57 74 f4 56 50 83 1a   ZH#..IOa.Wt.VP..    0080 - 1b 74 6c ea 09 99 42 f5-d6 3c 6d 4f 5b 98 ca b3   .tl...B..&lt;mO....    0090 - c7 72 56 5c 6c 67 71 77-8d 68 f7 54 e5 e3 7b d3   .rV\\lgqw.h.T..&#123;.    00a0 - 24 ff 42 0c 3f 12 27 42-7f 9e 0a 4c c2 79 60 45   $.B.?.&#x27;B...L.y`E    00b0 - 2d 77 a2 c8 2f f5 85 34-fa ce 79 ee 0b ea 00 c1   -w../..4..y.....    00c0 - 74 33 f0 6c af 7a 1a 55-f8 35 bd 5e 49 66 6f 06   t3.l.z.U.5.^Ifo.    00d0 - c6 38 ed a6 82 e2 c8 77-99 b7 34 9a 4a 9a 31 40   .8.....w..4.J.1@    00e0 - f1 93 a0 94 7f 1e 8d e0-54 29 dc e3 6f 5c 93 21   ........T)..o\\.!    Start Time: 1549886406    Timeout   : 7200 (sec)    Verify return code: 20 (unable to get local issuer certificate)    Extended master secret: no    Max Early Data: 16384---read R BLOCK---Post-Handshake New Session Ticket arrived:SSL-Session:    Protocol  : TLSv1.3    Cipher    : TLS_AES_256_GCM_SHA384    Session-ID: B7E28DE5DF2C95F2E3DE43732E4F9A45A8943ED3856B73CAB5E7260E7    Session-ID-ctx:    Resumption PSK: BF2BA2304BEB2B948F7BF6617D0KDRNFB9CD5466DEC1EB9697D2543B7BB913BC7854359D7F5DF7559D67    PSK identity: None    PSK identity hint: None    SRP username: None    TLS session ticket lifetime hint: 86400 (seconds)    TLS session ticket:    0000 - 0b 8d e5 44 b2 62 71 9d-f9 0a ec da f0 d0 6a 0b   ...D.bq.......j.    0010 - b4 9f cc 17 63 9a 70 c8-63 f8 2e c4 9f d4 a1 f8   ....c.p.c.......    0020 - 22 34 22 03 d0 f9 78 66-a0 d4 2f 62 53 d3 d8 e3   &quot;4&quot;...xf../bS...    0030 - 55 2c a5 7c 0b 19 b3 fc-77 55 8c de 0b 2d 00 bd   U,.|....wUL..-..    0040 - b8 fa 2e 00 30 78 c8 dc-35 14 d3 61 f0 69 38 59   ...%0x..5..a.i8Y    0050 - ee 2a 75 7e 50 34 3f e3-25 04 71 1c 6e c9 c8 20   .*u~P4?.%.q.n..    0060 - d7 4e 44 b3 69 56 50 23-38 c2 f1 1e ac 10 a7 ff   .ND.iVP#8.......    0070 - 96 cf fe ff 4d 07 7e 08-2d 37 49 78 ab 1d 78 6e   ....M.~.-7Ix..xn    0080 - 62 4b 99 e7 37 03 3e a2-89 de 61 48 a1 c5 77 18   bK..7.&gt;...aH..w.    0090 - 6f 1c 95 8a 0d 1d 17 68-88 8a 01 5b f0 dc ea 06   o......h...[....    00a0 - 98 dc 7e 94 f8 ef 4a 72-ff ba e5 03 07 c7 3d d0   ..~...Jr......=.    00b0 - c8 91 a6 ae 9a df 92 25-05 63 77 03 b0 bc b4 ab   .......%.c......    00c0 - 36 cb 0f 8c 5d ec 58 65-7c 97 2a 30 57 4a 96 b9   6...].Xe|.*0WJ..    00d0 - 60 21 12 76 77 4c 6d 0d-12 0c 50 cc f5 da 54 4e   `!.vwLm...P...TN    00e0 - 4b 27 5f 1b dd 11 b1 8d-7f e0 37 43 34 a3 88 34   K&#x27;_.......7C4..4    Start Time: 1549886406    Timeout   : 7200 (sec)    Verify return code: 20 (unable to get local issuer certificate)    Extended master secret: no    Max Early Data: 16384---read R BLOCK \n\n接下来在复用刚刚的连接，命令如下:\n1$ openssl s_client -connect halfrost.com:443 -tls1_3 -keylogfile=/Users/ydz/Documents/sslkeylog.log -sess_in=/Users/ydz/Documents/tls13.sess -early_data=/Users/ydz/Documents/req.txt \n\nreq.txt 里面只是简单的写一个 GET 请求:\n123GET / HTTP/1.1HOST: halfrost.comEarly-Data: 657567765 \n\n执行 s_client 以后，输出如下:\n12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849CONNECTED(00000006)---Server certificate-----BEGIN CERTIFICATE-----MIIElzCCA3+gAwIBAgISA604VEs+7Wwch5cNQDshC4t+MA0GCSqGSIb3DQEBCwUAMEoxCzAJBgNVBAYTAlVTMRYwFAYDVQQKEw1MZXQncyBFbmNyeXB0MSMwIQYDVQQDExpMZXQncyBFbmNyeXB0IEF1dGhvcml0eSBYMzAeFw0xODEyMDgxMzQzMzhaFw0xOTAzMDgxMzQzMzhaMBcxFTATBgNVBAMTDGhhbGZyb3N0LmNvbTBZMBMGByqGSM49AgEGCCqGSM49AwEHA0IABA7sYzIwq29BkT1mQ2TSZRPe34BlnuqN65xoLY+A87M8PpblV0IvNyj4ZdcgiSmSZffocVF6wzck6TmsQ/j2/sujggJzMIICbzAOBgNVHQ8BAf8EBAMCB4AwHQYDVR0lBBYwFAYIKwYBBQUHAwEGCCsGAQUFBwMCMAwGA1UdEwEB/wQCMAAwHQYDVR0OBBYEFOD4YIpf+PkD1Jvy+eayPn0csEi/MB8GA1UdIwQYMBaAFKhKamMEfd265tE5t6ZFZe/zqOyhMG8GCCsGAQUFBwEBBGMwYTAuBggrBgEFBQcwAYYiaHR0cDovL29jc3AuaW50LXgzLmxldHNlbmNyeXB0Lm9yZzAvBggrBgEFBQcwAoYjaHR0cDovL2NlcnQuaW50LXgzLmxldHNlbmNyeXB0Lm9yZy8wKQYDVR0RBCIwIIIMaGFsZnJvc3QuY29tghB3d3cuaGFsZnJvc3QuY29tMEwGA1UdIARFMEMwCAYGZ4EMAQIBMDcGCysGAQQBgt8TAQEBMCgwJgYIKwYBBQUHAgEWGmh0dHA6Ly9jcHMubGV0c2VuY3J5cHQub3JnMIIBBAYKKwYBBAHWeQIEAgSB9QSB8gDwAHUA4mlLribo73qkwe6lN9vZWu1dJV8+Q41cFLGYMJhDD56x7QIgL+V6g1CQst9UDXobdkAEnjahKiJWihr/Qn3plzgzjiIAdwApPFGWVMg5ZbqqUPxYB9S3b79Yeily3KTDDPTlRUf0eAAAAWeORhq2AAAEAwBIMEYCIQD1Mf1GtmegyTqIu0S3Q4afNDt0srIFyrtROtn0jQAV1gIhAJwXIGyMj87kjHtRc/mHJOOCZRSUvoasvWrytCv2dPwXMA0GCSqGSIb3DQEBCwUAA4IBAQB3sC7jKVGHR8MnAOWnECO/V5Z4oBqbahogwyhOSrbxuutijhyk8kb3A73Q++Ey150Y+hlNUQStmG9JBGg9pyLG2Yug9p5L13a6VrNaL1VQ1Dq6YgS55J8ElsalUgr+9jvTJesdYzfXPdsc8IK67tBXhukqc0/cT3I1QHNwAVru/AKWrkneH4AcadSeLGe5he2X9OV3JJg+gb/vE90UaVmqwUuSGMzluyBXPMuznTa/+7+31vWVQ8aWE32X+E5qHSyeLU808mZHYjvKHvuDnNNu6I0KlNcVJf1s0jOQOjgo7hIP/OR4OlW6ywk07IupV4w07xykP1/tWBsSCviXECcZ-----END CERTIFICATE-----subject=CN = halfrost.comissuer=C = US, O = Let&#x27;s Encrypt, CN = Let&#x27;s Encrypt Authority X3---No client certificate CA names sentServer Temp Key: X25519, 253 bits---SSL handshake has read 245 bytes and written 649 bytesVerification error: unable to get local issuer certificate---Reused, TLSv1.3, Cipher is TLS_AES_256_GCM_SHA384Server public key is 256 bitSecure Renegotiation IS NOT supportedCompression: NONEExpansion: NONENo ALPN negotiatedEarly data was acceptedVerify return code: 20 (unable to get local issuer certificate)--- \n\n从输出中可以看到 Early data was accepted。这个时候转到 wireshark，看抓到的包是怎么样的。\n\n可以看到 Client 在 ClientHello 之后，就立即发送了 Application Data。\n在 wireshark 中首选项，把下图中的勾去掉。\n\n配置生效以后，可以看到 Application Data 里面的请求了。\n\n普通的 GET 请求中 header 中带了 Early-Data 的值。这个值就会传给 Server 处理了。\n十一. TLS 1.3 的状态机TLS 1.3 相对 TLS 1.2 握手流程发生了巨大的变化，所以状态机也发生了巨大的变化。下面放 2 张状态流转图，最为总结，对应的也是本篇文章的精华。\n123456789101112131415161718192021222324252627282930START &lt;----+              Send ClientHello |        | Recv HelloRetryRequest         [K_send = early data] |        |                               v        |          /                 WAIT_SH ----+          |                    | Recv ServerHello          |                    | K_recv = handshake      Can |                    V     send |                 WAIT_EE    early |                    | Recv EncryptedExtensions     data |           +--------+--------+          |     Using |                 | Using certificate          |       PSK |                 v          |           |            WAIT_CERT_CR          |           |        Recv |       | Recv CertificateRequest          |           | Certificate |       v          |           |             |    WAIT_CERT          |           |             |       | Recv Certificate          |           |             v       v          |           |              WAIT_CV          |           |                 | Recv CertificateVerify          |           +&gt; WAIT_FINISHED &lt;+          |                  | Recv Finished          \\                  | [Send EndOfEarlyData]                             | K_send = handshake                             | [Send Certificate [+ CertificateVerify]]   Can send                  | Send Finished   app data   --&gt;            | K_send = K_recv = application   after here                v                         CONNECTED \n\n这图是 Client 在握手流程上的状态机。如果读者还不清楚中间的某个步骤，可以对照上文中的内容查缺补漏。\n12345678910111213141516171819202122232425262728293031323334353637383940START &lt;-----+              Recv ClientHello |         | Send HelloRetryRequest                               v         |                            RECVD_CH ----+                               | Select parameters                               v                            NEGOTIATED                               | Send ServerHello                               | K_send = handshake                               | Send EncryptedExtensions                               | [Send CertificateRequest]Can send                       | [Send Certificate + CertificateVerify]app data                       | Send Finishedafter   --&gt;                    | K_send = applicationhere                  +--------+--------+             No 0-RTT |                 | 0-RTT                      |                 |  K_recv = handshake  |                 | K_recv = early data[Skip decrypt errors] |    +------&gt; WAIT_EOED -+                      |    |       Recv |      | Recv EndOfEarlyData                      |    | early data |      | K_recv = handshake                      |    +------------+      |                      |                        |                      +&gt; WAIT_FLIGHT2 &lt;--------+                               |                      +--------+--------+              No auth |                 | Client auth                      |                 |                      |                 v                      |             WAIT_CERT                      |        Recv |       | Recv Certificate                      |       empty |       v                      | Certificate |    WAIT_CV                      |             |       | Recv                      |             v       | CertificateVerify                      +-&gt; WAIT_FINISHED &lt;---+                               | Recv Finished                               | K_recv = application                               v                           CONNECTED \n\n这图是 Server 在握手流程上的状态机。如果读者还不清楚中间的某个步骤，可以对照上文中的内容查缺补漏。读者能理解透上面这 2 张状态机，TLS 1.3 也就掌握透彻了。\n全文完。\n\nReference：\nRFC 8466TLS1.3 draft-28\n\n\n\n\n\n\n\n\n\nGitHub Repo：Halfrost-Field\nFollow: halfrost · GitHub\nSource: https://halfrost.com/HTTPS_handshake&#x2F;\n\n","slug":"NETWORK/直观感受 TLS 握手流程(下)","date":"2024-03-08T10:08:28.000Z","categories_index":"TLS,NETWORK","tags_index":"https,RTT,PSK","author_index":"dandeliono"},{"id":"18647109cd1271b82271b0b5bf74c52c","title":"直观感受 TLS 握手流程(上)","content":"直观感受 TLS 握手流程(上)在 HTTPS 开篇的文章中，笔者分析了 HTTPS 之所以安全的原因是因为 TLS 协议的存在。TLS 能保证信息安全和完整性的协议是记录层协议。(记录层协议在上一篇文章中详细分析了)。看完上篇文章的读者可能会感到疑惑，TLS 协议层加密的密钥是哪里来的呢？客户端和服务端究竟是如何协商 Security Parameters 加密参数的？这篇文章就来详细的分析一下 TLS 1.2 和 TLS 1.3 在 TLS 握手层上的异同点。\nTLS 1.3 在 TLS 1.2 的基础上，针对 TLS 握手协议最大的改进在于提升速度和安全性。本篇文章会重点分析这两块。\n一. TLS 对网络请求速度的影响由于部署了 HTTPS，传输层增加了 TLS，对一个完成的请求耗时又会多增加一些。具体会增加几个 RTT 呢？\n先来看看一个请求从零开始，完整的需要多少个 RTT。假设访问一个 HTTPS 网站，用户从 HTTP 开始访问，到收到第一个 HTTPS 的 Response，大概需要经历一下几个步骤(以目前最主流的 TLS 1.2 为例)：\n\n\n\n流程\n消耗时间\n总计\n\n\n\n1. DNS 解析网站域名\n1-RTT\n\n\n\n2. 访问 HTTP 网页 TCP 握手\n1-RTT\n\n\n\n3. HTTPS 重定向 302\n1-RTT\n\n\n\n4. 访问 HTTPS 网页 TCP 握手\n1-RTT\n\n\n\n5. TLS 握手第一阶段 Say Hello\n1-RTT\n\n\n\n6. 【证书校验】CA 站点的 DNS 解析\n1-RTT\n\n\n\n7. 【证书校验】CA 站点的 TCP 握手\n1-RTT\n\n\n\n8. 【证书校验】请求 OCSP 验证\n1-RTT\n\n\n\n9. TLS 握手第二阶段 加密\n1-RTT\n\n\n\n10. 第一个 HTTPS 请求\n1-RTT\n\n\n\n\n\n10-RTT\n\n\n在上面这些步骤中，1、10 肯定无法省去，6、7、8 如果浏览器本地有缓存，是可选的。将剩下的画在流程图上，见下图：\n\n针对上面的步骤进行一些说明：\n用户第一次访问网页需要解析 DNS，DNS 解析以后会被浏览器缓存下来，只要是 TTL 没有过期，期间的所有访问都不需要再耗 DNS 的解析时间了。另外如果有 HTTPDNS，也会缓存解析之后的结果。所以第一步并非每次都要花费 1-RTT。\n如果网站做了 HSTS (HTTP Strict Transport Security)，那么上面的第 3 步就不存在，因为浏览器会直接替换掉 HTTP 的请求，变成 HTTPS 的，防止重定向的中间人攻击。\n如果浏览器有主流 CA 的域名解析缓存，也不需要进行上面的第 6 步，直接访问即可。\n如果浏览器关闭掉了 OCSP 或者是有本地缓存，那么也不需要进行上面的第 7 和第 8 步。\n上面这 10 步是最最完整的流程，一般有各种缓存不会经历上面每一步。如果有各种缓存，并且有 HSTS 策略，所以用户每次访问网页都必须要经历的流程如下：\n\n\n\n流程\n消耗时间\n总计\n\n\n\n1. 访问 HTTPS 网页 TCP 握手\n1-RTT\n\n\n\n2. TLS 握手第一阶段 Say Hello\n1-RTT\n\n\n\n3. TLS 握手第二阶段 加密\n1-RTT\n\n\n\n4. 第一个 HTTPS 请求\n1-RTT\n\n\n\n\n\n4-RTT\n\n\n除去 4 是无论如何都无法省掉的以外，剩下的就是 TCP 和 TLS 握手了。 TCP 想要减至 0-RTT，目前来看有点难。那 TLS 呢？目前 TLS 1.2 完整一次握手需要 2-RTT，能再减少一点么？答案是可以的。\n二. TLS&#x2F;SSL 协议概述TLS 握手协议运行在 TLS 记录层之上，目的是为了让服务端和客户端就协议版本达成一致, 选择加密算法, 选择性的彼此相互验证对方, 使用公钥加密技术生成共享密钥——即协商出 TLS 记录层加密和完整性保护需要用到的 Security Parameters 加密参数。协商的过程中还必须要保证网络中传输的信息不能被篡改，伪造。由于协商需要在网络上来来回回花费几个来回，所以 TLS 的网络耗时基本上很大一部分花费在网络 RTT 上了。\n和加密参数关系最大的是密码套件。客户端和服务端在协商过程中需要匹配双方的密码套件。然后双方握手成功以后，基于密码套件协商出所有的加密参数，加密参数中最重要的就是主密钥(master secret)。\n握手协议主要负责协商一个会话，这个会话由以下元素组成:\n\nsession identifier:由服务端选取的一个任意字节的序列用于辨识一个活动的或可恢复的连接状态。\n\npeer certificate:对端的 X509v3 [PKIX]证书。这个字段可以为空。\n\ncompression method:加密之前的压缩算法。这个字段在 TLS 1.2 中用的不多。在 TLS 1.3 中这个字段被删除。\n\ncipher spec:指定用于产生密钥数据的伪随机函数(PRF)，块加密算法(如：空，AES 等)，和 MAC 算法(如：HMAC-SHA1)。它也定义了密码学属性如 mac_length。这个字段在 TLS 1.3 标准规范中已经删除，但是为了兼容老的 TLS 1.2 之前的协议，实际使用中还可能存在。在 TLS 1.3 中，密钥导出用的是 HKDF 算法。具体 PRF 和 HKDF 的区别会在之后的一篇文章中详细分析。\n\nmaster secret:client 和 server 之间共享的 48 字节密钥。\n\nis resumable:一个用于标识会话是否能被用于初始化新连接的标签。\n\n\n上面这些字段随后会被用于产生安全参数并由记录层在保护应用数据时使用。利用TLS握手协议的恢复特性，使用相同的会话可以实例化许多连接。\nTLS 握手协议包含如下几步:\n\n交换 Hello 消息, 交换随机数和支持的密码套件列表, 以协商出密码套件和对应的算法。检查会话是否可恢复\n交换必要的密码参数以允许 client 和 server 协商预备主密钥 premaster secret\n交换证书和密码信息以允许 client 和 server 进行身份认证\n从预备主密钥 premaster secret 和交换的随机数中生成主密钥 master secret\n为 TLS 记录层提供安全参数(主要是密码块)\n允许 client 和 server 验证它们的对端已经计算出了相同的安全参数, 而且握手过程不被攻击者篡改\n\n下面行文思路会按照 TLS 首次握手，会话恢复的顺序，依次对比 TLS 1.2 和 TLS 1.3 在握手上的不同，并且结合 Wireshark 抓取实际的网络包进行分析讲解。最后分析一下 TLS 1.3 新出的 0-RTT 是怎么回事。\n三. TLS 1.2 首次握手流程TLS 1.2 握手协议主要流程如下：\nClient 发送一个 ClientHello 消息，Server 必须回应一个 ServerHello 消息或产生一个验证的错误并且使连接失败。ClientHello 和 ServerHello 用于在 Client 和 Server 之间建立安全性增强的能力。ClientHello 和 ServerHello 建立了如下的属性: 协议版本，会话 ID，密码套件，压缩算法。此外，产生并交换两个随机数: ClientHello.random 和 ServerHello.random。\n密钥交换中使用的最多 4 个消息: Server Certificate, ServerKeyExchange, Client Certificate 和 ClientKeyExchange。新的密钥交换方法可以通过这些方法产生:为这些消息指定一个格式, 并定义这些消息的用法以允许 Client 和 Server 就一个共享密钥达成一致。这个密钥必须很长；当前定义的密钥交换方法交换的密钥大于 46 字节。\n在 hello 消息之后, Server 会在 Certificate 消息中发送它自己的证书，如果它即将被认证。此外，如果需要的话，一个 ServerKeyExchange 消息会被发送(例如, 如果 Server 没有证书, 或者它的证书只用于签名，RSA 密码套件就不会出现 ServerKeyExchange 消息)。如果 Server 被认证过了，如果对于已选择的密码套件来说是合适的话，它可能会要求 Client 发送证书。接下来，Server 会发送 ServerHelloDone 消息，至此意味着握手的 hello 消息阶段完成。Server 将会等待 Client 的响应。如果 Server 发送了一个 CertificateRequest 消息，Client 必须发送 Certificate 消息。现在 ClientKeyExchange 消息需要发送, 这个消息的内容取决于 ClientHello 和 ServerHello 之间选择的公钥算法。如果 Client 发送了一个带签名能力的证书, 则需要发送以一个数字签名的 CertificateVerify 消息，以显式验证证书中私钥的所有权。\n这时，Client 发送一个 ChangeCipherSpec 消息，并且复制 pending 的 Cipher Spec 到当前的 Cipher Spec 中. 然后 Client 在新算法, 密钥确定后立即发送 Finished 消息。作为回应，Server 会发送它自己的 ChangeCipherSpec 消息, 将 pending 的 Cipher Spec 转换为当前的 Cipher Spec，在新的 Cipher Spec 下发送 Finished 消息。这时，握手完成，Client 和 Server 可以开始交换应用层数据。应用数据一定不能在第一个握手完成前(在一个非TLS_NULL_WITH_NULL_NULL 类型的密码套件建立之前)发送。\n用经典的图表示一次完成的握手就是下图：\n12345678910111213141516Client                                               Server     ClientHello                  --------&gt;                                                     ServerHello                                                    Certificate*                                              ServerKeyExchange*                                             CertificateRequest*                                  &lt;--------      ServerHelloDone     Certificate*     ClientKeyExchange     CertificateVerify*     [ChangeCipherSpec]     Finished                     --------&gt;                                              [ChangeCipherSpec]                                  &lt;--------             Finished     Application Data             &lt;-------&gt;     Application Data \n\n* 号意味着可选择的或者并不会一直被发送的条件依赖形消息。\n为了防止 pipeline stalls，ChangeCipherSpec 是一种独立的 TLS 协议内容类型，并且事实上它不是一种 TLS 消息。所以图上 “[]“ 代表的是，ChangeCipherSpec 并不是 TLS 的消息的意思。\nTLS 握手协议是 TLS 记录协议的一个已定义的高层客户端。这个协议用于协商一个会话的安全属性。握手消息封装传递给 TLS 记录层，这里它们会被封装在一个或多个 TLSPlaintext 结构中，这些结构按照当前活动会话状态所指定的方式被处理和传输。\n123456789101112131415161718192021222324252627282930enum &#123;         hello_request(0),          client_hello(1),          server_hello(2),         certificate(11),          server_key_exchange (12),         certificate_request(13),          server_hello_done(14),         certificate_verify(15),          client_key_exchange(16),         finished(20),          (255)     &#125; HandshakeType;     struct &#123;         HandshakeType msg_type;             uint24 length;                      select (HandshakeType) &#123;             case hello_request:       HelloRequest;             case client_hello:        ClientHello;             case server_hello:        ServerHello;             case certificate:         Certificate;             case server_key_exchange: ServerKeyExchange;             case certificate_request: CertificateRequest;             case server_hello_done:   ServerHelloDone;             case certificate_verify:  CertificateVerify;             case client_key_exchange: ClientKeyExchange;             case finished:            Finished;         &#125; body;     &#125; Handshake; \n\n握手协议消息在下文中会以发送的顺序展现；没有按照期望的顺序发送握手消息会导致一个致命错误，握手失败。然而，不需要的握手消息会被忽略。需要注意的是例外的顺序是：证书消息在握手（从 Server 到 Client，然后从 Client到 Server）过程中会使用两次。不被这些顺序所约束的一个消息是 HelloRequest 消息，它可以在任何时间发送，但如果在握手中间收到这条消息，则应该被 Client 忽略。\n1. Hello 子消息Hello 阶段的消息用于在 Client 和 Server 之间交换安全增强能力。当一个新的会话开始时，记录层连接状态加密，hash，和压缩算法被初始化为空。当前连接状态被用于重协商消息。\n(1) Hello RequestHelloRequest 消息可以在任何时间由 Server 发送。\n这个消息的含义: HelloRequest 是一个简单的通知，告诉 Client 应该开始重协商流程。在响应过程中，Client 应该在方便的时候发送一个 ClientHello 消息。这个消息并不是意图确定哪端是 Client 或 Server，而仅仅是发起一个新的协商。Server 不应该在 Client 发起连接后立即发送一个 HelloRequest。\n如果 Client当前正在协商一个会话时，HelloRequest 这个消息会被 Client忽略。如果 Client 不想重新协商一个会话，或 Client 希望响应一个 no_renegotiation alert 消息，那么也可能忽略 HelloRequest 消息。因为握手消息意图先于应用数据被传送，它希望协商会在少量记录消息被 Client 接收之前开始。如果 Server 发送了一个 HelloRequest 但没有收到一个 ClientHello 响应，它应该用一个致命错误 alert 消息关闭连接。在发送一个 HelloRequest 之后，Server 不应该重复这个请求直到随后的握手协商完成。\nHelloRequest 消息的结构:\n1struct &#123; &#125; HelloRequest; \n\n这个消息不能被包含在握手消息中维护的消息 hash 中, 也不能用于结束的消息和证书验证消息。\n(2) Client Hello当一个 Client 第一次连接一个 Server 时，发送的第一条消息必须是 ClientHello。Client 也能发送一个 ClientHello 作为对 HelloRequest 的响应，或用于自身的初始化以便在一个已有连接中重新协商安全参数。\n123456789101112131415161718struct &#123;            uint32 gmt_unix_time;            opaque random_bytes[28];        &#125; Random;             struct &#123;         ProtocolVersion client_version;         Random random;         SessionID session_id;         CipherSuite cipher_suites&lt;2..2^16-2&gt;;         CompressionMethod compression_methods&lt;1..2^8-1&gt;;         select (extensions_present) &#123;             case false:                 struct &#123;&#125;;             case true:                 Extension extensions&lt;0..2^16-1&gt;;         &#125;;     &#125; ClientHello; \n\n\ngmt_unix_time:依据发送者内部时钟以标准 UNIX 32 位格式表示的当前时间和日期(从1970年1月1日UTC午夜开始的秒数, 忽略闰秒)。基本 TLS 协议不要求时钟被正确设置；更高层或应用层协议可以定义额外的需求. 需要注意的是，出于历史原因，该字段使用格林尼治时间命名，而不是 UTC 时间。\n\nrandom_bytes:由一个安全的随机数生成器产生的 28 个字节数据。\n\nclient_version:Client 愿意在本次会话中使用的 TLS 协议的版本. 这个应当是 Client 所能支持的最新版本(值最大)，TLS 1.2 是 3.3，TLS 1.3 是 3.4。\n\nrandom:一个 Client 所产生的随机数结构 Random。随机数的结构体 Random 在上面展示出来了。客户端的随机数，这个值非常有用，生成预备主密钥的时候，在使用 PRF 算法计算导出主密钥和密钥块的时候，校验完整的消息都会用到，随机数主要是避免重放攻击。\n\nsession_id:Client 希望在本次连接中所使用的会话 ID。如果没有 session_id 或 Client 想生成新的安全参数，则这个字段为空。这个字段主要用在会话恢复中。\n\ncipher_suites:Client 所支持的密码套件列表，Client最倾向使用的排在最在最前面。如果 session_id 字段不空(意味着是一个会话恢复请求)，这个向量必须至少包含那条会话中的 cipher_suite。cipher_suites 字段可以取的值如下：\n\n\n12345678910111213141516171819202122232425262728293031CipherSuite TLS_NULL_WITH_NULL_NULL               = &#123; 0x00,0x00 &#125;;     CipherSuite TLS_RSA_WITH_NULL_MD5                 = &#123; 0x00,0x01 &#125;;     CipherSuite TLS_RSA_WITH_NULL_SHA                 = &#123; 0x00,0x02 &#125;;     CipherSuite TLS_RSA_WITH_NULL_SHA256              = &#123; 0x00,0x3B &#125;;     CipherSuite TLS_RSA_WITH_RC4_128_MD5              = &#123; 0x00,0x04 &#125;;     CipherSuite TLS_RSA_WITH_RC4_128_SHA              = &#123; 0x00,0x05 &#125;;     CipherSuite TLS_RSA_WITH_3DES_EDE_CBC_SHA         = &#123; 0x00,0x0A &#125;;     CipherSuite TLS_RSA_WITH_AES_128_CBC_SHA          = &#123; 0x00,0x2F &#125;;     CipherSuite TLS_RSA_WITH_AES_256_CBC_SHA          = &#123; 0x00,0x35 &#125;;     CipherSuite TLS_RSA_WITH_AES_128_CBC_SHA256       = &#123; 0x00,0x3C &#125;;     CipherSuite TLS_RSA_WITH_AES_256_CBC_SHA256       = &#123; 0x00,0x3D &#125;;     CipherSuite TLS_DH_DSS_WITH_3DES_EDE_CBC_SHA      = &#123; 0x00,0x0D &#125;;     CipherSuite TLS_DH_RSA_WITH_3DES_EDE_CBC_SHA      = &#123; 0x00,0x10 &#125;;     CipherSuite TLS_DHE_DSS_WITH_3DES_EDE_CBC_SHA     = &#123; 0x00,0x13 &#125;;     CipherSuite TLS_DHE_RSA_WITH_3DES_EDE_CBC_SHA     = &#123; 0x00,0x16 &#125;;     CipherSuite TLS_DH_DSS_WITH_AES_128_CBC_SHA       = &#123; 0x00,0x30 &#125;;     CipherSuite TLS_DH_RSA_WITH_AES_128_CBC_SHA       = &#123; 0x00,0x31 &#125;;     CipherSuite TLS_DHE_DSS_WITH_AES_128_CBC_SHA      = &#123; 0x00,0x32 &#125;;     CipherSuite TLS_DHE_RSA_WITH_AES_128_CBC_SHA      = &#123; 0x00,0x33 &#125;;     CipherSuite TLS_DH_DSS_WITH_AES_256_CBC_SHA       = &#123; 0x00,0x36 &#125;;     CipherSuite TLS_DH_RSA_WITH_AES_256_CBC_SHA       = &#123; 0x00,0x37 &#125;;     CipherSuite TLS_DHE_DSS_WITH_AES_256_CBC_SHA      = &#123; 0x00,0x38 &#125;;     CipherSuite TLS_DHE_RSA_WITH_AES_256_CBC_SHA      = &#123; 0x00,0x39 &#125;;     CipherSuite TLS_DH_DSS_WITH_AES_128_CBC_SHA256    = &#123; 0x00,0x3E &#125;;     CipherSuite TLS_DH_RSA_WITH_AES_128_CBC_SHA256    = &#123; 0x00,0x3F &#125;;     CipherSuite TLS_DHE_DSS_WITH_AES_128_CBC_SHA256   = &#123; 0x00,0x40 &#125;;     CipherSuite TLS_DHE_RSA_WITH_AES_128_CBC_SHA256   = &#123; 0x00,0x67 &#125;;     CipherSuite TLS_DH_DSS_WITH_AES_256_CBC_SHA256    = &#123; 0x00,0x68 &#125;;     CipherSuite TLS_DH_RSA_WITH_AES_256_CBC_SHA256    = &#123; 0x00,0x69 &#125;;     CipherSuite TLS_DHE_DSS_WITH_AES_256_CBC_SHA256   = &#123; 0x00,0x6A &#125;;     CipherSuite TLS_DHE_RSA_WITH_AES_256_CBC_SHA256   = &#123; 0x00,0x6B &#125;; \n\n\ncompression_methods:这是 Client 所支持的压缩算法的列表，按照 Client所倾向的顺序排列。如果 session_id 字段不空(意味着是一个会话恢复请求)，它必须包含那条会话中的 compression_method。这个向量中必须包含, 所有的实现也必须支持 CompressionMethod.null。因此，一个 Client 和 Server 将能就压缩算法协商打成一致。\n\nextensions:Clients 可以通过在扩展域中发送数据来请求 Server 的扩展功能。和证书中的扩展一样，TLS&#x2F;SSL 协议中也支持扩展，可以在不用修改协议的基础上提供更多的可扩展性。\n\n\n如果一个 Client 使用扩展来请求额外的功能, 并且这个功能 Server 并不支持, 则 Client可以中止握手。一个 Server 必须接受带有或不带扩展域的 ClientHello 消息，并且(对于其它所有消息也是一样)它必须检查消息中数据的数量是否精确匹配一种格式；如果不是，它必须发送一个致命”decode_error” alert 消息。\n发送 ClientHello 消息之后，Client 会等待 ServerHello 消息。Server 返回的任何握手消息，除 HelloRequest 外, 均被作为一个致命的错误。\nTLS 允许在 compression_methods 字段之后的 extensions 块中添加扩展。通过查看在 ClientHello 结尾处，compression_methods 后面是否有多余的字节就能检测到扩展是否存在。需要注意的是这种检测可选数据的方法与正常的 TLS 变长域不一样，但它可以用于与扩展还没有定义之前的 TLS 相互兼容。\nClientHello 消息包含一个变长的 Session ID 会话标识符。如果非空，这个值标识了同一对 Client 和 Server 之间的会话，Client 希望重新使用这个会话中 Server 的安全参数。\nSession ID 会话标识符可能来自于一个早期的连接，本次连接，或来自另一个当前活动的连接。第二个选择是有用的，如果 Client 只是希望更新随机数据结构并且从一个连接中导出数值；第三个选择使得在无需重复全部握手协议的情况下就能够建立若干独立的安全连接。这些独立的连接可能先后顺序建立或同时建立。一个 Session ID 在双方交换 Finished 消息，握手协商完成是开始有效，并持续到由于过期或在一个与会话相关联的连接上遭遇致命错误导致它被删除为止。Session ID 的实际内容由 Server 定义。\n1opaque SessionID&lt;0..32&gt;; \n\n由于 Session ID 在传输时没有加密或直接的 MAC 保护，Server 一定不能将机密信息放在 Session ID 会话标识符中或使用伪造的会话标识符的内容，都是违背安全原则。(需要注意的是握手的内容作为一个整体, 包括 SessionID, 是由在握手结束时交换的 Finished 消息再进行保护的)。\n密码族列表, 在 ClientHello 消息中从 Client 传递到 Server，以 Client 所倾向的顺序(最推荐的在最先)包含了 Client 所支持的密码算法。每个密码族定义了一个密钥交互算法，一个块加密算法(包括密钥长度)，一个 MAC 算法，和一个随机数生成函数 PRF。Server 将选择一个密码套件，如果没有可以接受的选择，在返回一个握手失败 alert 消息然后关闭连接。如果列表包含了 Server 不能识别，支持或希望使用的密码套件，Server 必须忽略它们，并正常处理其余的部分。\n1uint8 CipherSuite[2]; \n\nClientHello 保护了 Client 所支持的压缩算法列表，按照 Client 的倾向进行排序。\n(3) Server Hello当 Server 能够找到一个可接受的算法集时，Server 发送这个消息作为对 ClientHello 消息的响应。如果不能找到这样的算法集, 它会发送一个握手失败 alert 消息作为响应。\nServer Hello 消息的结构是:\n12345678910111213struct &#123;         ProtocolVersion server_version;         Random random;         SessionID session_id;         CipherSuite cipher_suite;         CompressionMethod compression_method;         select (extensions_present) &#123;             case false:                 struct &#123;&#125;;             case true:                 Extension extensions&lt;0..2^16-1&gt;;         &#125;;     &#125; ServerHello; \n\n通过查看 compression_methods 后面是否有多余的字节在 ServerHello 结尾处就能探测到扩展是否存在。\n\nserver_version:这个字段将包含 Client 在 Client hello 消息中建议的较低版本和 Server 所能支持的最高版本。TLS 1.2 版本是 3.3，TLS 1.3 是 3.4 。\n\nrandom:这个结构由 Server 产生并且必须独立于 ClientHello.random 。这个随机数值和 Client 的随机数一样，这个值非常有用，生成预备主密钥的时候，在使用 PRF 算法计算导出主密钥和密钥块的时候，校验完整的消息都会用到，随机数主要是避免重放攻击。\n\nsession_id:这是与本次连接相对应的会话的标识。如果 ClientHello.session_id 非空，Server 将在它的会话缓存中进行匹配查询。如果匹配项被找到，且 Server 愿意使用指定的会话状态建立新的连接，Server 会将与 Client 所提供的相同的值返回回去。这意味着恢复了一个会话并且规定双方必须在 Finished 消息之后继续进行通信。否则这个字段会包含一个不同的值以标识新会话。Server 会返回一个空的 session_id 以标识会话将不再被缓存从而不会被恢复。如果一个会话被恢复了，它必须使用原来所协商的密码套件。需要注意的是没有要求 Server 有义务恢复任何会话，即使它之前提供了一个 session_id。Client 必须准备好在任意一次握手中进行一次完整的协商，包括协商新的密码套件。\n\ncipher_suite:由 Server 在 ClientHello.cipher_suites 中所选择的单个密码套件。对于被恢复的会话, 这个字段的值来自于被恢复的会话状态。从安全性考虑，应该以服务器配置为准。\n\ncompression_method:由 Server 在 ClientHello.compression_methods 所选择的单个压缩算法。对于被恢复的会话，这个字段的值来自于被恢复的会话状态。\n\nextensions:扩展的列表. 需要注意的是只有由 Client 给出的扩展才能出现在 Server 的列表中。\n\n\n2. Server Certificate无论何时经过协商一致以后的密钥交换算法需要使用证书进行认证的，Server 就必须发送一个 Certificate。Server Certificate 消息紧跟着 ServerHello 之后，通常他们俩者在同一个网络包中，即同一个 TLS 记录层消息中。\n如果协商出的密码套件是 DH_anon 或者 ECDH_annon，则 Server 不应该发送该消息，因为可能会遇到中间人攻击。其他的情况，只要不是需要证书进行认证的，Server 都可以选择不发送此条子消息。\n这个消息的作用是：\n这个消息把 Server 的证书链传给 Client。\n证书必须适用于已协商的密码套件的密钥交互算法和任何已协商的扩展。\n这个消息的结构是：\n12345opaque ASN.1Cert&lt;1..2^24-1&gt;;     struct &#123;         ASN.1Cert certificate_list&lt;0..2^24-1&gt;;     &#125; Certificate; \n\n\ncertificate_list:这是一个证书序列(链)。每张证书都必须是 ASN.1Cert 结构。发送者的证书必须在列表的第一个位置。每个随后的证书必须直接证明它前面的证书。假设远端必须已经拥有它以便在任何情况下验证它，在这个假设下，因为证书验证要求根密钥是独立分发的，所以可以从链中省略指定根证书颁发机构的自签名证书。根证书集成到了 Client 的根证书列表中，没有必要包含在 Server 证书消息中。\n\n相同的消息类型和结果将用于 Client 端对一个证书请求消息的响应。需要注意的是一个 Client 可能不发送证书, 如果它没有合适的证书来发送以响应 Server 的认证请求。\n如下的规则会被应用于 Server 发送的证书:\n\n证书类型必须是 X.509v3, 除非显式协商了其它的类型(如 [TLSPGP])。\n终端实体证书的公钥(和相关的限制)必须与选择的密钥交互算法兼容。\n“server_name”和”trusted_ca_keys”扩展 [TLSEXT] 被用于指导证书选择。\n\n\n\n\n密钥交换算法\n证书类型\n\n\n\nRSA\n\n\n\nRSA_PSK\n证书中包含 RSA 公钥，该公钥可以进行密码协商，也就是使用 RSA 密码协商算法；证书必须允许密钥用于加密(如果 key usage 扩展存在的话，则 keyEncipherment 位必须被设置，表示允许服务器公钥用于密码协商)\n\n\n注:RSA_PSK 定义于 [TLSPSK]\n\n\n\nDHE_RSA\n\n\n\nECDHE_RSA\n证书中包含 RSA 公钥，可以使用 ECDHE 或者 DHE 进行密钥协商；证书必须允许密钥使用 Server 密钥交互消息中的签名机制和 hash 算法进行签名 (如果 key usage 扩展存在的话，digitalSignature 位必须设置，RSA 公钥就可以进行数字签名)\n\n\n注: ECDHE_RSA定义于 [TLSECC]\n\n\n\nDHE_DSS\n证书包含 DSA 公钥; 证书必须允许密钥用于使用将在 Server 密钥交换消息中使用的散列算法进行签名\n\n\nDH_DSS\n\n\n\nDH_RSA\n证书中包含 DSS 或 RSA 公钥，使用 Diffie-Hellman 进行密钥协商; 如果 key usage 扩展存在的话，keyAgreement 位必须设置，目前这种套件已经很少见了。\n\n\nECDH_ECDSA\n\n\n\nECDH_RSA\n证书包含 ECDSA 或 RSA 公钥，使用 ECDH-capable 进行密钥协商。由于是静态密钥协商算法，ECDH 的参数和公钥包含在证书中; 公钥必须使用一个能够被 Client 支持的曲线和点格式, 正如 [TLSECC] 中描述的那样。目前这种套件已经很少见了，因为 ECDH 不支持前向安全性\n\n\nECDHE_ECDSA\n证书包含 ECDSA-capable 公钥，使用 ECDHE 算法协商预备主密钥; 证书必须允许密钥用于使用将在 Server 密钥交换消息中使用的散列算法进行签名;公钥必须使用一个能够被 Client 支持的曲线和点格式，Client 通过 Client Hello 消息中的 ec_point_formats 扩展指定支持的命名曲线，正如 [TLSECC] 中描述的那样。这是 TLS 1.2 中最安全，性能最高的密码套件。\n\n\n如果 Client 提供了一个 “signature_algorithms” 扩展，则 Server 提供的所有证书必须由出现在这个扩展中的一个 hash&#x2F;签名算法对进行签名。需要注意的是这意味着一个包含了一个签名算法密钥的证书应该被一个不同的签名算法签名(例如，RSA 密钥被 DSA 密钥签名)。这个与 TLS 1.1 不同，TLS 1.1 中要求算法是相同的。更进一步也说明了 DH_DSS，DH_RSA，ECDH_ECDSA，和 ECDH_RSA 套件的后半部分对应的公钥不会用来加密或者数字签名，没有存在的必要性，并且后半部分也不限制 CA 机构签发证书所选用的数字签名算法。固定的 DH 证书可以被出现在扩展中的任意 hash&#x2F;签名算法对签名。DH_DSS，DH_RSA，ECDH_ECDSA，和 ECDH_RSA 是历史上的名称。\n如果 Server 有多个证书, 它基于上述标准(此外其它的标准有:传输层端点，本地配置和偏好等)选择其中一个。如果 Server 只有一个证书，它应该尝试使这个证书符合这些标准。\n需要注意的是有很多证书使用无法与 TLS 兼容的算法或算法组合。例如，一个使用 RSASSA-PSS 签名密钥的证书(在 SubjectPublicKeyInfo 中是 id-RSASSA-PSS OID)不能被使用因为 TLS 没有定义相应的签名算法。\n正如密钥套件指定了用于 TLS 协议的新的密钥交换方法，它们也同样指定了证书格式和要求的编码的按键信息。\n至此已经涉及到了 Client 签名算法、证书签名算法、密码套件、Server 公钥，这 4 者相互有关联，也有没有关系的。\n\nClient 签名算法需要和证书签名算法相互匹配，如果 Client Hello 中的 signature_algorithms 扩展与证书链中的证书签名算法不匹配的话，结果是握手失败。\n\nServer 公钥与证书签名算法无任何关系。证书中包含 Server 证书，证书签名算法对 Server 公钥进行签名，但是 Server 公钥的加密算法可以是 RSA 也可以是 ECDSA。\n\n密码套件和 Server 公钥存在相互匹配的关系，因为密码套件中的身份验证算法指的就是 Server 公钥类型。\n\n\n例如 TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 这个密码套件：\n密钥协商算法是 ECDHE，身份验证算法是 ECDSA，加密模式是 AES_256_GCM，由于 GCM 是属于 AEAD 加密模式，所以整个密码套件无须另外的 HMAC，SHA384 指的是 PRF 算法。\n这里的身份验证并不是指的证书由哪种数字签名算法签名的，而是指的证书中包含的 Server 公钥是什么类型的公钥。\n所以 Client Hello 中的 signature_algorithms 扩展需要和证书链中的签名算法匹配，如果不匹配，就无法验证证书中的 Server 公钥，也需要和双方协商出来的密码套件匹配，如果不匹配，就无法使用 Server 公钥。\n3. Server Key Exchange Message这个消息会紧随在 Server 证书消息之后发送(如果是一个匿名协商的话，会紧随在 Server Hello消息之后发送)；\nServerKeyExchange 消息由 Server 发送，但仅在 Server 证书消息(如果发送了)没有包含足够的数据以允许 Client 交换一个预密钥时。这个限制对于如下的密钥交换算法是成立的:\n123456DHE_DSS        DHE_RSA        ECDHE_ECDSA        ECDHE_RSA        DH_anon        ECDH_anon \n\n对于上面前 4 个密码套件，由于使用的临时的 DH&#x2F;ECDH 密钥协商算法，证书中是不包含这些动态的 DH 信息(DH 参数和 DH 公钥)，所以需要使用 Server Key Exchange 消息传递这些信息。传递的动态 DH 信息需要使用 Server 私钥进行签名加密。\n对于上面后 2 个密码套件，是匿名协商，使用的静态的 DH&#x2F;ECDH 密钥协商算法，而且它们也没有证书消息(Server Certificate 消息)，所以同样需要使用 Server Key Exchange 消息传递这些信息。传递的静态 DH 信息需要使用 Server 私钥进行签名加密。\n对于如下密钥交换算法发送 ServerKeyExchange 是非法的:\n123RSA        DH_DSS        DH_RSA \n\n对于 RSA 加密套件，Client 不需要额外参数就可以计算出预备主密钥，然后使用 Server 的公钥加密发送给 Server 端，所以不需要 Server Key Exchange 可以完成协商。\n对于 DH_DSS 和 DH_RSA，在证书中就会包含静态的 DH 信息，也不需要发送 Server Key Exchange，Client 和 Server 双方可以各自协商出预备主密钥的一半密钥，合起来就是预备主密钥了。这两种密码套件目前很少用，一般 CA 不会在证书中包含静态的 DH 信息，也不太安全。\n其它的密钥交换算法，如在 [TLSECC] 中所定义的那些，必须指定是否发送 ServerKeyExchange；如果消息发送了，则必须指定发送内容。\nServerKeyExchange 这个消息的目的就是传递了必要的密码信息，使得 Client 可以完成预备主密钥的通信：获得一个 Client 可用于完成一个密钥交换的 Diffie-Hellman 公钥(结果就是生成预备主密钥)或一个其它算法的公钥。\nDH 参数的结构是:\n123456789enum &#123; dhe_dss, dhe_rsa, dh_anon, rsa, dh_dss, dh_rsa,ec_diffie_hellman                     &#125; KeyExchangeAlgorithm;     struct &#123;         opaque dh_p&lt;1..2^16-1&gt;;         opaque dh_g&lt;1..2^16-1&gt;;         opaque dh_Ys&lt;1..2^16-1&gt;;     &#125; ServerDHParams; \n\n\ndh_p:用于 Diffie-Hellman 操作的素模数，即大质数。\n\ndh_g:用于 Diffie-Hellman 操作的生成器\n\ndh_Ys:Server 的 Diffie-Hellman 公钥 (g^X mod p)\n\n\nServer 需要传递额外参数的密码套件主要 6 种，之前提到过，DHE_DSS、DHE_RSA、ECDHE_ECDSA、ECDHE_RSA、DH_anon、ECDH_anon，其他的密码套件不可用于 ServerKeyExchange 这个消息中。一般 HTTPS 都会部署这 4 种密码套件：ECDHE_RSA、DHE_RSA、ECDHE_ECDSA、RSA。\n\n\n\n\n\n\n\n\n\n关于 TLS 中 ECC 相关描述在 RFC4492 这篇文档中\n\n\n\n密钥交换算法\n描述\n\n\n\nECDH_ECDSA\n静态的 ECDH + ECDSA 签名证书\n\n\nECDHE_ECDSA\n动态的 ECDH + ECDSA 签名证书\n\n\nECDH_RSA\n静态的 ECDH + RSA 签名证书\n\n\nECDHE_RSA\n动态的 ECDH + RSA 签名证书\n\n\nECDH_anon\n匿名的 ECDH + 无签名证书\n\n\nECDHE 参数的结构是:\n1234struct &#123;           ECParameters    curve_params;           ECPoint         public;       &#125; ServerECDHParams; \n\nECC public 公钥的数据结构如下：\n123struct &#123;           opaque point &lt;1..2^8-1&gt;;       &#125; ECPoint; \n\nECC 椭圆曲线的类型:\n12345678910111213enum &#123;            explicit_prime (1),            explicit_char2 (2),           named_curve (3),            reserved(248..255)        &#125; ECCurveType;               struct &#123;           opaque a &lt;1..2^8-1&gt;;           opaque b &lt;1..2^8-1&gt;;       &#125; ECCurve;                enum &#123; ec_basis_trinomial, ec_basis_pentanomial &#125; ECBasisType; \n\n支持的所有命名曲线:\n123456789101112131415enum &#123;           sect163k1 (1), sect163r1 (2), sect163r2 (3),           sect193r1 (4), sect193r2 (5), sect233k1 (6),           sect233r1 (7), sect239k1 (8), sect283k1 (9),           sect283r1 (10), sect409k1 (11), sect409r1 (12),           sect571k1 (13), sect571r1 (14), secp160k1 (15),           secp160r1 (16), secp160r2 (17), secp192k1 (18),           secp192r1 (19), secp224k1 (20), secp224r1 (21),           secp256k1 (22), secp256r1 (23), secp384r1 (24),           secp521r1 (25),           reserved (0xFE00..0xFEFF),           arbitrary_explicit_prime_curves(0xFF01),           arbitrary_explicit_char2_curves(0xFF02),           (0xFFFF)       &#125; NamedCurve; \n\nECDH 参数的数据结构：\n12345678910111213141516171819202122232425262728struct &#123;           ECCurveType    curve_type;           select (curve_type) &#123;               case explicit_prime:                   opaque      prime_p &lt;1..2^8-1&gt;;                   ECCurve     curve;                   ECPoint     base;                   opaque      order &lt;1..2^8-1&gt;;                   opaque      cofactor &lt;1..2^8-1&gt;;               case explicit_char2:                   uint16      m;                   ECBasisType basis;                   select (basis) &#123;                       case ec_trinomial:                           opaque  k &lt;1..2^8-1&gt;;                       case ec_pentanomial:                           opaque  k1 &lt;1..2^8-1&gt;;                           opaque  k2 &lt;1..2^8-1&gt;;                           opaque  k3 &lt;1..2^8-1&gt;;                   &#125;;                   ECCurve     curve;                   ECPoint     base;                   opaque      order &lt;1..2^8-1&gt;;                   opaque      cofactor &lt;1..2^8-1&gt;;               case named_curve:                   NamedCurve namedcurve;           &#125;;       &#125; ECParameters; \n\nECCurveType 表示 ECC 类型每个人可以自行指定椭圆曲线的公式，基点等参数，但是在 TLS&#x2F;SSL 协议中一般都是使用已经命名好的命名曲线 NamedCurve，这样也更加安全。\nServerECDHParams 中包含了 ECParameters 参数和 ECPoint 公钥。\n最后来看看 ServerKeyExchange 消息的数据结构：\n12345678910111213141516171819202122struct &#123;         select (KeyExchangeAlgorithm) &#123;             case dh_anon:                 ServerDHParams params;             case dhe_dss:             case dhe_rsa:                 ServerDHParams params;                 digitally-signed struct &#123;                     opaque client_random[32];                     opaque server_random[32];                     ServerDHParams params;                 &#125; signed_params;             case rsa:             case dh_dss:             case dh_rsa:                 struct &#123;&#125; ;                             case ec_diffie_hellman:                 ServerECDHParams    params;                 Signature           signed_params;         &#125;;     &#125; ServerKeyExchange; \n\n\nparams:Server 密钥协商需要的参数。\n\nsigned_params:对于非匿名密钥交换, 这是一个对 Server 密钥协商参数的签名。\n\n\nServerKeyExchange 根据 KeyExchangeAlgorithm 类型的不同，加入了不同的参数。对于匿名协商，不需要证书，所以也不需要身份验证，没有证书。DHE 开头的协商算法，Server 需要发给 Client 动态的 DH 参数 ServerDHParams 和 数字签名。这里的数字签名会包含 Client 端传过来的随机数，Server 端生成的随机数和 ServerDHParams。\nRSA、DH_DSS、DH_RSA 这 3 个不需要 ServerKeyExchange 消息。\n如果是动态的 ECDH 协商算法，Server 需要把 ServerECDHParams 参数和签名发给 Client。签名的数据结构如下：\n123456789101112enum &#123; ecdsa &#125; SignatureAlgorithm;         select (SignatureAlgorithm) &#123;             case ecdsa:                 digitally-signed struct &#123;                     opaque sha_hash[sha_size];                 &#125;;         &#125; Signature;                ServerKeyExchange.signed_params.sha_hash           SHA(ClientHello.random + ServerHello.random +                                             ServerKeyExchange.params); \n\n这里的签名里面包含的是 Client 随机数、Server 随机数 和 ServerKeyExchange.params 三者求 SHA。\n如果 Client已经提供了 “signature_algorithms” 扩展，签名算法和 hash 算法必须成对出现在扩展中。需要注意的是这里可能会有不一致的可能，例如，Client 可能提供 DHE_DSS 密钥交换算法，但却在 “signature_algorithms” 扩展中忽略任何与 DSA 配对的组合。为了达成正确的密码协商，Server 必须在选择密码套件之前检查与 “signature_algorithms” 扩展可能冲突的密码套件。这并不算是一个优雅的方案，只能算是一个折中的方案，对原来密码套件的设计改动最小。\n此外，hash 和签名算法必须与位于 Server 的终端实体证书中的密钥相兼容。RSA 密钥可以与任何允许的 hash 算法配合使用, 并满足任何证书的约束(如果有的话)。\n4. Certificate Request一个非匿名的 Server 可以选择性地请求一个 Client 发送的证书，如果相互选定的密码套件合适的话。如果 ServerKeyExchange 消息发送了的话，就紧跟在 ServerKeyExchange 消息的后面。如果 ServerKeyExchange 消息没有发送的话，就跟在 Server Certificate 消息后面。\n这个消息的结构是:\n12345678910111213141516171819202122enum &#123;         rsa_sign(1),          dss_sign(2),          rsa_fixed_dh(3),          dss_fixed_dh(4),         rsa_ephemeral_dh_RESERVED(5),          dss_ephemeral_dh_RESERVED(6),         fortezza_dms_RESERVED(20),          ecdsa_sign(64),          rsa_fixed_ecdh(65),         ecdsa_fixed_ecdh(66),         (255)     &#125; ClientCertificateType;          opaque DistinguishedName&lt;1..2^16-1&gt;;     struct &#123;         ClientCertificateType certificate_types&lt;1..2^8-1&gt;;         SignatureAndHashAlgorithm           supported_signature_algorithms&lt;2^16-1&gt;;         DistinguishedName certificate_authorities&lt;0..2^16-1&gt;;     &#125; CertificateRequest; \n\n\ncertificate_types:client 可以提供的证书类型的列表。rsa_sign: 一个包含 RSA 密钥的证书dss_sign: 一个包含 DSA 密钥的证书rsa_fixed_dh: 一个包含静态 DH 密钥的证书dss_fixed_dh: 一个包含静态 DH 密钥的证书\n\nsupported_signature_algorithms:一个 hash&#x2F;签名算法对列表供 Server选择，按照偏好降序排列\n\ncertificate_authorities:可接受的 certificate_authorities 的名称列表，以 DER 编码的格式体现。这些名称可以为一个根 CA 或一个次级 CA 指定一个期望的名称；因此，这个消息可以被用于描已知的根和期望的认证空间。如果 certificate_authorities 列表为空，则 Client 可以发送 ClientCertificateType 中的任意证书，除非存在有一些属于相反情况的外部设定。\n\n\ncertificate_types 和 supported_signature_algorithms 域的交互关系某种程度上有些复杂，certificate_type 自从 SSLv3 开始就在 TLS 中存在，但某种程度上并不规范。它的很多功能被 supported_signature_algorithms 所取代。应遵循下述 3 条规则:\n\nClient 提供的任何证书必须使用在 supported_signature_algorithms 中存在的 hash&#x2F;签名算法对来签名。\n\nClinet 提供的终端实体的证书必须包含一个与 certificate_types 兼容的密钥。如果这个密钥是一个签名密钥，它必须能与 supported_signature_algorithms 中的一些 hash&#x2F;签名算法对一起使用。\n\n出于历史原因，一些 Client 证书类型的名称包含了签名证书的算法。例如，在早期版本的 TLS 中，rsa_fixed_dh 意味着一个用 RSA 签名并且还包含一个静态 DH 密钥的证书。在 TLS 1.2 中，这个功能被 supported_signature_algorithms 废除，证书类型不再限制签名证书的算法。例如，如果 Server 发送了 dss_fixed_dh 证书类型和 ((sha1, dsa), (sha1, rsa)) 签名类型，Client 可以回复一个包含一个静态 DH 密钥的证书，用 RSA-SHA1 签名。\n\n\n\n\n\n\n\n\n\n\n\n注: 一个匿名 Server 请求认证 Client 会产生一个致命的 handshake_failure 警告错误。\n5. Server Hello DoneServerHelloDone 消息已经被 Server 发送以表明 ServerHello 及其相关消息的结束。发送这个消息之后, Server 将会等待 Client 发过来的响应。\n这个消息意味着 Server 发送完了所有支持密钥交换的消息，Client 能继续它的密钥协商，证书校验等步骤。\n在收到 ServerHelloDone 消息之后，Client 应当验证 Server 提供的是否是有效的证书，如果有要求的话, 还需要进一步检查 Server hello 参数是否可以接受。\n这个消息的结构:\n1struct &#123; &#125; ServerHelloDone; \n\n6. Client Certificate这是 Client 在收到一个 ServerHelloDone 消息后发送的第一个消息。这个消息只能在 Server 请求一个证书时发送。如果没有合适的证书，Client 必须发送一个不带证书的证书消息。即, certificate_list 结构体的长度是 0。如果 Client 不发送任何证书，Server 可以自行决定是否可以在不验证 Client 的情况下继续握手，或者回复一个致命 handshake_failure 警报 alert 信息。而且, 如果证书链某些方面不能接受(例如, 它没有被一个知名的可信 CA 签名)，Server 可以自行决定是否继续握手(考虑到 Client 无认证)或发送一个致命的警报 alert 信息。\nClient 证书的数据结构和 Server Certificate 是相同的。\nClient Certificate 消息的目的是传递 Client 的证书链给 Server；当验证 CertificateVerify 消息时(当 Client 的验证基于签名时)Server 会用它来验证或计算预备主密钥(对于静态的 Diffie-Hellman)。证书必须适用于已协商的密码套件的密钥交换算法, 和任何已协商的扩展.\n尤其是:\n\n证书类型必须是 X.509v3, 除非显示协商其它类型(例如, [TLSPGP])。\n\n终端实体证书的公钥(和相关的限制)应该与列在 CertificateRequest 中的证书类型兼容:\n\n\n\n\n\nClient 证书类型\n证书密钥类型\n\n\n\nrsa_sign\n证书包含 RSA公钥；证书必须允许这个密钥被用于签名, 并与签名方案和 hash 算法一起被用于证书验证消息。\n\n\ndss_sign\n证书 DSA公钥；证书必须允许这个密钥被用于签名, 并与 hash 算法一起被用于证书验证消息。\n\n\necdsa_sign\n证书包含 ECDSA 的公钥；证书必须允许这个密钥被用于签名, 并与 hash 算法一起被用于证书验证消息；这个公钥必须使用一个曲线和 Server 支持的点的格式。\n\n\nrsa_fixed_dh\n\n\n\ndss_fixed_dh\n证书包含 Diffie-Hellman 公钥；必须使用与 Server 的密钥相同的参数\n\n\nrsa_fixed_ecdh\n\n\n\necdsa_fixed_ecdh\n证书包含 ECDH 公钥；必须使用与 Server 密钥相同的曲线，并且必须使用 Server 支持的点格式\n\n\n\n如果列出在证书请求中的 certificate_authorities 非空，证书链中的一个证书应该被一个列出来的 CA 签发。\n\n证书必须被一个可接受的 hash&#x2F;签名算法对签名，正如 Certificate Request 那部分描述的那样。需要注意的是这放宽了在以前的 TLS 版本中对证书签名算法的限制。\n\n\n需要注意的是, 与 Server 证书一样，有一些证书使用了当前不能用于当前 TLS 的算法&#x2F;算法组合。\n7. Client Key Exchange Message这个消息始终由 Client 发送。如果有 Client Certificate 消息的话，Client Key Exchange 紧跟在 Client Certificate 消息之后发送。如果不存在Client Certificate 消息的话，它必须是在 Client 收到 ServerHelloDone 后发送的第一个消息。\n这个消息的含义是，在这个消息中设置了预备主密钥，或者通过 RSA 加密后直接传输，或者通过传输 Diffie-Hellman 参数来允许双方协商出一致的预备主密钥。\n当 Client 使用一个动态的 Diffie-Hellman 指数时，这个消息就会包含 Client 的 Diffie-Hellman 公钥。如果 Client 正在发送一个包含一个静态 DH 指数(例如，它正在进行 fixed_dh Client 认证)的证书时，这个消息必须被发送但必须为空。\n这个消息的结构:\n这个消息的选项依赖于选择了哪种密钥交互方法。关于 KeyExchangeAlgorithm 的定义，见 Server Key Exchange Message 这一节。\nClientKeyExchange 消息的数据结构如下：\n1234567891011121314151617181920212223enum &#123; implicit, explicit &#125; PublicValueEncoding;              struct &#123;           select (PublicValueEncoding) &#123;               case implicit: struct &#123; &#125;;               case explicit: ECPoint ecdh_Yc;           &#125; ecdh_public;       &#125; ClientECDiffieHellmanPublic;            struct &#123;         select (KeyExchangeAlgorithm) &#123;             case rsa:                 EncryptedPreMasterSecret;             case dhe_dss:             case dhe_rsa:             case dh_dss:             case dh_rsa:             case dh_anon:                 ClientDiffieHellmanPublic;             case ec_diffie_hellman:                  ClientECDiffieHellmanPublic;         &#125; exchange_keys;     &#125; ClientKeyExchange; \n\n从 exchange_keys 的 case 中可以看到主要分为 3 种处理方式：EncryptedPreMasterSecret、ClientDiffieHellmanPublic、ClientECDiffieHellmanPublic。那么接下来就依次分析这 3 种处理方式的不同。\n(1) RSA&#x2F;ECDSA 加密预备主密钥如果 RSA 被用于密钥协商和身份认证(RSA 密码套件)，Client 会生成一个 48 字节的预备主密钥，使用 Server 证书中的公钥加密，然后以一个加密的预备主密钥消息发送。这个结构体是 ClientKeyExchange 消息的一个变量，它本身并非一个消息。\n这个消息的结构是:\n1234struct &#123;      ProtocolVersion client_version;      opaque random[46];  &#125; PreMasterSecret; \n\n\nclient_version:client_version 是 Client 支持的最高 TLS 协议版本。这个版本号是为了防止回退攻击。\n\nrandom:紧跟着是一个 46 字节的随机数。\n\n\nClient 将这 48 字节的预备主密钥用 Server 的 RSA 公钥加密以后生成 EncryptedPreMasterSecret，再发回 Server。\nEncryptedPreMasterSecret 的数据结构如下：\n123struct &#123;      public-key-encrypted PreMasterSecret pre_master_secret;  &#125; EncryptedPreMasterSecret; \n\n\nPreMasterSecret 中的 client_version 字段不是协商出来的 TLS 版本号，而是 ClientHello 中传递的版本号，这样做是为了防止回退攻击。不幸的是，一些旧的 TLS 实现使用了协商的版本，因此检查版本号会导致与这些不正确的 Client 实现之间的互操作失败。\nClient 生成的 EncryptedPreMasterSecret，仅仅是加密之后的结果，并没有完整性保护，消息可能会被篡改。有两种加密方式，一种是 RSAES-PKCS1-v1_5，另外一个种 RSAES-OAEP 加密方式。后者更加安全，但是在 TLS 1.2 中普遍用的是前者。\n\nServer 拿到 EncryptedPreMasterSecret 以后，用自己的 RSA 私钥解密。解密以后还需要再次校验 PreMasterSecret 中的 ProtocolVersion 和 ClientHello 中传递的 ProtocolVersion 是否一致。如果不相等，校验失败，Server 会根据下面说的规则重新生成 PreMasterSecret，并继续进行握手。\n如果 ClientHello.client_version 是 TLS 1.1 或更高，Server 实现必须按照以下的描述检查版本号。如果版本号是 TLS 1.0 或更早，Server 实现应该检查版本号，但可以有一个可配置的选项来禁止这个检查。需要注意的是如果检查失败，PreMasterSecret 应该按照以下的描述将PreMasterSecret 重新随机化生成。\n由 Bleichenbacher [BLEI] 和 Klima et al.[KPR03] 发现的攻击能被用于攻击 TLS Server，这种攻击表明一个特定的消息在解密时，已经被格式化为 PKCS#1，包含一个有效的 PreMasterSecret 结构，或着表明了有正确的版本号。\n正如 Klima [KPR03] 所描述的, 这些弱点能够被避免，通过处理不正确的格式消息块，或者在正确格式的 RSA 块中不区分错误的版本号。换句话说:\n\n\n生成一个 46 字节随机字符串 R；\n\n\n\n解密这消息来恢复明文 M；\n\n\n\n如果 PKCS#1 填充不正确，或消息 M 的长度不是精确的 48 字节:pre_master_secret = ClientHello.client_version || R，再如果 ClientHello.client_version &lt;= TLS 1.0，且版本号检查被显示禁用：pre_master_secret = M。如果以上 2 种情况都不符合，那么就pre_master_secret = ClientHello.client_version || M[2..47]。\n\n\n\n需要注意的是,如果 Client 在原始的 pre_master_secret 中发生了错误的版本的话，那么使用 ClientHello.client_version 显式构造产生出来的 pre_master_secret 是一个无效的 master_secret。\n另外一个可供选择的方法是将版本号不匹配作为一个 PKCS-1 格式错误来处理，并将预备主密钥完全随机化：\n\n\n生成一个 46 字节随机字符串 R；\n\n\n\n解密这消息来恢复明文 M；\n\n\n\n如果 PKCS#1 填充不正确，或消息 M 的长度不是精确的 48 字节:pre_master_secret = R，再如果ClientHello.client_version &lt;= TLS 1.0，且版本号检查被显示禁用:pre_master_secret = M。再如果 M 的前两个字节 M[0..1] 不等于ClientHello.client_version:premaster secret = R，如果以上 3 种情况都不满足，就pre_master_secret = M。\n\n\n\n虽然没有已知的针对这个结构体的攻击，Klima et al. [KPR03] 描述了一些理论上的攻击， 因此推荐第一种结构描述来处理。\n在任何情况下，如果处理一个 RSA 加密的预备主密钥消息失败的时候，或版本号不是期望的时候，一个 TLS Server 一定不能产生一个警报。作为替代，它必须以一个随机生成的预备主密钥继续握手流程。出于定位问题的意图将失败的真正原因记录在日志中可能是有帮助的。但必须注意避免泄露信息给攻击者（例如，计时，日志文件或其它渠道）\n在 [PKCS1] 中定义的 RSAES-OAEP 加密方案对于 Bleichenbacher 攻击是更安全的。然而，为了最大程度上兼容早期的 TLS 版本，TLS 1.2 规范使用 RSAES-PKCS1-v1_5 方案。如果上述建议被采纳的话，不会有多少已知的Bleichenbacher 能够奏效。\n公钥加密数据被表现为一个非透明向量 &lt;0..2^16-1&gt;。因此，一个 ClientKeyExchange 消息中的 RSA 加密的预备主密钥以两个长度字节为先导。这些字节对于 RSA 是冗余的因为 EncryptedPreMasterSecret 是 ClientKeyExchange 中仅有的数据，它的长度会明确地确定。SSLv3 规范对公钥加密数据的编码没有明确指定，因此很多 SSLv3 实现没有包含长度字节，它们将 RSA 加密数据直接编码进 ClientKeyExchange 消息中。\nTLS 1.2 要求 EncryptedPreMasterSecret 和长度字节一起正确地编码。结果 PDU 会与很多 SSLv3 实现不兼容。实现者从 SSLv3 升级时必须修改他们的实现以生成和接受正确的编码。希望兼容 SSLv3 和 TLS 的实现者必须使他们的实现的行为依赖于版本号。\n现在得知对 TLS 进行基于计时的攻击是可能的，至少当 Client 和 Server 在相同局域网中时是可行的。相应地，使用静态 RSA 密钥的实现必须使用 RSA 混淆或其它抗计时攻击技术，如 [TIMING] 所述。\n(2) 静态 DH 公钥算出预备主密钥如果这个值没有被包含在 Client 的证书中，这个结构体传递了 Client 的 Diffie-Hellman 公钥(Yc)。Yc 所用的编码由 PublicValueEncoding 罗列。这个结构是 Client 密钥交换消息的一个变量，它本身并非一个消息。\n这个消息的结构是：\n1enum &#123; implicit, explicit &#125; PublicValueEncoding; \n\n\nimplicit:如果 Client 发送了一个证书其中包含了一个合适的 Diffie-Hellman 密钥(用于 fixed_dh 类型的 Client认证)，则 Yc 是隐式的且不需要再次发送。这种情况下，Client 密钥交换消息会被发送，单必须是空。\n\nexplicit:Yc 需要被发送。\n\n\n123456struct &#123;         select (PublicValueEncoding) &#123;             case implicit: struct &#123;&#125;;             case explicit: opaque dh_Yc&lt;1..2^16-1&gt;;         &#125; dh_public;     &#125; ClientDiffieHellmanPublic; \n\n\ndh_Yc:Client 的 Diffie-Hellman 公钥(Yc)。DH 公钥是明文传递的。就是算明文传递，被中间人窃听了，也无法得到最终的主密钥。具体原因可以看笔者之前密码学这篇文章的分析。\n\n(3) 动态 DH 公钥算出预备主密钥如果协商出来的密码套件密钥协商算法是 ECDHE，Client 需要发送 ECDH 公钥，结构体如下:\n12345678910struct &#123;           opaque point &lt;1..2^8-1&gt;;       &#125; ECPoint;              struct &#123;           select (PublicValueEncoding) &#123;               case implicit: struct &#123;&#125;;               case explicit: ECPoint ecdh_Yc;           &#125; ecdh_public;       &#125; ClientECDiffieHellmanPublic; \n\n\necdh_Yc：Client 的 ECDH 公钥(Yc)。ECDH 公钥也是明文传递的。就是算明文传递，被中间人窃听了，也无法得到最终的主密钥。具体原因可以看笔者之前密码学这篇文章的分析。\n\n所有涉及 ECC 操作的，Server 和 Client 必须选用双方都支持的命名曲线，Client Hello 消息中 ecc_curve 扩展指定了 Client 支持的 ECC 命名曲线。\n8. Certificate Verify这个消息用于对一个 Client 的证书进行显式验证。这个消息只能在一个 Client 证书具有签名能力时才能发送(例如，所有除了包含固定 Diffie-Hellman 参数的证书)。当发送时，它必须紧随着 client key exchange 消息。\n这条消息的结构是：\n12345struct &#123;       digitally-signed struct &#123;           opaque handshake_messages[handshake_messages_length];       &#125;  &#125; CertificateVerify; \n\n这里 handshake_messages 是指发送或接收到的所有握手消息，从 client hello 开始到但不包括本消息，包含握手消息的类型和长度域。这是到目前为止所有握手结构（在这一节定义的）的级联。需要注意的是这要求两端要么缓存消息，要么计算用所有可用的 hash 算法计算运行时的 hash 值直到计算 CertificateVerify 的 hash 值为止。Server 可以通过在 CertificateRequest 消息中提高一个受限的摘要算法及来最小化这种计算代价。\n在签名中使用的 hash 和签名算法必须是 CertificateRequest 消息中 supported_signature_algorithms 字段所列出的算法中的一种。此外，hash 和签名算法必须与 Client 的终端实体证书相兼容。RSA 密钥可以与任何允许的 hash 算法一起使用，但需要服从证书中的限制(如果有的话)。\n由于 DSA 签名不包含任何安全的 hash 算法的方法，如果任意密钥使用多个 hash 的话会产生一个 hash 替代风险。目前 DSA [DSS] 可以与 SHA-1 一起使用。将来版本的 DSS [DSS-3] 被希望允许与 DSA 一起使用其它的摘要算法。以及指导哪些摘要算法应与每个密钥大小一起使用。此外，将来版本的 [PKIX] 可以指定机制以允许证书表明哪些摘要算法能与 DSA 一起使用。\n9. Finished一个 Finished 消息一直会在一个 change cipher spec 消息后立即发送，以证明密钥交换和认证过程是成功的。一个 change cipher spec 消息必须在其它握手消息和结束消息之间被接收。\nFinished 消息是第一个被刚刚协商的算法，密钥和机密保护的消息。Finished 消息的接收者必须验证内容是正确的。一旦一方已经发送了 Finished 消息且接收并验证了对端发送的 Finished 消息，就可以在连接上开始发送和接收应用数据。\nFinished 消息的结构：\n1234567struct &#123;         opaque verify_data[verify_data_length];     &#125; Finished;     verify_data =         PRF(master_secret, finished_label, Hash(handshake_messages))           [0..verify_data_length-1]; \n\n\nfinished_label:对于由 Client 发送的结束消息，字符串是 “client finished”。 对于由 Server 发送的结束消息，字符串是”server finished”。\n\nHash 指出了握手消息的一个 hash。hash 必须用作 PRF 的基础。任何定义了一个不同 PRF 的密码套件必须定义 Hash 用于 Finished 消息的计算。\n在 TLS 1.2 之前的版本中，verify_data 一直是 12 字节长。在 TLS 1.2 版本中，verify_data 的长度取决于密码套件。任何没有显式指定 verify_data_length 的密码套件都默认 verify_data_length 等于 12。需要注意的是这种表示的编码与之前的版本相同。将来密码套件可能会指定其它长度但这个长度必须至少是 12 字节。\n\nhandshake_messages:所有在本次握手过程（不包括任何 HelloRequest 消息）到但不包括本消息的消息中的数据。这是只能在握手层中看见的数据且不包含记录层头。这是到目前为止所有在这一节中定义的握手结构体的关联。\n\n如果一个 Finished 消息在握手的合适环节上没有一个 ChangeCipherSpec 在其之前则是致命错误。\nhandshake_messages 的值包括了从 ClientHello 开始一直到（但不包括）Finished 消息的所有握手消息。这一节中的 handshake_messages 不同，因为它包含 CertificateVerify 消息（如果发送了）。同样，client 发送的 Finished 消息的 handshake_messages 与 Server 发送的 Finished 消息不同，因为第二个被发送的要包含前一个。Server 的 Finished 消息会包含 Client 的 Finished 子消息。\n注意：ChangeCipherSpec 消息，alert 警报，和任何其它记录类型不是握手消息，不会被包含在 hash 计算中。同样，HelloRequest 消息也被握手 hash 忽略。\nFinished 子消息是 TLS 记录层加密保护的第一条消息。Finished 子消息的存在的意义是什么呢？\n在所有的握手协议中，所有的子消息都没有加密和完整性保护，消息很容易篡改，改掉以后如果不检验，就会出现不安全的攻击。为了避免握手期间存在消息被篡改的情况，所以 Client 和 Server 都需要校验一下对方的 Finished 子消息。\n如果中间人在握手期间把 ClientHello 的 TLS 最高支持版本修改为 TLS 1.0，企图回退攻击，利用 TLS 旧版本中的漏洞。Server 收到中间人的 ClientHello 并不知道是否存在篡改，于是也按照 TLS 1.0 去协商。握手进行到最后一步，校验 Finished 子消息的时候，校验不通过，因为 Client 原本发的 ClientHello 中 TLS 最高支持版本是 TLS 1.2，那么产生的 Finished 子消息的 verify_data 与 Server 拿到篡改后的 ClientHello 计算出来的 verify_data 肯定不同。至此也就发现了中间存在篡改，握手失败。\n四. 直观感受 TLS 1.2 首次握手流程至此，TLS 1.2 首次握手的所有细节都已经分析完了。这一节让我们小结一下上面的流程，并用 Wireshark 直观感受一下 TLS 1.2 协议。\n首先是基于 RSA 密钥协商算法的首次握手：\n\n握手开始，Client 先发送 ClientHello ，在这条消息中，Client 会上报它支持的所有“能力”。client_version 中标识了 Client 能支持的最高 TLS 版本号；random 中标识了 Client 生成的随机数，用于预备主密钥和主密钥以及密钥块的生成，总长度是 32 字节，其中前 4 个字节是时间戳，后 28 个字节是随机数；cipher_suites 标识了 Client 能够支持的密码套件。extensions 中标识了 Client 能够支持的所有扩展。\n\n\n\n\n\n\n\n\n\n关于 extensions 本篇文章涉及的少，因为笔者打算把 TLS 1.2 和 TLS 1.3 中涉及到的 extensions 都整理到一篇文章中，在这篇握手的流程中没有详细分析。extensions 更加详细的分析请见 《HTTPS 温故知新（六） —— TLS 中的 Extensions》\nServer 在收到 ClientHello 之后，如果能够继续协商，就会发送 ServerHello，否则发送 Hello Request 重新协商。在 ServerHello 中，Server 会结合 Client 的能力，选择出双方都支持的协议版本以及密码套件进行下一步的握手流程。server_version 中标识了经过协商以后，Server 选出了双方都支持的协议版本。random 中标识了 Server 生成的随机数，用于预备主密钥和主密钥以及密钥块的生成，总长度是 32 字节，其中前 4 个字节是时间戳，后 28 个字节是随机数；cipher_suites 标识了经过协商以后，Server 选出了双方都支持的密码套件。extensions 中标识了 Server 处理 Client 的 extensions 之后的结果。\n当协商出了双方都能满足的密钥套件，根据需要 Server 会发送 Certificate 消息。Certificate 消息会带上 Server 的证书链。Certificate 消息的目的一是为了验证 Server 身份，二是为了让 Client 根据协商出来的密码套件从证书中获取 Server 的公钥。Client 拿到 Server 的公钥和 server 的 random 会生成预备主密钥。\n由于密钥协商算法是 RSA，需要 Server 在发送完 Certificate 消息以后就直接发送 ServerHelloDone 消息了。\nClient 收到 ServerHelloDone 消息以后，会开始计算预备主密钥，计算出来的预备主密钥会经过 RSA&#x2F;ECDSA 算法加密，并通过 ClientKeyExchange 消息发送给 Server。RSA 密码套件的预备主密钥是 48 字节。前 2 个字节是 client_version，后 46 字节是随机数。Server 收到 ClientKeyExchange 消息以后就会开始计算主密钥和密钥块了。同时 Client 也会在自己本地算好主密钥和密钥块。\n\n\n\n\n\n\n\n\n\n有些人会说“主密钥和会话密钥”，这里的会话密钥和密钥块是相同的意思。主密钥是由预主密钥、客户端随机数和服务器随机数通过 PRF 函数来生成；会话密钥是由主密钥、客户端随机数和服务器随机数通过 PRF 函数来生成。\n会话密钥 &#x3D; key_block &#x3D; 密钥块，这三者的意思是一样的，只是翻译不同罢了。\nClient 发送完 ClientKeyExchange 消息紧接着还会继续发送 ChangeCipherSpec 消息和 Finished 消息。Server 也会回应 ChangeCipherSpec 消息和 Finished 消息。如果 Finished 消息校验完成以后，代表握手最终成功。\n再来看看基于 DH 密钥协商算法的首次握手：\n\n基于 DH 密钥协商算法和基于 RSA 密码协商的区别在 Server 和 Client 协商 DH 参数上面。这里只说明一下 DH 密钥协商过程比 RSA 多的几步，其他的流程和 RSA 的流程基本一致。\n在 Server 发送完 Certificate 消息以后，还会继续发送 ServerKeyExchange 消息，在这条消息里面传递 DH 参数。\n另外一个不同点在于 ClientKeyExchange 消息中传递给 Server 预备主密钥长度不是 48 字节。基于 DH&#x2F;ECDH 算法的协商密钥长度取决于 DH&#x2F;ECDH 算法的公钥。\n为了让读者能更加直观的理解 TLS 1.2 的流程，笔者用 Wireshark 抓取了 Chrome 和笔者博客 TLS 握手中的网络包。结合 Wireshark 的抓包分析，让我们更加深入的理解 TLS 1.2 握手吧。下面的例子以 TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 密码套件为例：\n\n上面这是一次从 TLS 握手开始到 TCP 4 次挥手完整的过程。这里我们重要关注 protocol &#x3D; TLS 1.2 的所有消息。整体流程和上面分析的基于 DH 密钥交换算法的是一致的。在 TCP 4 次挥手之前，TLS 层会先收到 Close Notify 的 Alert 消息。\n\n\n\n\n\n\n\n\n\n读到这里能有读者有疑问，为什么经过 TLS 加密以后的上层数据会以明文展示在抓包中？HTTPS 不安全？这里需要解释一下，因为笔者利用 export SSLKEYLOGFILE=/Users/XXXX/sslkeylog.log 把 ECDHE 协商的密钥保存成 log 文件了，解析上层 HTTP&#x2F;2 的加密包的时候就可以利用 log 中的 TLS key 进行解密。上图中绿色的部分，就是通过解密出来得到的 HTTP&#x2F;2 的解密内容。具体这块内容笔者会在 HTTP&#x2F;2 相关的文章里面会提一提，这里读者就认为是笔者利用某些手段解析出了 HTTPS 加密的内容即可。\n接下来一条条的看看网络是如何传输数据包的。从 ClientHello 开始。\n\n从 TLS 1.2 Record Layer 的 Length 字段中我们可以看到 TLS 记录层的这条 TLS 握手消息长度是 512 字节，其中 ClientHello 消息中占 508 字节。ClientHello 中标识了 Client 最高支持的 TLS 版本是 TLS 1.2 (0x0303)。Client 支持的密码套件有 17 种，优先支持的是 TLS_AES_128_GCM_SHA256 。Session ID 的长度是 32 字节，这里不为空。压缩算法是 null。signature_algorithms 中标识了 Client 支持 9 对数字签名算法。\n\n\n\n\n\n\n\n\n\n默认情况下 TLS 压缩都是关闭的，因为 CRIME 攻击会利用 TLS 压缩恢复加密认证 cookie，实现会话劫持，而且一般配置 gzip 等内容压缩后再压缩 TLS 分片效益不大又额外占用资源，所以一般都关闭 TLS 压缩。\n\nClientHello 中发送了 status_request 扩展，查询 OCSP 封套消信息。发送了 signed_certificate_timestamp 扩展，查询 SCT 信息。发送了 application_layer_protocol_negotiation (ALPN)扩展，询问服务端是否支持 HTTP&#x2F;2 协议。supported_group 扩展里面标识了 Client 中支持的椭圆曲线。SessionTicket TLS 扩展表明了 Client 支持基于 Session Ticket 的会话恢复。\n再看看 ServerHello 消息。\n\n从 TLS 1.2 Record Layer 的 Length 字段中我们可以看到 TLS 记录层的这条 TLS 握手协议消息长度是 82 字节，其中 ServerHello 消息中占 78 字节。Server 选择用 TLS 1.2 版本与 Client 进行接下来的握手流程。Server 与 Client 协商出来的密码套件是 TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384。\nServer 支持 HTTP&#x2F;2，在 ALPN 扩展中进行了回复。\n\n从 TLS 1.2 Record Layer 的 Length 字段中我们可以看到 TLS 记录层的这条 TLS 握手消息长度是 2544 字节，其中 Certificate 消息中占 2540 字节，Certificates 证书链包含 2 张证书，Server 实体证书 1357 字节，中间证书 1174 字节。中间证书是 Let’s Encrypt 签发的。两张证书都是用 sha256WithRSAEncryption 签名算法进行签名的。\n\n从 TLS 1.2 Record Layer 的 Length 字段中我们可以看到 TLS 记录层的这条 TLS 握手协议消息长度是 535 字节，其中 Certificate Status 消息中占 531 字节。Server 将 OCSP 的 response 发送给 Client。\n从 TLS 1.2 Record Layer 的 Length 字段中我们可以看到 TLS 记录层的这条 TLS 握手协议消息长度是 116 字节，其中 ServerKeyExchange 消息中占 112 字节。由于协商出来的是 ECDHE 密钥协商算法，所以 Server 需要把 ECDH 的参数和公钥通过 ServerKeyExchange 消息发给 Client。这里 ECDHE 使用的 ECC 命名曲线是 x25519 。Server 的公钥是 (62761b5……)，签名算法是 ECDSA_secp256r1_SHA256 。签名值是 (3046022……)。\nServerHelloDone 消息结构很简单，见上图。\n\n由于是 ECDHE 协商算法，所以 Client 需要发送 ECC DH 公钥，对应的公钥值是 (1e58cf……)。公钥长度 32 字节。\nChangeCipherSpec 消息结构很简单，发送这条消息是为了告诉 Server ，Client 可以使用 TLS 记录层协议进行密码学保护了。第一条进行密码学保护的消息是 Finished 消息。\nFinished 消息结构很简单，见上图。\n\nServer 如果只是 SessionTicket，那么会生成新的 NewSessionTicket 返给 Client，然后同样返回 ChangeCipherSpec 消息和 Finished 消息。\n\n当页面关闭的时候，Server 会给 Client 发送 TLS Alert 消息，这条消息里面的描述就是 Close Notify。同时 Server 会发送 FIN 包开始 4 次挥手。\n五. TLS 1.2 会话恢复Client 和 Server 只要一关闭连接，短时间内再次访问 HTTPS 网站的时候又需要重新连接。新的连接会造成网络延迟，并消耗双方的计算能力。有没有办法能复用之前的 TLS 连接呢？办法是有的，这就涉及到了 TLS 会话复用机制。\n当 Client 和 Server 决定继续一个以前的会话或复制一个现存的会话(取代协商新的安全参数)时，消息流如下:\nClient 使用需要恢复的当前会话的 ID 发送一个 ClientHello。Server 检查它的会话缓存以进行匹配。如果匹配成功，并且 Server 愿意在指定的会话状态下重建连接，它将会发送一个带有相同会话 ID 值的 ServerHello 消息。这时，Client 和 Server 必须都发送 ChangeCipherSpec 消息并且直接发送 Finished 消息。一旦重建立完成，Client 和 Server 可以开始交换应用层数据(见下面的流程图)。如果一个会话 ID 不匹配，Server 会产生一个新的会话 ID，然后 TLS Client 和 Server 需要进行一次完整的握手。\n123456789Client                                                Server     ClientHello                   --------&gt;                                                      ServerHello                                               [ChangeCipherSpec]                                   &lt;--------             Finished     [ChangeCipherSpec]     Finished                      --------&gt;     Application Data              &lt;-------&gt;     Application Data \n\nSession ID 由服务器端支持，协议中的标准字段，因此基本所有服务器都支持，服务器端保存会话 ID 以及协商的通信信息，占用服务器资源较多。\n1. 基于 Session ID 的会话恢复当 Client 通过一次完整的握手，与 Server 建立了一次完整的 Session，Server 会记录这次 Session 的信息，以备恢复会话的时候使用:\n\n会话标识符(session identifier):每个会话的唯一标识符\n对端的证书(peer certificate):对端的证书，一般为空\n压缩算法(compression method):一般不启用\n密码套件(cipher spec):Client 和 Server 协商共同协商出来的密码套件\n主密钥(master secret):每个会话都会保存一份主密钥，注意不是预备主密钥。(读者可以想想为什么，如果还是想不通，见 《HTTPS 温故知新（五） —— TLS 中的密钥计算》)\n会话可恢复标识(is resumable):标识会话是否可恢复\n\n当 Server 保存了以上的信息，可以再次计算出 TLS 记录层需要的 security parameters 加密参数，从而加密应用数据。\n基于 Session ID 会话恢复的流程如下：\n123456789Client                                                Server     ClientHello                   --------&gt;                                                      ServerHello                                               [ChangeCipherSpec]                                   &lt;--------             Finished     [ChangeCipherSpec]     Finished                      --------&gt;     Application Data              &lt;-------&gt;     Application Data \n\n\nClient 发现请求的网站是之前请求过的，即内存中存在 Session ID，那么再次建立连接的时候会在 ClientHello 中附带与网站对应的 Session ID。Server 的内存中会保存一份 Session Cache 的字典，key 是 Session ID，value 是会话信息。Server 收到 ClientHello 以后，根据传过来的 Session ID 查看是否有相关的会话信息，如果有，就会允许 Client 进行会话恢复，直接发送 ChangeCipherSpec 和 Finished 消息。如果没有相关的会话信息，就会开始一次完整的握手，并在 ServerHello 中生成新的 Session ID 返回给 Client。Client 收到 Server 发来的 ChangeCipherSpec 和 Finished 消息，代表会话恢复成功，也发送 ChangeCipherSpec 和 Finished 消息作为回应。\nSession ID 的来源：\n\n上次完全握手生成的 Session ID\n使用另外一条连接的 Session ID\n直接使用本次连接的 Session ID\n\n会话恢复中 ClientHello 各个参数的必要性：\n\nServer 通过 ClientHello 中协商出来的密钥套件必须和会话中的密钥套件是一致的，否则会话恢复失败，进行完整的握手。\nClientHello 中的随机数和恢复之前会话所用的随机数是不同的，所以即使会话恢复了，由于 ClientHello 中随机数的不同，再次通过 PRF 生成的密钥块(会话密钥)也是不同的。增加了安全性。\nClientHello 中的 Session ID 是明文传输，所以不应该在 Session ID 中包含敏感信息。并且握手最后一步的 Finished 校验非常有必要，防止 Session ID 被篡改。\n\n最后需要注意的是，会话恢复取决于 Server 端，即使 Session ID 正确，并且 Server 内存中也存在相关的会话信息，Server 依旧可以要求 Client 进行完整的握手。即会话恢复不是强制的。\n基于 Session ID 的会话恢复的优点是:\n\n减少网络延迟，握手耗时从 2-RTT -&gt; 1-RTT\n减少了 Client 和 Server 端的负载，减少了加密运算的 CPU 资源消耗\n\n基于 Session ID 的会话恢复的缺点是:\n\nServer 存储会话信息，限制了 Server 的扩展能力。\n分布式系统中，如果只是简单的在 Server 的内存中存储 Session Cache，那么多台机器的数据同步也是一个问题。\n\nNginx 官方并没有提供支持分布式服务器的 Session Cache 的实现。可以使用第三方补丁，但是安全和维护成本也会增加。\n由于上面 2 个缺点，也就引出了基于 Session Ticket 的会话恢复方案。\n2. 基于 Session Ticket 的会话恢复用来替代 Session ID 会话恢复的方案是使用会话票证（Session ticket）。使用这种方式，除了所有的状态都保存在客户端（与 HTTP Cookie 的原理类似）之外，其消息流与服务器会话缓存是一样的。\n其思想是服务器取出它的所有会话数据（状态）并进行加密 (密钥只有服务器知道)，再以票证的方式发回客户端。在接下来的连接中，客户端恢复会话时在 ClientHello 的扩展字段 session_ticket 中携带加密信息将票证提交回服务器，由服务器检查票证的完整性，解密其内容，再使用其中的信息恢复会话。\n对于 Server 来说，解密 ticket 就可以得到主密钥，(注意这里和 SessionID 不同，有 Session ID 可以得到主密钥的信息)。对于 Client 来说，完整握手的时候收到 Server 下发的 NewSessionTicket 子消息的时候，Client 会将 Ticket 和对应的预备主密钥存在 Client，简短握手的时候，一旦 Server 验证通过，可以进行简单握手的时候，Client 通过本地存储的预备主密钥生成主密钥，最终再生成会话密钥(密钥块)。\n这种方法有可能使扩展服务器集群更为简单，因为如果不使用这种方式，就需要在服务集群的各个节点之间同步 Session Cache。Session ticket 需要服务器和客户端都支持，属于一个扩展字段，占用服务器资源很少。\nSession Ticket 的优点也就决定了它特别适合在以下场景中使用：\n\n大型 HTTPS 网站，访问量非常大，在 Server 中存储 Session 信息需要消耗大量内存\nHTTPS 网站所有者希望会话信息的生命周期能足够长，让 Client 尽量都使用简短握手的方式\nHTTPS 网站所有者希望用户能跨地域跨主机访问\n\n基于 Session Ticket 会话恢复的流程如下：\n(1). 获取 SessionTicketClient 在进行一次完整握手以后才能获取到 SessionTicket。\n12345678910111213141516171819Client                                               Server     ClientHello     (empty SessionTicket extension)--------&gt;                                                     ServerHello                                  (empty SessionTicket extension)                                                    Certificate*                                              ServerKeyExchange*                                             CertificateRequest*                                  &lt;--------      ServerHelloDone     Certificate*     ClientKeyExchange     CertificateVerify*     [ChangeCipherSpec]     Finished                     --------&gt;     \t\t\t\t\t\t\t\t\t\t\t         NewSessionTicket                                              [ChangeCipherSpec]                                  &lt;--------             Finished     Application Data             &lt;-------&gt;     Application Data \n\nClient 在 ClientHello 的扩展中包含空的 SessionTicket 扩展，如果 Server 支持 SessionTicket 会话恢复，则会在 ServerHello 中回复一个空的 SessionTicket 扩展。Server 将会话信息进行加密保护，生成一个 ticket，通过 NewSessionTicket 子消息发给 Client 。注意，虽然 NewSessionTicket 子消息在 ChangeCipherSpec 消息之前，但是它也是一条加密消息。\nServer 将会话信息加密以后以 ticket 的方式发送给 Client，Server 就不再存储任何信息了。Client 将接收到的 ticket 存储在内存中，什么时候想会话恢复了，发给 Server，如果解密以后确认无误，即可进行简短握手了。\n(2). 基于 SessionTicket 的会话恢复当 Client 本地获取了 SessionTicket 以后，下次想要进行简短握手，就可以使用这个 SessionTicket 了。\n123456789101112Client                                                Server     ClientHello     (SessionTicket extension)     --------&gt;                                                      ServerHello                                   (empty SessionTicket extension)                                                 NewSessionTicket                                               [ChangeCipherSpec]                                   &lt;--------             Finished     [ChangeCipherSpec]     Finished                      --------&gt;     Application Data              &lt;-------&gt;     Application Data \n\nClient 在 ClientHello 的扩展中包含非空的 SessionTicket 扩展，如果 Server 支持 SessionTicket 会话恢复，则会在 ServerHello 中回复一个空的 SessionTicket 扩展。Server 将会话信息进行加密保护，生成一个新的 ticket，通过 NewSessionTicket 子消息发给 Client。发送完 NewSessionTicket 消息以后，紧跟着发送 ChangeCipherSpec 和 Finished 消息。Client 收到上述消息以后，回应 ChangeCipherSpec 和 Finished 消息，会话恢复成功。\n(3). Server 不支持 SessionTicket有读者可能会问了，既然 Client 发送了非空的 SessionTicket extension，为什么 Server 必须在 ServerHello 中回复一个空的 SessionTicket 扩展呢？因为当 Server 不支持 SessionTicket 的时候，ServerHello 中是不包含 SessionTicket extension 的，所以是否包含 SessionTicket extension，区分出了 Server 是否支持 SessionTicket。\n1234567891011121314151617Client                                               Server        ClientHello        (SessionTicket extension)    --------&gt;                                                        ServerHello                                                       Certificate*                                                 ServerKeyExchange*                                                CertificateRequest*                                     &lt;--------      ServerHelloDone        Certificate*        ClientKeyExchange        CertificateVerify*        [ChangeCipherSpec]        Finished                     --------&gt;                                                 [ChangeCipherSpec]                                     &lt;--------             Finished        Application Data             &lt;-------&gt;     Application Data \n\nServer 如果不支持 SessionTicket，那么在 ServerHello 不响应 SessionTicket TLS 扩展，并且也不发送 NewSessionTicket 子消息。\n(4). Server 校验 SessionTicket 失败如果 Server 校验 SessionTicket 失败，那么握手会回退到完整握手。\n12345678910111213141516171819Client                                               Server        ClientHello        (SessionTicket extension) --------&gt;                                                        ServerHello                                    (empty SessionTicket extension)                                                       Certificate*                                                 ServerKeyExchange*                                                CertificateRequest*                                 &lt;--------          ServerHelloDone        Certificate*        ClientKeyExchange        CertificateVerify*        [ChangeCipherSpec]        Finished                 --------&gt;                                                   NewSessionTicket                                                 [ChangeCipherSpec]                                 &lt;--------                 Finished        Application Data         &lt;-------&gt;         Application Data \n\n如果 Server 接受了票证但最终握手失败，则 Client 应该删除 ticket。\n正常的基于 SessionTicket 的会话恢复流程如下图：\n\n(5). NewSessionTicket 子消息这一节内容主要都来自 [RFC5077]。\n如果 ServerHello 消息中包含了 Session Ticket TLS 扩展，那么必须在 ChangeCipherSpec 之前发送加密过的 NewSessionTicket 子消息。如果 ServerHello 消息中不包含了 Session Ticket TLS 扩展，表示 Server 或 Client 不想使用 SessionTicket 会话恢复机制。\n由于 NewSessionTicket 子消息也算是握手的一部分，所以 Finished 子消息中也需要校验它。如果 Server 成功校验了 Client 发送的 ticket，那么也必须重新生成一个全新的 ticket，通过 NewSessionTicket 子消息发送给 Client，Client 下次会使用这个新的 SessionTicket。\n在握手协议中，由于扩展的加入，也加入了几条新的握手消息:\n12345678910111213141516171819struct &#123;         HandshakeType msg_type;         uint24 length;         select (HandshakeType) &#123;             case hello_request:       HelloRequest;             case client_hello:        ClientHello;             case server_hello:        ServerHello;             case certificate:         Certificate;             case certificate_url:     CertificateURL;                 case certificate_status:  CertificateStatus;              case server_key_exchange: ServerKeyExchange;             case certificate_request: CertificateRequest;             case server_hello_done:   ServerHelloDone;             case certificate_verify:  CertificateVerify;             case client_key_exchange: ClientKeyExchange;             case finished:            Finished;             case session_ticket:      NewSessionTicket;          &#125; body;     &#125; Handshake; \n\nCertificateURL、CertificateStatus、NewSessionTicket 这 3 条握手子消息都算是扩展带来的新的子消息。\nNewSessionTicket 消息的数据结构如下：\n1234struct &#123;         uint32 ticket_lifetime_hint;         opaque ticket&lt;0..2^16-1&gt;;     &#125; NewSessionTicket; \n\nNewSessionTicket 最重要的一个字段就是 ticket_lifetime_hint。它标识了 ticket 是否过期，Server 会校验这个字段，如果过期就不能进行会话恢复。ticket 的生成和校验全部都由 Server 进行，Client 仅仅只是接收和存储。\nServer 生成 ticket 没有固定的规范，每个 Server 生成的方式也可以不同，需要注意的一点就是前向安全性，防止被破解。在 RFC5077 中建议按照如下方式生成:\n123456struct &#123;         opaque key_name[16];         opaque iv[16];         opaque encrypted_state&lt;0..2^16-1&gt;;         opaque mac[32];     &#125; ticket; \n\n\nkey_name:ticket 加密使用的密钥文件\n\niv:初始化向量，AES 加密算法需要使用\n\nencrypted_state:ticket 详细信息，存储的就是会话信息\n\nmac:ticket 需要的完整和安全性保护\n\n\n会话信息的数据结构如下：\n12345678struct &#123;         ProtocolVersion protocol_version;         CipherSuite cipher_suite;         CompressionMethod compression_method;         opaque master_secret[48];         ClientIdentity client_identity;         uint32 timestamp;     &#125; StatePlaintext; \n\nStatePlaintext 中的 client_identity 是 Client 标识符，timestamp 是 ticket 过期时间。ClientIdentity 的数据结构如下：\n12345678910111213141516enum &#123;        anonymous(0),        certificate_based(1),        psk(2)    &#125; ClientAuthenticationType;     struct &#123;         ClientAuthenticationType client_authentication_type;         select (ClientAuthenticationType) &#123;             case anonymous: struct &#123;&#125;;             case certificate_based:                 ASN.1Cert certificate_list&lt;0..2^24-1&gt;;             case psk:                 opaque psk_identity&lt;0..2^16-1&gt;;            &#125;;      &#125; ClientIdentity; \n\nClientIdentity 有 2 种认证方式，一种是基于 certificate_based 证书的方式，另外一种是基于 psk PSK 的方式。\n使用给定 IV，在 CBC 模式下使用 128 位 AES 加密 encrypted_state 中的实际状态信息。使用 HMAC-SHA-256 通过 key_name（16个字节）和 IV（16个字节）计算消息验证代码（MAC），然后是 encrypted_state 字段的长度（2个字节）及其内容（可变长度），这样就生成了 ticket。\n六. 直观感受 TLS 1.2 会话恢复这一节，笔者用 Wireshark 展示一下 TLS 1.2 的会话恢复，让读者加深理解。\n\n\nServerHello 中 SessionID 为空，并且 SessionTicket TLS 扩展也会空。这个时候表明了 Server 会在接下来的子消息中发送 NewSessionTicket 子消息。\n这里用的例子比较特殊，Client 在上次握手的时候，ClientHello 中带有 SessionID 和空的 SessionTicket TLS 扩展，Server 收到这个扩展以后，在内存的 Session Cache 中找到了这个 Session ID 对应的会话信息，在 ServerHello 中以相同的 SessionID 响应了 Client，并且也回应了空的 SessionTicket TLS 扩展。以此为背景，进行会话恢复，结果会是怎么样的呢？\n最终结果抓包截图如下:\n\n看上面的截图没有看到 NewSessionTicket 子消息，是否说明会话恢复不是基于 SessionTicket 的呢？我们继续看细节。\n\n在 ClientHello 中，可以看到 Client 同时带了 Session ID 和非空的 SessionTicker TLS 扩展。\n\nServer 在 ServerHello 中回应了相同的 Session ID，说明了可以在 Session Cache 中找到相应的会话信息。在 ClientHello 中也发送了 SessionTicket，这里 Server 为什么什么扩展消息都没有回应呢？难道是因为 Server 不支持 SessionTicket TLS 扩展？Server 在前一次握手中发送了 NewSessionTicket 子消息说明了 Server 支持 SessionTicket TLS 扩展，那为什么这里什么关于 SessionTicket 的信息都没有回复呢？原因就是因为 ClientHello 包含了可以用来会话恢复的 SessionID。[RFC 5077 3.4. Interaction with TLS Session ID] 中规定：如果 Client 在 ClientHello 中同时发送了 Session ID 和 SessionTicket TLS 扩展，Server 必须是用 ClientHello 中相同的 Session ID 进行相应。但是在校验 SessionTicket 时，Sever 不能依赖这个特定的 Session ID，即不能用 ClientHello 中的 Session ID 进行会话恢复。Server 优先使用 SessionTicket 进行会话恢复(SessionTicket 优先级高于 Session ID)，如果 Session 校验通过，就继续发送 ChangeCipherSpec 和 Finished 消息。不发送 NewSessionTicket 消息。\n\nClient 收到 Server 发过来的 ChangeCipherSpec 和 Finished 消息，作为回应，也会发送 ChangeCipherSpec 和 Finished 消息。\n\n把会话恢复过程中 Client 同时带有 Session ID 和 SessionTicket TLS 扩展的情况总结成一张图，如上图。\n至此，直观感受 TLS 握手流程的上篇就结束了，上篇将 TLS 1.2 中所有的握手流程都详细分析完成了。《HTTPS 温故知新（四） —— 直观感受 TLS 握手流程(下)》 下篇会着重分析 TLS 1.3 的握手流程，与 TLS 1.2 握手流程进行对比。另外还会讲解 TLS 1.3 新增的 0-RTT 是怎么一回事。\n当然在 TLS 1.2 和 TLS 1.3 握手流程中所有涉及到密钥计算的内容，都放在 《HTTPS 温故知新（五） —— TLS 中的密钥计算》 这篇文章里面了，TLS 1.2 和 TLS 1.3 握手流程中所有涉及到扩展的内容，都放在 《HTTPS 温故知新（六） —— TLS 中的 Extensions》 这篇文章里面了。\n\nReference：\nRFC 5247RFC 5077RFC 8466TLS1.3 draft-28大型网站的 HTTPS 实践（二）– HTTPS 对性能的影响\n\n\n\n\n\n\n\n\n\nGitHub Repo：Halfrost-Field\nFollow: halfrost · GitHub\nSource: https://halfrost.com/HTTPS-TLS1.2_handshake&#x2F;\n\n","slug":"NETWORK/直观感受 TLS 握手流程(上)","date":"2024-03-08T10:07:56.000Z","categories_index":"Server,NETWORK","tags_index":"https,Client,TLS","author_index":"dandeliono"},{"id":"889ca5b7a321750970fca61562d5cfb3","title":"用纯CPU环境搭建大模型环境","content":"用纯CPU环境搭建大模型环境配套视频：\n大模型现在正处于高速发展时期，各厂商都推出了自己的大模型，各种开源模型也层出不穷。尽管各厂商都有应用接入 API，但在使用过程中还是存在一些问题：\n\n数据安全：接入其他厂商的大模型，就需要将自己的数据交给对方处理，这些数据可能存在一些敏感信息，存在安全风险。\n\n费用：大模型应用通常是按照 Token 数量收费的，对于普通开发者来说，长时间使用也是一个不小的开销。\n\n\n既然开源大模型那么多，是否可以在本地搭建一个大模型的应用呢？目前开源的很多大模型，如 ChatGLM、Baichuan 等部署都需要 GPU 支持，这对个人开发者来说是一个小门槛，对于一个不玩游戏、不做图像处理开发的开发者来说，可能还真没有一块像样点的 GPU，但是可能有一颗强劲的 CPU，那么，是否可以在 CPU 上跑大模型呢？当然可以！今天就介绍一个开源项目，将大模型跑在 CPU 上。\nChatGPM.cpp 是一个纯 C++实现的大模型运行时，可以将经过转换的开源大模型放在 CPU 上运行。这里是开源地址：https://github.com/li-plus/chatglm.cpp  \nChatGLM.cpp 具有以下特点：\n\n基于 ggml，纯 C++实现\n\n具有 int4&#x2F;int8 量化、优化 KV 缓存和并行计算的加速内存高效 CPU 推理\n\n具有打字机效果的流式生成\n\nPython 绑定，Web 演示，API 服务器和更多的可能性\n\n\n部署流程基本上就是按照项目 readme 进行，但是其中有一些隐藏的坑需要注意，接下来就一步步进行，中间的注意点会着重描述。\n纯源码物理机部署纯源码物理机部署是指不使用 docker 虚拟化的情况下部署。\n源码获取从 github 下载源码：\n1git clone --recursive https://github.com/li-plus/chatglm.cpp.git &amp;&amp; cd chatglm.cpp\n\n这里的--recursive参数是为了将子模块也拉取下来。\n如果拉取的时候没有带此参数，也可以在拉取之后使用以下命令拉取子模块。\n1git submodule update --init --recursive\n\n依赖安装ChatGLM.cpp 依赖 python 转换模型，因此需要安装 python。\n此处有一个坑，对 python 版本是有要求的，因为转换过程中还需要用到 torch 包，而某些版本的 python 没有对应的 torch 包。\n这里推荐使用 Anaconda 来安装环境。\nAnaconda 就是可以便捷获取包且对包能够进行管理，同时对环境可以统一管理的发行版本。Anaconda 包含了 conda、Python 在内的超过 180 个科学包及其依赖项。\n这里是官网：https://www.anaconda.com/  \n安装配置好 conda 之后，创建并切换到虚拟环境：\n12conda create -n chatglm-cppconda activate chatglm-cpp\n\n查找 pytorch 纯 cpu 版本支持的 python 版本：\n1conda search pytorch | grep cpu\n\n以下是输出：\n12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152pytorch                        1.2.0 cpu_py27h00be3c6_0  pkgs/mainpytorch                        1.2.0 cpu_py36h00be3c6_0  pkgs/mainpytorch                        1.2.0 cpu_py37h00be3c6_0  pkgs/mainpytorch                        1.3.1 cpu_py27h62f834f_0  pkgs/mainpytorch                        1.3.1 cpu_py36h62f834f_0  pkgs/mainpytorch                        1.3.1 cpu_py37h62f834f_0  pkgs/mainpytorch                        1.4.0 cpu_py36h7e40bad_0  pkgs/mainpytorch                        1.4.0 cpu_py37h7e40bad_0  pkgs/mainpytorch                        1.4.0 cpu_py38h7e40bad_0  pkgs/mainpytorch                        1.5.0 cpu_py37hd91cbb3_0  pkgs/mainpytorch                        1.7.1 cpu_py37h6a09485_0  pkgs/mainpytorch                        1.7.1 cpu_py38h6a09485_0  pkgs/mainpytorch                        1.7.1 cpu_py39h6a09485_0  pkgs/mainpytorch                        1.8.1 cpu_py37h60491be_0  pkgs/mainpytorch                        1.8.1 cpu_py38h60491be_0  pkgs/mainpytorch                        1.8.1 cpu_py39h60491be_0  pkgs/mainpytorch                       1.10.2 cpu_py310h6894f24_0  pkgs/mainpytorch                       1.10.2 cpu_py37hfa7516b_0  pkgs/mainpytorch                       1.10.2 cpu_py38hfa7516b_0  pkgs/mainpytorch                       1.10.2 cpu_py39hfa7516b_0  pkgs/mainpytorch                       1.12.1 cpu_py310h9dbd814_1  pkgs/mainpytorch                       1.12.1 cpu_py310hb1f1ab4_1  pkgs/mainpytorch                       1.12.1 cpu_py310he8d8e81_0  pkgs/mainpytorch                       1.12.1 cpu_py37h9dbd814_1  pkgs/mainpytorch                       1.12.1 cpu_py37hb1f1ab4_1  pkgs/mainpytorch                       1.12.1 cpu_py37he8d8e81_0  pkgs/mainpytorch                       1.12.1 cpu_py38h9dbd814_1  pkgs/mainpytorch                       1.12.1 cpu_py38hb1f1ab4_1  pkgs/mainpytorch                       1.12.1 cpu_py38he8d8e81_0  pkgs/mainpytorch                       1.12.1 cpu_py39h9dbd814_1  pkgs/mainpytorch                       1.12.1 cpu_py39hb1f1ab4_1  pkgs/mainpytorch                       1.12.1 cpu_py39he8d8e81_0  pkgs/mainpytorch                       1.13.1 cpu_py310h92724a6_0  pkgs/mainpytorch                       1.13.1 cpu_py310h9dc8d95_0  pkgs/mainpytorch                       1.13.1 cpu_py310ha02dd7b_1  pkgs/mainpytorch                       1.13.1 cpu_py311h92724a6_0  pkgs/mainpytorch                       1.13.1 cpu_py311h9dc8d95_0  pkgs/mainpytorch                       1.13.1 cpu_py311ha02dd7b_1  pkgs/mainpytorch                       1.13.1 cpu_py38h92724a6_0  pkgs/mainpytorch                       1.13.1 cpu_py38h9dc8d95_0  pkgs/mainpytorch                       1.13.1 cpu_py38ha02dd7b_1  pkgs/mainpytorch                       1.13.1 cpu_py39h92724a6_0  pkgs/mainpytorch                       1.13.1 cpu_py39h9dc8d95_0  pkgs/mainpytorch                       1.13.1 cpu_py39ha02dd7b_1  pkgs/mainpytorch                        2.0.1 cpu_py310hab5cca8_0  pkgs/mainpytorch                        2.0.1 cpu_py310hdc00b08_0  pkgs/mainpytorch                        2.0.1 cpu_py311h53e38e9_0  pkgs/mainpytorch                        2.0.1 cpu_py311h6d93b4c_0  pkgs/mainpytorch                        2.0.1 cpu_py38hab5cca8_0  pkgs/mainpytorch                        2.0.1 cpu_py38hdc00b08_0  pkgs/mainpytorch                        2.0.1 cpu_py39hab5cca8_0  pkgs/mainpytorch                        2.0.1 cpu_py39hdc00b08_0  pkgs/main\n\n可以看到最新支持到 python 3.11 了。\n安装 python 3.11\n1conda install python==3.11\n\n安装依赖包：\n1pip install torch tabulate tqdm transformers==4.33.2 accelerate sentencepiece aiohttp\n\n这里需要注意的是，transformers 包的版本指定到 4.33.2，因为更高版本的 transformers 的 api 发生了变化，导致模型转换中出错。\n另外增加了 aiohttp 包，因为 openai 包依赖这个包，但是没有被自动安装。\n至此，转换模型的环境已经搭建好了。\n模型转换模型转换过程中需要和模型相当大小的内存（chatglm 模型需要 25G 内存），如果本机没有这么大的内存，可以使用以下方法解决：\n\n找一台大内存的机器\n\n使用在其他机器转换好的模型。我这里有两个转换后的模型：\n\n链接：https://pan.baidu.com/s/1nXcd6NMOtIvPGX_i_rIaAQ提取码：s7g9\n\n在一些免费的云服务器上转换，如：https://gitpod.io（gitpod 创建环境的时候选择大存储，有 50G 存储空间，否则默认只有 30G，可能不太够）\n\n\n下载接下来下载大模型。\n当前支持的大模型如下：\n\nChatGLM-6B: THUDM/chatglm-6b, THUDM/chatglm-6b-int8, THUDM/chatglm-6b-int4  \n\nChatGLM2-6B: THUDM/chatglm2-6b, THUDM/chatglm2-6b-int4  \n\nChatGLM3-6B: THUDM/chatglm3-6b  \n\nCodeGeeX2: THUDM/codegeex2-6b, THUDM/codegeex2-6b-int4  \n\nBaichuan &amp; Baichuan2: baichuan-inc/Baichuan-13B-Chat, baichuan-inc/Baichuan2-7B-Chat, baichuan-inc/Baichuan2-13B-Chat\n\n\n可以使用 git 直接从 Hugging Face 直接下载模型，这里需要注意的是，git 需要支持 lfs，才能将大模型完整下载下来。各软件仓库应该都有 git-lfs 包，安装过程这里就省略了。\n安装配置完 lfs 后，就可以下载大模型了，这里以 chatglm 模型举例：\n1git clone https://huggingface.co/THUDM/chatglm-6b\n\n其他模型类似。\n注意一点是这个命令会卡住，实际上不是卡住了，是 lfs 正在拉取二进制的模型。可以另开一个终端使用du -h命令查看目录的大小变化。\nchatglm-6b 模型大约 25G，因此这个下载时间取决于网速，git 不支持断点续传，所以一定不要断开网络。还有一些其他下载模型的方式，可以自行搜索。\n转换使用以下命令转换模型为 ggm 模型：\n1python3 chatglm_cpp/convert.py -i THUDM/chatglm-6b -t q4_0 -o chatglm-ggml.bin\n\n其中 THUDM&#x2F;chatglm-6b 是模型路径（上一步模型 git 库的路径），chatglm-ggml.bin 是输出的 CPU 模型文件。\nq4_0 为转换类型，解释如下：\n\nq4_0: 使用 fp16 标度的 4 位整数量化。\n\nq4_1: 具有 fp16 标度和最小值的 4 位整数量化。\n\nq5_0: 基于 fp16 标度的 5 位整数量化。\n\nq5_1: 具有 fp16 标度和最小值的 5 位整数量化。\n\nq8_0:8 位整数量化与 fp16 尺度。\n\nf16: 无量化的半精度浮点权重。\n\nf32: 无量化的单精度浮点权重。\n\n\n不同类型效果不同，消耗的资源也不同，可以多尝试几个。\n这里选用了资源消耗最小的 q4_0 类型。\n执行后生成 chatglm-ggml.bin 文件。\n构建可执行文件进入到 ChatGLM.cpp 的源码所在目录，执行：\n123cmake -B buildcmake --build build -j --config Releasepython setup.py build_ext --inplace\n\n编译后生成 ./build/bin/main  \n命令行运行使用命令行：\n1./build/bin/main -m chatglm-ggml.bin -p &quot;你好，介绍一下你自己&quot;\n\nchatglm-ggml.bin 就是前面转换出来的 ggml 模型。\n我这边得到的输出如下：\n1你好，我是 ChatGLM-6B，是清华大学KEG实验室和智谱AI公司共同训练的语言模型。我的目标是通过回答用户提出的问题来帮助他们解决问题。由于我是一个计算机程序，所以我没有自我意识，也不能像人类一样感知世界。我只能通过分析我所学到的信息来回答问题。\n\n交互式运行将-p参数替换为-i就可以进入交互式运行：\n1./build/bin/main -m chatglm-ggml.bin -i\n\n1234567891011121314151617________          __  ________    __  ___   / ____/ /_  ____ _/ /_/ ____/ /   /  |/  /_________  ____  / /   / __ \\/ __ `/ __/ / __/ /   / /|_/ // ___/ __ \\/ __ \\ / /___/ / / / /_/ / /_/ /_/ / /___/ /  / // /__/ /_/ / /_/ / \\____/_/ /_/\\__,_/\\__/\\____/_____/_/  /_(_)___/ .___/ .___/                                              /_/   /_/Welcome to ChatGLM.cpp! Ask whatever you want. Type &#x27;clear&#x27; to clear context. Type &#x27;stop&#x27; to exit.Prompt  &gt; 你好ChatGLM &gt; 你好👋！我是人工智能助手 ChatGLM-6B，很高兴见到你，欢迎问我任何问题。Prompt  &gt; 介绍一下你自己ChatGLM &gt; 我是一个大型语言模型，被训练用来回答人类提出的问题。我使用了大规模数据和算法来生成回答，可以帮助人们解决各种问题。我的能力覆盖了许多主题，包括历史、科学、技术、文化、语言和娱乐等等。Prompt  &gt; GPT的原理是什么ChatGLM &gt; GPT(Generative Pretrained Transformer)是一种基于Transformer架构的自然语言处理模型。它的原理是使用大量文本数据进行预训练，以便在后续任务中能够生成自然语言文本。在预训练期间，GPT学习到了大量的语言模式和规律，这些知识和模式可以用于生成自然语言文本。在后续任务中，GPT可以根据这些知识和模式生成文本，帮助人们完成各种任务，例如回答问题、撰写文章、翻译文本等等。GPT是由 OpenAI开发的，它被广泛应用于自然语言处理领域。Prompt  &gt;\n\nweb 执行前面介绍了在命令行运行大模型的步骤，如果想要在 web 中运行，需要安装一些额外的包。\n安装 chatglm_cpp 包：\n或者直接从仓库安装：\n1pip install -U chatglm-cpp\n\n安装 chatglm-cpp 包之后，就可以使用 python 直接调用 ChatGLM.cpp 了：\n1234import chatglm_cpppipeline = chatglm_cpp.Pipeline(&quot;../chatglm-ggml.bin&quot;)pipeline.chat([&quot;你好&quot;])\n\n作者提供了一些 demo，位于 examples 目录下，运行其中的 web demo：\n12pip install -U gradiopython3 examples/web_demo.py -m chatglm-ggml.bin\n\n打开浏览器，访问http://127.0.0.1:7860  \n\n开放 API可以将聊天能力开放出来，供其他应用程序使用。\n要想使用开放 API，需要安装 chatglm-app[api] 包：\n1pip install &#x27;chatglm-cpp[api]&#x27;\n\nLangChain API启动 API Server：\n1MODEL=./chatglm-ggml.bin uvicorn chatglm_cpp.langchain_api:app --host 127.0.0.1 --port 8000\n\n测试：\n1curl http://127.0.0.1:8000 -H &#x27;Content-Type: application/json&#x27; -d &#x27;&#123;&quot;prompt&quot;: &quot;你好&quot;&#125;&#x27;\n\nOpenAI API启动 API Server：\n1MODEL=./chatglm-ggml.bin uvicorn chatglm_cpp.openai_api:app --host 127.0.0.1 --port 8000\n\n测试：\n12curl http://127.0.0.1:8000/v1/chat/completions -H &#x27;Content-Type: application/json&#x27; \\    -d &#x27;&#123;&quot;messages&quot;: [&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;你好&quot;&#125;]&#125;&#x27;\n\ndocker 部署本地构建镜像使用以下命令构建并运行本地镜像：\n1234567891011docker build . --network=host -t chatglm.cppdocker run -it --rm -v $PWD:/opt chatglm.cpp ./build/bin/main -m /opt/chatglm-ggml.bin -p &quot;你好&quot;docker run -it --rm -v $PWD:/opt chatglm.cpp python3 examples/cli_chat.py -m /opt/chatglm-ggml.bin -p &quot;你好&quot;docker run -it --rm -v $PWD:/opt -p 8000:8000 -e MODEL=/opt/chatglm-ggml.bin chatglm.cpp \\    uvicorn chatglm_cpp.langchain_api:app --host 0.0.0.0 --port 8000docker run -it --rm -v $PWD:/opt -p 8000:8000 -e MODEL=/opt/chatglm-ggml.bin chatglm.cpp \\    uvicorn chatglm_cpp.openai_api:app --host 0.0.0.0 --port 8000\n\n启用 CUDA 支持：\n1234docker build . --network=host -t chatglm.cpp-cuda \\    --build-arg BASE_IMAGE=nvidia/cuda:12.2.0-devel-ubuntu20.04 \\    --build-arg CMAKE_ARGS=&quot;-DGGML_CUBLAS=ON&quot;docker run -it --rm --gpus all -v $PWD:/chatglm.cpp/models chatglm.cpp-cuda ./build/bin/main -m models/chatglm-ggml.bin -p &quot;你好&quot;\n\n使用预构建的镜像使用以下命令使用预构建的镜像：\n12docker run -it --rm -v $PWD:/opt liplusx/chatglm.cpp:main \\    ./build/bin/main -m /opt/chatglm-ggml.bin -p &quot;你好&quot;\n\n或者：\n12docker run -it --rm -v $PWD:/opt ghcr.io/li-plus/chatglm.cpp:main \\    ./build/bin/main -m /opt/chatglm-ggml.bin -p &quot;你好&quot;\n\n本教程基本是按照 Chat GLM.cpp 项目中的 README 编写的，但是中间有一些坑在 README 中没有提到，按照本教程可以有效避坑。\n","slug":"WORK/用纯CPU环境搭建大模型环境","date":"2024-03-05T09:40:56.000Z","categories_index":"chatglm,WORK","tags_index":"https,cpp,THUDM","author_index":"dandeliono"},{"id":"f3f0b4e51e77515a8031a4140316d20f","title":"NullPointerException 没有打印异常栈问题追踪","content":"NullPointerException 没有打印异常栈问题追踪今天去服务器后台看日志，发现有很多NullPointerException异常。我下意识的去找异常栈信息，想看下到底是哪行代码导致了空指针。但是发现日志中只打印出了如下日志：\n12Copy`nulljava.lang.NullPointerException: null` \n\n我怀疑是不是打印日志的时候没有将异常栈打印出来，于是又去翻看应用代码核实。但是发现日志打印的代码也是正常的。\n1`logger.error(&quot;msg:&quot;,e);` \n\n问题原因#我在网上找到了这么一段描述：\n\n\n\n\n\n\n\n\n\nJVM 虚拟机会对异常信息进行优化，当相同异常出现很多次，会认为它是热点异常，忽略掉异常堆栈信息；通过增加 JVM 参数：-XX:-OmitStackTraceInFastThrow 可解决。\n这个描述能很好的解释我发现的问题。代码中出现空指针异常的地方是一个定时任务在不停地调用，当这个异常出现次数太多时 JVM 就会将其过滤掉。\n为了验证我的猜想，我去找了下这个服务刚刚启动时的代码，发现这个异常栈是正常打出的，这也验证了自己的猜想，通过异常栈信息也找到了导致空指针异常的代码。\n问题重现#下面是自己写的一段代码来显示这个额问题：\n123456789101112131415161718192021Copy`public class NullPointStackMissBug &#123;    static Logger logger = LoggerFactory.getLogger(NullPointStackMissBug.class);    public static void main(String[] args) &#123;        for (int i = 0; i &lt; 100000 ; i++) &#123;            try&#123;                System.out.println(&quot;Loop:&quot;+(i+1));                String str = &quot;test&quot;;                if(true)&#123;                    str = null;                &#125;                str.toUpperCase();            &#125;catch (Exception e)&#123;                logger.error(e.getMessage(),e);            &#125;        &#125;    &#125;&#125;` \n\n在JVM启动参数中增加：-XX:-OmitStackTraceInFastThrow后,异常就能正常输出。\n","slug":"JAVA/NullPointerException 没有打印异常栈问题追踪","date":"2024-01-31T16:27:01.000Z","categories_index":"JVM,JAVA","tags_index":"https,www,NullPointerException","author_index":"dandeliono"},{"id":"cf58df7d33c8bd4e506c71804d861dcc","title":"如何在 Docker 容器中使用 systemctl","content":"如何在 Docker 容器中使用 systemctl我们有时在使用 docker 的时候，会发现在容器中使用 systemctl 或者 service 的时候，会遇到下面的错误：\n1Failed to connect to bus: No such file or directory\n\n一、原因\nThat’s because “systemctl” talks to the systemd daemon by using the d-bus. In a container there is no systemd-daemon. Asking for a start will probably not quite do what you expect - the dev-mapping need to be a bit longer.\n容器里面是没有 systemd 进程的，所以不能正常开启 systemctl 。为什么 docker 会这样呢：\nThis is by design. Docker should be running a process in the foreground in your container and it will be spawned as PID 1 within the container’s pid namespace. Docker is designed for process isolation, not for OS virtualization, so there are no other OS processes and daemons running inside the container (like systemd, cron, syslog, etc), only your entrypoint or command you run.\nIf they included systemd commands, you’d find a lot of things not working since your entrypoint replaces init. Systemd also makes use to cgroups which docker restricts inside of containers since the ability to change cgroups could allow a process to escape the container’s isolation. Without systemd running as init inside your container, there’s no daemon to process your start and stop commands.\ndocker只是提供了进程隔离，不是操作系统的虚拟。\n二、解决方案\n1、网上流传一种错误的解决方法。 \n我们可以在启动容器的时候将在启动参数加上 &#x2F;sbin&#x2F;init 来让其生效。\n以centos为例：\n1docker run -d -v /sys/fs/cgroup/:/sys/fs/cgroup:ro --cap-add SYS_ADMIN --name systemd_websrv centos /sbin/init\n\n以上为网上流传的方法，我先为大家解释一下这条命令：\n12345-d：表示以后台进程的形式运行 Docker 容器。-v /sys/fs/cgroup/:/sys/fs/cgroup:ro：表示将主机的 /sys/fs/cgroup 目录挂载到容器中的 /sys/fs/cgroup 目录，并以只读的方式挂载。这个目录包含了 cgroups 系统的配置文件，用于管理系统资源的分配。（:ro表示以只读方式挂载，当然:rw表示以读写方式挂载）--cap-add SYS_ADMIN：表示为容器添加 SYS_ADMIN 的能力，以便容器能够访问系统的一些管理功能。--name systemd_websrv：表示将容器命名为 &quot;systemd_websrv&quot;。centos /sbin/init：表示使用 CentOS 镜像来创建容器，并在容器中运行 /sbin/init 程序。/sbin/init 是系统启动时运行的第一个进程，它负责启动系统的其他进程。\n\n我们尝试运行这条命令，然后使用如下命令查询 systemctl 在容器里是否正常工作。\n12docker exec -it systemd_websrv systemctlFailed to connect to bus: No such file or directory\n\n错误的原因可能是在启动 Docker 容器时没有正确挂载容器内的 &#x2F;run&#x2F;dbus 目录。我们使用如下命令将 &#x2F;run&#x2F;dbus 挂载上。\n1234 docker stop systemd_websrvdocker rm systemd_websrvdocker run -d -v /sys/fs/cgroup:/sys/fs/cgroup:ro -v /run/dbus:/run/dbus -v /etc/machine-id:/etc/machine-id --cap-add SYS_ADMIN --name systemd_websrv centos /sbin/init\n\n再次尝试运行这条命令，然后使用如下命令查询 systemctl 在容器里是否正常工作。\n12docker exec -it systemd_websrv systemctlFailed to connect to bus: Permission denied\n\n这次提示的是没有权限了，继续改命令。\n1234 docker stop systemd_websrvdocker rm systemd_websrvdocker run -ti -v /run/dbus/system_bus_socket:/run/dbus/system_bus_socket:ro -v /sys/fs/cgroup/:/sys/fs/cgroup:ro --cap-add SYS_ADMIN --name systemd_websrv centos /bin/bash\n\n这次可以了，进入 Docker容器了，我们尝试查询 systemctl 能否正常使用。\n123systemctl statusSystem has not been booted with systemd as init system (PID 1). Can&#x27;t operate.Failed to connect to bus: Host is down\n\nPID 1 是什么东东？我们尝试使用如下命令查询一下。\n1234ps auxUSER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMANDroot         1  0.0  0.4  12036  2160 ?        Ss   08:16   0:00 /bin/bashroot        16  0.0  0.4  44652  1780 ?        R+   08:20   0:00 ps aux\n\n由于 init 进程 PID 1 是 &#x2F;bin&#x2F;bash，而不是 systemd，即使挂载了 dbus 也无济于事。因此这条路是行不通的。\n2、正确的解决方法：替换 systemctl\n使用 docker-systemctl-replacement 替换容器中的 systemctl 。\n以 ubuntu 镜像为例：\n1). 安装python2\n1sudo apt install python\n\n2). 替换 systemctl (注意路径，可以使用 where is systemctl 查看当前默认路径)\n1wget https://raw.githubusercontent.com/gdraheim/docker-systemctl-replacement/master/files/docker/systemctl.py -O /bin/systemctl\n\n3). 给定权限\n1sudo chmod a+x /bin/systemctl\n\n这样就可以使用非 systemd 的 systemctl ，但是因为是非官方的 systemctl 所以可能存在一些未知问题。\n最好还是建议将 docker 作为进程隔离环境，single app single container， 但是遇到非常特殊的情况下，可以使用上述解决方案，如果有更好的方案，欢迎反馈。\n三、参考\nhttps://stackoverflow.com/questions/49285658/how-to-solve-docker-issue-failed-to-connect-to-bus-no-such-file-or-directoryhttps://stackoverflow.com/questions/39169403/systemd-and-systemctl-within-ubuntu-docker-images[https://super-unix.com/ubuntu/ubuntu-systemctl-failed-to-connect-to-bus-docker-ubuntu16-04-container/]\n","slug":"OCI/如何在 Docker 容器中使用 systemctl","date":"2024-01-29T14:59:56.000Z","categories_index":"systemctl,OCI","tags_index":"docker,systemd,the","author_index":"dandeliono"},{"id":"c4719cbd659f86d7d7d8c9fcfcd72c3e","title":"linux TOP命令各参数详解","content":"linux TOP命令各参数详解[toc]\nTOP是一个动态显示过程,即可以通过用户按键来不断刷新当前状态.如果在前台执行该命令,它将独占前台,直到用户终止该程序为止.比较准确的说,top命令提供了实时的对系统处理器的状态监视.\n下面是使用top命令来进行性能检测的截图：\n\n各参数含义一、top前5行统计信息第1行：  top - 05:43:27 up 4:52, 2 users, load average: 0.58, 0.41, 0.30 第1行是任务队列信息，其参数如下：\n\n\n\n内容\n含义\n\n\n\n05:43:27\n表示当前时间\n\n\nup 4:52\n系统运行时间 格式为时：分\n\n\n2 users\n当前登录用户数\n\n\nload average: 0.58, 0.41, 0.30\n系统负载，即任务队列的平均长度。 三个数值分别为 1分钟、5分钟、15分钟前到现在的平均值。\n\n\nload average: 如果这个数除以逻辑CPU的数量，结果高于5的时候就表明系统在超负荷运转了。  \n第2行：  Tasks: 159 total, 1 running, 158 sleeping, 0 stopped, 0 zombie 第3行：  %Cpu(s): 37.0 us, 3.7 sy, 0.0 ni, 59.3 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st 第2、3行为进程和CPU的信息当有多个CPU时，这些内容可能会超过两行，其参数如下：\n\n\n\n内容\n含义\n\n\n\n159 total\n进程总数\n\n\n1 running\n正在运行的进程数\n\n\n158 sleeping\n睡眠的进程数\n\n\n0 stopped\n停止的进程数\n\n\n0 zombie\n僵尸进程数\n\n\n37.0 us\n用户空间占用CPU百分比\n\n\n3.7 sy\n内核空间占用CPU百分比\n\n\n0.0 ni\n用户进程空间内改变过优先级的进程占用CPU百分比\n\n\n59.3 id\n空闲CPU百分比\n\n\n0.0 wa\n等待输入输出的CPU时间百分比\n\n\n0.0 hi\n硬中断（Hardware IRQ）占用CPU的百分比,CPU服务于硬中断所耗费的时间总额\n\n\n0.0 si、0.0 st\nCPU服务于软中断所耗费的时间总额、Steal Time\n\n\n第4行：  KiB Mem: 1530752 total, 1481968 used, 48784 free, 70988 buffers 第5行：  KiB Swap: 3905532 total, 267544 used, 3637988 free. 617312 cached Mem 第4、5行为内存信息其参数如下：\n\n\n\n内容\n含义\n\n\n\nKiB Mem: 1530752 total\n物理内存总量\n\n\n1481968 used\n使用的物理内存总量\n\n\n48784 free\n空闲内存总量\n\n\n70988 buffers\n用作内核缓存的内存量\n\n\nKiB Swap: 3905532 total\n交换区总量\n\n\n267544 used\n使用的交换区总量\n\n\n3637988 free\n空闲交换区总量\n\n\n617312 cached Mem\n缓冲的交换区总量。\n\n\n上述最后提到的缓冲的交换区总量，这里解释一下，所谓缓冲的交换区总量，即内存中的内容被换出到交换区，而后又被换入到内存，但使用过的交换区尚未被覆盖，该数值即为这些内容已存在于内存中的交换区的大小。相应的内存再次被换出时可不必再对交换区写入。\n计算可用内存数有一个近似的公式：第四行的free + 第四行的buffers + 第五行的cached\n二、进程信息\n\n\n列名\n含义\n\n\n\nPID\n进程id\n\n\nPPID\n父进程id\n\n\nRUSER\nReal user name\n\n\nUID\n进程所有者的用户id\n\n\nUSER\n进程所有者的用户名\n\n\nGROUP\n进程所有者的组名\n\n\nTTY\n启动进程的终端名。不是从终端启动的进程则显示为 ?\n\n\nPR\n优先级\n\n\nNI\nnice值。负值表示高优先级，正值表示低优先级\n\n\nP\n最后使用的CPU，仅在多CPU环境下有意义\n\n\n%CPU\n上次更新到现在的CPU时间占用百分比\n\n\nTIME\n进程使用的CPU时间总计，单位秒\n\n\nTIME+\n进程使用的CPU时间总计，单位1&#x2F;100秒\n\n\n%MEM\n进程使用的物理内存百分比\n\n\nVIRT\n进程使用的虚拟内存总量，单位kb。VIRT&#x3D;SWAP+RES\n\n\nSWAP\n进程使用的虚拟内存中，被换出的大小，单位kb\n\n\nRES\n进程使用的、未被换出的物理内存大小，单位kb。RES&#x3D;CODE+DATA\n\n\nCODE\n可执行代码占用的物理内存大小，单位kb\n\n\nDATA\n可执行代码以外的部分(数据段+栈)占用的物理内存大小，单位kb\n\n\nSHR\n共享内存大小，单位kb\n\n\nnFLT\n页面错误次数\n\n\nnDRT\n最后一次写入到现在，被修改过的页面数。\n\n\nS\n进程状态。D&#x3D;不可中断的睡眠状态 R&#x3D;运行 S&#x3D;睡眠 T&#x3D;跟踪&#x2F;停止 Z&#x3D;僵尸进程\n\n\nCOMMAND\n命令名&#x2F;命令行\n\n\nWCHAN\n若该进程在睡眠，则显示睡眠中的系统函数名\n\n\nFlags\n任务标志\n\n\n默认情况下仅显示比较重要的 PID、USER、PR、NI、VIRT、RES、SHR、S、%CPU、%MEM、TIME+、COMMAND 几个列！\n  可以通过下面的快捷键来更改显示内容：   更改显示内容通过 f 键可以选择显示的内容（按 f 键之后会显示列的列表，按 a-z 即可显示或隐藏对应的列，最后按回车键确定）   按 o 键可以改变列的显示顺序（按小写的 a-z 可以将相应的列向右移动，而大写的 A-Z 可以将相应的列向左移动，最后按回车键确定）   按大写的 F 或 O 键，然后按 a-z 可以将进程按照相应的列进行排序，而大写的 R 键可以将当前的排序倒转.\n三、 使用技巧1.top命令默认值显示前面一部分的进程，若是想要查看全部的进程信息，则需要使用命令：top -ab -n 1，在这个命令中，a表示按内存倒序排列，b表示将所有列输出，n表示只执行一次。\n2.在使用top -ab -n 1将所有进程信息输出之后，可以使用AWK等类似工具统计信息，例如：统计出COMMAND为httpd的所有进程占用的内存。\n3.也可以使用命令top -an -n 1 | grep httpd，查看所有httpd的进程信息，如下图：\n\n4. 常用命令\n123456top   // 每隔5秒显式所有进程的资源占用情况top -d 2  // 每隔2秒显式所有进程的资源占用情况top -c  // 每隔5秒显式进程的资源占用情况，并显示进程的命令行参数(默认只有进程名)top -p 12345 -p 6789// 每隔5秒显示pid是12345和pid是6789的两个进程的资源占用情况top -d 2 -c -p 123456 // 每隔2秒显示pid是12345的进程的资源使用情况，并显式该进程启动的命令行参数\n","slug":"LINUX/linux TOP命令各参数详解","date":"2024-01-18T11:57:19.000Z","categories_index":"CPU,LINUX","tags_index":"top,单位,total","author_index":"dandeliono"},{"id":"4bd9e9434ed4ab6b72d76312de1a0204","title":"Elasticsearch中如何进行日期(数值)范围查询","content":"Elasticsearch中如何进行日期(数值)范围查询\n\n\n符号\n含义\n\n\n\ngte\ngreater-than or equal to, 大于或等于\n\n\ngt\ngreater-than, 大于\n\n\nlte\nless-than or equal to, 小于或等于\n\n\nlt\nless-than, 小于\n\n\n需求: 查询商品中40 &lt;= price &lt;= 80的文档:\n12345678910111213GET book_shop/_search&#123;    &quot;query&quot;: &#123;        &quot;range&quot;: &#123;            &quot;price&quot;: &#123;                &quot;gte&quot;: 40,                &quot;lte&quot;: 80,                &quot;boost&quot;: 2.0\t            &#125;        &#125;    &#125;&#125;\n\n3.1 简单查询示例需求: 查询网站中最近一天发布的博客:\n123456789101112GET website/_search&#123;    &quot;query&quot;: &#123;        &quot;range&quot;: &#123;            &quot;post_date&quot;: &#123;            \t&quot;gte&quot;: &quot;now-1d/d&quot;,\t            \t&quot;lt&quot;:  &quot;now/d&quot;\t\t        \t&#125;        &#125;    &#125;&#125;\n\n3.2 关于时间的数学表达式(date-math)Elasticsearch中时间可以表示为now, 也就是系统当前时间, 也可以是以||结尾的日期字符串表示.\n在日期之后, 可以选择一个或多个数学表达式:\n\n+1h —— 加1小时;\n-1d —— 减1天;\n/d —— 四舍五入到最近的一天.\n\n下面是Elasticsearch支持数学表达式的时间单位:\n\n\n\n表达式\n含义\n表达式\n含义\n\n\n\ny\n年\nM\n月\n\n\nw\n星期\nd\n天\n\n\nh\n小时\nH\n小时\n\n\nm\n分钟\ns\n秒\n\n\n说明: 假设系统当前时间now = 2018-10-01 12:00:00 :\n\nnow+1h: now的毫秒值 + 1小时, 结果是: 2018-10-01 13:00:00.\nnow-1h: now的毫秒值 - 1小时, 结果是: 2018-10-01 11:00:00.\nnow-1h/d: now的毫秒值 - 1小时, 然后四舍五入到最近的一天的起始, 结果是: 2018-10-01 00:00:00.\n2018.10.01||+1M/d: 2018-10-01的毫秒值 + 1月, 再四舍五入到最近一天的起始, 结果是: 2018-11-01 00:00:00.\n\n3.3 关于时间的四舍五入\n\n\n\n\n\n\n\n\n对日期中的日、月、小时等 进行四舍五入时, 取决于范围的结尾是包含(include)还是排除(exclude).\n向上舍入: 移动到舍入范围的最后一毫秒;\n向下舍入: 一定到舍入范围的第一毫秒.\n举例说明:\n① “gt”: “2018-12-18||&#x2F;M” —— 大于日期, 需要向上舍入, 结果是2018-12-31T23:59:59.999, 也就是不包含整个12月.\n② “gte”: “2018-12-18||&#x2F;M” —— 大于或等于日期, 需要向下舍入, 结果是 2018-12-01, 也就是包含整个12月.\n③ “lt”: “2018-12-18||&#x2F;M” —— 小于日期, 需要向上舍入, 结果是2018-12-01, 也就是不包含整个12月.\n④ “lte”: “2018-12-18||&#x2F;M” —— 小于或等于日期, 需要向下舍入, 结果是2018-12-31T23:59:59.999, 也就是包含整个12月.\n格式化日期查询时, 将默认使用日期field中指定的格式进行解析, 当然也可以通过format参数来覆盖默认配置.\n示例:\n12345678910111213GET website/_search&#123;    &quot;query&quot;: &#123;        &quot;range&quot;: &#123;            &quot;post_date&quot;: &#123;                &quot;gte&quot;: &quot;2/1/2018&quot;,                 &quot;lte&quot;: &quot;2019&quot;,                &quot;format&quot;: &quot;dd/MM/yyyy||yyyy&quot;            &#125;        &#125;    &#125;&#125;\n\n注意: 如果日期中缺失了部分年、月、日, 缺失的部分将被填充为unix系统的初始值, 也就是1970年1月1日.\n比如, 将dd指定为format, 像&quot;gte&quot;: 10将转换为1970-01-10T00:00:00.000Z.\n如果日期field的格式允许, 也可以通过在日期值本身中指定时区, 从而将日期从另一个时区的时间转换为UTC时间, 或者为其指定特定的time_zone参数.\n示例:\n1234567891011121314GET website/_search&#123;    &quot;query&quot;: &#123;        &quot;range&quot;: &#123;            &quot;post_date&quot;: &#123;                &quot;gte&quot;: &quot;2018-01-01 00:00:00&quot;,                &quot;lte&quot;: &quot;now&quot;,                &quot;format&quot;: &quot;yyyy-MM-dd hh:mm:ss&quot;,                &quot;time_zone&quot;: &quot;+1:00&quot;            &#125;        &#125;    &#125;&#125;\n\nES中的日期类型必须按照UTC时间格式存储, 所以, 上述的2018-01-01 00:00:00将被转换为2017-12-31T23:00:00 UTC.\n另外需要注意的是, now是不受time_zone影响的.\n","slug":"MIDDLEWARE/Elasticsearch中如何进行日期(数值)范围查询","date":"2024-01-09T13:56:38.000Z","categories_index":"now,MIDDLEWARE","tags_index":"结果是,小时,than","author_index":"dandeliono"},{"id":"4483f94e0db17009da0543d6cde026da","title":"ElasticSearch里面如何分组后根据sum值排序","content":"ElasticSearch里面如何分组后根据sum值排序ElasticSearch里面的聚合机制非常灵活和强大，今天我们来看下如何在ElasticSearch里面实现分组后，根据sum值进行排序？类似的数据库SQL如下：\n1select id,sum(c1) as c1 , sum(c2) as c2  from table1 group id order by c1 desc, c2 asc\n\n这是一个比较常见的统计需求，在es也能比较轻松的实现，先看看curl的一个实现例子查询：\n1&quot;field&quot;:&quot;FIELD2.SUBFIELD&quot;\n\n然后，我们看下，如何在Java Api里面操作：\n首先我们看下造的数据\n总共三个字段id,count,code都是int类型的\n然后，我们可以将上面的数据插入到es里面，具体的插入代码不在给出，比较简单，直接通过client.prepareIndex方法插入json即可。\n下面看下查询代码：\n123456789101112131415161718192021SearchRequestBuilder search = client.prepareSearch(&quot;gv_test&quot;).setTypes(&quot;gv_test&quot;);TermsBuilder tb= AggregationBuilders.terms(&quot;id&quot;).field(&quot;id&quot;).order(Terms.Order.compound(    Terms.Order.aggregation(&quot;sum_count&quot;,false), // 先按count，降序排    Terms.Order.aggregation(&quot;sum_code&quot;,true)   // 如果count相等情况下，使用code的和排序));SumBuilder sb= AggregationBuilders.sum(&quot;sum_count&quot;).field(&quot;count&quot;);SumBuilder sb_code= AggregationBuilders.sum(&quot;sum_code&quot;).field(&quot;code&quot;);tb.subAggregation(sb); //添加到分组聚合请求中tb.subAggregation(sb_code); //添加到分组聚合请求中search.addAggregation(tb);Terms tms= search.get().getAggregations().get(&quot;id&quot;);for(Terms.Bucket tbb:tms.getBuckets())&#123;    Sum sum= tbb.getAggregations().get(&quot;sum_count&quot;);    Sum sum2=tbb.getAggregations().get(&quot;sum_code&quot;);    System.out.println(tbb.getKey()+&quot; &quot; + tbb.getDocCount() +&quot; &quot;+sum.getValue()+&quot; &quot;+sum2.getValue());&#125;\n\n最终的结果如下：\n通过对比，我们可以到到结果是准确的，虽然代码量比sql多很多，但是ElasticSearch的聚合功能却是非常的强大和灵活，用来做一些OLAP分析是非常方便的。\n","slug":"MIDDLEWARE/ElasticSearch里面如何分组后根据sum值排序","date":"2024-01-09T13:55:15.000Z","categories_index":"ElasticSearch,MIDDLEWARE","tags_index":"sum,然后,里面如何分组后根据","author_index":"dandeliono"},{"id":"9c243b726e495059eca1f4eed1c4f935","title":"使用 libfaketime 修改 docker 容器时间","content":"使用 libfaketime 修改 docker 容器时间容器的时间问题：如果想要直接进入容器，使用date -s修改日期，则会出现一个\n1\n\n的错误，而且也不会成功。\n这是由于docker容器的隔离是基于Linux的Capability机制实现的, Linux的Capability机制允许你将超级用户相关的高级权限划分成为不同的小单元。目前Docker容器默认只用到了以下的Capability.\n\nCHOWN,\nDAC_OVERRIDE,\nFSETID,\nFOWNER,\nMKNOD,\nNET_RAW,\nSETGID,\nSETUID,\nSETFCAP,\nSETPCAP,\nNET_BIND_SERVICE,\nSYS_CHROOT,\nKILL,\nAUDIT_WRITE\n\n而要修改系统时间需要有SYS_TIME权限。使用 --cap-add, --cap-drop 可以添加或禁用特定的权限。--privileged参数也可以达到开放权限的作用, 与--cap-add的区别就是, --privileged是将所有权限给容器.\ndocker使用--privileged, --cap-add, --cap-drop 来对容器本身的能力进行开放或限制。\n那么使用如下命令就可以直接改变时间了：  \n1\n\n接着进入容器实行date命令修改时间，如果没有修改成功，，那么可能就是因为宿主机做了共享主机的localtime（比如laradock就做了）：  \n1\n\n如果修改成功一会就又恢复了，那么就可能要查看一下宿主机是否做了定时校准的任务。\n但是如此执行之后，那就是容器时间变更为5月28日之后，宿主机的时间也跟着变更了，因为上边操作的 --cap-add SYS_TIME是为了将宿主机的内核时间挂载进来与容器共享，因此容器时间更改了，宿主机时间也会跟着更改。\n使用 libfaketime由此可见，直接修改docker容器的时间是比较危险的，所以选择如下方案。\n首先在宿主机上安装：libfaketime  \n1\n\n安装完成之后，把安装后的库文件拷贝到docker中：  \n1\n\n然后再进入docker中执行命令改变环境变量：\n1\n\n改变环境变量之后，再执行脚本会发现时间已经改变。若想要恢复，直接把环境变量修改为空即可：  \n1\n\n\n\n\n\n\n\n\n\n\nLD_PRELOAD是Linux系统的一个环境变量，它可以影响程序的运行时的链接（Runtime linker），它允许你定义在程序运行前优先加载的动态链接库。这个功能主要就是用来有选择性的载入不同动态链接库中的相同函数。通过这个环境变量，我们可以在主程序和其动态链接库的中间加载别的动态链接库，甚至覆盖正常的函数库。一方面，我们可以以此功能来使用自己的或是更好的函数（无需别人的源码），而另一方面，我们也可以以向别人的程序注入程序，从而达到特定的目的。\n如果需要修改容器中的各种web服务的时间，只需要在改变环境变量之后，重启服务即可。但是注意，镜像必须使用比较基础的镜像，因为如果直接使用服务的镜像（例如php镜像），会在重启的时候，整个容器会退出，faketime就会修改无效。\n例如：  \n1\n\n修改 faketime 步骤第一步：需要通过dockerfile把libfaketime拷贝部分制作到基础镜像当中。第二步：通过uuid来寻找要执行的pod。第三步：修改pod的yaml文件（容器启动），把\nexport LD_PRELOAD=/usr/local/lib/libfaketime.so.1 FAKETIME=&quot;-5d&quot;\n加入yaml，重启pod（会自动重启相应服务php、kong、nginx、go等），此时faketime生效。\n","slug":"OCI/使用 libfaketime 修改 docker 容器时间","date":"2023-12-14T14:07:31.000Z","categories_index":"使用,OCI","tags_index":"docker,libfaketime,cap","author_index":"dandeliono"},{"id":"82b3a52976f247e94d00aebe443d97b1","title":"Linux 系统监控工具 atop","content":"Linux 系统监控工具 atop系统监控是运维工作中重要的一环，本文以 atop 工具为例来介绍系统的重要监控项。\natop可以使用yum或apt包管理器进行安装。atop man page 中详细说明了 atop 中各监控项含义及atop命令用法。\n\n\n如上图所示, atop 的界面分为上半部分的系统监控项和下半部分的进程列表。\natop 每10s更新一次系统监控项以及在这段时间内状态发生变化的进程，按下A键可以查看全部进程。\n进程#第一行PRC显示总体进程状况:\n\nsys, user 表示 CPU 在内核态和用户态的运行时间比例\n#proc 为当前总进程数,\n#trun 表示 running 状态线程数\n#tslpi 表示 sleeping interruptible 状态的进线程数\n#tslpu 表示 sleeping uninterruptible 状态线程数\n#zombie 表示僵尸进程数\n\n\nclones 表示在监控周期（默认10s）内 clone() 系统调用次数\n\nlinux 中进程有两种 sleep 状态:\n\ninterruptible sleep: 进程接收系统信号，可以被系统信号中断\nuninterruptible sleep: 进程不接收系统信号，不可被系统信号中断，包括kill -9 (SIGKILL 信号)。当一个进程向磁盘读写数据时，为了保证数据的一致性，在得到磁盘回复前，它是不能被其他进程或者中断打断的，这个时候的进程就处于不可中断状态。\n\n一个进程使用fork创建子进程，如果子进程退出，而父进程并没有调用wait或waitpid获取子进程的状态信息，那么子进程的进程描述符仍然保存在系统中，这种进程称之为僵尸进程。大量僵尸进程可能会占用进程描述符空间导致无法创建进程。\n孤儿进程是容易与僵尸进程混淆的一类进程，孤儿进程是父进程终止的进程，它们会被 init 进程接管并不会产生危害。\nCPU#在 atop 中每个 CPU 逻辑核心拥有一个 cpu 行表示自身状态, 最前面的 CPU 行则展示系统总览。\n\nsys 表示CPU在内核态工作时间比例\nuser 表示 CPU 在用户态工作时间比例\nirq 表示 CPU 处理系统中断所消耗的时间比例\nidle 表示 CPU 空闲时间比例\n\nCPL 行表示 CPU 负载（CPU Load）:\n\navg1, avg5, avg15: 过去 1min、5min和 15min 内的平均系统负载\ncsw 表示监控周期内上下文切换的次数\nintr 表示监控周期内中断发生的次数\n\n系统负载\nCPU 负载或称为系统负载是一个容易被误解的监控项，它的定义为内核运行队列中 running 或 uninterruptible sleep 状态的进程的平均数与CPU计算能力的比值。\n系统负载 1.0 说明CPU恰好满载，当系统负载大于1.0时会有进程因为等待CPU而阻塞。在多核系统中，系统负载等于CPU核心数表示恰好满载，如在上图所示双核系统中，load&#x3D;2说明恰好满载。\n上文已经说明，uninterruptible sleep 进程通常是在等待IO, 当网络异常或磁盘故障时会导致大量进程处于 uninterruptible sleep 状态从而导致 Load 急剧上升。\n常见的服务器程序大多数为IO密集型程序，常见的CPU密集型任务包括:\n\n大规模的排序计算, 如 mysql filesort\n大量的正则表达式匹配\n大量的 Hash Code 计算\n大规模的加解密或压缩解压计算\n\n当我们发现 CPU 使用率上升时，我们可以优先考虑是否在上述CPU密集型任务。\n内存#MEM 行描述内存使用情况:\n\ntot: 物理内存总量\nfree: 空闲内存总量\ncache: 文件缓存用量\nbuff: 块设备缓存用量\nslab: 系统内核 slab 内存用量\ndirty: 需要写回磁盘的脏页用量，这部分内存使用已包含在cache中\n\n这里出现了两个两个缓存: cache 和 buffer:\ncache 是指 page cache， 是在文件系统级别的缓存。用于缓存从文件中读取的数据，下次读取文件时可以从内存中快速获取，不需要进行磁盘IO。\nbuffer 是磁盘等块设备的缓存，不经过文件系统直接对磁盘进行读写的数据会缓存在 buffer 中。\n文件需要映射到物理磁盘的块上，这层映射关系由文件系统负责维护。没有文件系统支持的数据读写都会使用 buffer 缓存，比如文件系统元数据的缓存，以及 dd 等工具直接对磁盘进行读写时需要的缓存。\ncache 和 buffer 两个缓存既会被用在读请求中，也会被用在写请求中。\nPAG 行表示页缓存的使用情况:\n\nscan: 当可用内存不足时扫描的页数，这个值过高说明可用内存不足\nstall: 内核紧急将页加载到内存中的次数，这个值过高说明可用内存不足\nsteal: 虚拟机相关指标\nswin: 从 Swap 分区将页加载到内存的次数\nswout: 将内存页写入 Swap 分区的次数\n\nscan 和 steal 的解释比较难理解，附上 man page 中的原文:\n\n\n\n\n\n\n\n\n\nThis line contains the number of scanned pages (‘scan’) due to the fact that free memory drops below a particular threshold and the number times that the kernel tries to reclaim pages due to an urgent need (‘stall’)\nSWP 行表示 Swap 分区使用状态:\n\ntot: Swap 分区总大小\nfree: Swap 分区空闲空间大小\n\n当物理内存不足时，内核会将进程内存中不常用的页逐出内存写入磁盘中的 Swap 分区，当进程需要读取这些页时再将它们从磁盘中加载到内存。\n磁盘#DSK 列描述磁盘使用情况:\n\nvda: 该列为磁盘设备名，每个设备拥有一行\nbusy: 设备处理IO请求的时间占比\nread: 监控周期内读请求数\nwrite: 监控周期内写请求数\nKiB&#x2F;r: 每次读请求的平均数据量\nKiB&#x2F;w: 每次写请求的平均数据量\nMBr&#x2F;s: 每秒读取的数据量\nMBw&#x2F;s: 每秒写入的数据量\navq: io 队列的平均长度\navio: 单次读写请求需要的毫秒数\n\n网络#网络层通常包含 transport、network、 eth 和 lo 行， 分别表示传输层、网络层、以太网（数据链路层）和本地回环的监控指标。\n\ntcpi&#x2F;udpi&#x2F;ipi: 接收的 tcp&#x2F;udp&#x2F;ip 数据包\ntcpo&#x2F;udpo&#x2F;ipo : 发出的 tcp&#x2F;udp&#x2F;ip 数据包\ntcpao: 主动建立的tcp连接数(active open)\ntcppo: 被动建立的tcp连接数(passive open), 即通过 listen() 建立的连接数\ntcprs: tcp 重传次数\ntcpie: 读取时发生错误的次数\n\n进程列表有多个视图分别展示不同方面的数据:\n\n默认视图(Generic information): 按G键回到默认视图\n内存视图(Memory information): 按M键进入内存视图，显示进程的内存占用情况\n命令行视图(Command Line information): 按C键进入命令行视图，显示进程启动时详细命令行参数\n调度器视图(Scheduling information): 按S键进入视图，显示线程调度、CPU使用和运行统计\n磁盘视图(Disk information): 按D键进入视图，显示进程的磁盘IO使用情况\n网络视图(Network information): 按N键进入视图，显示进程的网络IO使用情况\n\natop 默认展示过去10s内状态发生变化的进程，按下A键可以查看全部进程。\n默认视图#默认视图展示常用的监控项:\n\nPID: 进程ID\nSYSCPU: 在内核态下使用CPU时间\nUSERCPU: 在用户态下使用CPU时间\nVGROW: 过去一个监控周期内进程的虚拟内存空间增长，包括malloc()分配内存、使用共享内存以及free()释放内存造成的空间变化\nRGROW: 过去一个监控周期内进程常驻内存空间(resident memory)增长, 即进程内存空间中驻留在物理内存中未被逐出到SWAP分区的部分。\nRUID, EUID, SUID: 启动进程的UID\nRUID: 登录时的用户ID\nEUID: Effective Uid。通常EUID&#x3D;RUID, setuid 或 sudo 等指令能以另一个用户身份执行命令，这个被“代理”的用户即为 Effective User。\n\n\nEXC: 进程退出时的返回码\nTHR: 进程中的线程数\nS: 进程状态，与ps命令的进程描述符相同\n\n简单介绍一下进程状态\n\nR: Runing\nS: sleeping interruptible 等待某个事件\nD: sleeping non-interruptible 通常在等待IO\nZ: Zombie 僵尸进程\nE: 进程在上个监控周期内退出\nT: TASK_STOPPED 或 TASK_TRACED 状态\nTASK_STOPPED: 进程收到 SIGSTOP 信号进入暂停状态\nTASK_TRACED: 进程进入暂停状态等待跟踪它的进程，比如进程被 gdb 的断点暂停\n\n\n\n内存视图#\n\n按M键可以进入内存视图查看进程的内存使用情况:\n\nMINFLT: 进程缺页小错误(minor page fault)的次数\nMAJFLT: 进程缺页大错误(major page fault)的次数\nVSIZE: 虚拟内存空间的总大小\nRSIZE: 常驻内存(resident memory)的总大小\nVGROW: 虚拟内存空间在上个监控周期的增长\nRGROW: 虚拟内存空间在上个监控周期的增长\nMEM: 物理内存使用占比\n\n在 Linux 的内存管理系统中需要读取磁盘才能解决缺页中断称为大错误(Major Page Fault), 不需要读取磁盘可以解决的缺页中断被称为小错误(Minor Page Fault)。\n一般情况下 MINFLT 是因为频繁分配&#x2F;回收大内存块导致的，可以考虑使用内存池优化程序来减少缺页错误; MAJFLT 是由于物理内存不足导致。\n调度视图#\n\n按S键可以进入调度视图（Scheduling View）查看进程运行和CPU情况:\n\nTRUN: running 状态的线程数\nTSLPI: sleeping interruptible 状态线程数\nTSLPU: 表示 sleeping uninterruptible 状态进程数\nPILI: 调度策略\nPRI,NICE: 优先级，PRI+NICE越低优先级越高\nCPU: CPU 使用时间占比\n\n除了查看当前的状态外，atop 还可以服务方式运行在后台监控并记录系统状态。\n使用 service atop start 或 systemctl start atop 命令启动atop监控服务。\natop 默认将数据保存在/var/log/atop目录下，10 分钟采集一次，保留最近28天的数据。上述配置可以在 /etc/atop/atop.daily 文件中进行修改。\n使用 atop -r &lt;filename&gt; 命令读取日志文件。按t键向前翻页，T键向后翻页，b键跳转到指定时间，时间格式为hh:mm。\n","slug":"LINUX/Linux 系统监控工具 atop","date":"2023-12-13T09:25:22.000Z","categories_index":"CPU,LINUX","tags_index":"https,atop,img","author_index":"dandeliono"},{"id":"57ca05a26664334bc519d817eff06219","title":"使用prlimit命令不重启进程修改其limits等运行参数","content":"使用prlimit命令不重启进程修改其limits等运行参数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566这里我以MySQL服务为例  $ ps auxf | grep -v grep | grep mysqld     mysql      1204  1.3  3.4 7311604 1134388 ?     Sl   Aug31  68:40 ./bin/mysqld --defaults-file=/usr/local/mysql/my.cnf --user=mysql --log-error-verbosity=3  $ cat /proc/1204/limits | grep file   # 看到文件句柄限制为10240了    Max file size             unlimited            unlimited            bytes         Max core file size        0                    unlimited            bytes         Max open files            10240                10240                files         Max file locks            unlimited            unlimited            locks       # 用prlimit 搞一下    $ prlimit --nofile=65535:65535 --pid 1204  $ cat /proc/1204/limits | grep file        # 再次查看，可以看到已经变成 65535了     Max file size             unlimited            unlimited            bytes         Max core file size        0                    unlimited            bytes         Max open files            65535                65535                files         Max file locks            unlimited            unlimited            locks     prlimit 还支持其它的参数修改，具体如下    $ prlimit --help                                                Usage:    prlimit \\[options\\] \\[-p PID\\]    prlimit \\[options\\] COMMAND  General Options:    -p, --pid &lt;pid&gt;        process id    -o, --output &lt;list&gt;    define which output columns to use      --noheadings       don&#x27;t print headings      --raw              use the raw output format      --verbose          verbose output    -h, --help             display this help and exit    -V, --version          output version information and exit  Resources Options:    -c, --core             maximum size of core files created    -d, --data             maximum size of a process&#x27;s data segment    -e, --nice             maximum nice priority allowed to raise    -f, --fsize            maximum size of files written by the process    -i, --sigpending       maximum number of pending signals    -l, --memlock          maximum size a process may lock into memory    -m, --rss              maximum resident set size    -n, --nofile           maximum number of open files        # 最常用    -q, --msgqueue         maximum bytes in POSIX message queues    -r, --rtprio           maximum real-time scheduling priority    -s, --stack            maximum stack size    -t, --cpu              maximum amount of CPU time in seconds    -u, --nproc            maximum number of user processes        # 常用    -v, --as               size of virtual memory    -x, --locks            maximum number of file locks    -y, --rttime           CPU time in microseconds a process scheduled                under real-time scheduling  Available columns (for --output):    DESCRIPTION  resource description      RESOURCE  resource name        SOFT  soft limit        HARD  hard limit (ceiling)      UNITS  units  For more details see prlimit(1).\n","slug":"LINUX/使用prlimit命令不重启进程修改其limits等运行参数","date":"2023-11-29T14:25:53.000Z","categories_index":"使用,LINUX","tags_index":"prlimit,命令不重启进程修改其,limits","author_index":"dandeliono"},{"id":"e31577451631f8a2adc07bcea9f94111","title":"Elasticsearch 数值类型也能存String 类型问题分析","content":"Elasticsearch 数值类型也能存String 类型问题分析最近经常遇到遇到某个客户问数值类型的字段也能存字符串，或者说已经将字段类型设置成了float，但是实际存储的仍然是字符串，该如何解决，今天花点时间我们来梳理整个流程。\n1,定义一个索引mapping,并指定类型为float.单精度浮点型\n12345678910PUT nginxindex&#123;  &quot;mappings&quot;: &#123;    &quot;properties&quot;: &#123;      &quot;price&quot;:&#123;        &quot;type&quot;: &quot;float&quot;      &#125;    &#125;  &#125;&#125;\n\n2，写入几个文档，看看效果\n1234567891011121314POST nginxindex/_doc&#123;  &quot;price&quot;:4.68        &#125;POST nginxindex/_doc&#123;  &quot;price&quot;: &quot;4.69&quot;      &#125;POST nginxindex/_doc&#123;  &quot;price&quot;: &quot;free for charge&quot;    &#125; \n\n3，对比结果\n第一个，正常写入，有返回，\n第二个，正常写入，有返回，\n第三个，无法写入，报错提示。提示如下图所示\n\n第三个报错\n这个报错大概就是无法解析字符串内容到floa类型的type.这个比较容易理解，但是第二个文档那个字符串类型数字又能写入，这又是为何？那么这也是本节要关注的内容，也是前面客户需要解决的问题：\n用户在存储字符串形式的数字，无论是那种数字类型，都能默认识别并存储，那么最终客户在终端搜索的时候会出现很多返回结果数量不一致的问题。这个是什么原因呢？我们看一下官网,翻译如下：\n数据并不总是干净的。根据它的生成方式，一个数字可能在JSON体中呈现为一个真正的JSON数字，例如。5，但它也可能呈现为一个字符串，例如。“5”。或者，一个应该是整数的数字可以呈现为浮点，例如5.0，甚至“5.0”。\n需要配置强制程序来清理脏值，以适应字段的数据类型。具体参考如下链接：\nhttps://www.elastic.co/guide/en/elasticsearch/reference/current/coerce.html#coerce\n解决方案：就是在索引的mapping字段属性定义里，加一个coerce参数，并将其值设置为false.默认为true.\n123456789101112131415161718192021PUT nginxnewindex&#123;  &quot;mappings&quot;: &#123;    &quot;properties&quot;: &#123;      &quot;price&quot;:&#123;        &quot;type&quot;: &quot;float&quot;,        &quot;coerce&quot;: false              &#125;    &#125;  &#125;&#125;POST nginxnewindex/_doc        &#123;  &quot;price&quot;:4.68&#125;POST nginxnewindex/_doc      &#123;  &quot;price&quot;: &quot;4.69&quot;&#125; \n\n结果会发现，第三个文档，写入报错，截图如下：\n\n字段解析错误\n错误提示也是说你定义一个float类型的字段，但是写入的是字符串。\n那么这样的话，用户就能第一时间发现写入报错信息，及时扭转前端写入格式，以防后续影响业务了。那么这就是这个严格匹配参数的作用。\nmapping字段类型一旦定义，就不能再修改。那么实际用户生产环境，新数据可以通过修改新索引mapping参数解决。那么对于存量的索引数据，如何实现平滑更改呢。没错，大家想到的是reindex.再造索引。还是以本文前面报错的索引为列，如何实现字符串类型的float转换为纯float数字类型。\n先看源索引的文档类型，price是字符串类型，后面要实现更改为float.\n\n1，先创建一个目标索引，指定mapping参数\n1234567891011PUT nginxnewindex2&#123;  &quot;mappings&quot;: &#123;    &quot;properties&quot;: &#123;      &quot;price&quot;:&#123;        &quot;type&quot;: &quot;float&quot;,        &quot;coerce&quot;: false      &#125;    &#125;  &#125;&#125;\n\n2, 执行reindex拷贝，这里有问题，注意\n123456789POST _reindex&#123;  &quot;source&quot;: &#123;    &quot;index&quot;: &quot;nginxindex&quot;  &#125;,  &quot;dest&quot;: &#123;    &quot;index&quot;: &quot;nginxnewindex2&quot;  &#125;&#125;\n\n返回报错如下：\n\nreindex索引报错\n这个错误大家都很明白了，就是写入的是字符串，但是实际目标存储的字段类型是数字类型，解析失败。reindex也中断。\n因此，reindex在某些场景下是有限制的。两个索引mapping不一致的问题，会导致索引同步失败。\n那么有没有一种办法，将存量索引的字段类型进行更改，然后再拷贝到目标索引呢？ 答案是有的，这里要用到pipeline，管道预处理。就是在reindex拷贝之前，将源索引的字段类型先进行更改，然后再写入目标索引。\n需求：实现reindex的时候，转化源索引格式到float.\n第一步，创建一个processors\n123456789101112 PUT _ingest/pipeline/my-pipeline-id&#123;  &quot;description&quot;: &quot;converts the content of the price field to an float&quot;,  &quot;processors&quot; : [    &#123;      &quot;convert&quot; : &#123;        &quot;field&quot; : &quot;price&quot;,        &quot;type&quot;: &quot;float&quot;      &#125;    &#125;  ]&#125;\n\n第二步：创建一个目标索引\n1234567891011PUT nginxnewindex2&#123;  &quot;mappings&quot;: &#123;    &quot;properties&quot;: &#123;      &quot;price&quot;:&#123;        &quot;type&quot;: &quot;float&quot;,        &quot;coerce&quot;: false      &#125;    &#125;  &#125;&#125;\n\n第三步：执行reindex+pipeline\n12345678910POST _reindex&#123;  &quot;source&quot;: &#123;    &quot;index&quot;: &quot;nginxindex&quot;      &#125;,  &quot;dest&quot;: &#123;    &quot;index&quot;: &quot;nginxnewindex2&quot;,    &quot;pipeline&quot;: &quot;my-pipeline-id&quot;  &#125;&#125;\n\n返回如下：没有报错，执行成功。\n\n返回成功\n那么我们看看目标拷贝的索引的mapping，看看新的字段类型是否变成强float了。如下：更改成功。\n\n那么至此，彻底解决客户这个问题。问题整理完毕。以后两个字段类型不一样需要转格式，大家也可以这么操作。\n","slug":"MIDDLEWARE/Elasticsearch 数值类型也能存String 类型问题分析","date":"2023-11-22T10:47:50.000Z","categories_index":"https,MIDDLEWARE","tags_index":"com,float,developer","author_index":"dandeliono"},{"id":"a3ec52c6805d10835480c023a68f988e","title":"从 apache-common-pool 看如何写一个通用池","content":"从 apache-common-pool 看如何写一个通用池对象的创建和销毁在一定程度上会消耗系统的资源，虽然 jvm 的性能在近几年已经得到了很大的提高，对于多数对象来说，没有必要利用对象池技术来进行对象的创建和管理。但是对于有些对象来说，其创建的代价还是比较昂贵的，比如线程、tcp 连接、数据库连接等对象。对于那些创建耗时较长，或者资源占用较多的对象，比如网络连接，线程之类的资源，通常使用池化来管理这些对象，从而达到提高性能的目的。\napache-common-pool 提供了一个通用的对象池技术的实现。可以很方便的基于它来实现自己的对象池。比如 DBCP 和 Jedis 他们的内部对象池的实现就是依赖于 apache-common-pool (本文分析的是 apache common pool2)。\n组件分析先来看看 apache-common-pool 的组成结构:\n\nPooledObjectFactory|KeyedPooledObjectFactory: 池化对象工厂，负责对 PooledOjbect 的创建，状态验证，销毁，钝化，激活工作。\nPooledObject: 池化对象，这个就是前面所说需要池化的资源，被池化的对象可以抽离出共有属性，如，创建时间，状态，最近一次使用时间等。\nObjectPool|KeyedObjectPool: 对象池，它是负责和对象使用者直接打交道的，对使用者提供获取对象，返还对象接口。\n\n池化对象工厂PooledObjectFactoryPooledObjectFactory 是一个池化对象工厂接口，定义了生成对象、激活对象、钝化对象、销毁对象的方法，其方法和继承关系如下图：\n\n\nPooledObjectFactory\n12345678910111213141516public interface PooledObjectFactory&lt;T&gt; &#123;    PooledObject&lt;T&gt; makeObject() throws Exception;    void destroyObject(PooledObject&lt;T&gt; p) throws Exception;    boolean validateObject(PooledObject&lt;T&gt; p);    void activateObject(PooledObject&lt;T&gt; p) throws Exception;      void passivateObject(PooledObject&lt;T&gt; p) throws Exception;&#125;\n\n从类图中可以看到 PooledObjectFactory 有两个实现类:\n\nBasePooledObjectFactory&lt;T&gt;\nSynchronizedPooledObjectFactory&lt;T&gt;\n\nBasePooledObjectFactory&lt;T&gt; 是有个抽象类，提供了两个抽象方法:\n\npublic abstract T create() throws Exception;: 用来创建泛型实例\npublic abstract PooledObject&lt;T&gt; wrap(T obj);: 用来将泛型实例包装为池化对象\n\nBasePooledObjectFactory&lt;T&gt; 的继承方法基本是空实现或默认实现，除了 makeObject 方法。\n1234@Overridepublic PooledObject&lt;T&gt; makeObject() throws Exception &#123;  return wrap(create());&#125;\n\n下面来看看 SynchronizedPooledObjectFactory&lt;T&gt;, 这是一个基于装饰器模式设计的同步对象工厂，可以看到内部存在两个属性，一个是可重入读写锁的写锁，一个是对象工厂的实例。所有继承方法都进行了锁的功能扩展，例如:\n123456789@Overridepublic PooledObject&lt;T&gt; makeObject() throws Exception &#123;  writeLock.lock();   try &#123;    return factory.makeObject();  &#125; finally &#123;    writeLock.unlock();  &#125;&#125;\n\nKeyedPooledObjectFactoryKeyedPooledObjectFactory 也是一个池化对象工厂接口，和 PooledObjectFactory 相比，特点是可以通过 key 来查找池化对象。\n12345678910111213141516public interface KeyedPooledObjectFactory&lt;K, V&gt; &#123;    PooledObject&lt;V&gt; makeObject(K key) throws Exception;    void destroyObject(K key, PooledObject&lt;V&gt; p) throws Exception;    boolean validateObject(K key, PooledObject&lt;V&gt; p);    void activateObject(K key, PooledObject&lt;V&gt; p) throws Exception;    void passivateObject(K key, PooledObject&lt;V&gt; p) throws Exception;&#125;\n\n\n\nKeyedPooledObjectFactory\n从类图中可以看到 KeyedPooledObjectFactory 也有两个实现类:\n\nBaseKeyedPooledObjectFactory&lt;K, V&gt;\nSynchronizedKeyedPooledObjectFactory&lt;K, V&gt;\n\n这两个实现类的特点和 PooledObjectFactory 的两个实现类类似，此处就不多说了。\n生命周期当通过对象工厂创建对象时:\n\n每当需要新实例时，都会调用 makeObject()。\n在从池中借用之前已钝化的实例时都会调用 activateObject()。\n在激活的实例上调用 validateObject，以确保它们可以从池中借用；在归还池的实例钝化之前调用 validateObject.validateObject 只会在已被激活的实例上使用。\n当每个实例返回池时，会调用 passivateObject。\n当从池中 “删除” 时，会在每个实例上调用 destroyObject\n\n\n\n\n\n\n\n\n\n\nPooledObjectFactory 必须是线程安全的。ObjectPool 唯一的承诺是，对象的同一个实例不会一次传递给 PoolableObjectFactory 的多个方法。\n池化对象池化对象是对象池中对象的包装类，用于记录对象池需要的额外信息，例如状态，时间等。\nPooledObject12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970public interface PooledObject&lt;T&gt; extends Comparable&lt;PooledObject&lt;T&gt;&gt; &#123;        T getObject();        long getCreateTime();        long getActiveTimeMillis();        long getIdleTimeMillis();        long getLastBorrowTime();        long getLastReturnTime();        long getLastUsedTime();        @Override    int compareTo(PooledObject&lt;T&gt; other);    @Override    boolean equals(Object obj);    @Override    int hashCode();    @Override    String toString();        boolean startEvictionTest();        boolean endEvictionTest(Deque&lt;PooledObject&lt;T&gt;&gt; idleQueue);        boolean allocate();        boolean deallocate();        void invalidate();        void setLogAbandoned(boolean logAbandoned);        void use();        void printStackTrace(PrintWriter writer);        PooledObjectState getState();        void markAbandoned();        void markReturning();&#125;\n\n\n\nDefaultPooledObject从上面的类图中我们可以看到 DefaultPooledObject 实现了 PooledObject 接口。\n1234567891011121314public class DefaultPooledObject&lt;T&gt; implements PooledObject&lt;T&gt; &#123;private final T object;@GuardedBy(&quot;this&quot;)private PooledObjectState state = PooledObjectState.IDLE;private final long createTime = System.currentTimeMillis();private volatile long lastBorrowTime = createTime;private volatile long lastUseTime = createTime;private volatile long lastReturnTime = createTime;private volatile boolean logAbandoned = false;private volatile CallStack borrowedBy = NoOpCallStack.INSTANCE;private volatile CallStack usedBy = NoOpCallStack.INSTANCE;private volatile long borrowedCount = 0;...&#125;\n\nDefaultPooledObject 被设计为线程安全的类。可以看到 属性上都有 volatile 修饰，涉及到修改属性的方法，在方法上都添加了对象级的 synchronize 锁。注意到有 3 个属性没有 volatile 修饰，其中 createTime 和 object 对于实例来说是不变的，一旦赋值，不会修改，所以不需要同步处理，state 则是在每处读写的地方都加了对象锁。PooledSoftReference 是对 DefaultPooledObject 的扩展，用于包装软引用。\n\n\n\n\n\n\n\n\n\nCallStack 用于记录和打印调用堆栈。\nPooledObject 的状态流转PooledObject 有下面几个状态，(在 PooledObjectState 中定义):\n\nIDLE: 空闲状态，在空闲队列中。\nALLOCATED: 使用中\nEVICTION: 正在进行驱逐检查，在空闲队列中。\nEVICTION_RETURN_TO_HEAD: 正在进行驱逐检查时，试图分配该对象，从空闲队列中去除，由于正在检查，分配失败，当通过空闲检查后，重新放回到空闲队列头部。\nVALIDATION: 验证状态，在空闲队列中。\nVALIDATION_PREALLOCATED: 验证时试图分配，验证完后分配。\nVALIDATION_RETURN_TO_HEAD: 驱逐检查后正在验证，验证完后，放入空闲队列头部。\nINVALID: 无效，将会 &#x2F; 已被销毁。\nABANDONED: 抛弃状态，即将无效。\nRETURNING: 返回到池中。\n\n\n\nPooledObjectState\n对象池ObjectPool123456789101112131415161718192021222324252627282930313233343536373839404142434445public interface ObjectPool&lt;T&gt; extends Closeable &#123;        T borrowObject() throws Exception, NoSuchElementException, IllegalStateException;        void returnObject(T obj) throws Exception;        void invalidateObject(T obj) throws Exception;        void addObject() throws Exception, IllegalStateException ,UnsupportedOperationException;        int getNumIdle();        int getNumActive();        void clear() throws Exception, UnsupportedOperationException;        @Override    void close();&#125;\n\nObjectPool 是一个简单的对象池接口。下面是一个简单的使用示例:\n12345678910111213141516171819202122Object obj = null;try &#123;        pool.addObject();        obj = pool.borrowObject();    try &#123;            &#125; catch(Exception e) &#123;                pool.invalidateObject(obj);                obj = null;    &#125; finally &#123;                if(null != obj) &#123;            pool.returnObject(obj);       &#125;    &#125;&#125; catch(Exception e) &#123;      &#125;\n\nObjectPool 有几个实现类。下面来看看它们:\nProxiedObjectPoolProxiedObjectPool 实现了代理模式。\n1234567891011121314151617181920212223242526272829303132333435363738394041public class ProxiedObjectPool&lt;T&gt; implements ObjectPool&lt;T&gt; &#123;    private final ObjectPool&lt;T&gt; pool;    private final ProxySource&lt;T&gt; proxySource;        public ProxiedObjectPool(final ObjectPool&lt;T&gt; pool, final ProxySource&lt;T&gt; proxySource) &#123;        this.pool = pool;        this.proxySource = proxySource;    &#125;    @SuppressWarnings(&quot;unchecked&quot;)    @Override    public T borrowObject() throws Exception, NoSuchElementException, IllegalStateException &#123;        UsageTracking&lt;T&gt; usageTracking = null;        if (pool instanceof UsageTracking) &#123;            usageTracking = (UsageTracking&lt;T&gt;) pool;        &#125;        final T pooledObject = pool.borrowObject();        final T proxy = proxySource.createProxy(pooledObject, usageTracking);        return proxy;    &#125;    @Override    public void returnObject(final T proxy) throws Exception &#123;        final T pooledObject = proxySource.resolveProxy(proxy);        pool.returnObject(pooledObject);    &#125;    @Override    public void invalidateObject(final T proxy) throws Exception &#123;        final T pooledObject = proxySource.resolveProxy(proxy);        pool.invalidateObject(pooledObject);    &#125;    &#125;\n\n从源码可以看到，ProxiedObjectPool 通过 ProxySource 实现对内部的 ObjectPool 的动态代理，可以给对象添加一些额外的信息，实现更好的控制。\n包内还提供了 ProxySource 的 JDK 和 cglib 的实现，详细代码可以看 JdkProxySource 和 CglibProxySource。\nErodingObjectPoolErodingObjectPool 提供了一个基于时间和空闲对象数判断的动态收缩的对象池 (漏池)。\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263private final ObjectPool&lt;T&gt; pool;private final ErodingFactor factor;public ErodingObjectPool(final ObjectPool&lt;T&gt; pool, final float factor) &#123;    this.pool = pool;    this.factor = new ErodingFactor(factor);&#125;@Overridepublic void returnObject(final T obj) &#123;    boolean discard = false;    final long now = System.currentTimeMillis();    synchronized (pool) &#123;        if (factor.getNextShrink() &lt; now) &#123;                                                         final int numIdle = pool.getNumIdle();            if (numIdle &gt; 0) &#123;                discard = true;            &#125;            factor.update(now, numIdle);        &#125;    &#125;    try &#123;                if (discard) &#123;            pool.invalidateObject(obj);        &#125; else &#123;            pool.returnObject(obj);        &#125;    &#125; catch (final Exception e) &#123;            &#125;&#125;private static final class ErodingFactor &#123;private final float factor;private transient volatile long nextShrink;private transient volatile int idleHighWaterMark;public ErodingFactor(final float factor) &#123;    this.factor = factor;    nextShrink = System.currentTimeMillis() + (long) (900000 * factor);         idleHighWaterMark = 1; &#125; public void update(final long now, final int numIdle) &#123;     final int idle = Math.max(0, numIdle);     idleHighWaterMark = Math.max(idle, idleHighWaterMark);     final float maxInterval = 15f;     final float minutes = maxInterval +             ((1f - maxInterval) / idleHighWaterMark) * idle;     nextShrink = now + (long) (minutes * 60000f * factor); &#125; public long getNextShrink() &#123;     return nextShrink; &#125;&#125;\n\n根据上面的源码，可以看到在向池中返回对象时，会进行判断，此时可能返回对象或把对象设为无效。而判断条件是传入的 factor 和 空闲对象数 numIdle。nextShrink 的动态变化 (可以在 ErodingFactor 的 update 方法中看到)，从而实现对象池的动态收缩。\nSynchronizedObjectPoolSynchronizedObjectPool 是基于装饰器模式设计的。通过内置的 ReentrantReadWriteLock 装饰了 pool 的各个方法，实现了具有读写锁的线程安全的对象池。\n12345private static final class SynchronizedObjectPool&lt;T&gt; implements ObjectPool&lt;T&gt; &#123;    private final ReentrantReadWriteLock readWriteLock = new ReentrantReadWriteLock();    private final ObjectPool&lt;T&gt; pool;     &#125;\n\nSoftReferenceObjectPoolSoftReferenceObjectPool 管理的对象的软引用，，SoftReferenceObjectPool 是线程安全的。SoftReferenceObjectPool 继承自 BaseObjectPool，BaseObjectPool 是一个抽象类，方法大多是空实现或是空实现。下面我们来看看 SoftReferenceObjectPool 的内容。\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869public class SoftReferenceObjectPool&lt;T&gt; extends BaseObjectPool&lt;T&gt; &#123;        private final PooledObjectFactory&lt;T&gt; factory;        private final ReferenceQueue&lt;T&gt; refQueue = new ReferenceQueue&lt;&gt;();        private int numActive = 0;        private long destroyCount = 0;        private long createCount = 0;        private final LinkedBlockingDeque&lt;PooledSoftReference&lt;T&gt;&gt; idleReferences =        new LinkedBlockingDeque&lt;&gt;();        private final ArrayList&lt;PooledSoftReference&lt;T&gt;&gt; allReferences =        new ArrayList&lt;&gt;();    @Override  public synchronized void addObject() throws Exception &#123;    assertOpen();    if (factory == null) &#123;        throw new IllegalStateException(&quot;Cannot add objects without a factory.&quot;);    &#125;    final T obj = factory.makeObject().getObject();    createCount++;        final PooledSoftReference&lt;T&gt; ref = new PooledSoftReference&lt;&gt;(            new SoftReference&lt;&gt;(obj, refQueue));    allReferences.add(ref);    boolean success = true;    if (!factory.validateObject(ref)) &#123;        success = false;    &#125; else &#123;        factory.passivateObject(ref);    &#125;    final boolean shouldDestroy = !success;    if (success) &#123;        idleReferences.add(ref);        notifyAll();             &#125;    if (shouldDestroy) &#123;        try &#123;            destroy(ref);        &#125; catch (final Exception e) &#123;                    &#125;    &#125;  &#125;  private void destroy(final PooledSoftReference&lt;T&gt; toDestroy) throws Exception &#123;    toDestroy.invalidate();    idleReferences.remove(toDestroy);    allReferences.remove(toDestroy);    try &#123;        factory.destroyObject(toDestroy);    &#125; finally &#123;        destroyCount++;                        toDestroy.getReference().clear();    &#125;  &#125;  &#125;\n\n其他方法都较简单，这里就不赘述了。\nGenericObjectPool 详解GenericObjectPool 是最常用的对象池，继承自抽象类 BaseGenericObjectPool，提供了一些可配置的对象池功能，GenericObjectPool 被设计为线程安全。\nconfig\n\n\n配置\n默认值\n描述\n\n\n\nmaxTotal\n8\n池中最多可用的实例个数\n\n\nmaxIdle\n8\n池中最大空闲的个数\n\n\nminIdle\n0\n池中最少空闲的个数\n\n\nlifo\ntrue\n是否 LIFO，后进先出\n\n\nfairness\nfalse\n等待线程拿空闲连接的方式，为 true 是先进先出的方式获取空闲对象\n\n\nmaxWaitMillis\n-1\n当连接池资源耗尽时，调用者最大阻塞的时间，超时将跑出异常，单位：毫秒数。默认为 - 1 时表示永不超时.\n\n\nevictorShutdownTimeoutMillis\n10L * 1000L\n驱逐者线程 shutdown 的等待时间\n\n\nminEvictableIdleTimeMillis\n1000L * 60L * 30L\n对象空闲的最小时间，达到此值后空闲对象将可能会被移除，单位毫秒。负值表示不移除\n\n\nsoftMinEvictableIdleTimeMillis\n1000L * 60L * 30L\n对象空闲的最小时间，达到此值后空闲对象将可能会被移除，但是会保留 minIdle 个空闲对象，单位毫秒。若 minEvictableIdleTimeMillis 为正数，那么该配置会被覆盖\n\n\nnumTestsPerEvictionRun\n3\n对于驱逐者线程而言，每次检测的链接资源的个数。如果 numTestsPerEvictionRun&gt;&#x3D;0, 每次检查 numTestsPerEvictionRun 和空闲对象数的较小值，否则，返回 Math.ceil(idleObjects.size() /Math.abs((double) numTestsPerEvictionRun))\n\n\nevictionPolicy\nnull\n驱逐者线程驱逐策略，2.6 版本提供\n\n\nevictionPolicyClassName\nDefaultEvictionPolicy.class.getName()\n驱逐者线程驱逐策略，2.6 之前版本提供\n\n\ntestOnCreate\nfalse\n创建对象时，是否使用 validateObject 验证\n\n\ntestOnBorrow\nfalse\n借出对象时，是否使用 validateObject 验证\n\n\ntestOnReturn\nfalse\n归还对象时，是否使用 validateObject 验证\n\n\ntestWhileIdle\nfalse\n空闲对象在驱逐者线程检查后，是否使用 validateObject 验证\n\n\ntimeBetweenEvictionRunsMillis\n-1L\n驱逐者检测线程检测的周期，毫秒数。如果为负值，表示不运行.\n\n\nblockWhenExhausted\ntrue\n当池中 active 数量达到阀值时，是否阻塞 borrowObject。\n\n\n创建对象池对象池的构造方法接受参数，对象工厂和对象池配置。在构造器中会初始化空闲对象容器 idleObjects 和所有对象容器 allObjects。此外，还可以设置 AbandonedConfig，用于_丢弃借出，但是长时间未使用的对象_。\n123private final Map&lt;IdentityWrapper&lt;T&gt;, PooledObject&lt;T&gt;&gt; allObjects =        new ConcurrentHashMap&lt;&gt;();idleObjects = new LinkedBlockingDeque&lt;&gt;(config.getFairness());\n\n对象租借123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104public T borrowObject(final long borrowMaxWaitMillis) throws Exception &#123;        assertOpen();        final AbandonedConfig ac = this.abandonedConfig;    if (ac != null &amp;&amp; ac.getRemoveAbandonedOnBorrow() &amp;&amp;        (getNumIdle() &lt; 2) &amp;&amp;(getNumActive() &gt; getMaxTotal() - 3) ) &#123;        removeAbandoned(ac);    &#125;    PooledObject&lt;T&gt; p = null;            final boolean blockWhenExhausted = getBlockWhenExhausted();    boolean create;    final long waitTime = System.currentTimeMillis();    while (p == null) &#123;        create = false;                p = idleObjects.pollFirst();        if (p == null) &#123;                        p = create();            if (p != null) &#123;                create = true;            &#125;        &#125;                if (blockWhenExhausted) &#123;            if (p == null) &#123;                                if (borrowMaxWaitMillis &lt; 0) &#123;                                        p = idleObjects.takeFirst();                &#125; else &#123;                                        p = idleObjects.pollFirst(borrowMaxWaitMillis,TimeUnit.MILLISECONDS);                &#125;            &#125;            if (p == null) &#123;                throw new NoSuchElementException(&quot;Timeout waiting for idle object&quot;);            &#125;        &#125; else &#123;            if (p == null) &#123;                throw new NoSuchElementException(&quot;Pool exhausted&quot;);            &#125;        &#125;        if (!p.allocate()) &#123;                        p = null;        &#125;        if (p != null) &#123;            try &#123;                                factory.activateObject(p);            &#125; catch (final Exception e) &#123;                try &#123;                    destroy(p);                &#125; catch (final Exception e1) &#123;                                    &#125;                p = null;                if (create) &#123;                    final NoSuchElementException nsee = new NoSuchElementException(&quot;Unable to activate object&quot;);                    nsee.initCause(e);                    throw nsee;                &#125;            &#125;            if (p != null &amp;&amp; (getTestOnBorrow() || create &amp;&amp; getTestOnCreate())) &#123;                boolean validate = false;                Throwable validationThrowable = null;                try &#123;                                        validate = factory.validateObject(p);                &#125; catch (final Throwable t) &#123;                    PoolUtils.checkRethrow(t);                    validationThrowable = t;                &#125;                if (!validate) &#123;                    try &#123;                        destroy(p);                        destroyedByBorrowValidationCount.incrementAndGet();                    &#125; catch (final Exception e) &#123;                                            &#125;                    p = null;                    if (create) &#123;                        final NoSuchElementException nsee = new NoSuchElementException(&quot;Unable to validate object&quot;);                        nsee.initCause(validationThrowable);                        throw nsee;                    &#125;                &#125;            &#125;        &#125;    &#125;        updateStatsBorrow(p, System.currentTimeMillis() - waitTime);    return p.getObject();&#125;\n\n对象归还123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384public void returnObject(final T obj) &#123;        final PooledObject&lt;T&gt; p = allObjects.get(new IdentityWrapper&lt;&gt;(obj));    if (p == null) &#123;        if (!isAbandonedConfig()) &#123;            throw new IllegalStateException(&quot;Returned object not currently part of this pool&quot;);        &#125;        return;     &#125;        markReturningState(p);    final long activeTime = p.getActiveTimeMillis();    if (getTestOnReturn() &amp;&amp; !factory.validateObject(p)) &#123;                try &#123;            destroy(p);        &#125; catch (final Exception e) &#123;            swallowException(e);        &#125;        try &#123;                        ensureIdle(1, false);        &#125; catch (final Exception e) &#123;            swallowException(e);        &#125;                updateStatsReturn(activeTime);        return;    &#125;    try &#123;                factory.passivateObject(p);    &#125; catch (final Exception e1) &#123;        swallowException(e1);        try &#123;            destroy(p);        &#125; catch (final Exception e) &#123;            swallowException(e);        &#125;        try &#123;                        ensureIdle(1, false);        &#125; catch (final Exception e) &#123;            swallowException(e);        &#125;                 updateStatsReturn(activeTime);        return;    &#125;        if (!p.deallocate()) &#123;        throw new IllegalStateException(                &quot;Object has already been returned to this pool or is invalid&quot;);    &#125;    final int maxIdleSave = getMaxIdle();    if (isClosed() || maxIdleSave &gt; -1 &amp;&amp; maxIdleSave &lt;= idleObjects.size()) &#123;                try &#123;            destroy(p);        &#125; catch (final Exception e) &#123;            swallowException(e);        &#125;    &#125; else &#123;                if (getLifo()) &#123;            idleObjects.addFirst(p);        &#125; else &#123;            idleObjects.addLast(p);        &#125;        if (isClosed()) &#123;                                                clear();        &#125;    &#125;        updateStatsReturn(activeTime);&#125;\n\nevictor 检查启动 evictor 入口启动 “空闲对象的驱逐者线程” 的入口是：BaseGenericObjectPool.setTimeBetweenEvictionRunsMillis 方法\n1234 public final void setTimeBetweenEvictionRunsMillis(final long timeBetweenEvictionRunsMillis) &#123;    this.timeBetweenEvictionRunsMillis = timeBetweenEvictionRunsMillis;    startEvictor(timeBetweenEvictionRunsMillis);&#125;\n\n启动1234567891011121314151617181920212223242526final void startEvictor(final long delay) &#123;    synchronized (evictionLock) &#123;        if (null != evictor) &#123;             EvictionTimer.cancel(evictor, evictorShutdownTimeoutMillis, TimeUnit.MILLISECONDS);            evictor = null;            evictionIterator = null;        &#125;        if (delay &gt; 0) &#123;            evictor = new Evictor();            EvictionTimer.schedule(evictor, delay, delay);        &#125;    &#125;&#125;static synchronized void schedule(            final BaseGenericObjectPool&lt;?&gt;.Evictor task, final long delay, final long period) &#123;    if (null == executor) &#123;        executor = new ScheduledThreadPoolExecutor(1, new EvictorThreadFactory());        executor.setRemoveOnCancelPolicy(true);    &#125;    final ScheduledFuture&lt;?&gt; scheduledFuture =            executor.scheduleWithFixedDelay(task, delay, period, TimeUnit.MILLISECONDS);    task.setScheduledFuture(scheduledFuture);&#125;\n\n驱逐者实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051class Evictor implements Runnable &#123;          @Override    public void run() &#123;        final ClassLoader savedClassLoader =                Thread.currentThread().getContextClassLoader();        try &#123;            if (factoryClassLoader != null) &#123;                                                final ClassLoader cl = factoryClassLoader.get();                if (cl == null) &#123;                                                                                cancel();                    return;                &#125;                Thread.currentThread().setContextClassLoader(cl);                                            &#125;                        try &#123;                evict();            &#125; catch(final Exception e) &#123;                swallowException(e);            &#125; catch(final OutOfMemoryError oome) &#123;                                                oome.printStackTrace(System.err);            &#125;                        try &#123;                ensureMinIdle();            &#125; catch (final Exception e) &#123;                swallowException(e);            &#125;        &#125; finally &#123;                        Thread.currentThread().setContextClassLoader(savedClassLoader);        &#125;    &#125;&#125;\n\nevictor 方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110public void evict() throws Exception &#123;        assertOpen();    if (idleObjects.size() &gt; 0) &#123;        PooledObject&lt;T&gt; underTest = null;                final EvictionPolicy&lt;T&gt; evictionPolicy = getEvictionPolicy();        synchronized (evictionLock) &#123;                        final EvictionConfig evictionConfig = new EvictionConfig(                    getMinEvictableIdleTimeMillis(),                    getSoftMinEvictableIdleTimeMillis(),                    getMinIdle());            final boolean testWhileIdle = getTestWhileIdle();                        for (int i = 0, m = getNumTests(); i &lt; m; i++) &#123;                                if (evictionIterator == null || !evictionIterator.hasNext()) &#123;                    evictionIterator = new EvictionIterator(idleObjects);                &#125;                if (!evictionIterator.hasNext()) &#123;                                        return;                &#125;                try &#123;                    underTest = evictionIterator.next();                &#125; catch (final NoSuchElementException nsee) &#123;                                                            i--;                    evictionIterator = null;                    continue;                &#125;                                if (!underTest.startEvictionTest()) &#123;                                                            i--;                    continue;                &#125;                                                                boolean evict;                try &#123;                                        evict = evictionPolicy.evict(evictionConfig, underTest,                            idleObjects.size());                &#125; catch (final Throwable t) &#123;                                                            PoolUtils.checkRethrow(t);                    swallowException(new Exception(t));                                        evict = false;                &#125;                if (evict) &#123;                                         destroy(underTest);                    destroyedByEvictorCount.incrementAndGet();                &#125; else &#123;                                        if (testWhileIdle) &#123;                        boolean active = false;                        try &#123;                                                        factory.activateObject(underTest);                            active = true;                        &#125; catch (final Exception e) &#123;                            destroy(underTest);                            destroyedByEvictorCount.incrementAndGet();                        &#125;                        if (active) &#123;                            if (!factory.validateObject(underTest)) &#123;                                                                destroy(underTest);                                destroyedByEvictorCount.incrementAndGet();                            &#125; else &#123;                                try &#123;                                                                        factory.passivateObject(underTest);                                &#125; catch (final Exception e) &#123;                                    destroy(underTest);                                    destroyedByEvictorCount.incrementAndGet();                                &#125;                            &#125;                        &#125;                    &#125;                                         if (!underTest.endEvictionTest(idleObjects)) &#123;                                                                    &#125;                &#125;            &#125;        &#125;    &#125;    final AbandonedConfig ac = this.abandonedConfig;    if (ac != null &amp;&amp; ac.getRemoveAbandonedOnMaintenance()) &#123;                removeAbandoned(ac);    &#125;&#125;\n\n驱逐策略在 evict () 方法中最后对象是否要被驱逐是调用了 evictionPolicy.evict () 的方法来判断的，commons-pool 提供的驱逐策略如下：\n123456789101112131415public class DefaultEvictionPolicy&lt;T&gt; implements EvictionPolicy&lt;T&gt; &#123;    @Override    public boolean evict(EvictionConfig config, PooledObject&lt;T&gt; underTest,            int idleCount) &#123;                                if ((config.getIdleSoftEvictTime() &lt; underTest.getIdleTimeMillis() &amp;&amp;                config.getMinIdle() &lt; idleCount) ||                config.getIdleEvictTime() &lt; underTest.getIdleTimeMillis()) &#123;            return true;        &#125;        return false;    &#125;&#125;\n\n驱逐策略是支持自定义的，这里使用的是设计模式中的策略模式，我们只要实现 EvictionPolicy 接口，然后调用 setEvictionPolicy () 方法既可以更换驱逐策略.\n","slug":"JAVA/从 apache-common-pool 看如何写一个通用池","date":"2023-11-16T11:44:17.000Z","categories_index":"PooledObjectFactory,JAVA","tags_index":"https,www,victorchu","author_index":"dandeliono"},{"id":"b17fc263eaa274b6bfcba03d60c157eb","title":"SpringBoot 扩展接口","content":"SpringBoot 扩展接口1、背景Spring的核心思想就是容器，当容器refresh的时候，外部看上去风平浪静，其实内部则是一片惊涛骇浪，汪洋一片。Springboot更是封装了Spring，遵循约定大于配置，加上自动装配的机制。很多时候我们只要引用了一个依赖，几乎是零配置就能完成一个功能的装配。\n我非常喜欢这种自动装配的机制，所以在自己开发中间件和公共依赖工具的时候也会用到这个特性。让使用者以最小的代价接入。想要把自动装配玩的转，就必须要了解spring对于bean的构造生命周期以及各个扩展接口。当然了解了bean的各个生命周期也能促进我们加深对spring的理解。业务代码也能合理利用这些扩展点写出更加漂亮的代码。\n在这篇文章里，我总结了几乎Spring &amp; Springboot所有的扩展接口，以及各个扩展点的使用场景。并且整理出了一个bean在spring内部从被加载到最后初始化完成所有可扩展点的顺序调用图。从而我们也能窥探到bean是如何一步步加载到spring容器中的。\n2、可扩展的接口启动调用顺序图以下是spring容器中Bean的生命周期内所有可扩展的点的调用顺序，下面会一个个分析\n\n3、ApplicationContextInitializer\n\n\n\n\n\n\n\n\norg.springframework.context.ApplicationContextInitializer\n这是整个spring容器在刷新之前初始化ConfigurableApplicationContext的回调接口，简单来说，就是在容器刷新之前调用此类的initialize方法。这个点允许被用户自己扩展。用户可以在整个spring容器还没被初始化之前做一些事情。\n可以想到的场景可能为，在最开始激活一些配置，或者利用这时候class还没被类加载器加载的时机，进行动态字节码注入等操作。\n扩展方式为：\n1234567public class TestApplicationContextInitializer implements ApplicationContextInitializer &#123;          @Override          public void initialize(ConfigurableApplicationContext applicationContext) &#123;              System.out.println(&quot;[ApplicationContextInitializer]&quot;);          &#125;      &#125;      \n\n因为这时候spring容器还没被初始化，所以想要自己的扩展的生效，有以下三种方式：\n\n在启动类中用springApplication.addInitializers(new TestApplicationContextInitializer())语句加入\n配置文件配置context.initializer.classes=com.example.demo.TestApplicationContextInitializer\nSpring SPI扩展，在spring.factories中加入org.springframework.context.ApplicationContextInitializer=com.example.demo.TestApplicationContextInitializer\n\n4、BeanDefinitionRegistryPostProcessor\n\n\n\n\n\n\n\n\norg.springframework.beans.factory.support.BeanDefinitionRegistryPostProcessor\n这个接口在读取项目中的beanDefinition之后执行，提供一个补充的扩展点\n使用场景：你可以在这里动态注册自己的beanDefinition，可以加载classpath之外的bean\n扩展方式为:\n123456789101112public class TestBeanDefinitionRegistryPostProcessor implements BeanDefinitionRegistryPostProcessor &#123;          @Override          public void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) throws BeansException &#123;              System.out.println(&quot;[BeanDefinitionRegistryPostProcessor] postProcessBeanDefinitionRegistry&quot;);          &#125;                @Override          public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException &#123;              System.out.println(&quot;[BeanDefinitionRegistryPostProcessor] postProcessBeanFactory&quot;);          &#125;      &#125;      \n\n5、BeanFactoryPostProcessor\n\n\n\n\n\n\n\n\norg.springframework.beans.factory.config.BeanFactoryPostProcessor\n这个接口是beanFactory的扩展接口，调用时机在spring在读取beanDefinition信息之后，实例化bean之前。\n在这个时机，用户可以通过实现这个扩展接口来自行处理一些东西，比如修改已经注册的beanDefinition的元信息。\n扩展方式为：\n1234567public class TestBeanFactoryPostProcessor implements BeanFactoryPostProcessor &#123;          @Override          public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException &#123;              System.out.println(&quot;[BeanFactoryPostProcessor]&quot;);          &#125;      &#125;      \n\n6、InstantiationAwareBeanPostProcessor\n\n\n\n\n\n\n\n\norg.springframework.beans.factory.config.InstantiationAwareBeanPostProcessor\n该接口继承了BeanPostProcess接口，区别如下：\nBeanPostProcess接口只在bean的初始化阶段进行扩展（注入spring上下文前后），而InstantiationAwareBeanPostProcessor接口在此基础上增加了3个方法，把可扩展的范围增加了实例化阶段和属性注入阶段。 \n该类主要的扩展点有以下5个方法，主要在bean生命周期的两大阶段：实例化阶段和初始化阶段，下面一起进行说明，按调用顺序为：\n\npostProcessBeforeInstantiation：实例化bean之前，相当于new这个bean之前\npostProcessAfterInstantiation：实例化bean之后，相当于new这个bean之后\npostProcessPropertyValues：bean已经实例化完成，在属性注入时阶段触发，@Autowired,@Resource等注解原理基于此方法实现\npostProcessBeforeInitialization：初始化bean之前，相当于把bean注入spring上下文之前\npostProcessAfterInitialization：初始化bean之后，相当于把bean注入spring上下文之后\n\n使用场景：这个扩展点非常有用 ，无论是写中间件和业务中，都能利用这个特性。比如对实现了某一类接口的bean在各个生命期间进行收集，或者对某个类型的bean进行统一的设值等等。\n扩展方式为：\n1234567891011121314151617181920212223242526272829303132public class TestInstantiationAwareBeanPostProcessor implements InstantiationAwareBeanPostProcessor &#123;                @Override          public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123;              System.out.println(&quot;[TestInstantiationAwareBeanPostProcessor] before initialization &quot; + beanName);              return bean;          &#125;                @Override          public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123;              System.out.println(&quot;[TestInstantiationAwareBeanPostProcessor] after initialization &quot; + beanName);              return bean;          &#125;                @Override          public Object postProcessBeforeInstantiation(Class&lt;?&gt; beanClass, String beanName) throws BeansException &#123;              System.out.println(&quot;[TestInstantiationAwareBeanPostProcessor] before instantiation &quot; + beanName);              return null;          &#125;                @Override          public boolean postProcessAfterInstantiation(Object bean, String beanName) throws BeansException &#123;              System.out.println(&quot;[TestInstantiationAwareBeanPostProcessor] after instantiation &quot; + beanName);              return true;          &#125;                @Override          public PropertyValues postProcessPropertyValues(PropertyValues pvs, PropertyDescriptor[] pds, Object bean, String beanName) throws BeansException &#123;              System.out.println(&quot;[TestInstantiationAwareBeanPostProcessor] postProcessPropertyValues &quot; + beanName);              return pvs;          &#125;      \n\n7、SmartInstantiationAwareBeanPostProcessor\n\n\n\n\n\n\n\n\norg.springframework.beans.factory.config.SmartInstantiationAwareBeanPostProcessor\n该扩展接口有3个触发点方法：\n\npredictBeanType：该触发点发生在postProcessBeforeInstantiation之前(在图上并没有标明，因为一般不太需要扩展这个点)，这个方法用于预测Bean的类型，返回第一个预测成功的Class类型，如果不能预测返回null；当你调用BeanFactory.getType(name)时当通过bean的名字无法得到bean类型信息时就调用该回调方法来决定类型信息。\ndetermineCandidateConstructors：该触发点发生在postProcessBeforeInstantiation之后，用于确定该bean的构造函数之用，返回的是该bean的所有构造函数列表。用户可以扩展这个点，来自定义选择相应的构造器来实例化这个bean。\ngetEarlyBeanReference：该触发点发生在postProcessAfterInstantiation之后，当有循环依赖的场景，当bean实例化好之后，为了防止有循环依赖，会提前暴露回调方法，用于bean实例化的后置处理。这个方法就是在提前暴露的回调方法中触发。\n\n扩展方式为：\n123456789101112131415161718192021public class TestSmartInstantiationAwareBeanPostProcessor implements SmartInstantiationAwareBeanPostProcessor &#123;                @Override          public Class&lt;?&gt; predictBeanType(Class&lt;?&gt; beanClass, String beanName) throws BeansException &#123;              System.out.println(&quot;[TestSmartInstantiationAwareBeanPostProcessor] predictBeanType &quot; + beanName);              return beanClass;          &#125;                @Override          public Constructor&lt;?&gt;[] determineCandidateConstructors(Class&lt;?&gt; beanClass, String beanName) throws BeansException &#123;              System.out.println(&quot;[TestSmartInstantiationAwareBeanPostProcessor] determineCandidateConstructors &quot; + beanName);              return null;          &#125;                @Override          public Object getEarlyBeanReference(Object bean, String beanName) throws BeansException &#123;              System.out.println(&quot;[TestSmartInstantiationAwareBeanPostProcessor] getEarlyBeanReference &quot; + beanName);              return bean;          &#125;      &#125;      \n\n8、BeanFactoryAware\n\n\n\n\n\n\n\n\norg.springframework.beans.factory.BeanFactoryAware\n这个类只有一个触发点，发生在bean的实例化之后，注入属性之前，也就是Setter之前。这个类的扩展点方法为setBeanFactory，可以拿到BeanFactory这个属性。\n使用场景为，你可以在bean实例化之后，但还未初始化之前，拿到 BeanFactory，在这个时候，可以对每个bean作特殊化的定制。也或者可以把BeanFactory拿到进行缓存，日后使用。\n扩展方式为：\n1234567public class TestBeanFactoryAware implements BeanFactoryAware &#123;          @Override          public void setBeanFactory(BeanFactory beanFactory) throws BeansException &#123;              System.out.println(&quot;[TestBeanFactoryAware] &quot; + beanFactory.getBean(TestBeanFactoryAware.class).getClass().getSimpleName());          &#125;      &#125;      \n\n9、ApplicationContextAwareProcessor\n\n\n\n\n\n\n\n\norg.springframework.context.support.ApplicationContextAwareProcessor\n该类本身并没有扩展点，但是该类内部却有6个扩展点可供实现 ，这些类触发的时机在bean实例化之后，初始化之前\n\n可以看到，该类用于执行各种驱动接口，在bean实例化之后，属性填充之后，通过执行以上红框标出的扩展接口，来获取对应容器的变量。所以这里应该来说是有6个扩展点，这里就放一起来说了\n\nEnvironmentAware：用于获取EnviromentAware的一个扩展类，这个变量非常有用， 可以获得系统内的所有参数。当然个人认为这个Aware没必要去扩展，因为spring内部都可以通过注入的方式来直接获得。\nEmbeddedValueResolverAware：用于获取StringValueResolver的一个扩展类， StringValueResolver用于获取基于String类型的properties的变量，一般我们都用@Value的方式去获取，如果实现了这个Aware接口，把StringValueResolver缓存起来，通过这个类去获取String类型的变量，效果是一样的。\nResourceLoaderAware：用于获取ResourceLoader的一个扩展类，ResourceLoader可以用于获取classpath内所有的资源对象，可以扩展此类来拿到ResourceLoader对象。\nApplicationEventPublisherAware：用于获取ApplicationEventPublisher的一个扩展类，ApplicationEventPublisher可以用来发布事件，结合ApplicationListener来共同使用，下文在介绍ApplicationListener时会详细提到。这个对象也可以通过spring注入的方式来获得。\nMessageSourceAware：用于获取MessageSource的一个扩展类，MessageSource主要用来做国际化。\nApplicationContextAware：用来获取ApplicationContext的一个扩展类，ApplicationContext应该是很多人非常熟悉的一个类了，就是spring上下文管理器，可以手动的获取任何在spring上下文注册的bean，我们经常扩展这个接口来缓存spring上下文，包装成静态方法。同时ApplicationContext也实现了BeanFactory，MessageSource，ApplicationEventPublisher等接口，也可以用来做相关接口的事情。\n\n10、BeanNameAware\n\n\n\n\n\n\n\n\norg.springframework.beans.factory.BeanNameAware\n可以看到，这个类也是Aware扩展的一种，触发点在bean的初始化之前，也就是postProcessBeforeInitialization之前，这个类的触发点方法只有一个：setBeanName\n使用场景为：用户可以扩展这个点，在初始化bean之前拿到spring容器中注册的的beanName，来自行修改这个beanName的值。\n扩展方式为：\n1234567891011public class NormalBeanA implements BeanNameAware&#123;          public NormalBeanA() &#123;              System.out.println(&quot;NormalBean constructor&quot;);          &#125;                @Override          public void setBeanName(String name) &#123;              System.out.println(&quot;[BeanNameAware] &quot; + name);          &#125;      &#125;      \n\n11、@PostConstruct\n\n\n\n\n\n\n\n\njavax.annotation.PostConstruct\n这个并不算一个扩展点，其实就是一个标注。其作用是在bean的初始化阶段，如果对一个方法标注了@PostConstruct，会先调用这个方法。这里重点是要关注下这个标准的触发点，这个触发点是在postProcessBeforeInitialization之后，InitializingBean.afterPropertiesSet之前。\n使用场景：用户可以对某一方法进行标注，来进行初始化某一个属性\n扩展方式为：\n1234567891011public class NormalBeanA &#123;          public NormalBeanA() &#123;              System.out.println(&quot;NormalBean constructor&quot;);          &#125;                @PostConstruct          public void init()&#123;              System.out.println(&quot;[PostConstruct] NormalBeanA&quot;);          &#125;      &#125;      \n\n12、InitializingBean\n\n\n\n\n\n\n\n\norg.springframework.beans.factory.InitializingBean\n这个类，顾名思义，也是用来初始化bean的。InitializingBean接口为bean提供了初始化方法的方式，它只包括afterPropertiesSet方法，凡是继承该接口的类，在初始化bean的时候都会执行该方法。这个扩展点的触发时机在postProcessAfterInitialization之前。\n使用场景：用户实现此接口，来进行系统启动的时候一些业务指标的初始化工作。\n扩展方式为：\n1234567public class NormalBeanA implements InitializingBean&#123;          @Override          public void afterPropertiesSet() throws Exception &#123;              System.out.println(&quot;[InitializingBean] NormalBeanA&quot;);          &#125;      &#125;      \n\n13、FactoryBean\n\n\n\n\n\n\n\n\norg.springframework.beans.factory.FactoryBean\n一般情况下，Spring通过反射机制利用bean的class属性指定支线类去实例化bean，在某些情况下，实例化Bean过程比较复杂，如果按照传统的方式，则需要在bean中提供大量的配置信息。配置方式的灵活性是受限的，这时采用编码的方式可能会得到一个简单的方案。Spring为此提供了一个org.springframework.bean.factory.FactoryBean的工厂类接口，用户可以通过实现该接口定制实例化Bean的逻辑。\nFactoryBean接口对于Spring框架来说占用重要的地位，Spring自身就提供了70多个FactoryBean的实现。它们隐藏了实例化一些复杂bean的细节，给上层应用带来了便利。从Spring3.0开始，FactoryBean开始支持泛型，即接口声明改为FactoryBean&lt;T&gt;的形式\n使用场景：用户可以扩展这个类，来为要实例化的bean作一个代理，比如为该对象的所有的方法作一个拦截，在调用前后输出一行log，模仿ProxyFactoryBean的功能。\n扩展方式为：\n1234567891011121314151617181920212223public class TestFactoryBean implements FactoryBean&lt;TestFactoryBean.TestFactoryInnerBean&gt; &#123;                @Override          public TestFactoryBean.TestFactoryInnerBean getObject() throws Exception &#123;              System.out.println(&quot;[FactoryBean] getObject&quot;);              return new TestFactoryBean.TestFactoryInnerBean();          &#125;                @Override          public Class&lt;?&gt; getObjectType() &#123;              return TestFactoryBean.TestFactoryInnerBean.class;          &#125;                @Override          public boolean isSingleton() &#123;              return true;          &#125;                public static class TestFactoryInnerBean&#123;                &#125;      &#125;      \n\n14、SmartInitializingSingleton\n\n\n\n\n\n\n\n\norg.springframework.beans.factory.SmartInitializingSingleton\n这个接口中只有一个方法afterSingletonsInstantiated，其作用是是 在spring容器管理的所有单例对象（非懒加载对象）初始化完成之后调用的回调接口。其触发时机为postProcessAfterInitialization之后。\n使用场景：用户可以扩展此接口在对所有单例对象初始化完毕后，做一些后置的业务处理。\n扩展方式为：\n1234567public class TestSmartInitializingSingleton implements SmartInitializingSingleton &#123;          @Override          public void afterSingletonsInstantiated() &#123;              System.out.println(&quot;[TestSmartInitializingSingleton]&quot;);          &#125;      &#125;      \n\n15、CommandLineRunner\n\n\n\n\n\n\n\n\norg.springframework.boot.CommandLineRunner\n这个接口也只有一个方法：run(String... args)，触发时机为整个项目启动完毕后，自动执行。如果有多个CommandLineRunner，可以利用@Order来进行排序。\n使用场景：用户扩展此接口，进行启动项目之后一些业务的预处理。\n扩展方式为：\n12345678public class TestCommandLineRunner implements CommandLineRunner &#123;                @Override          public void run(String... args) throws Exception &#123;              System.out.println(&quot;[TestCommandLineRunner]&quot;);          &#125;      &#125;      \n\n16、DisposableBean\n\n\n\n\n\n\n\n\norg.springframework.beans.factory.DisposableBean\n这个扩展点也只有一个方法：destroy()，其触发时机为当此对象销毁时，会自动执行这个方法。比如说运行applicationContext.registerShutdownHook时，就会触发这个方法。\n扩展方式为：\n1234567public class NormalBeanA implements DisposableBean &#123;          @Override          public void destroy() throws Exception &#123;              System.out.println(&quot;[DisposableBean] NormalBeanA&quot;);          &#125;      &#125;      \n\n17、ApplicationListener\n\n\n\n\n\n\n\n\norg.springframework.context.ApplicationListener\n准确的说，这个应该不算spring&amp;springboot当中的一个扩展点，ApplicationListener可以监听某个事件的event，触发时机可以穿插在业务方法执行过程中，用户可以自定义某个业务事件。\n但是spring内部也有一些内置事件，这种事件，可以穿插在启动调用中。我们也可以利用这个特性，来自己做一些内置事件的监听器来达到和前面一些触发点大致相同的事情。\n接下来罗列下spring主要的内置事件：\n\nContextRefreshedEvent\nApplicationContext 被初始化或刷新时，该事件被发布。这也可以在ConfigurableApplicationContext接口中使用 refresh()方法来发生。此处的初始化是指：所有的Bean被成功装载，后处理Bean被检测并激活，所有Singleton Bean 被预实例化，ApplicationContext容器已就绪可用。\n\nContextStartedEvent\n当使用 ConfigurableApplicationContext （ApplicationContext子接口）接口中的 start() 方法启动 ApplicationContext时，该事件被发布。你可以调查你的数据库，或者你可以在接受到这个事件后重启任何停止的应用程序。\n\nContextStoppedEvent\n当使用 ConfigurableApplicationContext接口中的 stop()停止ApplicationContext 时，发布这个事件。你可以在接受到这个事件后做必要的清理的工作\n\nContextClosedEvent\n当使用 ConfigurableApplicationContext接口中的 close()方法关闭 ApplicationContext 时，该事件被发布。一个已关闭的上下文到达生命周期末端；它不能被刷新或重启\n\nRequestHandledEvent\n这是一个 web-specific 事件，告诉所有 bean HTTP 请求已经被服务。只能应用于使用DispatcherServlet的Web应用。在使用Spring作为前端的MVC控制器时，当Spring处理用户请求结束后，系统会自动触发该事件\n\n\n18、最后我们从这些spring&amp;springboot的扩展点当中，大致可以窥视到整个bean的生命周期。在业务开发或者写中间件业务的时候，可以合理利用spring提供给我们的扩展点，在spring启动的各个阶段内做一些事情。以达到自定义初始化的目的。\n","slug":"JAVA/SpringBoot 扩展接口","date":"2023-11-15T16:23:04.000Z","categories_index":"bean,JAVA","tags_index":"spring,org,springframework","author_index":"dandeliono"},{"id":"ad2282a09eaef1269a01ab0f55818331","title":"C2 CompilerThread9 长时间占用CPU解决方案","content":"一、描述：异常线程的堆栈如下：1234567`&quot;C2 CompilerThread9&quot; #48 daemon prio=9 os_prio=0 tid=0x00007f45f0b80000 nid=0x188 runnable [0x0000000000000000]   java.lang.Thread.State: RUNNABLE` *   1*   2\n\n补充描述：我的应用类型为后台接口服务，系统秒级调用峰值在10W+，JRE版本如下：\n123456789`java version &quot;1.8.0_231&quot;Java(TM) SE Runtime Environment (build 1.8.0_231-b11)Java HotSpot(TM) 64-Bit Server VM (build 25.231-b11, mixed mode)` *   1*   2*   3\n\n二、问题解决过程：关闭JIT分层编译步骤一：了解JIT编译原理\n因为之前对JIT的编译原理并不了解，不敢随意修改线上服务器的编译类型，担心会有一些其他的副作用，所以在网上开始了查阅资料学习的过程。\n什么是JIT编译？编译器在编译过程中通常会考虑很多因素。比如：汇编指令的顺序。假设我们要将两个寄存器的值进行相加，执行这个操作一般只需要一个CPU周期；但是在相加之前需要将数据从内存读到寄存器中，这个操作是需要多个CPU周期的。编译器一般可以做到，先启动数据加载操作，然后执行其它指令，等数据加载完成后，再执行相加操作。由于解释器在解释执行的过程中，每次只能看到一行代码，所以很难生成上述这样的高效指令序列。而编译器可以事先看到所有代码，因此，一般来说，解释性代码比编译性代码要慢。\njava 作为静态语言十分特殊，他需要编译，但并不是在执行之前就编译为本地机器码。Java的实现在解释性和编译性之间进行了折中，Java代码是编译性的，它会被编译成一个平台独立的字节码程序。JVM负责加载、解释、执行这些字节码程序，在这个过程中，还可能会将这些字节码实时编译成目标机器码，以便提升性能。\n所以，在谈到 java的编译机制的时候，其实应该按时期，分为两个部分。一个是 javac指令 将java源码变为 java字节码的静态编译过程。 另一个是 java字节码编译为本地机器码的过程，并且因为这个过程是在程序运行时期完成的所以称之为即时编译（JIT：Just In Time）。\nJIT编译类型：C1编译器、C2编译器、分层编译器\n通常我们说即时编译器有两种类型，Client Compiler（C1编译器）和Server Compiler（C2编译器）。这两种编译器最大的区别就是，编译代码的时间点不一样。C1编译器会更早的对代码进行编译，因此在程序刚启动的时候，C1编译器比C2编译器执行的更快，所以C1编译器适用于一些GUI应用，可以缩短应用启动时间。C2编译器会收集更多的信息，然后才对代码进行编译优化，所以从长远角度考虑，C2编译器最终可以产生比C1编译器更优秀的代码，适用于长时间运行的后台接口服务。\n可能大家都有一个困扰，JVM为什么要将编译器分为client和server，为什么不在程序启动时，使用client编译器，在程序运行一段时间后，自动切换为server编译器？ 其实，这种技术是存在的，一般称之为 Tiered Compiler（分层编译器）。Java7 和Java 8可以使用选项-XX:+TieredCompilation来打开（-server选项也要打开）。在Java 8中，-XX:+TieredCompilation默认是打开的。\n分层编译将 JVM 的执行状态分为了 5 个层次：\n\n第 0 层：程序解释执行，默认开启性能监控功能（Profiling），如果不开启，可触发第二层编译；\n第 1 层：可称为 C1 编译，将字节码编译为本地代码，进行简单、可靠的优化，不开启 Profiling；\n第 2 层：也称为 C1 编译，开启 Profiling，仅执行带方法调用次数和循环回边执行次数 profiling 的 C1 编译；\n第 3 层：也称为 C1 编译，执行所有带 Profiling 的 C1 编译；\n第 4 层：可称为 C2 编译，也是将字节码编译为本地代码，但是会启用一些编译耗时较长的优化，甚至会根据性能监控信息进行一些不可靠的激进优化。\n\n在一些特殊情况下，激进优化后的代码并不能有更高的性能。需要进行优化回退，将重新对代码进行解释执行。因此\n123456789`C2编译器相对于C1编译器更适用于我们系统分层编译器是综合考虑C1和C2编译器的优点衍生出的一种进化版本编译器，但是由于我们是纯后台应用，这种衍生优化是否有效未可知。分层编译器在一些特殊情况下可能比较激进、不可靠。` *   1*   2*   3\n\nJIT学习参考博文：https://blog.csdn.net/qq_28674045/article/details/51896129https://www.cnblogs.com/insistence/p/5901457.htmlhttps://www.cnblogs.com/death00/p/11722130.html\n步骤二：关闭分层编译，启用C2编译器JVM启动脚本中添加如下参数\n123`-XX:-TieredCompilation -server` \n\n以下是一些常见的 JVM 参数和示例代码：\n\n设置堆大小：\n\n增大堆大小：-Xmx 参数用于设置最大堆大小，例如 -Xmx2g 表示将堆的最大大小设置为 2GB。\n增大初始堆大小：-Xms 参数用于设置初始堆大小，例如 -Xms512m 表示将初始堆大小设置为 512MB。\n\n1java -Xmx2g -Xms512m -jar YourApp.jar\n\n选择垃圾收集器：\n\n选择垃圾收集器：使用 -XX:+UseG1GC 来启用 G1 垃圾收集器。\n\n1java -XX:+UseG1GC -jar YourApp.jar\n\nJIT 编译优化：\n\n禁用 JIT 编译：可以使用 -Djava.compiler=NONE 来禁用 JIT 编译器。\n\n1java -Djava.compiler=NONE -jar YourApp.jar\n\n设置编译阈值：\n\n调整编译阈值：使用 -XX:CompileThreshold 参数来调整编译阈值。\n\n1java -XX:CompileThreshold=1000 -jar YourApp.jar\n\n更改编译器：\n\n使用 -XX:TieredStopAtLevel 参数来更改编译器级别，例如 -XX:TieredStopAtLevel=1 可以将编译器级别设置为 1。\n\n1java -XX:TieredStopAtLevel=1 -jar YourApp.jar\n\n启用或禁用特定的编译器优化：\n\n使用 -XX:CompileCommand 来控制编译器优化，例如 -XX:CompileCommand=inline,YourClass.yourMethod 可以强制内联特定方法。\n\n1java -XX:CompileCommand=inline,YourClass.yourMethod -jar YourApp.jar\n\n这些示例展示了如何在启动 Java 应用程序时传递不同的 JVM 参数。请注意，JVM 参数的调整应该根据应用程序的性能需求和性能分析的结果来选择。不同的应用程序可能需要不同的参数配置。确保在生产环境中小心谨慎地测试和优化参数，以获得最佳性能。\n","slug":"JAVA/C2 CompilerThread9 长时间占用CPU解决方案","date":"2023-11-02T18:06:45.000Z","categories_index":"JIT,JAVA","tags_index":"编译,编译器,JVM","author_index":"dandeliono"},{"id":"f6ebfd744882aafe6959a28651edde47","title":"Kafka内外网访问的设置","content":"Kafka内外网访问的设置kafka的两个配置listeners和advertised.listenerslistenerskafka监听的网卡的ip，假设你机器上有两张网卡，内网192.168.0.213和外网101.89.163.1 如下配置\n1listeners=PLAINTEXT://192.168.0.213:9092\n那么kafka只监听内网网卡，即只接收内网网卡的数据，如果你不能把外网网卡流量转发到内网网卡（为什么要强调这一点，下面说），那么kafka就接收不到外网网卡数据。如果配置成外网ip同理。当然你可以配置成0.0.0.0，监听所有网卡。\nadvertised.listeners我们观察kafka的配置文件server.properties，会发现里面记录了zookeeper集群的各个节点的访问地址，但是并没有记录kafka兄弟节点的地址。kafka节点启动后，会向zookeeper注册自己，同时从zookeeper中获取兄弟节点的地址，以便与兄弟节点通信。同样，我们使用客户端连接kafka后，kafka返回给客户端的是集群各节点的访问地址，这个地址也是上面说的从zookeeper中获得的地址。这个地址哪里来，就是kafka节点向zookeeper注册时提供的advertised.listeners。如果没有，就会使用listeners。\n三种情景，搭配使用这两个配置只需要内网访问kafka1listeners=PLAINTEXT://192.168.0.213:9092\n只需要内网访问kafka你肯定想到了最简单的一个方法，listeners使用外网ip1listeners=PLAINTEXT://101.89.163.1:9092需要外网访问如果宿主机有外网网卡，这么配当然没问题。如果没有（ifconfig看不到外网ip的网卡，基本上就不存在这个外网网卡），很可能和我使用的的宿主机一样是通过NAT映射或者啥办法搞出来的外网ip，此时kafka无法监听这个外网ip（因为不存在，启动就会报错）。这时候就是advertised.listeners真正发挥作用的时候了。使用如下配置：\n12listeners=PLAINTEXT://192.168.0.213:9092advertised.listeners=PLAINTEXT://101.89.163.1:9092\n此时一个完整的kafka客户端访问服务端的流程：\n\n客户端访问101.89.163.1:9092，被kafka宿主机所在环境映射到内网192.168.0.213:9092，访问到了kafka节点，请求获得kafka服务端的访问地址\nkafka从zookeeper拿到自己和其他兄弟节点通过advertised.listeners注册到zookeeper的101.89.163.1:9092等外网地址，作为kafka的服务端访问地址返回给客户端\n客户端拿这些地址访问kafka集群，被kafka宿主机所在环境映射到各kafka节点的内网ip，访问到了kafka服务端……完美循环\n\n你可能会问已经配置了访问地址，为什么还要在第一次访问的时候请求获得kafka的访问地址。因为如果是kafka集群，你可以选择只给客户端配置一个kafka节点的地址（这样是不推荐的），但是客户端必须要访问集群中的每一个节点，所以必须通过这个节点获得集群中每一个节点的访问地址。\n如果不配置advertised.listeners&#x3D;PLAINTEXT:&#x2F;&#x2F;101.89.163.1:9092，你会发现虽然你给kafka客户端配置的访问地址是101.89.163.1:9092，但是kafka客户端访问时报错，报错原因是Connection to node -1[192.168.0.213:9092] could not be established. Broker may not be available.。这就是因为不配置advertised.listeners则advertised.listeners默认使用listeners配置的地址，客户端拿到的就是listeners配置的内网地址\n内外网分流上面说的有外网ip的情况，直接配置外网ip有没有问题呢？如果既要内网访问，又要外网访问，本来可以走内网的流量都走外网网卡，显然不合适；而且有的环境可能被配置成这些kafka宿主机是没有外网访问权限的，即虽然他可以访问自己的外网ip，但是访问不了兄弟节点的外网ip。这时候就要配置内外网。配置1：\n1234listener.security.protocol.map=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXTlisteners=INTERNAL://192.168.0.213:9092,EXTERNAL://192.168.0.213:19092advertised.listeners=INTERNAL://192.168.0.213:9092,EXTERNAL://101.89.163.9:19092inter.broker.listener.name=INTERNAL\n配置2：\n1234listener.security.protocol.map=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXTlisteners=INTERNAL://192.168.0.213:9092,EXTERNAL://101.89.163.9:19092advertised.listeners=INTERNAL://192.168.0.213:9092,EXTERNAL://101.89.163.9:19092inter.broker.listener.name=INTERNAL\n注意这两的区别是listeners的EXTERNAL使用的ip不一样，一个使用内网ip，一个使用外网ip。\n\n如果你的kafka宿主机有外网网卡，只能用外网ip，若使用配置1，kafka通过listeners监听的两个端口都是内网网卡的数据，无法接收到外网网卡数据；\n如果你的kafka宿主机外网ip是映射来的，只能使用内网ip，原因也是上面说过的，不存在外网网卡，kafka启动监听就会报错，而使用内网ip有环境配置好的转发，可以接收到外网ip的数据。\n\n","slug":"MIDDLEWARE/Kafka内外网访问的设置","date":"2023-10-23T19:00:55.000Z","categories_index":"kafka,MIDDLEWARE","tags_index":"listeners,advertised,zookeeper","author_index":"dandeliono"},{"id":"98948a6b47eff1c83e919c4c7211acdc","title":"向量数据库","content":"向量数据库也许你最近可能听过这样的新闻，某向量数据库的初创公司刚写好 PPT，就获得了几千万的投资，某公司的开源的向量数据库因其代码的简陋而登上了 Hackernews 等等。在过去几个月时间中， AI 应用的发展如火如荼，带动了 AI 应用技术栈上下游的火爆，而向量数据库就是其中最热门的之一。\n笔者最近因为开发 ChatFiles 和 VectorHub 两款开源项目的需要从而对向量数据库（Vector Database）进行了学习，在对主流的向量数据库和搜索算法有了大概的了解后，笔者决定将这些知识整理成一篇文章，希望能够帮助到大家。\nGPT 的缺陷过去几个月的时间，我们正处于人工智能的革命中，其中最耀眼的莫过于 GPT-3.5&#x2F;4 的横空出世，而 GPT-3.5&#x2F;4 带给我们无限震撼的同时，其天然的缺陷和诸多的限制也让开发者头痛不已，例如其输入端上下文（tokens）大小的限制困扰着很多的开发者和消费者，像 gpt-3.5-turbo 模型它的限制是 4K tokens(～3000字)，这意味着使用者最多只能输入 3000 字给 GPT 来理解和推理答案。\n有人可能会疑惑，我使用的 ChatGPT 是有对话记忆功能的，既然它可以做到聊天记忆，那么它的输入端 token 有限制也没什么关系，只要我将给 ChatGPT 的文字内容拆分成多次输入，它自然就可以记住我之前的对话，从而做到解除 token 限制。\n这个想法是不太正确的，GPT 作为 LLM 模型是没有记忆功能的，所谓的记忆功能只是开发者将对话记录存储在内存或者数据库中，当你发送消息给 gpt 模型时，程序会自动将最近的几次对话记录（基于对话的字数限制在 4096 tokens 内）通过 prompt 组合成最终的问题，并发送给 ChatGPT。简而言之，如果你的对话记忆超过了 4096 tokens，那么它就会忘记之前的对话，这就是目前 GPT 在需求比较复杂的任务中无法克服的缺陷。\n目前，不同模型对于 token 的限制也不同，gpt-4 是 32K tokens 的限制，而目前最大的 token 限制是 Claude 模型的 100K，这意味可以输入大约 75000 字的上下文给 GPT，这也意味着 GPT 直接理解一部《哈利波特》的所有内容并回答相关问题。\n但这样就能解决我们所有的问题了吗？答案是否定的，首先 Claude 给出的例子是 GPT 处理 72K tokens 上下文的响应速度是 22 秒。如果我们拥有 GB 级别或更大的文档需要进行 GPT 理解和问答，目前的算力很难带来良好体验，更关键的是目前 GPT API 的价格是按照 tokens 来收费的，所以输入的上下文越多，其价格越按昂贵。\n这种情况有点类似于早期开发者面对内存只有几 MB 甚至几 KB 时期开发应用的窘境，一是‘内存’昂贵，二是‘内存’太小，所以在 GPT 模型在性能、成本、注意力机制等方面有重大革命性进展前，开发者不得不面对的绕过 GPT tokens 限制的难题。\n向量数据库的崛起在 GPT 模型的限制下，开发者们不得不寻找其他的解决方案，而向量数据库就是其中之一。向量数据库的核心思想是将文本转换成向量，然后将向量存储在数据库中，当用户输入问题时，将问题转换成向量，然后在数据库中搜索最相似的向量和上下文，最后将文本返回给用户。\n当我们有一份文档需要 GPT 处理时，例如这份文档是客服培训资料或者操作手册，我们可以先将这份文档的所有内容转化成向量（这个过程称之为 Vector Embedding），然后当用户提出相关问题时，我们将用户的搜索内容转换成向量，然后在数据库中搜索最相似的向量，匹配最相似的几个上下文，最后将上下文返回给 GPT。这样不仅可以大大减少 GPT 的计算量，从而提高响应速度，更重要的是降低成本，并绕过 GPT 的 tokens 限制。\n\n再比如我们和 ChatGPT 之间有一份很长的对话，我们可以将所有对话以向量的方式保存起来，当我们提问给 ChatGPT 时，我们可以将问题转化为向量对过去所有的聊天记录进行语义搜索，找到与当前问题最相关的‘记忆’，一起发送给 ChatGPT，极大的提高 GPT 的输出质量。\n向量数据库的作用当然不止步于文字语义搜索，在传统的 AI 和机器学习场景中，还包含人脸识别、图像搜索、语音识别等功能，但不可否认的是，这一轮向量数据库的火爆，正是因为它对于 AI 获得理解和维护长期记忆以执行复杂任务时有非常大的帮助。例如你可以试试 LangChainJs 的文档搜索&#x2F;Q&amp;A 功能 感受它的魅力，或者可以试试笔者的开源项目 VectorHub 和 ChatFiles，可以上传一份文档或者基于一份网页文档，然后询问文档相关问题。这些功能都是基于 Vector Embedding 和向量数据库的产品。\nVector Embeddings对于传统数据库，搜索功能都是基于不同的索引方式（B Tree、倒排索引等）加上精确匹配和排序算法（BM25、TF-IDF）等实现的。本质还是基于文本的精确匹配，这种索引和搜索算法对于关键字的搜索功能非常合适，但对于语义搜索功能就非常弱。\n例如，如果你搜索“小狗”，那么你只能得到带有“小狗”关键字相关的结果，而无法得到“柯基”、“金毛”等结果，因为“小狗”和“金毛”是不同的词，传统数据库无法识别它们的语义关系，所以传统的应用需要人为的将“小狗”和“金毛”等词之间打上特征标签进行关联，这样才能实现语义搜索。而如何将生成和挑选特征这个过程，也被称为 Feature Engineering (特征工程)，它是将原始数据转化成更好的表达问题本质的特征的过程。\n但是如果你需要处理非结构化的数据，就会发现非结构化数据的特征数量会开始快速膨胀，例如我们处理的是图像、音频、视频等数据，这个过程就变得非常困难。例如，对于图像，可以标注颜色、形状、纹理、边缘、对象、场景等特征，但是这些特征太多了，而且很难人为的进行标注，所以我们需要一种自动化的方式来提取这些特征，而这可以通过 Vector Embedding 实现。\nVector Embedding 是由 AI 模型（例如大型语言模型 LLM）生成的，它会根据不同的算法生成高维度的向量数据，代表着数据的不同特征，这些特征代表了数据的不同维度。例如，对于文本，这些特征可能包括词汇、语法、语义、情感、情绪、主题、上下文等。对于音频，这些特征可能包括音调、节奏、音高、音色、音量、语音、音乐等。\n例如对于目前来说，文本向量可以通过 OpenAI 的 text-embedding-ada-002 模型生成，图像向量可以通过 clip-vit-base-patch32 模型生成，而音频向量可以通过 wav2vec2-base-960h 模型生成。这些向量都是通过 AI 模型生成的，所以它们都是具有语义信息的。\n例如我们将这句话 “Your text string goes here” 用 text-embedding-ada-002 模型进行文本 Embedding，它会生成一个 1536 维的向量，得到的结果是这样：“-0.006929283495992422, -0.005336422007530928, ... -4547132266452536e-05,-0.024047505110502243”，它是一个长度为 1536 的数组。这个向量就包含了这句话的所有特征，这些特征包括词汇、语法，我们可以将它存入向量数据库中，以便我们后续进行语义搜索。\n特征和向量虽然向量数据库的核心在于相似性搜索(Similarity Search)，但在深入了解相似性搜索前，我们需要先详细了解一下特征和向量的概念和原理。\n我们先思考一个问题？为什么我们在生活中区分不同的物品和事物？\n如果从理论角度出发，这是因为我们会通过识别不同事物之间不同的特征来识别种类，例如分别不同种类的小狗，就可以通过体型大小、毛发长度、鼻子长短等特征来区分。如下面这张照片按照体型排序，可以看到体型越大的狗越靠近坐标轴右边，这样就能得到一个体型特征的一维坐标和对应的数值，从 0 到 1 的数字中得到每只狗在坐标系中的位置。\n\n然而单靠一个体型大小的特征并不够，像照片中哈士奇、金毛和拉布拉多的体型就非常接近，我们无法区分。所以我们会继续观察其它的特征，例如毛发的长短。\n\n这样每只狗对应一个二维坐标点，我们就能轻易的将哈士奇、金毛和拉布拉多区分开来，如果这时仍然无法很好的区分德牧和罗威纳犬。我们就可以继续再从其它的特征区分，比如鼻子的长短，这样就能得到一个三维的坐标系和每只狗在三维坐标系中的位置。\n在这种情况下，只要特征足够多，就能够将所有的狗区分开来，最后就能得到一个高维的坐标系，虽然我们想象不出高维坐标系长什么样，但是在数组中，我们只需要一直向数组中追加数字就可以了。\n实际上，只要维度够多，我们就能够将所有的事物区分开来，世间万物都可以用一个多维坐标系来表示，它们都在一个高维的特征空间中对应着一个坐标点。\n那这和相似性搜索 (Similarity Search) 有什么关系呢？你会发现在上面的二维坐标中，德牧和罗威纳犬的坐标就非常接近，这就意味着它们的特征也非常接近。我们都知道向量是具有大小和方向的数学结构，所以可以将这些特征用向量来表示，这样就能够通过计算向量之间的距离来判断它们的相似度，这就是相似性搜索。\n上面这几张图片和详细解释来源于这个视频，这个视频系列也包含了部分下方介绍的相似性搜索算法。如果你对这个向量数据库感兴趣，非常推荐看看这个视频。\n相似性搜索 (Similarity Search)既然我们知道了可以通过比较向量之间的距离来判断它们的相似度，那么如何将它应用到真实的场景中呢？如果想要在一个海量的数据中找到和某个向量最相似的向量，我们需要对数据库中的每个向量进行一次比较计算，但这样的计算量是非常巨大的，所以我们需要一种高效的算法来解决这个问题。\n高效的搜索算法有很多，其主要思想是通过两种方式提高搜索效率：\n\n减少向量大小——通过降维或减少表示向量值的长度。\n\n缩小搜索范围——可以通过聚类或将向量组织成基于树形、图形结构来实现，并限制搜索范围仅在最接近的簇中进行，或者通过最相似的分支进行过滤。\n\n\n我们首先来介绍一下大部分算法共有的核心概念，也就是聚类。\nK-Means 和 Faiss我们可以在保存向量数据后，先对向量数据先进行聚类。例如下图在二维坐标系中，划定了 4 个聚类中心，然后将每个向量分配到最近的聚类中心，经过聚类算法不断调整聚类中心位置，这样就可以将向量数据分成 4 个簇。每次搜索时，只需要先判断搜索向量属于哪个簇，然后再在这一个簇中进行搜索，这样就从 4 个簇的搜索范围减少到了 1 个簇，大大减少了搜索的范围。\n\n常见的聚类算法有 K-Means，它可以将数据分成 k 个类别，其中 k 是预先指定的。以下是 k-means 算法的基本步骤：\n\n选择 k 个初始聚类中心。\n将每个数据点分配到最近的聚类中心。\n计算每个聚类的新中心。\n重复步骤 2 和 3，直到聚类中心不再改变或达到最大迭代次数。\n\n但是这种搜索方式也有一些缺点，例如在搜索的时候，如果搜索的内容正好处于两个分类区域的中间，就很有可能遗漏掉最相似的向量。\n现实情况中，向量的分布也不会像图中一样区分的那么明显，往往区域的边界是相邻的，就像下图 Faiss 算法 一样。\n我们可以将向量想象为包含在 Voronoi 单元格中 - 当引入一个新的查询向量时，首先测量其与质心 (centroids) 之间的距离，然后将搜索范围限制在该质心所在的单元格内。\n\n那么为了解决搜索时可能存在的遗漏问题，可以将搜索范围动态调整，例如当 nprobe &#x3D; 1 时，只搜索最近的一个聚类中心，当 nprobe &#x3D; 2 时，搜索最近的两个聚类中心，根据实际业务的需求调整 nprobe 的值。\n\n实际上，除了暴力搜索能完美的搜索出最相邻，所有的搜索算法只能在速度和质量还有内存上做一个权衡，这些算法也被称为近似最相邻（Approximate Nearest Neighbor）。\nProduct Quantization (PQ)在大规模数据集中，聚类算法最大的问题在于内存占用太大。这主要体现在两个方面，首先因为需要保存每个向量的坐标，而每个坐标都是一个浮点数，占用的内存就已经非常大了。除此之外，还需要维护聚类中心和每个向量的聚类中心索引，这也会占用大量的内存。\n对于第一个问题，可以通过量化 (Quantization) 的方式解决，也就是常见的有损压缩。例如在内存中可以将聚类中心里面每一个向量都用聚类中心的向量来表示，并维护一个所有向量到聚类中心的码本，这样就能大大减少内存的占用。\n但这仍然不能解决所有问题，在前面一个例子中，在二维坐标系中划分了聚类中心，同理，在高维坐标系中，也可以划定多个聚类中心点，不断调整和迭代，直到找到多个稳定和收敛的中心点。\n但是在高维坐标系中，还会遇到维度灾难问题，具体来说，随着维度的增加，数据点之间的距离会呈指数级增长，这也就意味着，在高维坐标系中，需要更多的聚类中心点将数据点分成更小的簇，才能提高分类的质量。否者，向量和自己的聚类中心距离很远，会极大的降低搜索的速度和质量。\n但如果想要维持分类和搜索质量，就需要维护数量庞大的聚类中心。随之而来会带来另一个问题，那就是聚类中心点的数量会随着维度的增加而指数级增长，这样会导致我们存储码本的数量极速增加，从而极大的增加了内存的消耗。例如一个 128 维的向量，需要维护 2^64 个聚类中心才能维持不错的量化结果，但这样的码本存储大小已经超过维护原始向量的内存大小了。\n解决这个问题的方法是将向量分解为多个子向量，然后对每个子向量独立进行量化，比如将 128 维的向量分为 8 个 16 维的向量，然后在 8 个 16 维的子向量上分别进行聚类，因为 16 维的子向量大概只需要 256 个聚类中心就能得到还不错的量化结果，所以就可以将码本的大小从 2^64 降低到 8 * 256 &#x3D; 2048 个聚类中心，从而降低内存开销。\n\n而将向量进行编码后，也将得到 8 个编码值，将它们拼起来就是该向量的最终编码值。等到使用的时候，只需要将这 8 个编码值，然后分别在 8 个子码本中搜索出对应的 16 维的向量，就能将它们使用笛卡尔积的方式组合成一个 128 维的向量，从而得到最终的搜索结果。这也就是乘积量化（Product Quantization）的原理。\n使用 PQ 算法，可以显著的减少内存的开销，同时加快搜索的速度，它唯一的问题是搜索的质量会有所下降，但就像我们刚才所讲，所有算法都是在内存、速度和质量上做一个权衡。\nHierarchical Navigable Small Worlds (HNSW)除了聚类以外，也可以通过构建树或者构建图的方式来实现近似最近邻搜索。这种方法的基本思想是每次将向量加到数据库中的时候，就先找到与它最相邻的向量，然后将它们连接起来，这样就构成了一个图。当需要搜索的时候，就可以从图中的某个节点开始，不断的进行最相邻搜索和最短路径计算，直到找到最相似的向量。\n这种算法能保证搜索的质量，但是如果图中所以的节点都以最短的路径相连，如图中最下面的一层，那么在搜索的时候，就同样需要遍历所有的节点。\n\n解决这个问题的思路与常见的跳表算法相似，如下图要搜索跳表，从最高层开始，沿着具有最长“跳过”的边向右移动。如果发现当前节点的值大于要搜索的值-我们知道已经超过了目标，因此我们会在下一级中向前一个节点。\n\nHNSW 继承了相同的分层格式，最高层具有更长的边缘（用于快速搜索），而较低层具有较短的边缘（用于准确搜索）。\n具体来说，可以将图分为多层，每一层都是一个小世界，图中的节点都是相互连接的。而且每一层的节点都会连接到上一层的节点，当需要搜索的时候，就可以从第一层开始，因为第一层的节点之间距离很长，可以减少搜索的时间，然后再逐层向下搜索，又因为最下层相似节点之间相互关联，所以可以保证搜索的质量，能够找到最相似的向量。\n如果你对跳表和 HNSW 感兴趣，可以看看这个视频。\nHNSW 算法是一种经典的空间换时间的算法，它的搜索质量和搜索速度都比较高，但是它的内存开销也比较大，因为不仅需要将所有的向量都存储在内存中。还需要维护一个图的结构，也同样需要存储。所以这类算法需要根据实际的场景来选择。\nLocality Sensitive Hashing (LSH)局部敏感哈希（Locality Sensitive Hashing）也是一种使用近似最近邻搜索的索引技术。它的特点是快速，同时仍然提供一个近似、非穷举的结果。LSH 使用一组哈希函数将相似向量映射到“桶”中，从而使相似向量具有相同的哈希值。这样，就可以通过比较哈希值来判断向量之间的相似度。\n通常，我们设计的哈希算法都是力求减少哈希碰撞的次数，因为哈希函数的搜索时间复杂度是 O(1)，但是，如果存在哈希碰撞，即两个不同的关键字被映射到同一个桶中，那么就需要使用链表等数据结构来解决冲突。在这种情况下，搜索的时间复杂度通常是 O(n)，其中n是链表的长度。所以为了提高哈希函数的搜索的效率，通常会将哈希函数的碰撞概率尽可能的小。\n但是在向量搜索中，我们的目的是为了找到相似的向量，所以可以专门设计一种哈希函数，使得哈希碰撞的概率尽可能高，并且位置越近或者越相似的向量越容易碰撞，这样相似的向量就会被映射到同一个桶中。\n等搜索特定向量时，为了找到给定查询向量的最近邻居，使用相同的哈希函数将类似向量“分桶”到哈希表中。查询向量被散列到特定表中，然后与该表中的其他向量进行比较以找到最接近的匹配项。这种方法比搜索整个数据集要快得多，因为每个哈希表桶中的向量远少于整个空间中的向量数。\n那么这个哈希函数应该如何设计呢？为了大家更好理解，我们先从二维坐标系解释，如下所图示，在二维坐标系中可以通过随机生成一条直线，将二维坐标系划分为两个区域，这样就可以通过判断向量是否在直线的同一边来判断它们是否相似。例如下图通过随机生成 4 条直线，这样就可以通过 4 个二进制数来表示一个向量的位置，例如 A 和 B 表示向量在同一个区域。\n\n这个原理很简单，如果两个向量的距离很近，那么它们在直线的同一边的概率就会很高，例如直线穿过 AC 的概率就远大于直线穿过 AB 的概率。所以 AB 在同一侧的概率就远大于 AC 在同一侧的概率。\n当搜索一个向量时，将这个向量再次进行哈希函数计算，得到相同桶中的向量，然后再通过暴力搜索的方式，找到最接近的向量。如下图如果再搜索一个向量经过了哈希函数，得到了 0110 的值，就会直接找到和它同一个桶中相似的向量 D。从而大大减少了搜索的时间。\n\n关于更多 LSH 算法的细节，可以参考这篇博客。\nRandom Projection for LSH 随机投影如果在二维坐标系可以通过随机生成的直线区分相似性，那么同理，在三维坐标系中，就可以通过随机生成一个平面，将三维坐标系划分为两个区域。在多维坐标系中，同样可以通过随机生成一个超平面，将多维坐标系划分为两个区域，从而区分相似性。\n但是在高维空间中，数据点之间的距离往往非常稀疏，数据点之间的距离会随着维度的增加呈指数级增长。导致计算出来的桶非常多，最极端的情况是每个桶中就一个向量，并且计算速度非常慢。所以实际上在实现 LSH 算法的时候，会考虑使用随机投影的方式，将高维空间的数据点投影到低维空间，从而减少计算的时间和提高查询的质量。\n随机投影背后的基本思想是使用随机投影矩阵将高维向量投影到低维空间中。创建一个由随机数构成的矩阵，其大小将是所需的目标低维值。然后，计算输入向量和矩阵之间的点积，得到一个被投影的矩阵，它比原始向量具有更少的维度但仍保留了它们之间的相似性。\n当我们查询时，使用相同的投影矩阵将查询向量投影到低维空间。然后，将投影的查询向量与数据库中的投影向量进行比较，以找到最近邻居。由于数据的维数降低了，搜索过程比在整个高维空间中搜索要快得多。\n其基本步骤是：\n\n从高维空间中随机选择一个超平面，将数据点投影到该超平面上。\n重复步骤 1，选择多个超平面，将数据点投影到多个超平面上。\n将多个超平面的投影结果组合成一个向量，作为低维空间中的表示。\n使用哈希函数将低维空间中的向量映射到哈希桶中。\n\n同样，随机投影也是一种近似方法，并且投影质量取决于投影矩阵。通常情况下，随机性越大的投影矩阵，其映射质量就越好。但是生成真正随机的投影矩阵可能会计算成本很高，特别是对于大型数据集来说。关于更多 RP for LSH 算法的细节，可以参考这篇博客。\n相似性测量 (Similarity Measurement)上面我们讨论了向量数据库的不同搜索算法，但是还没有讨论如何衡量相似性。在相似性搜索中，需要计算两个向量之间的距离，然后根据距离来判断它们的相似度。\n而如何计算向量在高维空间的距离呢？有三种常见的向量相似度算法：欧几里德距离、余弦相似度和点积相似度。\n欧几里得距离（Euclidean Distance）欧几里得距离是指两个向量之间的距离，它的计算公式为：\nd(A,B)&#x3D;∑i&#x3D;1n(Ai−Bi)2d(\\mathbf{A}, \\mathbf{B}) &#x3D; \\sqrt{\\sum_{i&#x3D;1}^{n}(A_i - B_i)^2}\n其中，A\\mathbf{A} 和 B\\mathbf{B} 分别表示两个向量，nn 表示向量的维度。\n\n欧几里得距离算法的优点是可以反映向量的绝对距离，适用于需要考虑向量长度的相似性计算。例如推荐系统中，需要根据用户的历史行为来推荐相似的商品，这时就需要考虑用户的历史行为的数量，而不仅仅是用户的历史行为的相似度。\n余弦相似度（Cosine Similarity）余弦相似度是指两个向量之间的夹角余弦值，它的计算公式为：\ncos⁡(θ)&#x3D;A⋅B∣A∣∣B∣\\cos(\\theta) &#x3D; \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{|\\mathbf{A}| |\\mathbf{B}|}\n其中，A\\mathbf{A} 和 B\\mathbf{B} 分别表示两个向量，⋅\\cdot 表示向量的点积，∣A∣|\\mathbf{A}| 和 ∣B∣|\\mathbf{B}| 分别表示两个向量的模长。\n\n余弦相似度对向量的长度不敏感，只关注向量的方向，因此适用于高维向量的相似性计算。例如语义搜索和文档分类。\n点积相似度 (Dot product Similarity)向量的点积相似度是指两个向量之间的点积值，它的计算公式为：\nA⋅B&#x3D;∑i&#x3D;1nAiBi\\mathbf{A} \\cdot \\mathbf{B} &#x3D; \\sum_{i&#x3D;1}^{n}A_i B_i\n其中，A\\mathbf{A} 和 B\\mathbf{B} 分别表示两个向量，nn 表示向量的维度。\n\n点积相似度算法的优点在于它简单易懂，计算速度快，并且兼顾了向量的长度和方向。它适用于许多实际场景，例如图像识别、语义搜索和文档分类等。但点积相似度算法对向量的长度敏感，因此在计算高维向量的相似性时可能会出现问题。\n每一种相似性测量 (Similarity Measurement) 算法都有其优点和缺点，需要开发者根据自己的数据特征和业务场景来选择。\n过滤 (Filtering)在实际的业务场景中，往往不需要在整个向量数据库中进行相似性搜索，而是通过部分的业务字段进行过滤再进行查询。所以存储在数据库的向量往往还需要包含元数据，例如用户 ID、文档 ID 等信息。这样就可以在搜索的时候，根据元数据来过滤搜索结果，从而得到最终的结果。\n为此，向量数据库通常维护两个索引：一个是向量索引，另一个是元数据索引。然后，在进行相似性搜索本身之前或之后执行元数据过滤，但无论哪种情况下，都存在导致查询过程变慢的困难。\n\n过滤过程可以在向量搜索本身之前或之后执行，但每种方法都有自己的挑战，可能会影响查询性能：\n\nPre-filtering：在向量搜索之前进行元数据过滤。虽然这可以帮助减少搜索空间，但也可能导致系统忽略与元数据筛选标准不匹配的相关结果。\n\nPost-filtering：在向量搜索完成后进行元数据过滤。这可以确保考虑所有相关结果，在搜索完成后将不相关的结果进行筛选。\n\n\n为了优化过滤流程，向量数据库使用各种技术，例如利用先进的索引方法来处理元数据或使用并行处理来加速过滤任务。平衡搜索性能和筛选精度之间的权衡对于提供高效且相关的向量数据库查询结果至关重要。\n向量数据库选型笔者在本文中，花费了大量的笔墨来介绍向量数据库的相似性搜索算法的原理和实现，相似性搜索算法固然是一个向量数据库的核心和关键点，但是在实际的业务场景中，往往还需要考虑其它的因素，例如向量数据库的可用性、扩展性、安全性等，还有代码是否开源、社区是否活跃等等。\n分布式一个成熟的向量数据库，往往需要支持分布式部署，这样才能满足大规模数据的存储和查询。数据拥有的越多，需要节点就越多，出现的错误和故障也就越多，所以分布式的向量数据库需要具备高可用性和容错性。\n数据库的高可用性和容错性，往往需要实现分片和复制能力，在传统的数据库中，往往通过数据的主键或者根据业务需求进行分片，但是在分布式的向量数据库中，就需要考虑根据向量的相似性进行分区，以便查询的时候能够保证结果的质量和速度。\n其它类似复制节点数据的一致性、数据的安全性等等，都是分布式向量数据库需要考虑的因素。\n访问控制和备份除此之外，访问控制设计的是否充足，例如当组织和业务快速发展时，是否能够快速的添加新的用户和权限控制，是否能够快速的添加新的节点，审计日志是否完善等等，都是需要考虑的因素。\n另外，数据库的监控和备份也是一个重要的因素，当数据出现故障时，能够快速的定位问题和恢复数据，是一个成熟的向量数据库必须要考虑的因素。\nAPI &amp; SDK对比上面的因素选择，API &amp; SDK 可能是往往被忽略的因素，但是在实际的业务场景中，API &amp; SDK 往往是开发者最关心的因素。因为 API &amp; SDK 的设计直接影响了开发者的开发效率和使用体验。一个优秀良好的 API &amp; SDK 设计，往往能够适应需求的不同变化，向量数据库是一个新的领域，在如今大部分人不太清楚这方面需求的当下，这一点容易被人忽视。\n选型截至目前，汇总到目前的向量数据库有以下几种选择：\n\n\n\n向量数据库\nURL\nGitHub Star\nLanguage\nCloud\n\n\n\nchroma\nhttps://github.com/chroma-core/chroma\n7.4K\nPython\n❌\n\n\nmilvus\nhttps://github.com/milvus-io/milvus\n21.5K\nGo&#x2F;Python&#x2F;C++\n✅\n\n\npinecone\nhttps://www.pinecone.io/\n❌\n❌\n✅\n\n\nqdrant\nhttps://github.com/qdrant/qdrant\n11.8K\nRust\n✅\n\n\ntypesense\nhttps://github.com/typesense/typesense\n12.9K\nC++\n❌\n\n\nweaviate\nhttps://github.com/weaviate/weaviate\n6.9K\nGo\n✅\n\n\n传统数据的扩展除了选择专业的向量数据库，使用传统数据库进行扩展也是一种方法。类似 Redis 除了传统的 Key Value 数据库用途外，Redis 还提供了 Redis Modules，这是一种通过新功能、命令和数据类型扩展 Redis 的方式。例如使用 RediSearch 模块来扩展向量搜索的功能。\n同理的还有 PostgreSQL 的扩展，PostgreSQL 提供使用 extension 的方式来扩展数据库的功能，例如 pgvector 来开启向量搜索的功能。它不仅支持精确和相似性搜索，还支持余弦相似度等相似性测量算法。最重要的是，它是附加在 PostgreSQL 上的，因此可以利用 PostgreSQL 的所有功能，例如 ACID 事务、并发控制、备份和恢复等。还拥有所有的 PostgreSQL 客户端库，因此可以使用任何语言的 PostgreSQL 客户端来访问它。可以减少开发者的学习成本和服务的维护成本。\n像笔者的开源项目 ChatFiles 和 VectorHub 目前就暂时使用 pgvector 来实现向量搜索以实现 GPT 文档问答，基于 Supabase 提供的 PostgreSQL + pgvector 服务完成。\n总结本文主要介绍了向量数据库的原理和实现，包括向量数据库的基本概念、相似性搜索算法、相似性测量算法、过滤算法和向量数据库的选型等等。向量数据库是崭新的领域，目前大部分向量数据库公司的估值乘着 AI 和 GPT 的东风从而飞速的增长，但是在实际的业务场景中，目前向量数据库的应用场景还比较少，抛开浮躁的外衣，向量数据库的应用场景还需要开发者们和业务专家们去挖掘。\nReferences\nhttps://www.bilibili.com/video/BV11a4y1c7SW\nhttps://www.bilibili.com/video/BV1BM4y177Dk\nhttps://www.pinecone.io/learn/vector-database/\nhttps://github.com/guangzhengli/ChatFiles\nhttps://github.com/guangzhengli/vectorhub\nhttps://www.anthropic.com/index/100k-context-windows\nhttps://js.langchain.com/docs/\nhttps://www.pinecone.io/learn/series/faiss/locality-sensitive-hashing/\nhttps://www.pinecone.io/learn/series/faiss/product-quantization/\nhttps://www.pinecone.io/learn/series/faiss/locality-sensitive-hashing-random-projection/\nhttps://www.youtube.com/watch?v=QvKMwLjdK-s&t=168s&ab_channel&#x3D;JamesBriggs\nhttps://www.pinecone.io/learn/series/faiss/faiss-tutorial/\nhttps://www.youtube.com/watch?v=sKyvsdEv6rk&ab_channel&#x3D;JamesBriggs\nhttps://www.pinecone.io/learn/vector-similarity/\nhttps://github.com/chroma-core/chroma\nhttps://github.com/milvus-io/milvus\nhttps://www.pinecone.io/\nhttps://github.com/qdrant/qdrant\nhttps://github.com/typesense/typesense\nhttps://github.com/weaviate/weaviate\nhttps://redis.io/docs/interact/search-and-query/\nhttps://github.com/pgvector/pgvector\n\n","slug":"MIDDLEWARE/向量数据库","date":"2023-08-21T14:51:31.000Z","categories_index":"https,MIDDLEWARE","tags_index":"com,www,github","author_index":"dandeliono"},{"id":"4cc462e9c8fcfd4947ede10d296d33aa","title":"Producer Metadata 读取与更新机制","content":"Producer Metadata 读取与更新机制一、前言宏观上介绍了 Producer 的宏观模型，其中通过 waitOnMetadata() 方法获取 topic 的 metadata 。\n二、Metadata2.1 什么是 Metadata\nMetadata 是指 Kafka 集群的元数据，包含了 Kafka 集群的各种信息，直接看源码便可知：\n1234567891011121314151617181920212223242526272829public class Metadata implements Closeable &#123;    private final Logger log;        private final long refreshBackoffMs;        private final long metadataExpireMs;        private int updateVersion;        private int requestVersion;        private long lastRefreshMs;        private long lastSuccessfulRefreshMs;    private KafkaException fatalException;        private Set&lt;String&gt; invalidTopics;        private Set&lt;String&gt; unauthorizedTopics;        private MetadataCache cache = MetadataCache.empty();    private boolean needFullUpdate;    private boolean needPartialUpdate;        private final ClusterResourceListeners clusterResourceListeners;    private boolean isClosed;        private final Map&lt;TopicPartition, Integer&gt; lastSeenLeaderEpochs;&#125;\n\nMetadataCache：Kafka 集群中关于 node、topic 和 partition 的信息。（是只读的）\n12345678910public class MetadataCache &#123;    private final String clusterId;    private final Map&lt;Integer, Node&gt; nodes;    private final Set&lt;String&gt; unauthorizedTopics;    private final Set&lt;String&gt; invalidTopics;    private final Set&lt;String&gt; internalTopics;    private final Node controller;    private final Map&lt;TopicPartition, PartitionMetadata&gt; metadataByPartition;    private Cluster clusterInstance;&#125;\n\n关于 topic 的详细信息（leader 所在节点、replica 所在节点、isr 列表）都是在 Cluster 实例中保存的。\n123456789101112131415161718192021222324public final class Cluster &#123;    private final boolean isBootstrapConfigured;        private final List&lt;Node&gt; nodes;        private final Set&lt;String&gt; unauthorizedTopics;        private final Set&lt;String&gt; invalidTopics;        private final Set&lt;String&gt; internalTopics;    private final Node controller;        private final Map&lt;TopicPartition, PartitionInfo&gt; partitionsByTopicPartition;        private final Map&lt;String, List&lt;PartitionInfo&gt;&gt; partitionsByTopic;        private final Map&lt;String, List&lt;PartitionInfo&gt;&gt; availablePartitionsByTopic;        private final Map&lt;Integer, List&lt;PartitionInfo&gt;&gt; partitionsByNode;        private final Map&lt;Integer, Node&gt; nodesById;        private final ClusterResource clusterResource;&#125;\n\n12345678public class PartitionInfo &#123;    private final String topic;    private final int partition;    private final Node leader;    private final Node\\[\\] replicas;    private final Node\\[\\] inSyncReplicas;    private final Node\\[\\] offlineReplicas;&#125;\n\n看源码不难理解 Metadata 的主要数据结构，我们大概总结下包含哪些信息：\n\n集群中有哪些节点；\n集群中有哪些 topic，这些 topic 有哪些 partition；\n每个 partition 的 leader 副本分配在哪个节点上，follower 副本分配在哪些节点上；\n每个 partition 的 AR 有哪些副本，ISR 有哪些副本；\n\n2.2 Metadata 的应用场景\nMetadata 在 Kafka 中非常重要，很多场景中都需要从 Metadata 中获取数据或更新数据，例如：\n\nKafkaProducer 发送一条消息到指定的 topic 中，需要知道分区的数量，要发送的目标分区，目标分区的 leader，leader 所在的节点地址等，这些信息都要从 Metadata 中获取。\n当 Kafka 集群中发生了 leader 选举，节点中 partition 或副本发生了变化等，这些场景都需要更新Metadata 中的数据。\n\n三、Producer 的 Metadata 更新流程Producer 在调用 doSend() 方法时，第一步就是通过 waitOnMetadata 方法获取该 topic 的 metadata 信息。\n\n\n总结一下以上代码：\n\n首先会从缓存中获取 cluster 信息，并从中获取 partition 信息，如果可以取到则返回当前的 cluster 信息，如果不含有所需要的 partition 信息时就会更新 metadata；\n\n更新 metadata 的操作会在一个 do ….while 循环中进行，直到 metadata 中含有所需 partition 的信息，该循环中主要做了以下事情：\n\n调用 metadata.requestUpdateForTopic() 方法来获取 updateVersion，即上一次更新成功时的 version，并将 needUpdate 设为 true，强制更新；\n调用 sender.wakeup() 方法来唤醒 Sender 线程，Sender 线程中又会唤醒 NetworkClient 线程，在 NetworkClient 中会对 UpdateMetadataRequest 请求进行操作，待会下面会详细介绍；\n调用 metadata.awaitUpdate(version, remainingWaitMs) 方法来等待 metadata 的更新，通过比较当前的 updateVersion 与步骤 1 中获取的 updateVersion 来判断是否更新成功；\n\n\n\n3.1 org.apache.kafka.clients.NetworkClient#poll\n上面提到调用 sender.wakeup() 方法来唤醒 Sender 线程，Sender 线程中又会唤醒 NetworkClient 线程，在 NetworkClient 中会对 UpdateMetadataRequest 请求进行操作。在 NetworkClient 中真正处理请求的是 NetworkClient.poll() 方法，接下来让我们通过分析源码来看下 NetworkClient 是如何处理请求的。\n\n3.2 org.apache.kafka.clients.NetworkClient.DefaultMetadataUpdater#maybeUpdate(long)\n我们来看下 metadata 是如何更新的\n这里你可能会问，老周啊，最小负载节点是啥呀？\n别急，我们来看下面这张图，你就理解了。\n\nLeastLoadedNode 指 Kafka 集群中所有 Node 中负载最小的那一个 Node，它是由每个 Node 在 InFlightRequests 中还未确定的请求数决定的，未确定的请求越少则负载越小。如上图所示，Node1 即为 LeastLoadedNode。\n3.3 org.apache.kafka.clients.Metadata#updateRequested\n下次更新元数据信息的时间：当前 metadata 信息即将到期的时间即 timeToExpire 和 距离允许更新 metadata 信息的时间 即 timeToAllowUpdate 中的最大值。\ntimeToExpire：needUpdate 为 true，表示强制更新，此时该值为 0；否则的话，就按照定时更新时间，即元数据信息过期时间（默认是 300000 ms 即 5 分钟）进行周期性更新。\ntimeToAllowUpdate：默认就是 refreshBackoffMs 的默认值，即 100 ms。\n3.4 org.apache.kafka.clients.NetworkClient.DefaultMetadataUpdater#maybeUpdate(long, org.apache.kafka.common.Node)\n我们继续跟一下 maybeUpdate 方法：\n\n因此，每次 producer 请求更新 metadata 时，会有以下几种情况：\n\n通道已经 ready，node 可以发送请求，那么就直接发送请求。\n如果该 node 正在建立连接，则直接返回。\n如果该 node 还没建立连接，则向 broker 初始化连接。\n\n而 KafkaProducer 线程一直是阻塞在两个 while 循环中的，直到 metadata 更新：\n\nsender 线程第一次调用 poll，初始化与 node 的连接。\nsender 线程第二次调用 poll，发送 metadata 请求。\nsender 线程第三次调用 poll，获取 metadataResponse，并更新 metadata。\n\n3.5 接收 Server 端的响应，更新 Metadata 信息\nhandleCompletedReceives 是如何处理任何已完成的接收响应，如下：\n\n之后进一步调用 handleSuccessfulResponse。\n四、总结Metadata 会在下面两种情况下进行更新：\n\n强制更新：调用 Metadata.requestUpdate() 将 needFullUpdate 置为 true 来强制更新。\n周期性更新：通过 Metadata 的 lastSuccessfulRefreshMs 和 metadataExpireMs 来实现，一般情况下，默认周期时间就是 metadataExpireMs，5 分钟时长。\n\n在 NetworkClient 的 poll() 方法调用时，会去检查两种更新机制，只要达到一种，就会触发更新操作。\nMetadata 的强制更新会在以下几种情况下进行：\n\ninitConnect 方法调用时，初始化连接；\npoll() 方法中对 handleDisconnections() 方法调用来处理连接断开的情况，这时会触发强制更新；\npoll() 方法中对 handleTimedOutRequests() 来处理请求超时时；\n发送消息时，如果无法找到 partition 的 leader；\n处理 Producer 响应（handleProduceResponse），如果返回关于 Metadata 过期的异常，比如：没有 topic-partition 的相关 meta 或者 client 没有权限获取其 metadata。\n\n强制更新主要是用于处理各种异常情况。\n好了，Producer Metadata 读取与更新机制就说到这，我们下一期再见。\n","slug":"MIDDLEWARE/Producer Metadata 读取与更新机制","date":"2023-08-11T14:30:38.000Z","categories_index":"Metadata,MIDDLEWARE","tags_index":"partition,metadata,NetworkClient","author_index":"dandeliono"},{"id":"902bce5943bfcf4419fe702bc9520551","title":"小米AX9000（SSH后） 开启北京联通IPTV转发服务，全家设备可以看IPTV","content":"小米AX9000（SSH后） 开启北京联通IPTV转发服务，全家设备可以看IPTV北京联通，宽带+IPTV已经入户，我需求不用联通送的IPTV盒子，在一根网线（光猫Lan1口）、路由器相同WIFI下（上网、IPTV信号），全家手机、电视盒子播放北京联通IPTV节目。设置的前提 IPTV是DHCP协议，光猫有VLAN设置，光猫已经设好组播IGMP proxy，所以用光猫LAN1口，上网和同时看联通IPTV信号  \nAX9000的设置  \n\n获得AX9000的SSH看之前通过开发板docker获取ssh的文章\n登录SSH，安装udpxy\n\n\n自动安装  12opkg update  opkg install udpxy\n手动安装如果上面opkg intsall udpxy失败，手动下载安装登录小米路由器SSH\n\n12345cd /tmpwget https://downloads.openwrt.org/snapshots/packages/aarch64\\_cortex-a53/packages/udpxy\\_1.0-25.1-1\\_aarch64\\_cortex-a53.ipkopkg install udpxy\\_1.0-25.1-1\\_aarch64_cortex-a53.ipk\n\n\n改动配置文件  1cd /etc/config  \n备份三个配置文件  123cp udpxy udpxy.backup  cp network network.backup  cp firewall firewall.backup\n编辑 udpxy 配置文件  1vi /etc/config/udpxy  \n&#x3D;&#x3D;&#x3D; 配置文件内容 udpxy 端口4022 &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;  123456789config udpxy                                                                                           #option disabled &#x27;1&#x27;                                                                           option respawn &#x27;1&#x27;                                                                            option verbose &#x27;0&#x27;                                                                            option status &#x27;1&#x27;                                                                              # option bind &#x27;0.0.0.0&#x27;                                                                        option port &#x27;4022&#x27;                                                                            option source &#x27;eth4&#x27;                                                                           option disabled &#x27;0&#x27;  \n保存后，回到ssh 命令行，启动udpxy12service udpxy restart  service udpxy enable  \n\nPC web检测udpxy服务状态：http:&#x2F;&#x2F;路由器-ip:4022&#x2F;status\n&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;\n\n编辑 network 网络配置文件  1vi /etc/config/network\n\n&#x3D;&#x3D;&#x3D; 配置文件内容 wan 是机器配置，‘iptv’网络接口也同样设置在WAN上 eth4， 增加的\noption metric 10（wan口上网）,20（IPTV） 设置跃点&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;  \n1234567891011121314151617config interface &#x27;wan&#x27;                                                                                 option proto &#x27;pppoe&#x27;                                                                           option special &#x27;0&#x27;                                                                            option username &#x27;宽带账户&#x27;                                                                option mru &#x27;1480&#x27;                                                                              option password &#x27;宽带密码&#x27;                                                                      option ifname &#x27;eth4&#x27;   # 以下为新增数据            option metric &#x27;10&#x27;          config interface &#x27;iptv&#x27;                                                                               option ifname &#x27;eth4&#x27;                                                                          option \\_orig\\_ifname &#x27;eth4&#x27;                                                                     option \\_orig\\_bridge &#x27;false&#x27;                                                                   option proto &#x27;dhcp&#x27;                                                                            option delegate &#x27;0&#x27;                                                                            option metric &#x27;20&#x27;   \n\n编辑firewall防火墙配置1vi /etc/config/firewall\n12345678910111213141516config zone                                                                                                          option name &#x27;wan&#x27;                                                                                           option input &#x27;REJECT&#x27;                                                                                        option output &#x27;ACCEPT&#x27;                                                                                       option forward &#x27;REJECT&#x27;                                                                                     option masq &#x27;1&#x27;                                                                                              option mtu_fix &#x27;1&#x27;                                                                                  # 以下为新增数据        option network &#x27;wan wan6 iptv&#x27;    config rule                                                                                           option src &#x27;wan&#x27;                option proto &#x27;udp&#x27;                                                                            option dest_ip &#x27;224.0.0.0/4&#x27;                                                                   option target &#x27;ACCEPT&#x27;                                                                         option name &#x27;Allow-UDP-udpxy&#x27;  \n\n配置完重启，路由器。大功告成北京联通iptv列表\n","slug":"OTHER/小米AX9000（SSH后） 开启北京联通IPTV转发服务，全家设备可以看IPTV","date":"2023-07-26T23:55:11.000Z","categories_index":"IPTV,OTHER","tags_index":"udpxy,SSH,编辑","author_index":"dandeliono"},{"id":"a49fcb0de63d67d2f42e8c106ec83312","title":"一文讲解Elasticsearch java restful api 跨版本兼容解决方案","content":"一文讲解Elasticsearch java restful api 跨版本兼容解决方案之前的文章elasticsearch&amp;kibana从6.0升级到7.9完整过程记录已经已经介绍了该需求的来源，也已经将elasticsearch的server端从6.0.0升级到了7.9.0，server端的问题解决了，接下来就是client端的问题了，怎么解决下面的问题：\n\n之前6.0.0版本使用的是elasticsearch的JAVA api，这个版本的额API在7.X版本已经变成deprecated状态，并会在8.0版本被移除，所以需要将JAVA api转化为JAVA restful api\n由于公司的产品除了官服外都是在客户现场的私有化部署，由于elasticsearch的应用比较广泛加上客户为了方便管理，所以很多情况下elasticsearch集群都是直接使用客户提供或者构建的固有集群，集群的版本不固定，包含但不限于5.4.0，6.X以及未来将要升级到的7.9.0，繁杂的版本使得client端的代码兼容变成了一件无比繁杂且重要的一件事情\nclient端的支撑要考虑到代码变更的复杂性以及工作量，尽量使用最少的工作量达到目的\n\n1、one stack to rule them all(一套代码解决所有问题)\n2、尽量减少适配和代码维护工作量\n前后一共出了两版方案，第一版方案就是正面刚，彻底解决问题，但是适配和维护工作工作量过大，被迫放弃，第二版方案是一个这种方案，虽然可能在未来再次升级后会出现问题，但是能做到平稳过度，最大化减少代码升级以及维护的工作量，下面就分别说一说。\n首先先简单科普下elasticsearch的java restful client：\n\nrestful client分为high level restful api（下面简称high）和lowlevel restful api（下面简称low）\n其中high是通过low实现的，而low是直接执行底层命令来获得返回结果，其中底层命令就是在kibana中执行的命令，返回值也完全一样，需要自己去手动解析\n鉴于2中介绍的两者特点，理论上来说low是可以基本上做到版本完全兼容的，而high则和版本相关\nhigh出现的原因，更多的是为使用java api的项目快速适配以及方便后续使用而封装的，所以如果完全使用high的话，和java api的兼容性会很好，修改适配的工作量相对较少\nlow一直存在，high一直在发展，6.0.0版本的high只有document相关的操作，而7.9.0则所有的操作都有high相关的实现\n\n正面刚方案：\n针对6.X和7.9.0版本，各维护一套es-client的api包，封装同样的接口供业务模块使用，这样的结果是维护一套业务代码，两套es-client的代码\n这样设计的好处是业务代码解耦合，维护一套即可，屏蔽了es版本的差异；两套es-client的代码分别维护，可以最大程度的减少版本不兼容带来的各种问题，提升稳定性\n但是凡事都有两面性，这样做的缺点就是服务端需要维护的额代码变多，es-client的工作量加倍，而且最重要的是，公司的业务特点就是私有化部署的客户遍及全中国，运维团队需要维护的内容多到离谱，再给他们增加es-client的适配无疑是一个很大的压力，而且繁杂度的增加带来的是犯错误的几率提升，对整个流程来说并不友好（公司团队决定的，运维人员的资源并不多）\n\n侧面迂回方案：    既然强攻不成，那就迂回包抄，通过某些特殊的手段，包含但不限于使用low来实现部分high无法兼容的代码，调整使用方式避开跨版本不兼容的用法，甚至修改部分源码以达到一套es-client代码适配所有业务代码以及所有版本ES的目的，当然这么做的风险是仅仅能支持6.X和7.9.0版本，对后续版本的兼容性未知，但是当前版本可以作为过渡版本，当后续ES版本更迭的时候可以淘汰掉6.X版本而专心考虑高版本（6.X和7.X的差异实在太大，兼容性搞起来有很多坑需要填….）\n\n直接维护一套es-client的api包，在该包中封装6.X和7.9.0相关的对象以屏蔽版本的差异，另外尽量使用以前java client相关的方法体，里面的实现替换成rest-client的实现，以最大程度减少业务代码的修改。\n在填坑过程中最后还是没有绕过修改elasticsearch源码，针对SearchRequest进行了一些封装，来兼容各种版本的search属性\n\n    下面说说具体的方案实施过程： \n    版本差异收集：\n\n    seq_no_primary_term:6.7版本增加的默认search属性\n    ignore_throttled:6.8.1版本增加的默认search属性\n    ccsMinimizeRoundtrips:7.0.0版本增加的默认search属性\n    SearchHits类的totalHits属性在6.X版本是long类型，在7.X版本是TotalHits对象属性，导致使用6.X版本client访问7.X版本的server时得到的totalHits属性为0，该问题在6.8的client中做了适配\n\n    推进过程：\n\n最开始的时候想使用7.9.0的high来解决所有问题，后来发现在es6.0.0版本上2，3两个属性导致所有的search返回为null，报的错误就是search不支持ignore_throttled和ccsMinimizeRoundtrips两个属性\n见招拆招，选取6.8.0版本来跳过这两个我们暂时并不需要的属性，修改之后发现绝大多数的search返回结果都正常了，但是TopHitsAggregationBuilder相关的API却返回为null，报的错误是search不支持seq_no_primary_term属性\n将适配的工作进行到底，经过查询发现，seq_no_primary_term是es6.7版本增加的search属性，于是将rest-client版本降到6.6.2，在6.X版本上所有问题都解决了，所有业务系统代码正常运行，还没高兴多久，在7.9.0版本的es测试中发现，search返回的总条数总是0，于是再次网上学习查看源码，发现了上面第4条中出现的问题\n现在问题摆在眼前，版本冲突了，没有一个版本能兼容所有的问题了，于是痛下决心，在6.6.2的版本基础上修改elasticsearch的源码，来兼容第4条带来的问题\n\n    下面贴出具体的核心结构与代码： \n    \n   上面的代码就是es-client的restful实现，EsClient里面封装了es各种基础操作的restful实现，request包中是为了屏蔽es版本差异和兼容原有es的java api实现而做的抽象，都是继承对应的request后而做的封装\n   至于源码修改，就是在SearchRequest以及相关的requet中增加了totalHits4Object属性来判断，是否需要将topHits对象转为转为Long，下面就是在SearchRequest中增加的属性\n   \n   然后在EsClient中根据low的api获取es的版本，然后根据版本来判断该属性该如何设置\n   \n   hasType属性就是es版本小于7.X的标识，即es的版本为7.X时，需要将topHits对象转为转为Long，然后在search参数组织时进行判断，根据不同版本做不同的处理，具体如下：\n   \n   \n   判断SearchRequest中的totalHits4Object来判断是否需要增加rest_total_hits_as_int属性来处理SearchRequest\n    最后还需要在查询校验的时候去掉对于type的校验，防止高版本的es如es7及以上版本只会返回报错，具体做法是将IndexRequest中的相关校验注释掉。\n\n   至此所有的坑以及问题都得以解决，暂时已经编译通过并正常运行，后续的功能以及性能测试再出现坑再去填吧，反正已经修改了源码，理论上一切问题都可以通过修改源码来解决了\n","slug":"JAVA/一文讲解Elasticsearch java restful api 跨版本兼容解决方案","date":"2023-07-24T16:14:49.000Z","categories_index":"client,JAVA","tags_index":"search,api,high","author_index":"dandeliono"},{"id":"2b4f70bb6bf139f87ea4e015f7434bed","title":"检查网络延时测试 URL 汇总","content":"检查网络延时测试 URL 汇总\n\n\nURL\n备注\n推荐\n\n\n\nhttp://captive.apple.com\n苹果设备用于检测 Wi-Fi 是否需要认证登陆的链接\n推荐用于境外，但也没那么推荐\n\n\nhttp://www.apple.com/library/test/success.html\n苹果设备用于检测 Wi-Fi 是否需要认证登陆的链接\n中国大陆和境外都推荐\n\n\nhttp://cp.cloudflare.com/generate_204\nCloudflare的联通性测试地址\n推荐用于境外\n\n\nhttp://detectportal.firefox.com/success.txt\n火狐的网络联通性测试地址\n推荐用于境外\n\n\nhttp://www.gstatic.com/generate_204\nGoogle 的网络联通性测试地址\n好像境外大陆都能用\n\n\nhttp://www.google.com/generate_204\nGoogle 的网络联通性测试地址\n只能境外\n\n\nhttp://connectivitycheck.gstatic.com/generate_204\nGoogle 的网络联通性测试地址\n好像境外大陆都能用\n\n\nhttp://www.msftconnecttest.com/connecttest.txt\n微软的网络联通性测试地址\n自行尝试\n\n\nhttp://edge.microsoft.com/captiveportal/generate_204\nEdge 的网络联通性测试地址\n自行尝试\n\n\nhttp://g.alicdn.com\n阿里云 CDN 地址\n推荐中国大陆\n\n\nhttp://file.myqcloud.com\n腾讯云 CDN 地址\n推荐中国大陆\n\n\nhttp://www.qualcomm.cn/generate_204\n高通的联通性测试地址\n自行尝试\n\n\nhttp://wifi.vivo.com.cn/generate_204\nvivo 的联通性测试地址\n推荐中国大陆\n\n\nhttp://connectivitycheck.platform.hicloud.com/generate_204\n华为的联通性测试地址\n推荐中国大陆\n\n\nhttp://connect.rom.miui.com/generate_204\n小米的联通性测试地址\n推荐中国大陆\n\n\n","slug":"OTHER/检查网络延时测试 URL 汇总","date":"2023-07-22T15:26:02.000Z","categories_index":"com,OTHER","tags_index":"www,http,generate","author_index":"dandeliono"},{"id":"37fc72851c01fae66c3f1f8cc79de26f","title":"Java中的零拷贝","content":"Java中的零拷贝\n\n\n\n\n\n\n\n\n先提出两个问题：IO过程中，哪些步骤进行了拷贝？哪些地方零拷贝？Java支持哪些零拷贝？\n带着这俩问题，我们一起来看下面的探究。\n哪里听说过零拷贝?真的0次拷贝吗?相信大家伙在以往的学习中，或多或少在下面这些组件、框架中有听说过零拷贝 (Zero-Copy)?\n\n\n\n\n\n\n\n\n\nKafkaNettyrocketmqnginxapache\n什么是零拷贝?零拷贝(英语: Zero-copy) 技术是指计算机执行操作时，CPU不需要先将数据从某处内存复制到另一个特定区域。这种技术通常用于通过网络传输文件时节省CPU周期和内存带宽。\n➢零拷贝技术可以减少数据拷贝和共享总线操作的次数，消除传输数据在存储器之间不必要的中间拷贝次数，从而有效地提高数据传输效率➢零拷贝技术减少了用户进程地址空间和内核地址空间之间因为上:下文切换而带来的开销可以看出没有说不需要拷贝，只是说减少冗余[不必要]的拷贝。\nLinuxI&#x2F;O机制及零拷贝介绍IO中断与DMAIO中断，需要CPU响应，需要CPU参与，因此效率比较低。\n\n用户进程需要读取磁盘数据，需要CPU中断，发起IO请求，每次的IO中断，都带来CPU的上下文切换。\n因此出现了——DMA。 \nDMA(Direct Memory Access，直接内存存取) 是所有现代电脑的重要特色，它允许不同速度的硬件装置来沟通，而不需要依赖于CPU 的大量中断负载。DMA控制器，接管了数据读写请求，减少CPU的负担。这样一来，CPU能高效工作了。现代硬盘基本都支持DMA。\n\nLinux IO流程实际因此IO读取，涉及两个过程：1、DMA等待数据准备好，把磁盘数据读取到操作系统内核缓冲区；2、用户进程，将内核缓冲区的数据copy到用户空间。这两个过程，都是阻塞的。\n\n传统数据传送\n比如：读取文件，再用socket发送出去传统方式实现：先读取、再发送，实际经过1~4四次copy。\n12buffer = File.read Socket.send(buffer) \n\n1、第一次：将磁盘文件，读取到操作系统内核缓冲区；2、第二次：将内核缓冲区的数据，copy到application应用程序的buffer；3、第三步：将application应用程序buffer中的数据，copy到socket网络发送缓冲区(属于操作系统内核的缓冲区)；4、第四次：将socket buffer的数据，copy到网卡，由网卡进行网络传输。\n\n传统方式，读取磁盘文件并进行网络发送，经过的四次数据copy是非常繁琐的。实际IO读写，需要进行IO中断，需要CPU响应中断(带来上下文切换)，尽管后来引入DMA来接管CPU的中断请求，但四次copy是存在“不必要的拷贝”的。\n重新思考传统IO方式，会注意到实际上并不需要第二个和第三个数据副本。应用程序除了缓存数据并将其传输回套接字缓冲区之外什么都不做。相反，数据可以直接从读缓冲区传输到套接字缓冲区。\n显然，第二次和第三次数据copy 其实在这种场景下没有什么帮助反而带来开销，这也正是零拷贝出现的背景和意义。\n传统数据传送所消耗的成本：4次拷贝，4次上下文切换。4次拷贝，其中两次是DMA copy，两次是CPU copy。如下图所示拷贝是个IO过程，需要系统调用。\n\n注意一点的是 内核从磁盘上面读取数据 是 不消耗CPU时间的，是通过磁盘控制器完成；称之为DMA Copy。网卡发送也用DMA。\n零拷贝的出现目的：减少IO流程中不必要的拷贝零拷贝需要OS支持，也就是需要kernel暴露api。虚拟机不能操作内核，\n\nLinux支持的(常见)零拷贝一、mmap内存映射\n\n\n\n\n\n\n\n\ndata loaded from disk is stored in a kernel buffer by DMA copy. Then the pages of the application buffer are mapped to the kernel buffer, so that the data copy between kernel buffers and application buffers are omitted.\nDMA加载磁盘数据到kernel buffer后，应用程序缓冲区(application buffers)和内核缓冲区(kernel buffer)进行映射，数据再应用缓冲区和内核缓存区的改变就能省略。\n\nmmap内存映射将会经历：3次拷贝: 1次cpu copy，2次DMA copy；以及4次上下文切换\n二、sendfilelinux 2.1支持的sendfile\n\n\n\n\n\n\n\n\n\nwhen calling the sendfile() system call, data are fetched from disk and copied into a kernel buffer by DMA copy. Then data are copied directly from the kernel buffer to the socket buffer. Once all data are copied into the socket buffer, the sendfile() system call will return to indicate the completion of data transfer from the kernel buffer to socket buffer. Then, data will be copied to the buffer on the network card and transferred to the network.\n当调用sendfile()时，DMA将磁盘数据复制到kernel buffer，然后将内核中的kernel buffer直接拷贝到socket buffer；一旦数据全都拷贝到socket buffer，sendfile()系统调用将会return、代表数据转化的完成。socket buffer里的数据就能在网络传输了。\n\nsendfile会经历：3次拷贝，1次CPU copy 2次DMA copy；以及2次上下文切换\n三、Sendfile With DMA Scatter&#x2F;Gather Copy\n\n\n\n\n\n\n\n\nThen by using the DMA scatter&#x2F;gather operation, the network interface card can gather all the data from different memory locations and store the assembled packet in the network card buffer.\nScatter&#x2F;Gather可以看作是sendfile的增强版，批量sendfile。\n\nScatter&#x2F;Gather会经历2次拷贝: 0次cpu copy，2次DMA copy\nIO请求批量化DMA scatter&#x2F;gather：需要DMA控制器支持的。DMA工作流程：cpu发送IO请求给DMA，DMA然后读取数据。IO请求：相当于可以看作包含一个物理地址。从一系列物理地址(10)读数据:普通的DMA (10请求)dma scatter&#x2F;gather:一次给10个物理地址， 一个请求就可以（批量处理）。\n4、spliceLinux 2.6.17 支持splice\n\n\n\n\n\n\n\n\n\nit does not need to copy data between kernel space and user space.When using this approach, data are copied from disk to kernel buffer first. Then the splice() system call allows data to move between different buffers in kernel space without the copy to user space.Unlike the method sendfile() with DMA scatter&#x2F;gather copy, splice() does not need support from hardware.\n数据从磁盘读取到OS内核缓冲区后，在内核缓冲区直接可将其转成内核空间其他数据buffer，而不需要拷贝到用户空间。如下图所示，从磁盘读取到内核buffer后，在内核空间直接与socket buffer建立pipe管道。和sendfile()不同的是，splice()不需要硬件支持。\n\n注意splice和sendfile的不同，sendfile是将磁盘数据加载到kernel buffer后，需要一次CPU copy,拷贝到socket buffer。而splice是更进一步，连这个CPU copy也不需要了，直接将两个内核空间的buffer进行set up pipe。\nsplice会经历 2次拷贝: 0次cpu copy 2次DMA copy；以及2次上下文切换\nLinux零拷贝机制对比无论是传统IO方式，还是引入零拷贝之后，2次DMA copy 是都少不了的。因为两次DMA都是依赖硬件完成的。\n\n零拷贝的广义狭义之分\n实际上，零拷贝时有广义和狭义之分的。广义零拷贝： 能减少拷贝次数，减少不必要的数据拷贝，就算作“零拷贝”。这是目前，对零拷贝最为广泛的定义，我们需要知道的是，这是广义上的零拷贝，并不是操作系统 意义上的零拷贝。\n零拷贝的广义性最早的零拷贝定义，来源于\n\n\n\n\n\n\n\n\n\nLinux 2.4内核新增 sendfile 系统调用，提供了零拷贝。磁盘数据通过 DMA 拷贝到内核态 Buffer 后，直接通过 DMA 拷贝到 NIC Buffer(socket buffer)，无需 CPU 拷贝。这也是零拷贝这一说法的来源。这是真正操作系统 意义上的零拷贝(也就是狭义零拷贝)。\n但是我们知道，由OS内核提供的 操作系统意义上的零拷贝，发展到目前也并没有很多种，也就是这样的零拷贝并不是很多；\n随着发展，零拷贝的概念得到了延伸，就是目前的减少不必要的数据拷贝都算作零拷贝的范畴；\n糟糕的是，一些开发者、机构、某些框架，在产品推广或竞争中“滥用”零拷贝这个概念，包装并美其名曰“性能…有多高，采用零拷贝…”\n尤其在框架孵化、推广初期，和竞对争夺市场时，这样的宣传似乎会让不是很内行的人 不明觉厉。\n今天提及的目的，是要大家明白，在看到xxx框架底层采用零拷贝时，或许并不是真正意义上的零拷贝，或许只是借用概念。\n在此说明，并不是否认某些框架借用概念的行为，毕竟随着发展，零拷贝的概念得到了延伸，容纳了新的东西。\n想要强调的是，作为一线技术者，应该不被几句宣传蒙蔽双眼；需要清晰的知道，数据合并以减少拷贝和内核提供的API、在性能提升方面还是有天壤之别的。\n若能稍作深入了解，便能识透其真相，究竟只是偏向于优化数据操作，还是真正切合场景、灵活运用了操作系统意义上的零拷贝，都会浮出水面了。\n后文，也会对目前使用了零拷贝的常见框架进行分析。\nJava零拷贝机制解析Linux提供的领拷贝技术 Java并不是全支持，支持2种(内存映射mmap、sendfile)；\n\nNIO提供的内存映射 MappedByteBuffer\n首先要说明的是，JavaNlO中 的Channel (通道)就相当于操作系统中的内核缓冲区，有可能是读缓冲区，也有可能是网络缓冲区，而Buffer就相当于操作系统中的用户缓冲区。\n\n123MappedByteBuffer mappedByteBuffer = new RandomAccessFile(file, &quot;r&quot;)                                  .getChannel()                                 .map(FileChannel.MapMode.READ_ONLY, 0, len); \n\n底层就是调用Linux mmap()实现的。\nNIO中的FileChannel.map()方法其实就是采用了操作系统中的内存映射方式，底层就是调用Linux mmap()实现的。\n将内核缓冲区的内存和用户缓冲区的内存做了一个地址映射。这种方式适合读取大文件，同时也能对文件内容进行更改，但是如果其后要通过SocketChannel发送，还是需要CPU进行数据的拷贝。使用MappedByteBuffer，小文件，效率不高；一个进程访问，效率也不高。\nMappedByteBuffer只能通过调用FileChannel的map()取得，再没有其他方式。FileChannel.map()是抽象方法，具体实现是在 FileChannelImpl.c 可自行查看JDK源码，其map0()方法就是调用了Linux内核的mmap的API。使用 MappedByteBuffer类要注意的是：mmap的文件映射，在full gc时才会进行释放。当close时，需要手动清除内存映射文件，可以反射调用sun.misc.Cleaner方法。\nNIO提供的sendfile\nFileChannel.transferTo()方法直接将当前通道内容传输到另一个通道，没有涉及到Buffer的任何操作，NIO中 的Buffer是JVM堆或者堆外内存，但不论如何他们都是操作系统内核空间的内存\ntransferTo()的实现方式就是通过系统调用sendfile() (当然这是Linux中的系统调用)\n\n123 FileChannel sourceChannel = new RandomAccessFile(source, &quot;rw&quot;).getChannel();SocketChannel socketChannel = SocketChannel.open(sa);sourceChannel.transferTo(0, sourceChannel.size(), socketChannel); \n\nZeroCopyFile实现文件复制\n123456789101112class ZeroCopyFile &#123;    public void copyFile(File src, File dest) &#123;        try (FileChannel srcChannel = new FileInputStream(src).getChannel();             FileChannel destChannel = new FileInputStream(dest).getChannel()) &#123;            srcChannel.transferTo(0, srcChannel.size(), destChannel);        &#125; catch (IOException e) &#123;            e.printStackTrace();        &#125;    &#125;&#125; \n\n注意： Java NIO提供的FileChannel.transferTo 和 transferFrom 并不保证一定能使用零拷贝。实际上是否能使用零拷贝与操作系统相关，如果操作系统提供 sendfile 这样的零拷贝系统调用，则这两个方法会通过这样的系统调用充分利用零拷贝的优势，否则并不能通过这两个方法本身实现零拷贝。\n\nKafka中的零拷贝Kafka两个重要过程都使用了零拷贝技术，且都是操作系统层面的狭义零拷贝，一是Producer生产的数据存到broker，二是 Consumer从broker读取数据。\n\nProducer生产的数据持久化到broker，采用mmap文件映射，实现顺序的快速写入；\nCustomer从broker读取数据，采用sendfile，将磁盘文件读到OS内核缓冲区后，直接转到socket buffer进行网络发送。\n\n深入学习请移步 https://www.jianshu.com/p/1c27da322767\nNetty中的零拷贝Netty中的Zero-copy与上面我们所提到到OS层面上的Zero-copy不太一样, Netty的Zero-copy完全是在用户态(Java层面)的，它的Zero-copy的更多的是偏向于优化数据操作这样的概念。\nNetty的Zero-copy体现在如下几个个方面:\n\nNetty提供了CompositeByteBuf类，它可以将多个ByteBuf合并为一个逻辑上的ByteBuf，避免了各个ByteBuf之间的拷贝。\n通过wrap操作，我们可以将byte[]数组、ByteBuf、 ByteBuffer 等包装成一个 Netty ByteBuf对象，进而避免了拷贝操作。\nByteBuf支持slice 操作，因此可以将ByteBuf分解为多个共享同一个存储区域的ByteBuf，避免了内存的拷贝。\n通过FileRegion包装的FileChannel.tranferTo实现文件传输，可以直接将文件缓冲区的数据发送到目标Channel，避免了传统通过循环write方式导致的内存拷贝问题。\n\n认真阅读的读者，一定能够知道：前三个都是 广义零拷贝，都是减少不必要数据copy；偏向于应用层数据优化的操作。FileRegion包装的FileChannel.tranferTo，才是真正的零拷贝。下面我们分别来看其每一种实现。下面的分析，也不会刻意区别广义零拷贝和狭义零拷贝，读者只需要了解二者的区别，及其各自的实现对我们应用程序的影响即可。\n通过CompositeByteBuf实现零拷贝\n将多个ByteBuf合并为一个逻辑上的ByteBuf，简单理解就是类似于用一个链表，把分散的多个ByteBuf通过引用连接起来；\n分散的多个ByteBuf在内存中可能是大小各异、互不相连的区域，通过链表串联起来，作为一整块逻辑上的大区域。\n而在实际数据读取时，还是会去各自每一小块上读取。\n\n\n通过wrap操作实现零拷贝\n将byte[]数组、ByteBuf、 ByteBuffer 等包装成一个 Netty ByteBuf对象；\n这个比较简单，看过ByteBuf源码的同学一定会知道，ByteBuf其实就是组合(包含)了byte[]；\n通过 Unpooled.wrappedBuffer 方法来将 bytes 包装成为一个 UnpooledHeapByteBuf 对象, 而在包装的过程中, 是不会有拷贝操作的. 即最后我们生成的生成的 ByteBuf 对象是和 bytes 数组共用了同一个存储空间, 对 bytes 的修改也会反映到 ByteBuf 对象中.\n\n\n通过slice操作实现零拷贝\n将ByteBuf分解为多个共享同一个存储区域的ByteBuf\nslice恰好是将一整块区域，划分成逻辑上独立的小区域；\n在读取每个逻辑小区域时，实际会去按slice(int index, int length) index和length去读取原内存buffer的数据。\n\n\n通过FileRegion实现零拷贝\nFileRegion包装的FileChannel.tranferTo实现文件传输，可以直接将文件缓冲区的数据发送到目标Channel；\n这是操作系统级别的零拷贝\n\n\n扩展阅读 https://pdfs.semanticscholar.org/6a35/60046cb8d3258669c86072a7cab05e1d2300.pdf\n","slug":"JAVA/Java中的零拷贝","date":"2023-07-21T14:02:55.000Z","categories_index":"upload,JAVA","tags_index":"images,copy,DMA","author_index":"dandeliono"},{"id":"fb69e728d1b56159eab29357e4640574","title":"Redis - Redis分布式算法原理","content":"Redis - Redis分布式算法原理一、Redis集群的使用我们在使用Redis的时候，为了保证Redis的高可用，提高Redis的读写性能，最简单的方式我们会做主从复制，组成Master-Master或者Master-Slave的形式，或者搭建Redis集群，进行数据的读写分离，类似于数据库的主从复制和读写分离。如下所示：\n\n同样类似于数据库，当单表数据大于500W的时候需要对其进行分库分表，当数据量很大的时候（标准可能不一样，要看Redis服务器容量）我们同样可以对Redis进行类似的操作，就是分库分表。\n假设，我们有一个社交网站，需要使用Redis存储图片资源，存储的格式为键值对，key值为图片名称，value为该图片所在文件服务器的路径，我们需要根据文件名查找该文件所在文件服务器上的路径，数据量大概有2000W左右，按照我们约定的规则进行分库，规则就是随机分配，我们可以部署8台缓存服务器，每台服务器大概含有500W条数据，并且进行主从复制，示意图如下：\n\n由于规则是随机的，所有我们的一条数据都有可能存储在任何一组Redis中，例如上图我们用户查找一张名称为”a.png”的图片，由于规则是随机的，我们不确定具体是在哪一个Redis服务器上的，因此我们需要进行1、2、3、4，4次查询才能够查询到（也就是遍历了所有的Redis服务器），这显然不是我们想要的结果，有了解过的小伙伴可能会想到，随机的规则不行，可以使用类似于数据库中的分库分表规则：按照Hash值、取模、按照类别、按照某一个字段值等等常见的规则就可以出来了！好，按照我们的主题，我们就使用Hash的方式。\n二、为Redis集群使用Hash可想而知，如果我们使用Hash的方式，每一张图片在进行分库的时候都可以定位到特定的服务器，示意图如下：\n\n上图中，假设我们查找的是”a.png”，由于有4台服务器（排除从库），因此公式为 hash(a.png) % 4 &#x3D; 2，可知定位到了第2号服务器，这样的话就不会遍历所有的服务器，大大提升了性能！\n三、使用Hash的问题上述的方式虽然提升了性能，我们不再需要对整个Redis服务器进行遍历！但是，使用上述Hash算法进行缓存时，会出现一些缺陷，主要体现在服务器数量变动的时候，所有缓存的位置都要发生改变！\n试想一下，如果4台缓存服务器已经不能满足我们的缓存需求，那么我们应该怎么做呢？很简单，多增加几台缓存服务器不就行了！假设：我们增加了一台缓存服务器，那么缓存服务器的数量就由4台变成了5台。那么原本hash(a.png) % 4 &#x3D; 2 的公式就变成了hash(a.png) % 5 &#x3D; ？ ， 可想而知这个结果肯定不是2的，这种情况带来的结果就是当服务器数量变动时，所有缓存的位置都要发生改变！换句话说，当服务器数量发生改变时，所有缓存在一定时间内是失效的，当应用无法从缓存中获取数据时，则会向后端数据库请求数据（还记得上一篇的《缓存雪崩》吗？）！\n同样的，假设4台缓存中突然有一台缓存服务器出现了故障，无法进行缓存，那么我们则需要将故障机器移除，但是如果移除了一台缓存服务器，那么缓存服务器数量从4台变为3台，也是会出现上述的问题！\n所以，我们应该想办法不让这种情况发生，但是由于上述Hash算法本身的缘故，使用取模法进行缓存时，这种情况是无法避免的，为了解决这些问题，Hash一致性算法（一致性Hash算法）诞生了！\n四、一致性Hash算法的神秘面纱一致性Hash算法也是使用取模的方法，只是，刚才描述的取模法是对服务器的数量进行取模，而一致性Hash算法是对2^32取模，什么意思呢？简单来说，一致性Hash算法将整个哈希值空间组织成一个虚拟的圆环，如假设某哈希函数H的值空间为0-2^32-1（即哈希值是一个32位无符号整形），整个哈希环如下：\n\n整个空间按顺时针方向组织，圆环的正上方的点代表0，0点右侧的第一个点代表1，以此类推，2、3、4、5、6……直到2^32-1，也就是说0点左侧的第一个点代表2^32-1， 0和2^32-1在零点中方向重合，我们把这个由2^32个点组成的圆环称为Hash环。\n下一步将各个服务器使用Hash进行一个哈希，具体可以选择服务器的IP或主机名作为关键字进行哈希，这样每台机器就能确定其在哈希环上的位置，这里假设将上文中四台服务器使用IP地址哈希后在环空间的位置如下：\n\n接下来使用如下算法定位数据访问到相应服务器：将数据key使用相同的函数Hash计算出哈希值，并确定此数据在环上的位置，从此位置沿环顺时针“行走”，第一台遇到的服务器就是其应该定位到的服务器！\n例如我们有Object A、Object B、Object C、Object D四个数据对象，经过哈希计算后，在环空间上的位置如下：\n\n根据一致性Hash算法，数据A会被定为到Node A上，B被定为到Node B上，C被定为到Node C上，D被定为到Node D上。\n五、一致性Hash算法的容错性和可扩展性现假设Node C不幸宕机，可以看到此时对象A、B、D不会受到影响，只有C对象被重定位到Node D。一般的，在一致性Hash算法中，如果一台服务器不可用，则受影响的数据仅仅是此服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它不会受到影响，如下所示：\n\n下面考虑另外一种情况，如果在系统中增加一台服务器Node X，如下图所示：\n\n此时对象Object A、B、D不受影响，只有对象C需要重定位到新的Node X ！一般的，在一致性Hash算法中，如果增加一台服务器，则受影响的数据仅仅是新服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它数据也不会受到影响。\n综上所述，一致性Hash算法对于节点的增减都只需重定位环空间中的一小部分数据，具有较好的容错性和可扩展性。 \n六、Hash环的数据倾斜问题一致性Hash算法在服务节点太少时，容易因为节点分部不均匀而造成数据倾斜（被缓存的对象大部分集中缓存在某一台服务器上）问题，例如系统中只有两台服务器，其环分布如下：\n\n此时必然造成大量数据集中到Node A上，而只有极少量会定位到Node B上。为了解决这种数据倾斜问题，一致性Hash算法引入了虚拟节点机制，即对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点。具体做法可以在服务器IP或主机名的后面增加编号来实现。\n例如上面的情况，可以为每台服务器计算三个虚拟节点，于是可以分别计算 “Node A#1”、“Node A#2”、“Node A#3”、“Node B#1”、“Node B#2”、“Node B#3”的哈希值，于是形成六个虚拟节点：\n\n同时数据定位算法不变，只是多了一步虚拟节点到实际节点的映射，例如定位到“Node A#1”、“Node A#2”、“Node A#3”三个虚拟节点的数据均定位到Node A上。这样就解决了服务节点少时数据倾斜的问题。在实际应用中，通常将虚拟节点数设置为32甚至更大，因此即使很少的服务节点也能做到相对均匀的数据分布。\n七、总结上文中，我们一步步分析了什么是一致性Hash算法，主要是考虑到分布式系统每个节点都有可能失效，并且新的节点很可能动态的增加进来的情况，如何保证当系统的节点数目发生变化的时候，我们的系统仍然能够对外提供良好的服务，这是值得考虑的！\n","slug":"MIDDLEWARE/Redis - Redis分布式算法原理","date":"2023-07-06T17:05:32.000Z","categories_index":"Hash,MIDDLEWARE","tags_index":"Redis,Node,png","author_index":"dandeliono"},{"id":"86b13b8415e2a3ceb3c7a4d2cff47573","title":"小米路由器 AX9000 开发版固件直接获取 SSH","content":"小米路由器 AX9000 开发版固件直接获取 SSH1.下载安装开发版固件官网下载: http://www.miwifi.com/miwifi_download.html下载地址: miwifi_ra70_all_develop_1.0.140.binmd5sum: a0221ec10e2197922c975bc5cf961324sha256sum: 4471d3d9d1e047429e3baf7abc06e79f2ff949af570d9d0c2c39aea018252b66\n\n直接在界面更新即可。\n2.在路由器界面安装并开启 docker 服务，并安装第三方管理服务，打开 docker 管理界面\n3.在容器管理界面单击左上角 “Add container”，进入容器部署界面，按如下参数部署容器使用 busybox 镜像\n开启 tty\n挂载主机根目录\n最后，单击 “Deploy the container” 部署容器\n4.返回容器列表，单击 busybox 容器后的回形针图标\n5.chroot 到主机根目录，获取 SSH12chroot /mntvi /etc/init.d/dropbear\n\n编辑 &#x2F;etc&#x2F;init.d&#x2F;dropbear 注释以下几行：\n1234567891011121314151617start_service()&#123;                                                        [ -s /etc/dropbear/dropbear_rsa_host_key ] || keygen        . /lib/functions.sh        . /lib/functions/network.sh        config_load &quot;$&#123;NAME&#125;&quot;        config_foreach dropbear_instance dropbear&#125;\n\n\n最后启动 ssh 服务：\n1/etc/init.d/dropbear start\n\n\n6.修改 root 密码或添加 authorized_keys修改 root 密码\n1passwd root\n\n添加 authorized_keys，注意只支持 rsa 格式的公钥\n1vi /etc/dropbear/authorized_keys\n","slug":"OTHER/小米路由器 AX9000 开发版固件直接获取 SSH","date":"2023-06-21T00:09:21.000Z","categories_index":"com,OTHER","tags_index":"https,blog,miwifi","author_index":"dandeliono"},{"id":"19cbf23407e01413597628d6a1cf3439","title":"MATCH_RECOGNIZE","content":"MATCH_RECOGNIZE本文内容\n语法\n限制持续时间\nPARTITION BY\nMATCH 后跳转到下一行\n措施\nPATTERN\n模式限定符\n定义\n聚合方法\n限制\n另请参阅\n\nMATCH_RECOGNIZE 子句用于通过数据流搜索一组事件。 此子句使你可以使用正则表达式和聚合方法定义事件模式，以便从匹配项验证和提取值。\n下面的示例演示 MATCH_RECOGNIZE 子句的基本结构：\n123456789101112131415SELECT *INTO output FROM input TIMESTAMP BY time\tMATCH_RECOGNIZE (\t\tLIMIT DURATION (minute, 1)\t\tPARTITION BY tollBoothId\t\tMEASURES\t\t\tLast(Toyota.LicensePlate) AS toyotaLicensePlate,\t\t\tLast(Lexus.LicensePlate) AS lexusLicensePlate\t\tAFTER MATCH SKIP TO NEXT ROW\t\tPATTERN (Toyota+ Ford* Lexus+)\t\tDEFINE\t\t\tToyota AS Toyota.make = &#x27;Toyota&#x27;,\t\t\tFord AS Ford.make = &#x27;Ford&#x27;,\t\t\tLexus AS Lexus.make = &#x27;Lexus&#x27;\t) AS T \n\nMATCH_RECOGNIZE 的每个匹配项的匹配输出为默认值，这是唯一可用的匹配项。 这意味着匹配每个匹配生成一行结果，并且不返回匹配的行。\n1234567891011121314151617181920212223242526272829303132SELECT_star_query_definitionMATCH_RECOGNIZE (\tLIMIT DURATION (time_unit, time)\tPARTITION BY column_alias\tMEASURES\t\texpression AS column_alias [,...n]\tAFTER MATCH SKIP TO NEXT ROW\tPATTERN ( &lt;pattern_group&gt; )\tDEFINE\t\tpattern_name AS boolean_expression [,…n]) AS column_alias&lt;pattern_group&gt; ::=&#123;\t&lt;pattern_name_modifier&gt; [ | &lt;pattern_group&gt; ]*&#125;&lt;pattern_name_modifier&gt; ::=&#123;\t&lt;pattern_atom&gt; [ &lt;pattern_atom&gt; ]*&#125;&lt;pattern_atom&gt; ::=&#123;\t[ pattern_name | ( &lt;pattern_group&gt; ) ] [ &lt;pattern_modifier&gt; ]?&#125;&lt;pattern_name&gt; :: =&#123;\tname | .&#125;&lt;pattern_modifier&gt; ::=&#123;\t* | + | ?&#125; \n\n限制持续时间用于定义搜索模式的时间窗口。 事件按时间和时间戳排序，可用于 SELECT 子句以指定时间字段。\n分区允许在列名称上进行匹配并对其进行分区。 匹配将在 partition 语句指定的每个唯一键上发生。 这使得单个查询能够在所有键上进行匹配，并生成单独的匹配项，每个键一个。\n此 skip 子句定义从事件 s开始匹配模式后，下一次尝试匹配模式将在事件 s + 1开始。 在这种情况下，匹配可以重叠，因为模式可以包含内部的另一种模式的开头。 这是唯一可用的 skip 子句。\n度量值用于使用聚合方法定义匹配项中的投影值。 例如， LAST(A.id) AS aid 将输出在名称与字段名称 aid 匹配 A 的所有事件中找到的最后一个 id 值。\n可在度量值中使用分类器函数输出与输入事件匹配的模式名称。 函数返回一个字符串列表，其中每个字符串都具有与事件匹配的模式名称。\n模式定义要在数据流上搜索的事件的正则表达式。 模式变量是用户定义的，由空格分隔。 与和 ***** 类似 + 的修饰符可用于在匹配事件时修改变量的频率。\n1PATTERN (A+ (B | C)) \n\n此示例中的 模式至少定义一次变量， 后跟 B 或 C的串联。\n模式限定符用于更改如何在数据流中映射模式，以及定义模式需要匹配才能有效的次数。 以下限定符可用：\n\n“*”-零次或多次\n“+”-一次或多次\n“？”-零次或一次\n“|”-一种模式或其他模式\n\n示例： \n1PATTERN (A? B+) \n\n此示例定义 了一个 0 或1的时间，后跟 B 至少一次。\n定义指定用于将模式变量与事件进行匹配的规则。 规则是针对数据流中的聚合值的布尔表达式。\n123DEFINE    A AS Last(A.bigint) &gt; 5,    B AS Last(A.bigint) &lt; B.bigint \n\n此示例定义规则 a 和 b ，其中的最后 一个 值大于5，而 b 的最后 一个 值小于 b的当前值。当不在定义表达式上使用聚合函数时，所计算的当前事件将绑定到模式变量，例如，在 node.js 上，****b 值来自当前正在计算的事件。\n只能按顺序访问定义的模式，如果模式 a 在模式 B之前定义， 则 无法引用 B。\n允许\n12345...DEFINEA AS Last(A.value),B AS Max(A.value) + Max(B.value),... \n\n不允许\n12345...DEFINEA AS Last(A.value) + Last(B.Value),B AS Max(A.value) + Max(B.value),... \n\n以下聚合方法可用于度量值和定义：\n\n最小–迄今为止聚合的最小数目。\nMax –迄今为止聚合的最大数目。\nFirst –聚合的第一个值。\nLast –迄今为止聚合的最后一个值。\n\n示例： \n重填高压力 tanks 是一种危险的过程，需要密切监视，因为水箱的压力也增加了温度，压力需要增加，使箱在重填时冷却的时间更长。\n在此示例中，开发人员希望在开始增加压力时监视高压力箱的重填。 箱开始重填，在3分钟内不能增加压力，否则箱 overheats 并可能导致灾难性故障。\n以下查询可用于监视进度：\n12345678910111213SELECT *INTO output FROM input TIMESTAMP BY timeMATCH_RECOGNIZE (\tLIMIT DURATION (minute, 3)\tMEASURES \t\tMAX(Dangerous.pressure) as pressure,\t\tClassifier() as patterns\tAFTER MATCH SKIP TO NEXT ROW\tPATTERN (Normal+ Dangerous+)\tDEFINE\t\tNormal AS Normal.isFilling = 1,\t\tDangerous AS Max(Dangerous.pressure) &gt; 2* Max(Normal.pressure)) AS T \n\n此查询与填充箱的任何事件匹配 正常 ，以防在3分钟内压力超过 正常 填充的两倍，而不是使用 危险 模式的最大压力读数触发事件。\n\n只有字段值可用于聚合。 不能在聚合调用中调用函数。\n允许\n1234...DEFINE      A AS Max(A.value) &gt; 5,... \n\n不允许\n1234...DEFINE      A AS Max(udf.myUdf(A.value)) &gt; 5,... \n\n只有单个字段才能作为输入参数提供给聚合函数。\n允许\n1234...    DEFINE  \t    A AS Max(A.value) &gt; 5,  ... \n\n不允许\n1234... DEFINE     A AS Max(A.value1 + A.value2) &gt; 5, ... \n\n\n查询语言元素\n\n\n","slug":"BIGDATA/MATCH_RECOGNIZE","date":"2023-06-13T17:53:37.000Z","categories_index":"MATCH,BIGDATA","tags_index":"RECOGNIZE,skip,partition","author_index":"dandeliono"},{"id":"e7a83bdf4e44456c4b6fc5db7d1c3bb4","title":"Linux Tar Split压缩解压缩分片压缩解压缩","content":"Linux Tar Split压缩解压缩分片压缩解压缩文件压缩tar -zcvf test.tar.gz test/\n-z：是否同时具有gzip属性，是否需要gzip压缩；-c：建立一个压缩文件指令；-v：压缩过程中是否显示文件；-f ：使用档名；\n文件分割split -b 40M -d -a 2 test.tar.gz test.tar.gz.\n-b：指定每个文件的大小，单位可以为B、K、M ；-d：使用数字而不是字母作为后缀名 ；-a：后缀名长度，默认为2；\n文件压缩加分割tar -cvf - test | split -b 1M - test.tar.gz.\n注意一下指令中的两个“-”，如果分开执行，就不用”-”。为什么有这个”-”？\n\n\n\n\n\n\n\n\n\nman tar-f, –file [HOSTNAME:]FUse archive file or device F (default “-”, meaning stdin&#x2F;stdout). Note that “&#x2F;dev&#x2F;stdout” is not equivalent to “-”.Using “&#x2F;dev&#x2F;stdout” explicitly can lead to corrupted archive, especially when coupled with “-v”.\n文件合并cat test.tar.gz.* &gt; test_new.tar.gz\n文件解压tar -zxvf test_new.tar.gz -C ./Directory/\n-z：是否同时具有gzip属性，是否需要gzip压缩；-x：解压缩指令；-v：压缩过程中是否显示文件；-f ：使用档名；\n","slug":"LINUX/Linux Tar Split压缩解压缩分片压缩解压缩","date":"2023-06-12T14:15:57.000Z","categories_index":"tar,LINUX","tags_index":"test,gzip,stdout","author_index":"dandeliono"},{"id":"bdf0bc79680e5abcf1a951c9276dec8c","title":"Esper学习之十四：Pattern（一）","content":"Esper学习之十四：Pattern（一）1. Pattern Atoms and Pattern operatorsPattern是通过原子事件和操作符组合在一起构成模板。原子事件有3类，操作符有4类，具体如下：\n原子事件：**1).**普通事件：包括POJO，Map，Array，XML**2).**时间事件：包括间隔n个时间单位、crontab**3).**自定义插件：用于观察特定事件的发生\n操作符：**1).**重复操作符：every, every-distinct, [num] and until**2).**逻辑操作符：and, or, not**3).**顺序操作符：-&gt;（Followed by）**4).**事件生命周期操作符：timer:within, timer:withinmax, while-expression, 自定义插件\n关于操作符，自然会有优先级，具体如下：\n\n\n\nPrecedence\nOperator\nDescription\nExample\n\n\n\n1\n\n\n\n\n\nguard postfix\n | where timer:within and while (expression) (incl. withinmax and plug-in pattern guard) | \nMyEvent where timer:within(1 sec)\n || 2 | \nunary\n | every, not every, distinct | \nevery MyEventtimer:interval(5 min) and not MyEvent\n || 3 | repeat | [num], until | \n[5] MyEvent\n[1..3] MyEvent until MyOtherEvent\n || 4 | and | and | every (MyEvent and MyOtherEvent) || 5 | or | or | every (MyEvent or MyOtherEvent) || 6 | followed by | -&gt; | every (MyEvent -&gt; MyOtherEvent) |\n上面的内容各位可以先有个印象，方便理解之后的详解。\n2. Pattern Filter ExpressionPattern的Filter表达式和普通的表达式没有区别，我就不展开讲解了，各位看看下面几个例子就好，除了Filter之外的东西暂时不用关心是什么意思。\n1). every e1&#x3D;RfidEvent -&gt; e2&#x3D;RfidEvent(assetId&#x3D;e1.assetId)**2).**every e1&#x3D;RfidEvent -&gt; e2&#x3D;RfidEvent(MyLib.isInRadius(e1.x, e1.y, x, y) and zone in (1, e1.zone))**3).**every (RfidEvent(zone &gt; 1) and RfidEvent(zone &lt; 10))\n3. Controlling Event Consumption       上面说到了Filter，因为Pattern可以由多个原子事件组成，那么Filter自然也会有多个，正常情况下，所有的Filter都会对进入引擎的事件进行判定，但是我们也有只需要判定一次的时候，只要满足了某个Filter，那么其他的Filter就不用管这个事件了。Esper考虑到了这个需求，我们只需要在Filter表达式后面加个@consume注解即可，此注解可以跟随数字，表示过滤的优先级。默认优先级为1，数值越大优先级越高。\n为了结合上面几节的知识，我给了个完整的实例：\n执行结果：\n\nEPL1: every (a&#x3D;example.ConsumeEvent(id &#x3D; 1)@consume and b&#x3D;example.ConsumeEvent(name &#x3D; ‘luonq’))  \n\nSend Event: ConsumeEvent{id&#x3D;1, name&#x3D;’luonq’, age&#x3D;0}  \n\nSend Event: ConsumeEvent{id&#x3D;2, name&#x3D;’luonq’, age&#x3D;0}  \n\na: ConsumeEvent{id&#x3D;1, name&#x3D;’luonq’, age&#x3D;0}  \n\nb: ConsumeEvent{id&#x3D;2, name&#x3D;’luonq’, age&#x3D;0}  \n\nEPL2: every (a&#x3D;example.ConsumeEvent(id &#x3D; 1)@consume(2) or b&#x3D;example.ConsumeEvent(name &#x3D; ‘luonq’)@consume(3) or c&#x3D;example.ConsumeEvent(age &gt; 2))  \n\nSend Event: ConsumeEvent{id&#x3D;1, name&#x3D;’luonq’, age&#x3D;3}  \n\na: null  \n\nb: ConsumeEvent{id&#x3D;1, name&#x3D;’luonq’, age&#x3D;3}  \n\nc: null  \n\nSend Event: ConsumeEvent{id&#x3D;1, name&#x3D;’luonqin’, age&#x3D;1}  \n\na: ConsumeEvent{id&#x3D;1, name&#x3D;’luonqin’, age&#x3D;1}  \n\nb: null  \n\nc: null  \n\nSend Event: ConsumeEvent{id&#x3D;3, name&#x3D;’luonqin’, age&#x3D;5}  \n\na: null  \n\nb: null  \n\nc: ConsumeEvent{id&#x3D;3, name&#x3D;’luonqin’, age&#x3D;5}\n\n\n  \n这里先简单说明下，every关键字表示引擎把每个事件都进行Pattern的匹配，而不管上一个匹配是否完成。or和and就是或和且的意思，表示满足某个以及满足所有。\n去掉consume的执行结果：\n\nEPL1: every (a&#x3D;example.ConsumeEvent(id &#x3D; 1) and b&#x3D;example.ConsumeEvent(name &#x3D; ‘luonq’))  \n\nSend Event: ConsumeEvent{id&#x3D;1, name&#x3D;’luonq’, age&#x3D;0}  \n\na: ConsumeEvent{id&#x3D;1, name&#x3D;’luonq’, age&#x3D;0}  \n\nb: ConsumeEvent{id&#x3D;1, name&#x3D;’luonq’, age&#x3D;0}  \n\nSend Event: ConsumeEvent{id&#x3D;2, name&#x3D;’luonq’, age&#x3D;0}  \n\nEPL2: every (a&#x3D;example.ConsumeEvent(id &#x3D; 1) or b&#x3D;example.ConsumeEvent(name &#x3D; ‘luonq’) or c&#x3D;example.ConsumeEvent(age &gt; 2))  \n\nSend Event: ConsumeEvent{id&#x3D;1, name&#x3D;’luonq’, age&#x3D;3}  \n\na: ConsumeEvent{id&#x3D;1, name&#x3D;’luonq’, age&#x3D;3}  \n\nb: null  \n\nc: null  \n\nSend Event: ConsumeEvent{id&#x3D;1, name&#x3D;’luonqin’, age&#x3D;1}  \n\na: ConsumeEvent{id&#x3D;1, name&#x3D;’luonqin’, age&#x3D;1}  \n\nb: null  \n\nc: null  \n\nSend Event: ConsumeEvent{id&#x3D;3, name&#x3D;’luonqin’, age&#x3D;5}  \n\na: null  \n\nb: null  \n\nc: ConsumeEvent{id&#x3D;3, name&#x3D;’luonqin’, age&#x3D;5}\n\n\n  \n上面的例子可能看得不是很懂，不过没关系，等到后面讲操作符进行详解后再来回顾就很简单了。\n","slug":"BIGDATA/Esper学习之十四：Pattern（一）","date":"2023-06-09T11:31:58.000Z","categories_index":"ConsumeEvent,BIGDATA","tags_index":"name,age,every","author_index":"dandeliono"},{"id":"247a554806e84b69903a33066dd8bcf1","title":"Esper学习之十二：EPL语法（八）","content":"Esper学习之十二：EPL语法（八）  今天的内容十分重要，在Esper的应用中是十分常用的功能之一。它是一种事件集合，我们可以对这个集合进行增删查改，所以在复杂的业务场景中我们肯定不会缺少它。它就是Named Window。\n       由于本篇篇幅较长，希望各位童鞋慢慢阅读，并仔细研究文档中或者我给出的例子。\n1.Create Named Window\n       本篇的开头有说过named window是一种事件集合，它可以存储一种类型或多种类型的事件。如果我们不移除named window中的事件，那么事件应该存在生命周期，否则事件过多会有内存溢出的风险。所以我们在创建named window的时候会附带事件的过期策略，而各种过期策略就是用view来表达。named window的创建方式有三种：a.用已有的事件结构创建；b.自定义named window存放的事件格式；c.将已有的named window中包含的事件写入到新的named window中，即复制named window。\na.Creation by Modelling after an Existing Type语法如下：\n\n[context context_name] create window window_name.view_specifications [as] [select list_of_properties from] event\n\n  \n1）context是关键字，后面跟着之前定义的context的名称。关于context的内容，请参看《Esper学习之三：Context》2）create window后跟着要创建的named window的名字，且具有唯一性。名字后面紧跟着的“.”是用来连接事件过期策略的，即view。常用的view有win:length，win:length_batch，win:time，win:time_batch，std:unique，std:groupwin及自定义view等等，并且特定的view可以连用。PS：view的相关内容我打算在EPL语法讲解完之后就为各位呈上，以免影响其他章节的学习。3）select子句表示将某个事件定义中的全部或者某些属性作为named window所维护的事件属性。如果将某个事件的所有属性都加入到named window中，则可以通过select子句前的as连接事件名称，并且省略select子句。这个将在接下来的例子进行说明。\n举几个例子说明下：\n\n&#x2F;&#x2F; FruitWindow保持最近10分钟的Apple事件  \n\ncreate window FruitWindow.win:time(10 min) as Apple  \n\n&#x2F;&#x2F; FruitWindow保持最近5分钟的Apple事件，但只包含size和price属性  \n\ncreate window FruitWindow.win:time(5 min) as select size, price from Apple\n\n\n  \nb.Creation By Defining Columns Names and Types和上一种创建方式不同，Esper允许自己指定named window能维护哪些属性，而不是通过别的事件定义来指定。语法如下：\n\n[context context_name]  \ncreate window window_name.view_specifications [as] (column_name column_type[,column_name column_type [,…]])\n\n  \ncolumn_name表示属性名称，column_type表示属性类型。属性可以是一个，也可以是多个，多个用逗号分隔。示例如下：\n\n&#x2F;&#x2F; 创建包含普通属性的named window  \n\ncreate window SecurityEvent.win:time(30 sec) (ipAddress string, userId String, numAttempts int, properties String[])  \n\n&#x2F;&#x2F; 创建包含事件类型属性的named window  \n\ncreate schema SecurityData (name String, roles String[])  \n\ncreate window SecurityEvent.win:time(30 sec) (ipAddress string, userId String, secData SecurityData, historySecData SecurityData[])\n\n\n  \nc.Populating a Named Window from an Existing Named Window     这一节在文档中并没有放到create named window里，不过我认为这也是一种创建方式，所以就放在这里讲解了。这种创建方式很容易懂，就是将已经存在的named window中的event写到新的named window中。语法如下：\n\n[context context_name] create window window_name.view_specifications as windowname insert [where filter_expression]\n\n  \nwindowname后面紧跟insert，表示将该window中的事件插入到新建的named window中。where filter_expression表示过滤插入的事件。实例如下：\n\ncreate window ScratchBuyOrders.win:time(10) as OrdersNamedWindow insert where side &#x3D; ‘buy’\n\n  \n以上就是三种创建方式，另外还有一个注解的使用，可以使触发时往Listener中传入的事件是数组或者map形式。这个需要各位自己调试的时候才能看到效果。实例如下：\n\n&#x2F;&#x2F; 以数组形式反馈给UpdateListener  \n\n@EventRepresentation(array&#x3D;true)create window FooWindow.win:time(5 sec) as (string prop1)  \n\n&#x2F;&#x2F; 以Map形式反馈给UpdateListener  \n\n@EventRepresentation(array&#x3D;false)create window FooWindow.win:time(5 sec) as (string prop1)  \n\n… equals …  \n\ncreate window FooWindow.win:time(5 sec) as (string prop1)\n\n\n  \n2.Inserting Into Named Windows\n       创建好named window以后，我们就可以往里面插入事件了。插入的语法很简单，基本上和insert into语法一样。关于insert into，请参见《Esper学习之八：EPL语法（四）》。下面直接举几个例子：\n\n1）  \n\n&#x2F;&#x2F; create named window with some properties of OrderEvent  \n\ncreate window OrdersWindow.win:keepall() as select symbol, volume, price from OrderEvent  \n\n&#x2F;&#x2F; insert into events to named window  \n\ninsert into OrdersWindow(symbol, volume, price) select name, count, price from FXOrderEvent  \n\n… equals …  \n\ninsert into OrdersWindow select name as symbol, vol as volume, price from FXOrderEvent  \n\n2）  \n\n&#x2F;&#x2F; create named window with POJO  \n\ncreate window OrdersWindow.win:time(30) as com.mycompany.OrderEvent  \n\n&#x2F;&#x2F; insert into events to named window  \n\ninsert into OrdersWindow select * from com.mycompany.OrderEvent(symbol&#x3D;’IBM’)  \n\n3）  \n\n&#x2F;&#x2F; create named window  \n\ncreate window OrdersWindow.win:time(30) as select *, price as derivedPrice from OrderEvent  \n\n&#x2F;&#x2F; insert into events to named window with custom function  \n\ninsert into OrdersWindow select *, MyFunc.func(price, percent) as derivedPrice from OrderEvent  \n\n4）  \n\n&#x2F;&#x2F; create named window  \n\ncreate window OrdersWindow.win:time(30) as select *, price as priceTotal from OrderEvent  \n\n&#x2F;&#x2F; insert into events to named window  \n\ninsert into OrdersWindow select *, price * unit as priceTotal from ServiceOrderEvent  \n\n5)  \n\n&#x2F;&#x2F; create a named window for the base class  \n\ncreate window OrdersWindow.std:unique(name) as select * from ProductBaseEvent  \n\n&#x2F;&#x2F; The ServiceProductEvent class subclasses the ProductBaseEvent class  \n\ninsert into OrdersWindow select * from ServiceProductEvent  \n\n6）  \n\n&#x2F;**  \n\n * interface InterfaceEvent {  \n\n *         public int getPrice();  \n\n *         public String getName();  \n\n * }  \n\n *&#x2F;  \n\n&#x2F;&#x2F; create a named window for the inteface  \n\ncreate window InterfaceWindow.win: time(2 sec) as select * from InterfaceEvent  \n\n&#x2F;&#x2F; The InterfaceEventImpl is the implements for InterfaceEvent  \n\ninsert into InterfaceWindow select * from InterfaceEventImpl\n\n\n  \n       从以上几个例子可以看得出，创建named window时用的事件类型只是用了该事件类型的属性定义，只要insert的事件列出了对应的属性名称就可以，当然属性的数据类型也得对应。如果属性名不对应，可以用as来重命名，就像例3和例4那样。第5个例子表明子类可以插入到父类定义的named window中。第六个例子表明实现类可以插入到用接口定义的named window中。\n3.Selecting From Named Windows\n       这一节主要讲解如何查询named window中的事件。准确来说这并不是一种无条件的查询方式，监听器监听查询语句以后，必须发送对应的事件(创建window时指定的事件类型)，然后先看是否满足named window的输出条件，再看是否满足查询语句的输出条件，所以并不是named window中的事件都会发给监听器。咱们先看一个例子：\n\n&#x2F;&#x2F; first, you should create the AllOrdersNamedWindow  \n\ncreate named window AllOrdersNamedWindow.win:length_batch(3) as OrderEvent  \n\nselect * from AllOrdersNamedWindow\n\n\n   \n       这个select句子看似很简单，实际上并不是查询AllOrdersNamedWindow中的事件。实际过程是这样的：首先引擎检查AllOrdersNamedWindow中有多少事件，因为过期策略是length_batch(3），意思是当AllOrdersNamedWindow中包含3个事件，才会将这3个事件同时输出，然后再等待新的3个事件，然后再一起输出这3个事件。。。所以当输入某一个事件时，若不满足3个数量，则select语句不会返回任何内容。若满足3个数量，则监听select语句的监听器会得到AllOrdersNamedWindow中的最新的3各事件。\n下面的几个句子都只有在length_batch(3)满足条件时才能有事件输出。    \n如果想查到上一次过期的3个事件，则查询语句改成下面的样子：\n\nselect rstream * from AllOrdersNamedWindow\n\n   \n如果只想查询AllOrdernamedWindow里面的某几个属性，并且加上一些限制条件查看特定的事件，则查询语句改成下面的样子：\n\nselect symbol, avg(price) from AllOrdersNamedWindow(sector&#x3D;’energy’)  group by symbol\n\n  \n       因为上面所说的特点，所以查询语句可以在事件发送后再建立，也就是说我什么时候想查什么时候再建句子，named windows里有多少事件就如实返回多少。下面举个完整的例子总结一下：\n\npackage example;  \n\nimport com.espertech.esper.client.EPAdministrator;  \n\nimport com.espertech.esper.client.EPRuntime;  \n\nimport com.espertech.esper.client.EPServiceProvider;  \n\nimport com.espertech.esper.client.EPServiceProviderManager;  \n\nimport com.espertech.esper.client.EPStatement;  \n\nimport com.espertech.esper.client.EventBean;  \n\nimport com.espertech.esper.client.UpdateListener;  \n\nclass SelectEvent {  \n\n    private int price;  \n\n    private String name;  \n\n    public int getPrice() {  \n\n        return price;  \n\n    }  \n\n    public void setPrice(int price) {  \n\n        this.price &#x3D; price;  \n\n    }  \n\n    public String getName() {  \n\n        return name;  \n\n    }  \n\n    public void setName(String name) {  \n\n        this.name &#x3D; name;  \n\n    }  \n\n    @Override  \n\n    public String toString() {  \n\n        return “name&#x3D;”+name+”, price&#x3D;”+price;  \n\n    }  \n\n}  \n\nclass SelectNamedWindowListener implements UpdateListener{  \n\n    public void update(EventBean[] newEvents, EventBean[] oldEvents) {  \n\n        if (newEvents !&#x3D; null) {  \n\n            System.out.println(“There is “+newEvents.length+” events to be return!”);  \n\n            for (int i &#x3D; 0;  i &lt; newEvents.length;i++) {  \n\n                System.out.println(newEvents[i].getUnderlying());  \n\n            }  \n\n        }  \n\n    }  \n\n}  \n\npublic class SelectNamedWindowTest{  \n\n    public static void main(String[] args) {  \n\n        EPServiceProvider epService &#x3D; EPServiceProviderManager.getDefaultProvider();  \n\n        EPAdministrator admin &#x3D; epService.getEPAdministrator();  \n\n        EPRuntime runtime &#x3D; epService.getEPRuntime();  \n\n        String selectEvent &#x3D; SelectEvent.class.getName();  \n\n        String epl1 &#x3D; “create window SelectNamedWindow.win:length_batch(3) as “ + selectEvent;  \n\n        admin.createEPL(epl1);  \n\n        System.out.println(“Create named window: create window SelectNamedWindow.win:length_batch(3) as “+selectEvent);  \n\n        String epl2 &#x3D; “insert into SelectNamedWindow select * from “ + selectEvent;  \n\n        admin.createEPL(epl2);  \n\n        SelectEvent se1 &#x3D; new SelectEvent();  \n\n        se1.setName(“se1”);  \n\n        se1.setPrice(1);  \n\n        System.out.println(“Send SelecEvent1 “ + se1);  \n\n        runtime.sendEvent(se1);  \n\n        SelectEvent se2 &#x3D; new SelectEvent();  \n\n        se2.setName(“se2”);  \n\n        se2.setPrice(2);  \n\n        System.out.println(“Send SelecEvent2 “ + se2);  \n\n        runtime.sendEvent(se2);  \n\n        String epl3 &#x3D; “select * from SelectNamedWindow(price&gt;&#x3D;2)”;  \n\n        EPStatement state3 &#x3D; admin.createEPL(epl3);  \n\n        state3.addListener(new SelectNamedWindowListener());  \n\n        System.out.println(“Register select sentence: select * from SelectNamedWindow(price&gt;&#x3D;2)”);  \n\n        SelectEvent se3 &#x3D; new SelectEvent();  \n\n        se3.setName(“se3”);  \n\n        se3.setPrice(3);  \n\n        System.out.println(“Send SelecEvent3 “ + se3 + “\\n”);  \n\n        runtime.sendEvent(se3);  \n\n    }  \n\n}\n\n\n  \n执行结果：\n\nCreate named window: create window SelectNamedWindow.win:length_batch(3) as example.SelectEvent  \n\nSend SelecEvent1 name&#x3D;se1, price&#x3D;1  \n\nSend SelecEvent2 name&#x3D;se2, price&#x3D;2  \n\nRegister select sentence: select * from SelectNamedWindow(price&gt;&#x3D;2)  \n\nSend SelecEvent3 name&#x3D;se3, price&#x3D;3  \n\nThere is 2 events to be return!  \n\nname&#x3D;se2, price&#x3D;2  \n\nname&#x3D;se3, price&#x3D;3\n\n\n  \n      上面的例子有一段注释“select * from SelectamedWindow.win:time(3 sec)”，实际上是因为win:time(3 sec)这个东西，之前create named window的时候，已经使用了一个win的view，所以这里就不能再使用win的view了。这个与view相关，就一笔带过了。另外还有一点，在select句子中的filter如果使用了variable，当变量的值在句子创建后改变了，引擎不会读取新的值，这个需要额外注意。\n4.Triggered Select on Named Windows：On Select\n       上一节说到的查询方式实际上并不是很好用，而这一节就提供了一个非常简单的查询办法。他是通过发送一个触发事件即可得到当前window里有些什么事件，你还可以设置这个触发事件满足什么要求才可触发，或者这个触发事件和window中的事件达到某种关联后输出符合这个关联的事件或事件的部分属性。先来看看语法：\n\non event_type[(filter_criteria)] [as stream_name]  \nselect select_list from window_name [as stream_name]  \n[where criteria_expression] [group by grouping_expression_list] [having grouping_search_conditions] [order by order_by_expression_list]\n\n  \n       event_type表示用来触发的事件，可以是任何一种事件类型，也可以是pattern（用来表达较为复杂的触发条件）。后面用括号括起来的包含了触发事件的限制条件，必须满足这个里面约定的条件才可用来触发查询。as stream_name为可选参数，主要用于具体的查询语句或者与window做关联用。select语句就不说了，和普通的没太大区别。后面的where条件限制了查询结果，满足结果的才可返回给监听器。group by、having、order by和之前说的用法一样，这里不做说明了。我们先来看几个简单的例子：\n\n&#x2F;&#x2F; QueryEvent作为触发事件，查询OrderNamedWindow中的所有事件，并附带触发的事件作为结果返回（什么意思？）  \n\non QueryEvent select * from OrdersNamedWindow  \n\n&#x2F;&#x2F; QueryEvent作为触发事件，查询OrderNamedWindow中的所有事件作为结果返回（懂了吧）  \n\non QueryEvent select win.* from OrdersNamedWindow as win  \n\n&#x2F;&#x2F; volume大于0的QueryEvent作为触发事件，查询OrderNamedWindow中的事件，且事件的symbol要与QueryEvent的symbol一样，返回满足条件的window事件的symbol值，以及QueryEvent的symbol和volume值  \n\non QueryEvent(volume&gt;0) as query  \n\nselect query.symbol, query.volume, win.symbol from OrdersNamedWindow as win  \n\nwhere win.symbol &#x3D; query.symbol  \n\n&#x2F;&#x2F; group by, having, order by的用法与前面所讲无差别，都是针对查询出来的事件  \n\non QueryEvent  \n\nselect symbol, sum(volume) from OrdersNamedWindow as win  \n\ngroup by symbol having volume &gt; 0 order by symbol  \n\n&#x2F;&#x2F; 每一次OrderNamedWindow有变化并满足限制条件即可触发监听器返回window中的事件。  \n\non OrdersNamedWindow as trig  \n\nselect onw.symbol, sum(onw.volume) from OrdersNamedWindow as onw  \n\nwhere onw.symbol &#x3D; trig.symbol\n\n\n  \n      前两个例子我相信大家已经明白了。如果select子句里是*，则返回的不仅仅是window中的事件，还会返回触发查询的事件，并且返回的多行结果中每行都会包含这个触发事件。第三个例子可以看出as的用法。最后一个例子有些特别，触发的事件就是named window本身，目的就是为了named window变化了就能返回变化后的结果。但是这里的“变化”不是简单的事件有增加或者减少，而是指named window定义时指定的view达到触发条件有输出了，你才能真的看到查询结果。下面列一个完整的例子来概括一下上面说到的几点内容：\n执行结果：\n\nCreate Window:create window OnSelectWindow.win:length(2) as select * from example.OnSelectEvent  \n\nTrigger sentence: on example.OnSelectTrigger(trigger&gt;&#x3D;2) select osw.* from OnSelectWindow as osw  \n\nSend OnSelectEvent 1: name&#x3D;ose1, size&#x3D;1  \n\nSend OnSelectEvent 2: name&#x3D;ose2, size&#x3D;2  \n\nSend OnSelectEvent 3: name&#x3D;ose3, size&#x3D;3  \n\nSend OnSelectTrigger trigger&#x3D;1  \n\nSend OnSelectTrigger trigger&#x3D;2  \n\nTrigger On Select:  \n\nThere is 2 OnSelectEvent in OnSelectWindow!  \n\nname&#x3D;ose2, size&#x3D;2  \n\nname&#x3D;ose3, size&#x3D;3  \n\nTrigger sentence: on OnSelectWindow select osw.* from OnSelectWindow as osw  \n\nSend OnSelectEvent 4(also a Trigger): name&#x3D;ose4, size&#x3D;4  \n\nTrigger On Select:  \n\nThere is 2 OnSelectEvent in OnSelectWindow!  \n\nname&#x3D;ose3, size&#x3D;3  \n\nname&#x3D;ose4, size&#x3D;4  \n\nSend OnSelectEvent 5(also a Trigger): name&#x3D;ose5, size&#x3D;5  \n\nTrigger On Select:  \n\nThere is 2 OnSelectEvent in OnSelectWindow!  \n\nname&#x3D;ose4, size&#x3D;4  \n\nname&#x3D;ose5, size&#x3D;5\n\n\n  \n5.Triggered Delete on Named Windows：On Delete\n       除了可以用on select语句查询named window，还可以用on delete语句删除named window中的事件。语法和on select基本一样，同样能够设置一定条件限制触发事件，以及删除符合特定条件的事件。语法如下：\n\non event_type[(filter_criteria)] [as stream_name] delete from window_name [as stream_name]  \n[where criteria_expression]\n\n  \n和on select不同的是，delete后面不跟属性什么的，因为删除的就是事件，不存在删除事件中的某些属性这种情况。filter_criteria用来限制触发事件，where criteria_expression用来限制要删除的事件。而且没有group by, having, Order by。举例如下：\n执行结果：\n\nCreate Window: create window OnDeleteWindow.win:keepall() as select * from example.OnDeleteEvent  \n\nDelete Trigger: on example.OnDeleteTrigger(trigger&gt;0) as odt delete from OnDeleteWindow as odw where odt.trigger&#x3D;odw.size  \n\nSelect Trigger: on example.OnDeleteTrigger(trigger&#x3D;0) select odw.* from OnDeleteWindow as odw  \n\nSend OnDeleteEvent 1: name&#x3D;ose1, size&#x3D;1  \n\nSend OnDeleteEvent 2: name&#x3D;ose2, size&#x3D;2  \n\nSend OnDeleteEvent 3: name&#x3D;ose3, size&#x3D;3  \n\nSend OnSelectTrigger trigger&#x3D;0  \n\nTrigger On Select:  \n\nThere is 3 OnDeleteEvent in OnDeleteWindow!  \n\nname&#x3D;ose1, size&#x3D;1  \n\nname&#x3D;ose2, size&#x3D;2  \n\nname&#x3D;ose3, size&#x3D;3  \n\nSend OnDeleteTrigger trigger&#x3D;2  \n\nTrigger On Delete:  \n\nThere is 1 OnDeleteEvent to be deleted in OnDeleteWindow!  \n\nname&#x3D;ose2, size&#x3D;2  \n\nSend OnSelectTrigger trigger&#x3D;0  \n\nTrigger On Select:  \n\nThere is 2 OnDeleteEvent in OnDeleteWindow!  \n\nname&#x3D;ose1, size&#x3D;1  \n\nname&#x3D;ose3, size&#x3D;3\n\n\n  \n6.Triggered Select+Delete on Named Windows: the On Select Delete\n        如果我们想在从named window查询出结果的同时删掉查询结果，那么可以使用on select and delete语句。语法和on select语句几乎是一样的，只是多了个delete。语法如下：\n\non trigger  \nselect [and] delete select_list…  \n… (please see on-select for from, group by, having, order by)…\n\n  \n直接上一个完整例子：\n执行结果：\n\nCreate Window: create window OnSelectDeleteWindow.win:keepall() as select * from example.OnSelectDeleteEvent  \n\nSelect and Delete Trigger: on example.OnSelectDeleteTrigger select and delete osw.* from OnSelectDeleteWindow as osw  \n\nSend OnSelectDeleteEvent 1: name&#x3D;osde1, size&#x3D;1  \n\nSend OnSelectDeleteEvent 2: name&#x3D;osde2, size&#x3D;2  \n\nSend OnSelectDeleteEvent 3: name&#x3D;osde3, size&#x3D;3  \n\nSend OnSelectDeleteTrigger trigger&#x3D;1  \n\nTrigger On Select and Delete:  \n\nThere is 3 OnSelectDeleteEvent in OnSelectDeleteWindow!  \n\nname&#x3D;osde1, size&#x3D;1  \n\nname&#x3D;osde2, size&#x3D;2  \n\nname&#x3D;osde3, size&#x3D;3  \n\nSend OnSelectDeleteEvent 4: name&#x3D;osde4, size&#x3D;4  \n\nSend OnSelectDeleteTrigger trigger&#x3D;1  \n\nTrigger On Select and Delete:  \n\nThere is 1 OnSelectDeleteEvent in OnSelectDeleteWindow!  \n\nname&#x3D;osde4, size&#x3D;4\n\n\n  \n7.Updating Named Windows: the On Update clause\n除了查询和删除window里的事件，我们还可以更新事件，而且同样也是通过发送一个特定事件来出发更新操作。语法如下：\n\non event_type[(filter_criteria)] [as stream_name]  \nupdate window_name [as stream_name]  \nset property_name &#x3D; expression [, property_name &#x3D; expression [,…]] [where criteria_expression]\n\n  \nfilter_criteria用来过滤触发事件，criteria_expression用来过滤window中的事件不予更改。和sql类似，set后面跟着要修改的属性及要赋的值，多个属性名值对用逗号分隔。先看一个简单的例子：\n\non NewOrderEvent(volume&gt;0) as myNewOrders  \nupdate AllOrdersNamedWindow as myNamedWindow set price &#x3D; myNewOrders.price  \nwhere myNamedWindow.symbol &#x3D; myNewOrders.symbol\n\n  \n我想不用做太多解释大家就能很容易看懂。不过下面这个例子呢：\n\non UpdateEvent as upd  \nupdate MyWindow as win  \nset field_a &#x3D; 1,  \nfield_b &#x3D; win.field_a, &#x2F;&#x2F; assigns the value 1  \nfield_c &#x3D; initial.field_a &#x2F;&#x2F; assigns the field_a original value before update\n\n  \n       上面的例子中，field_a被更新为1，然后用修改后的field_a赋值给field_b，那么field_b实际上也变成1了。但是如果想用更新前的field_a的值赋给field_c，那么就要写成initial.field_a。initial是关键字，所以不能省略。我们可以用监听器监听on update语句，返回给监听器的就是被更新的事件。结合上面的两点内容，我们来看一个完整的例子：\n执行结果：\n\nCreate Window: create window OnUpdateWindow.win:keepall() as select * from example.OnUpdateEvent  \n\nUpdate Trigger sentence: on example.OnUpdateTrigger(trigger&gt;0) as out update OnUpdateWindow as ouw set size&#x3D;out.trigger, price&#x3D;initial.size where out.trigger&lt;ouw.price  \n\nSend OnUpdateEvent 1: name&#x3D;oue1, size&#x3D;1, price&#x3D;2  \n\nSend OnUpdateEvent 2: name&#x3D;oue2, size&#x3D;2, price&#x3D;3  \n\nSend OnUpdateEvent 3: name&#x3D;oue3, size&#x3D;3, price&#x3D;4  \n\nSend OnUpdateTrigger trigger&#x3D;0  \n\nTrigger On Select:  \n\nThere is 3 OnUpdateEvent in OnUpdateWindow!  \n\nname&#x3D;oue1, size&#x3D;1, price&#x3D;2  \n\nname&#x3D;oue2, size&#x3D;2, price&#x3D;3  \n\nname&#x3D;oue3, size&#x3D;3, price&#x3D;4  \n\nSend OnUpdateTrigger trigger&#x3D;2  \n\nTrigger On Update:  \n\nThere is 2 to be updated in OnUpdateWindow!  \n\nname&#x3D;oue2, size&#x3D;2, price&#x3D;2  \n\nname&#x3D;oue3, size&#x3D;2, price&#x3D;3  \n\nSend OnUpdateTrigger trigger&#x3D;0  \n\nTrigger On Select:  \n\nThere is 3 OnUpdateEvent in OnUpdateWindow!  \n\nname&#x3D;oue1, size&#x3D;1, price&#x3D;2  \n\nname&#x3D;oue2, size&#x3D;2, price&#x3D;2  \n\nname&#x3D;oue3, size&#x3D;2, price&#x3D;3\n\n\n  \n       OnUpdateEvent和平时我们定义的事件不太一样，它实现了Serializable接口。这是因为update更新属性前会复制一份同样的事件暂存，比如initial这种操作就需要更新前的值，所以就需要我们实现序列化接口。如果不想通过代码完成这个序列化要求，也可以通过配置完成，这个就不在这里说了。另外还有以下几点需要注意：a）需要更新的属性一定要是可写的b）XML格式的事件不能通过此语句更新c）嵌套属性不支持更新\n8.Triggered Upsert using the On-Merge Clause\n       除了上面的on select，on update，on delete操作，esper还支持on merge操作。他可以在满足不同条件的情况下完成对应的window中的事件的insert，update和delete操作，所以语法相对前面几种就复杂很多。语法如下：\n\non event_type[(filter_criteria)] [as stream_name] merge [into] window_name [as stream_name]  \n[where criteria_expression]  \nwhen [not] matched [and search_condition] then [  \ninsert [into streamname] [ (property_name [, property_name] [,…]) ]  \nselect select_expression [, select_expression[,…]] [where filter_expression]  \n|  \nupdate set property_name &#x3D; expression [, property_name &#x3D; expression [,…]] [where filter_expression]  \n|  \ndelete [where filter_expression]  \n]  \n  [then [insert|update|delete]] [,then …]  \n  [when …  then … […]]\n\n  \na.第一行就不多说了，和前面的用法都一样。b.第二行的where语句将事件分为了matched（满足where条件）和not matched（不满足where条件）两类c.第三行的when配合matched或者not matched表示“window中满足where条件的事件，执行下面的操作&#x2F;window中不满足where条件的事件，执行下面的操作”。search_condition为可选字段，表示再次过滤matched或not matched中的事件，只有没被过滤掉的事件才可以被then后面的语句操作。d.第四行的insert语句和之前说的insert into不太一样。虽然都表示插入事件，但是由于into streamname是可选，所以在只有insert关键字的情况下，会将触发的事件插入到当前的named window中。如果要指明插入到别的named window中就要在insert之后带上into及window的名字。再之后的圆括号中的内容表示要插入的事件的属性，一般情况是在将事件插入到别的window中时，用它来重命名第五行中列出的属性。e.第五行实际是配合第四行一起使用的。select子句不可少，不然引擎就不知道要往window中插入什么内容了。select的内容可以是*，也可以是属性列表。where语句再一次限制可插入的触发事件。注意select后面没有from，因为事件来源就是当时的触发事件。f.第七行用来更新符合条件的事件，可更新单个或多个属性，where条件判断是否可进行更新操作。g.第九行用来删除符合条件的事件，只包含关键字delete以及可选的where语句。h.最后两行表示on merge中可以有多个when，每个when可以有多个then及insert或updata或delete语句，这样就能组成一个非常复杂的merge操作了。\n语法内容有点多，但是应该都还容易理解吧。除了语法，还有几点需要注意的：1. when not matched后面只能跟insert语句，而when matched则没有任何限制。2. select语句中不能使用聚合函数。3. 当某一个when matched或者no matched和其search_condition满足，则这个when下的所有then都会执行，并且按照顺序执行。4. 避免在一个when中同时使用update和delete，esper不能保证两个都有效执行。\n接下来我给大家一个完整的例子，各位可以好好揣摩一下上面所讲的要点。\n\npackage example;  \n\nimport com.espertech.esper.client.EPAdministrator;  \n\nimport com.espertech.esper.client.EPRuntime;  \n\nimport com.espertech.esper.client.EPServiceProvider;  \n\nimport com.espertech.esper.client.EPServiceProviderManager;  \n\nimport com.espertech.esper.client.EPStatement;  \n\nimport com.espertech.esper.client.EventBean;  \n\nimport com.espertech.esper.client.UpdateListener;  \n\nimport java.io.Serializable;  \n\nimport java.util.HashMap;  \n\nimport java.util.Map;  \n\nclass MergeEvent implements Serializable {  \n\n    private int mergeId;  \n\n    private String mergeStr;  \n\n    private int mergeSize;  \n\n    private boolean deleteFlag;  \n\n    public int getMergeId() {  \n\n        return mergeId;  \n\n    }  \n\n    public void setMergeId(int mergeId) {  \n\n        this.mergeId &#x3D; mergeId;  \n\n    }  \n\n    public String getMergeStr() {  \n\n        return mergeStr;  \n\n    }  \n\n    public void setMergeStr(String mergeStr) {  \n\n        this.mergeStr &#x3D; mergeStr;  \n\n    }  \n\n    public int getMergeSize() {  \n\n        return mergeSize;  \n\n    }  \n\n    public void setMergeSize(int mergeSize) {  \n\n        this.mergeSize &#x3D; mergeSize;  \n\n    }  \n\n    public boolean isDeleteFlag() {  \n\n        return deleteFlag;  \n\n    }  \n\n    public void setDeleteFlag(boolean deleteFlag) {  \n\n        this.deleteFlag &#x3D; deleteFlag;  \n\n    }  \n\n    public String toString() {  \n\n        return “mergeId&#x3D;” + mergeId + “, mergeStr&#x3D;” + mergeStr + “, mergeSize&#x3D;” + mergeSize + “, deleteFlag&#x3D;” + deleteFlag;  \n\n    }  \n\n}  \n\nclass OnMergeWindowlistener implements UpdateListener {  \n\n    public void update(EventBean[] newEvents, EventBean[] oldEvents) {  \n\n        if (newEvents !&#x3D; null) {  \n\n            System.out.println(“Trigger MergeWindow:”);  \n\n            for (int i &#x3D; 0; i &lt; newEvents.length; i++) {  \n\n                System.out.println(newEvents[i].getUnderlying());  \n\n            }  \n\n        }  \n\n    }  \n\n}  \n\nclass SelectLogWindowlistener implements UpdateListener {  \n\n    public void update(EventBean[] newEvents, EventBean[] oldEvents) {  \n\n        if (newEvents !&#x3D; null) {  \n\n            for (int i &#x3D; 0; i &lt; newEvents.length; i++) {  \n\n                System.out.println(newEvents[i].getUnderlying());  \n\n            }  \n\n        }  \n\n    }  \n\n}  \n\nclass SelectMergeWindowlistener implements UpdateListener {  \n\n    public void update(EventBean[] newEvents, EventBean[] oldEvents) {  \n\n        if (newEvents !&#x3D; null) {  \n\n            for (int i &#x3D; 0; i &lt; newEvents.length; i++) {  \n\n                System.out.println(newEvents[i].getUnderlying());  \n\n            }  \n\n        }  \n\n    }  \n\n}  \n\npublic class OnMergeWindowTest {  \n\n    public static void main(String[] args) {  \n\n        EPServiceProvider epService &#x3D; EPServiceProviderManager.getDefaultProvider();  \n\n        EPAdministrator admin &#x3D; epService.getEPAdministrator();  \n\n        EPRuntime runtime &#x3D; epService.getEPRuntime();  \n\n        String mergeEvent &#x3D; MergeEvent.class.getName();  \n\n        String epl1 &#x3D; “create window MergeWindow.win:keepall() select * from “ + mergeEvent;  \n\n        String epl2 &#x3D; “create schema LogEvent as (id int, name string)”;  \n\n        String epl3 &#x3D; “create window LogWindow.win:keepall() as LogEvent”;  \n\n        String epl4 &#x3D; “on “ + mergeEvent + “(mergeSize &gt; 0) me merge MergeWindow mw where me.mergeId &#x3D; mw.mergeId “  \n\n                + “when matched and me.deleteFlag &#x3D; true then delete “  \n\n                + “when matched then update set mergeSize &#x3D; mergeSize + me.mergeSize where mw.mergeSize &gt; 3 “  \n\n                + “when not matched then insert select * then insert into LogWindow(id, name) select me.mergeId, me.mergeStr”;  \n\n        String epl5 &#x3D; “on LogEvent(id&#x3D;0) select lw.* from LogWindow as lw”;  \n\n        String epl6 &#x3D; “on “ + mergeEvent + “(mergeSize &#x3D; 0) select mw.* from MergeWindow as mw”;  \n\n        System.out.println(“Create Window: “ + epl1);  \n\n        System.out.println(“Merge Trigger: “ + epl4);  \n\n        System.out.println();  \n\n        admin.createEPL(epl1);  \n\n        admin.createEPL(epl2);  \n\n        admin.createEPL(epl3);  \n\n        EPStatement state1 &#x3D; admin.createEPL(epl4);  \n\n        state1.addListener(new OnMergeWindowlistener());  \n\n        EPStatement state2 &#x3D; admin.createEPL(epl5);  \n\n        state2.addListener(new SelectLogWindowlistener());  \n\n        EPStatement state3 &#x3D; admin.createEPL(epl6);  \n\n        state3.addListener(new SelectMergeWindowlistener());  \n\n        Map&lt;String, Object&gt; selectLog &#x3D; new HashMap&lt;String, Object&gt;();  \n\n        selectLog.put(“id”, 0);  \n\n        MergeEvent selectMerge &#x3D; new MergeEvent();  \n\n        selectMerge.setMergeSize(0);  \n\n        MergeEvent me1 &#x3D; new MergeEvent();  \n\n        me1.setDeleteFlag(false);  \n\n        me1.setMergeId(1);  \n\n        me1.setMergeSize(2);  \n\n        me1.setMergeStr(“me1”);  \n\n        System.out.println(“Send MergeEvent 1: “ + me1);  \n\n        runtime.sendEvent(me1);  \n\n        MergeEvent me2 &#x3D; new MergeEvent();  \n\n        me2.setDeleteFlag(false);  \n\n        me2.setMergeId(2);  \n\n        me2.setMergeSize(3);  \n\n        me2.setMergeStr(“me2”);  \n\n        System.out.println(“Send MergeEvent 2: “ + me2);  \n\n        runtime.sendEvent(me2);  \n\n        MergeEvent me3 &#x3D; new MergeEvent();  \n\n        me3.setDeleteFlag(false);  \n\n        me3.setMergeId(3);  \n\n        me3.setMergeSize(4);  \n\n        me3.setMergeStr(“me3”);  \n\n        System.out.println(“Send MergeEvent 3: “ + me3);  \n\n        runtime.sendEvent(me3);  \n\n        System.out.println(“\\nSend MergeEvent to Select MergeWindow!”);  \n\n        runtime.sendEvent(selectMerge);  \n\n        System.out.println(“\\nSend LogEvent to Select LogWindow!”);  \n\n        runtime.sendEvent(selectLog, “LogEvent”);  \n\n        MergeEvent me4 &#x3D; new MergeEvent();  \n\n        me4.setDeleteFlag(false);  \n\n        me4.setMergeId(3);  \n\n        me4.setMergeSize(5);  \n\n        me4.setMergeStr(“me4”);  \n\n        System.out.println(“\\nSend MergeEvent 4: “ + me4);  \n\n        runtime.sendEvent(me4);  \n\n        System.out.println(“\\nSend MergeEvent to Select MergeWindow!”);  \n\n        runtime.sendEvent(selectMerge);  \n\n        MergeEvent me5 &#x3D; new MergeEvent();  \n\n        me5.setDeleteFlag(true);  \n\n        me5.setMergeId(1);  \n\n        me5.setMergeSize(6);  \n\n        me5.setMergeStr(“me5”);  \n\n        System.out.println(“\\nSend MergeEvent 5: “ + me5);  \n\n        runtime.sendEvent(me5);  \n\n        System.out.println(“\\nSend MergeEvent to Select MergeWindow!”);  \n\n        runtime.sendEvent(selectMerge);  \n\n    }  \n\n}\n\n\n  \n执行结果：\n\nCreate Window: create window MergeWindow.win:keepall() select * from example.MergeEvent  \n\nMerge Trigger: on example.MergeEvent(mergeSize &gt; 0) me merge MergeWindow mw where me.mergeId &#x3D; mw.mergeId when matched and me.deleteFlag &#x3D; true then delete when matched then update set mergeSize &#x3D; mergeSize + me.mergeSize where mw.mergeSize &gt; 3 when not matched then insert select * then insert into LogWindow(id, name) select me.mergeId, me.mergeStr  \n\nSend MergeEvent 1: mergeId&#x3D;1, mergeStr&#x3D;me1, mergeSize&#x3D;2, deleteFlag&#x3D;false  \n\nTrigger MergeWindow:  \n\nmergeId&#x3D;1, mergeStr&#x3D;me1, mergeSize&#x3D;2, deleteFlag&#x3D;false  \n\nSend MergeEvent 2: mergeId&#x3D;2, mergeStr&#x3D;me2, mergeSize&#x3D;3, deleteFlag&#x3D;false  \n\nTrigger MergeWindow:  \n\nmergeId&#x3D;2, mergeStr&#x3D;me2, mergeSize&#x3D;3, deleteFlag&#x3D;false  \n\nSend MergeEvent 3: mergeId&#x3D;3, mergeStr&#x3D;me3, mergeSize&#x3D;4, deleteFlag&#x3D;false  \n\nTrigger MergeWindow:  \n\nmergeId&#x3D;3, mergeStr&#x3D;me3, mergeSize&#x3D;4, deleteFlag&#x3D;false  \n\nSend MergeEvent to Select MergeWindow!  \n\nmergeId&#x3D;1, mergeStr&#x3D;me1, mergeSize&#x3D;2, deleteFlag&#x3D;false  \n\nmergeId&#x3D;2, mergeStr&#x3D;me2, mergeSize&#x3D;3, deleteFlag&#x3D;false  \n\nmergeId&#x3D;3, mergeStr&#x3D;me3, mergeSize&#x3D;4, deleteFlag&#x3D;false  \n\nSend LogEvent to Select LogWindow!  \n\n{id&#x3D;1, name&#x3D;me1}  \n\n{id&#x3D;2, name&#x3D;me2}  \n\n{id&#x3D;3, name&#x3D;me3}  \n\nSend MergeEvent 4: mergeId&#x3D;3, mergeStr&#x3D;me4, mergeSize&#x3D;5, deleteFlag&#x3D;false  \n\nTrigger MergeWindow:  \n\nmergeId&#x3D;3, mergeStr&#x3D;me3, mergeSize&#x3D;9, deleteFlag&#x3D;false  \n\nSend MergeEvent to Select MergeWindow!  \n\nmergeId&#x3D;1, mergeStr&#x3D;me1, mergeSize&#x3D;2, deleteFlag&#x3D;false  \n\nmergeId&#x3D;2, mergeStr&#x3D;me2, mergeSize&#x3D;3, deleteFlag&#x3D;false  \n\nmergeId&#x3D;3, mergeStr&#x3D;me3, mergeSize&#x3D;9, deleteFlag&#x3D;false  \n\nSend MergeEvent 5: mergeId&#x3D;1, mergeStr&#x3D;me5, mergeSize&#x3D;6, deleteFlag&#x3D;true  \n\nSend MergeEvent to Select MergeWindow!  \n\nmergeId&#x3D;2, mergeStr&#x3D;me2, mergeSize&#x3D;3, deleteFlag&#x3D;false  \n\nmergeId&#x3D;3, mergeStr&#x3D;me3, mergeSize&#x3D;9, deleteFlag&#x3D;false\n\n\n  \n9.Selecting&#x2F;Updating&#x2F;Deleting From Named Windows Using Fire-And-Forget Queries\n       上面对于named window的所有操作，都是要发送一个事件到引擎才可达到目的。那是否可以不发送事件直接执行select&#x2F;update&#x2F;delete语句达到目的呢？Esper确实为大家提供了一种简单的方式来操作named window。和之前说的有很大的不同，首先不需要发送任何事件到引擎即可select&#x2F;update&#x2F;delete，其次不需要监听器就可立即获得操作结果。4.9版本只支持这三种操作，不过现在的4.11已经增加了很多别的操作，详细内容请各位自己去看官方文档吧。由于本节内容较为简单，所以我直接给大家展示语法及一个完整的例子。\n\n&#x2F;&#x2F; select  \n\nselect *[, property_name[,…]] from window_name [where criteria_expression] [oder by] [having] [subquery]  \n\n&#x2F;&#x2F; update 子查询，聚合函数等不能用于expression  \n\nupdate window_name [as stream_name]  \n\nset property_name &#x3D; expression [, property_name &#x3D; expression [,…]] [where criteria_expression]  \n\n&#x2F;&#x2F; delete  \n\ndelete from window_name [as stream_name] [where criteria_expression]\n\n\n  \n执行结果：\n\nSend SelectEvent 1: name&#x3D;se1, size&#x3D;1  \n\nSend SelectEvent 2: name&#x3D;se2, size&#x3D;2  \n\nSelect SelectWindow!  \n\nname&#x3D;se1, size&#x3D;1  \n\nname&#x3D;se2, size&#x3D;2  \n\nUpdate SelectEvent(size &#x3D; 2) in SelectWindow!  \n\nname&#x3D;update1, size&#x3D;2  \n\nSelect SelectWindow!  \n\nname&#x3D;se1, size&#x3D;1  \n\nname&#x3D;update1, size&#x3D;2  \n\nDelete SelectEvent(size &lt; 2) in SelectWindow!  \n\nname&#x3D;se1, size&#x3D;1  \n\nSelect SelectWindow!  \n\nname&#x3D;update1, size&#x3D;2\n\n\n  \n10.Explicitly Indexing Named Windows\n       因为named window中可以存放事件，所以对存有大量的事件的window进行操作时，效率肯定是一个很大的问题。因此Esper支持对named window建立索引，具体来说是对named window中存放的事件的属性建立索引。我没有实际测试过加过索引之后操作时间有没有缩短，所以如果有人测试过并的确有好效果，希望能在本篇评论里向我反馈下，谢谢。语法如下：\n\ncreate [unique] index index_name on named_window_name (property [hash| btree] [, property] [hash|btree] [,…] )\n\n  \n       unique代表建立唯一索引，如果插入了重复的行，则会抛出异常并阻止重复行插入。如果不使用此关键字，则表示可以插入重复行。index_name为索引的名称，named_window_name是要建立索引的named window。后面的括号中包含named window中的属性以及索引类型。索引类型分两种，hash索引不会排序，如果有&#x3D;操作，建议使用此类型索引。btree索引基于排序二叉树，适合&lt;, &gt;, &gt;&#x3D;, &lt;&#x3D;, between, in等操作。如果不显式声明hash或者btree，则默认为hash索引。举例如下：\n\n&#x2F;&#x2F; create a unique index on user id（hash） and profile id（hash）  \n\ncreate unique index UserProfileIndex on UserProfileWindow(userId, profileId)  \n\n&#x2F;&#x2F; create a non-unique index on symbol（hash） and buyPrice（btree）  \n\ncreate index idx1 on TickEventWindow(symbol hash, buyPrice btree)\n\n\n  \n11.Dropping or Removing Named Windows\n       注销named window的方式是直接调用EPStatement对象的destroy方法。虽然注销，但是named window的名字仍然被占用着，所以你只能重新建立和之前的named window一样结构的window，否则会抛出异常。例如：\n\n&#x2F;&#x2F; Create DropWindow  \n\ncreate window DropWindow.win:keepall() as select * from DropEvent  \n\n&#x2F;&#x2F; Destroy DropWindow  \n\nEPStatement state &#x3D; admin.createEPL(“create window DropWindow.win:keepall() as select * from DropEvent”);  \n\nstate.destroy();  \n\n&#x2F;&#x2F; Create DropEvent again(different with prior epl)  \n\ncreate window DropWindow.win:keepall() as select name from DropEvent  \n\n&#x2F;&#x2F; throw Exception\n\n\n","slug":"BIGDATA/Esper学习之十二：EPL语法（八）","date":"2023-06-09T10:49:00.000Z","categories_index":"window,BIGDATA","tags_index":"select,name,named","author_index":"dandeliono"},{"id":"6131b419f4b67eaeeae93021cf6fda95","title":"Esper学习之十一：EPL语法（七）","content":"Esper学习之十一：EPL语法（七） 上一篇说到了EPL如何访问关系型数据库这种数据源，实际上别的数据源，比如：webservice、分布式缓存、非关系型数据库等等，Esper提供了统一的数据访问接口。然后今天会讲解如何创建另外一种事件类型——Schema。\n1.Joining Method Invocation Results和执行sql的语法类似，调用方法的一种触发方式也是通过join别的事件的属性来达到效果，且调用方法的句子为from子句。语法如下：\n\nmethod:class_name.method_name(parameter_expressions)\n\n  \nmethod是固定关键字，class_name为类全名，方法名为返回外部数据的方法名，parameter_expressions为方法的参数列表，对应join的事件属性，多个属性之间用逗号分隔，参数整体用圆括号括起来。例如：\n\nselect * from AssetMoveEvent, method:MyLookupLib.lookupAsset(assetId) &#x2F;&#x2F; assetId为AssetMoveEvent的属性之一\n\n  \n除了简单join，还可以为join加上where条件过滤一些返回结果。例如：\n\nselect assetId, assetDesc from AssetMoveEvent as asset,  \n    method:MyLookupLib.getAssetDescriptions() as desc    &#x2F;&#x2F;调用的方法无参  \n        where asset.assetid &#x3D; desc.assetid\n\n  \nEsper不仅能缓存执行sql的查询结果，也能缓存执行方法的查询结果，并且缓存策略也是两种：LRU和Expire Time。具体可以参考上一篇缓存配置章节。若存在返回结果，且缓存生效后，Esper会自动为返回结果简历索引，加快查询速度。配置范例如下：\n\n&#x2F;&#x2F; LRU Cache  \n\n  \n\n      \n\n  \n\n&#x2F;&#x2F; Expire Time Cache  \n\n  \n\n      \n\n\n\n  \nclass-name表示方法所在的类，可以是类全名，可以只有类名，前提是包已经import。其他参数的解释请参见上一篇缓存配置章节\n2.Polling Method Invocation Results via Iterator除了Join别的事件来触发查询方法，进而触发Listener，Esper还支持通过API直接执行方法。例如：\n\nselect * from method:MyLookupLib.getAssetDescriptions(category_var)   &#x2F;&#x2F; category_var为注册到引擎的变量。\n\n  \n3.Method Definition为了能够以一种统一的结构访问外部数据（RDBMS除外），Esper提供了调用静态方法的形式访问外部数据。具体解释如下：**a.**返回数据的方法必须是公共静态方法。方法参数可以有多个也可以没有。**b.**如果返回一条数据或无返回数据，则方法的返回类型可以是Java类或者Map类型数据。如果返回多条数据（包括一条），则方法返回类型必须是Java类的数组或者Map数组。**c.**如果方法的返回类型是Java类或者Java类数组，则Java的类定义必须包含针对属性的get方法。**d.**如果方法的返回类型是Map或者Map数组，则Map的key-value定义固定为&lt;String, Object&gt;。**e.**如果返回的数据是Map或者Map数组，除了定义返回数据的方法外，还要定义返回元数据的方法（这个元数据针对返回的数据）。方法是公共静态方法，且必须是无参数方法。方法返回类型为Map&lt;String, Class&gt;，String表示返回的数据的名称，Class表示返回的数据的类型。返回元数据的方法名称&#x3D;返回数据的方法名称+Metadata。\n下面举一个完整的例子总结前面说的三点：\n\npackage example;  \n\nclass JavaObject {  \n\n    private String name;  \n\n    private int size;  \n\n    public String getName() {  \n\n        return name;  \n\n    }  \n\n    public void setName(String name) {  \n\n        this.name &#x3D; name;  \n\n    }  \n\n    public int getSize() {  \n\n        return size;  \n\n    }  \n\n    public void setSize(int size) {  \n\n        this.size &#x3D; size;  \n\n    }  \n\n    public String toString() {  \n\n        return “JavaObject{“ + “name&#x3D;’” + name + ‘&#39;‘ + “, size&#x3D;” + size + ‘}’;  \n\n    }  \n\n}  \n\npublic class InvocationMethodJava {  \n\n    public static JavaObject[] getJavaObject(int times) {  \n\n        JavaObject[] javaObjects &#x3D; new JavaObject[2];  \n\n        JavaObject javaObject1 &#x3D; new JavaObject();  \n\n        javaObject1.setName(“javaObject1”);  \n\n        javaObject1.setSize(1 * times);  \n\n        JavaObject javaObject2 &#x3D; new JavaObject();  \n\n        javaObject2.setName(“javaObject2”);  \n\n        javaObject2.setSize(2 * times);  \n\n        javaObjects[0] &#x3D; javaObject1;  \n\n        javaObjects[1] &#x3D; javaObject2;  \n\n        return javaObjects;  \n\n    }  \n\n}  \n\npackage example;  \n\nimport java.util.HashMap;  \n\nimport java.util.Map;  \n\npublic class InvocationMethodMap {  \n\n    public static Map&lt;String, Object&gt; getMapObject() {  \n\n        Map&lt;String, Object&gt; map &#x3D; new HashMap&lt;String, Object&gt;();  \n\n        map.put(“name”, “mapObject1”);  \n\n        map.put(“size”, 1);  \n\n        return map;  \n\n    }  \n\n    public static Map&lt;String, Class&gt; getMapObjectMetadata() {  \n\n        Map&lt;String, Class&gt; map &#x3D; new HashMap&lt;String, Class&gt;();  \n\n        map.put(“name”, String.class);  \n\n        map.put(“size”, int.class);  \n\n        return map;  \n\n    }  \n\n}  \n\npackage example;  \n\nimport java.util.Iterator;  \n\nimport com.espertech.esper.client.EPAdministrator;  \n\nimport com.espertech.esper.client.EPRuntime;  \n\nimport com.espertech.esper.client.EPServiceProvider;  \n\nimport com.espertech.esper.client.EPServiceProviderManager;  \n\nimport com.espertech.esper.client.EPStatement;  \n\nimport com.espertech.esper.client.EventBean;  \n\nimport com.espertech.esper.client.UpdateListener;  \n\nclass Times {  \n\n    private int times;  \n\n    public int getTimes() {  \n\n        return times;  \n\n    }  \n\n    public void setTimes(int times) {  \n\n        this.times &#x3D; times;  \n\n    }  \n\n}  \n\nclass InvocationMethodListener implements UpdateListener {  \n\n    public void update(EventBean[] newEvents, EventBean[] oldEvents) {  \n\n        if (newEvents !&#x3D; null) {  \n\n            System.out.println(newEvents[0].getUnderlying());  \n\n            System.out.println(newEvents[1].getUnderlying());  \n\n        }  \n\n    }  \n\n}  \n\npublic class InvocationMethodTest {  \n\n    public static void main(String arg[]) {  \n\n        EPServiceProvider epService &#x3D; EPServiceProviderManager.getDefaultProvider();  \n\n        EPRuntime runtime &#x3D; epService.getEPRuntime();  \n\n        EPAdministrator admin &#x3D; epService.getEPAdministrator();  \n\n        String timesName &#x3D; Times.class.getName();  \n\n        String ijName &#x3D; InvocationMethodJava.class.getName();  \n\n        String epl1 &#x3D; “select ij.* from “ + timesName + “ as t, method:” + ijName + “.getJavaObject(times) as ij”;  \n\n        System.out.println(epl1+”\\n”);  \n\n        EPStatement state1 &#x3D; admin.createEPL(epl1);  \n\n        state1.addListener(new InvocationMethodListener());  \n\n        Times times &#x3D; new Times();  \n\n        times.setTimes(2);  \n\n        runtime.sendEvent(times);  \n\n        System.out.println(“”);  \n\n        String imName &#x3D; InvocationMethodMap.class.getName();  \n\n        String epl2 &#x3D; “select * from method:” + imName + “.getMapObject()”;  \n\n        System.out.println(epl2+”\\n”);  \n\n        EPStatement state2 &#x3D; admin.createEPL(epl2);  \n\n        Iterator iter &#x3D; state2.iterator();  \n\n        while (iter.hasNext()) {  \n\n            EventBean event &#x3D; iter.next();  \n\n            System.out.println(event.getUnderlying());  \n\n        }  \n\n    }  \n\n}\n\n\n  \n4.Declare an Event Type by Providing Names and Types       我曾经在《Esper学习之二：事件类型》里说过，事件类型的定义可以是POJO，数组，Map，或者XML。实际上还有另一种定义事件类型的方法，那就是schema。这个schema可不是数据库中的schema，而是用EPL定义的事件类型，所以说此类事件类型是针对Esper设计的，并不能拿出来通用。\n我们先从语法开始说起。\n\ncreate [map | objectarray] schema schema_name [as]  \n (property_name property_type [,property_name property_type [,…])  \n [inherits inherited_event_type[, inherited_event_type] [,…]] [starttimestamp timestamp_property_name]  \n [endtimestamp timestamp_property_name]  \n [copyfrom copy_type_name [, copy_type_name] [,…]]\n\n  \n解释如下：\n**a.**map objectarray分别表示当前定义的schema是map结构还是数组结构。**b.**schema_name表示schema的名字，全局唯一。as为可选内容。**c.**property_name表示schema所包含的属性名称，property_type为属性的类型，多个属性用逗号分隔，所有属性用圆括号括起来可以使用的类型有：int、String等已经内置的Java类型；已经通过Configuration接口注册的事件类型，比如Map，数组，schema等；可以使用POJO类，如果没有import就要写全名，否则写类名即可。**d.**inherits表示继承别的事件类型，后面跟着的是要继承的事件类型。如果使用了继承，则当前定义的schema包含了继承的事件类型的所有属性。并且可以继承多个，用逗号分隔。**e.**starttimestamp和endtimestamp是两个特殊的关键字。单独使用starttimestamp时，表示为schema记一个时间戳。后面跟着已经声明的属性并且能够返回一个data-time值。如果联合endtimestamp使用，则表示这个schema只能在某段事件内使用。后面跟着的同样也是已经声明的属性并且能够返回一个data-time值。注意endtimestamp不能单独使用。**f.**copyfrom表示复制别的事件的所有属性到当前定义的schema中，并且可以copy多个事件，用逗号分隔。\n下面用几个例子来一一展示如何使用上面的语法：\n\n&#x2F;&#x2F; 声明SecurityEvent  \n\ncreate schema SecurityEvent as (ipAddress string, userId String, numAttempts int)  \n\n&#x2F;&#x2F; 声明AuthorizationEvent，并且包含com.mycompany.HostNameInfo类型的hostinfo属性  \n\ncreate schema AuthorizationEvent(group String, roles String[], hostinfo com.mycompany.HostNameInfo)  \n\n&#x2F;&#x2F; 声明CompositeEvent，并且包含了SecurityEvent数组作为innerEvents属性的类型  \n\ncreate schema CompositeEvent(group String, innerEvents SecurityEvent[])  \n\n&#x2F;&#x2F; 声明WebPageVisitEvent，自己定义了userId属性，并且继承了PageHitEvent的所有属性  \n\ncreate schema WebPageVisitEvent(userId String) inherits PageHitEvent  \n\n&#x2F;&#x2F; 声明RoboticArmMovement，并且开始于startts，结束于endts  \n\ncreate schema RoboticArmMovement (robotId string, startts long, endts long) starttimestamp startts endtimestamp endts  \n\n&#x2F;&#x2F; 声明ExtendedSecurityEvent，并且复制SecurityEvent事件的所有属性到ExtendedSecurityEvent  \n\ncreate schema ExtendedSecurityEvent (userName string) copyfrom SecurityEvent  \n\n&#x2F;&#x2F; 声明WebSecurityEvent，并且复制SecurityEvent和WebPageVisitEvent事件的所有属性到WebSecurityEvent  \n\ncreate schema WebSecurityEvent (userName string) copyfrom SecurityEvent, WebPageVisitEvent\n\n\n  \n这里要额外说一下，继承不仅仅继承了事件的属性，如果被继承的事件定义了starttimestamp或者endtimestamp，同样也会被继承下来。但是copyfrom是不会吧starttimestamp和endtimestamp复制的。这点一定要注意。\n对于map和objectarray这两个关键字，可以用注解达到同样的效果。例如：\n\n&#x2F;&#x2F; 声明数组类型的schema  \n\ncreate objectarray schema SchemaTest1 as (prop1 string);  \n\n… equals …  \n\n@EventRepresentation(array&#x3D;true)create schema SchemaTest1 as (prop1 string);  \n\n&#x2F;&#x2F; 声明Map类型的schema  \n\ncreate map schema SchemaTest2 as (prop1 string);  \n\n… equals …  \n\n@EventRepresentation(array&#x3D;false)create schema SchemaTest2 as (prop1 string);\n\n\n  \n5.Declare Variant StreamVariant Stream简单来说就是包含了各种不同的事件类型的事件类型。所以语法也很明了：\n\ncreate variant schema schema_name [as] eventtype_name|* [, eventtype_name|*] [,…]\n\n  \nvariant为关键字，表明这是Variant Stream，eventtype_name为事件的定义名，多个事件定义用逗号分隔开。*表示接收任何的事件类型，不过一般来说没有需求会到这种程度。举例如下：\n\n&#x2F;&#x2F; 声明SecurityVariant，包含了LoginEvent和LogoutEvent  \ncreate variant schema SecurityVariant as LoginEvent, LogoutEvent  \n&#x2F;&#x2F; 声明AnyEvent，包含任何类型的事件  \ncreate variant schema AnyEvent as *\n\n  \n以上就是调用外部方法以及schema的创建讲解，尤其是schema的创建，可能大家会用的更多一些，最好能记住。\n","slug":"BIGDATA/Esper学习之十一：EPL语法（七）","date":"2023-06-09T09:35:27.000Z","categories_index":"schema,BIGDATA","tags_index":"class,name,String","author_index":"dandeliono"},{"id":"404c6135f2e86a9010b5598b38ae4379","title":"Esper学习之十：EPL语法（六）","content":"Esper学习之十：EPL语法（六）  在esper的文档中，epl访问数据库的配置放在了比较靠后的位置，不过为了方便各位学习，这里会先说明和数据库交互的相关配置，然后再说epl怎么访问数据库。\n配置文件在官方esper包的etc文件夹下，大家可以参考着学习。\n1.连接数据库\na.JNDI获取连接\n配置如下：\n\n  \n    \n      \n    &lt;env-property name&#x3D;”java.naming.provider.url” value &#x3D;”iiop:&#x2F;&#x2F;localhost:1050”&#x2F; &gt;  \n    \n\n\n  \ndatabase-reference的name是要连接的数据库名字，其余的配置可参考JNDI的文档\n使用方法：\n\nif (envProperties.size() &gt; 0) {  \n  initialContext &#x3D; new InitialContext(envProperties);  \n} else {  \n  initialContext &#x3D; new InitialContext();  \n}  \nDataSource dataSource &#x3D; (DataSource) initialContext.lookup(lookupName);  \nConnection connection &#x3D; dataSource.getConnection();\n\n  \n更多内容可参考JNDI的文档\nb.从连接池获取连接\n配置如下：（以dbcp为例）\n\n  \n\n    \n\n      \n\n      \n\n      \n\n      \n\n      \n\n      \n\n    \n\n\n\n\n  \n\n\n相同的配置可以使用esper的api达到同样的效果。代码如下：\n\nProperties props &#x3D; new Properties();  \n\nprops.put(“username”, “myusername”);  \n\nprops.put(“password”, “mypassword”);  \n\nprops.put(“driverClassName”, “com.mysql.jdbc.Driver”);  \n\nprops.put(“url”, “jdbc:mysql:&#x2F;&#x2F;localhost&#x2F;test”);  \n\nprops.put(“initialSize”, 2);  \n\nprops.put(“validationQuery”, “select 1 from dual”);  \n\nConfigurationDBRef configDB &#x3D; new ConfigurationDBRef();  \n\nconfigDB.setDataSourceFactory(props, BasicDataSourceFactory.class.getName());  \n\nconfigDB.setConnectionLifecycleEnum(ConfigurationDBRef.ConnectionLifecycleEnum.POOLED);  \n\nConfiguration configuration &#x3D; new Configuration();  \n\nconfiguration.addDatabaseReference(“mydb3”, configDB);\n\n\n  \n同样，也可以自己实现数据源。示例如下：\n\nconfigDB.setDataSourceFactory(props, MyOwnDataSourceFactory.class.getName());  \n…  \nclass MyOwnDataSourceFactory {  \n  public static DataSource createDataSource(Properties properties) {  \n    return new MyDataSourceImpl(properties);  \n  }   \n}\n\n  \nc.JDBC获取连接\n前提是要将对应的jdbc驱动假如classpath\n\n  \n    \n      \n      \n      \n    \n\n\n  \n注意：drivermanager-connection中的user和password属性必须填写，即使增加了connection-arg参数也不行。所以实际上connection-arg的user和password是不需要写的。这点我觉得esper做的不够人性化。\nd.其他关于数据库连接的配置\n下面是一些和数据库交互的配置，更多配置可参考Javadoc\n\n  \n… configure data source or driver manager settings…  \n    \n\n\n  \n下面是关于连接的生命周期的配置\n\n  \n… configure data source or driver manager settings…  \n    \n\n\n  \n如果参数值为pooled，当配置了连接池，则会将每次获取的连接还给连接池。若没配置连接池，则每次获取的连接用完后就关闭。\n如果参数值为retain，则会将连接缓存到esper引擎中，这个epl用完后，另一个epl可以接着用\n2.查询结果缓存策略EPL和数据库交互时会产生查询结果，所以引擎若能缓存查询结果将大大提高执行效率，因此esper提供了两种缓存模式。\na.LRU CacheLRU即least-recently-used，中文释义为“最近最少使用”，学过OS的应该知道内存缓存策略里也有这个算法，不明白的请自行搜索。配置如下：\n\n  \n… configure data source or driver manager settings…  \n      \n\n\n  \nsize的参数值表示最多能缓存多少条查询结果，而不是大小\nb.Expiry-time Cache该策略为每一次的查询结果设置了存活期（注意不是每一条查询结果），并且定期清理过期的查询结果。配置如下：\n\n  \n… configure data source or driver manager settings…  \n      \n\n\n  \nmax-age-seconds表示存活时间，purge-interval-seconds表示每隔多久清理一次过期的内容，两者单位都是秒。ref-type有三个参数值：weak，soft，hard。表示查询结果的引用级别，JVM垃圾回收的时候，会根据此参数决定何时释放缓存。具体解释如下：1).weak表示弱引用，JVM在垃圾回收的时候将清除所有的缓存，释放内存。2).soft表示软引用，JVM在垃圾回收的时候，只有当所有的弱引用都被回收了才会清除所有的缓存并释放空间。3).hard表示强引用，JVM的垃圾回收不会清除缓存，所以引擎将按照规定的存活期和清理时间管理缓存。\n3.Column Change Case通常情况下，表字段是大小写不敏感的，但是也有设置为小写敏感的情况，我们可以通过设置使得查询返回的列结果为大写或者小写。配置如下：\n\n\n\n  \n4.SQL Types Mapping默认的数据库字段类型映射可以满足需求，不过想修改也是可以的。配置如下：\n\n\n\n  \nsql-type表示数据库字段类型，这里的2映射了具体类型，可在java.sql.Types类中查到，并且这个类里包含了大部分的数据库字段类型。java-type表示对应的java数据类型，大小写不敏感。\n以上就是EPL和数据库交互的相关配置，下面来讲解EPL是怎么和数据库交互的。\nEPL和数据库交互有两个前提，一是JDBC驱动能够预编译sql，而是JDBC驱动能获取数据库的元数据。\n5.Joining SQL Query Results通常我们想要的一种交互方式是：输入某个事件到引擎，然后引擎把事件的某个属性作为sql的查询条件交给JDBC驱动，执行sql。正好esper为此提供了相应的解决办法，参看语法：\n\nsql:database_name [“ parameterized_sql_query “]\n\n  \nsql是关键字不可少，parameterized_sql_query为sql语句，只与具体的DB有关，无关esper，所以数据库的那些个函数都可以用。先看一个简单的例子：\n\nselect custId, cust_name from CustomerCallEvent, sql:MyCustomerDB [‘ select cust_name from Customer where cust_id &#x3D; ${custId} ‘]\n\n  \n引擎接收CustomerCallEvent事件，将事件的custId属性作为查询值，执行MyCustomerDB数据库的Customer表，其中查询条件为Customer的cust_id字段值存在，然后返回相应的custId属性值和cust_name字段值给监听器\n该语法有几点需要注意：a.sql需要用单引号或者双引号引起来，然后再用方括号括起来。b.${expression}中可以是事件属性，可以是变量、常量等，也可以是用户自定义的函数。例如：\n\nselect * from LimitEvent le,  \n  sql:MyCustomerDB [‘ select cust_name from Customer where  \n      amount &gt; ${max(varLowerLimit, MyLib.getLimit(le))} ‘]\n\n  \nc.join的事件可以使用view，但是sql不可使用。不过可以将sql的查询结果通过insert into输出到另外的事件，然后再使用view。例如：\n\nselect customerId, customerName from CustomerCallEvent.win:time(30 sec) as cce,  \n  sql:MyCustomerDB [“select cust_id as customerId, cust_name as customerName  \n    from Customer  \n  where cust_id &#x3D; ${cce.custId}”] as cq\n\n  \nd.可以用as为表的字段设置别名，例如：\n\nselect custId, custName from CustomerCallEvent, sql:MyCustomerDB [‘ select cust_name as custName from Customer where cust_id &#x3D; ${custId} ‘]\n\n  \ne.当使用事件的属性作为查询值是，属性名不要和字段名重名，否则会报错，esper无法识别f.join的sql语句没有限制，并且可以使用where子句。例如：\n\nselect symbol, symbolDesc from OrderEvent as orders,  \n  sql:My_Oracle_DB [‘select symbolDesc from SymbolReference’] as reference,  \n  sql:My_MySQL_DB [‘select orderList from orderHistory’] as history  \n    where reference.symbol &#x3D; orders.symbol  \n    and history.symbol &#x3D; orders.symbol\n\n  \n除了普通的join，EPL也支持outer join sql语句，语法也没有什么改变。例如：\n\nselect custId, custName from  \n  CustomerCallEvent as cce  \n    left outer join  \n  sql:MyCustomerDB [“select cust_id, cust_name as custName from Customer where cust_id &#x3D; ${cce.custId}”] as cq  \n    on cce.custId &#x3D; cq.cust_id\n\n  \n6.Using Patterns to Request Data除了通过传递外部数据查询数据库，也可以用pattern定时或者以固定频率查询数据库。例如：\n\ninsert into NewOrders  \nselect orderId, orderAmount from  \n  pattern [every timer:interval(5 sec)],  \n    sql:MyCustomerDB [‘select orderId, orderAmount from NewOrders’]\n\n  \npattern语法之后再说，这里只让大家知道有这么一个用法。\n7.Polling SQL Queries via APIEsper提供了API直接执行EPL来达到访问数据库的目的。请看下面的代码：\n\npackage example;  \n\nimport com.espertech.esper.client.Configuration;  \n\nimport com.espertech.esper.client.EPAdministrator;  \n\nimport com.espertech.esper.client.EPRuntime;  \n\nimport com.espertech.esper.client.EPServiceProvider;  \n\nimport com.espertech.esper.client.EPServiceProviderManager;  \n\nimport com.espertech.esper.client.EPStatement;  \n\nimport com.espertech.esper.client.EventBean;  \n\nimport java.util.Iterator;  \n\npublic class IteratorSQLTest {  \n\n    public static void main(String[] args) throws InterruptedException {  \n\n        Configuration config &#x3D; new Configuration();  \n\n        config.configure(“esper.examples.cfg.xml”);  \n\n        config.addVariable(“vari”, Integer.class, 1);  \n\n        EPServiceProvider epService &#x3D; EPServiceProviderManager.getDefaultProvider(config);  \n\n        EPAdministrator admin &#x3D; epService.getEPAdministrator();  \n\n        EPRuntime runtime &#x3D; epService.getEPRuntime();  \n\n        String epl1 &#x3D; “select id, name from sql:test[‘select id, name from test1 where id&#x3D;${vari}’]“;  \n\n        EPStatement state &#x3D; admin.createEPL(epl1);  \n\n        Iterator iter &#x3D; state.iterator(); \n\n        while (iter.hasNext()) {  \n\n            EventBean eventBean &#x3D; iter.next();  \n\n            System.out.println(eventBean.get(“id”) + “ “ + eventBean.get(“name”));  \n\n        }  \n\n    }  \n\n}\n\n\n  \n执行结果：\n8.SQL Input Parameter and Column Output Conversion刚才数据库配置里面有说到可以修改数据库字段类型和java数据类型的映射关系，但是那只是针对全局的设置，如果想针对EPL来设置映射关系，可以实现SQLColumnTypeConversion接口，然后通过注解Hook调用实现类。具体代码及解释如下：\n\nimport com.espertech.esper.client.hook.SQLColumnTypeContext;  \n\nimport com.espertech.esper.client.hook.SQLColumnTypeConversion;  \n\nimport com.espertech.esper.client.hook.SQLColumnValueContext;  \n\nimport com.espertech.esper.client.hook.SQLInputParameterContext;  \n\npublic class MySQLColumnTypeConvertor implements SQLColumnTypeConversion{  \n\n    public Class getColumnType(SQLColumnTypeContext context) {  \n\n        Class clazz &#x3D; context.getColumnClassType();  \n\n        return clazz;  \n\n    }  \n\n    public Object getColumnValue(SQLColumnValueContext context) {  \n\n        Object obj &#x3D; context.getColumnValue();  \n\n        return obj;  \n\n    }  \n\n    public Object getParameterValue(SQLInputParameterContext context) {  \n\n        Object obj &#x3D; context.getParameterValue();  \n\n        return obj;  \n\n    }  \n\n}  \n\npackage example;  \n\nimport com.espertech.esper.client.Configuration;  \n\nimport com.espertech.esper.client.EPAdministrator;  \n\nimport com.espertech.esper.client.EPServiceProvider;  \n\nimport com.espertech.esper.client.EPServiceProviderManager;  \n\nimport com.espertech.esper.client.EPStatement;  \n\nimport com.espertech.esper.client.EventBean;  \n\nimport java.util.Iterator;  \n\npublic class SQLColumnTypeConversionTest {  \n\n    public static void main(String[] args) throws InterruptedException {  \n\n        Configuration config &#x3D; new Configuration();  \n\n        config.configure(“esper.examples.cfg.xml”);  \n\n        config.addVariable(“vari”, Integer.class, 1);  \n\n        EPServiceProvider epService &#x3D; EPServiceProviderManager.getDefaultProvider(config);  \n\n        EPAdministrator admin &#x3D; epService.getEPAdministrator();  \n\n        String epl1 &#x3D; “@Hook(type&#x3D;HookType.SQLCOL, hook&#x3D;’” + MySQLColumnTypeConvertor.class.getName()  \n\n                + “‘)select id, name from sql:test[‘select id, name from test1 where id&#x3D;${vari}’]“;  \n\n        System.out.println(epl1);  \n\n        EPStatement state1 &#x3D; admin.createEPL(epl1);  \n\n        Iterator iter &#x3D; state1.iterator();  \n\n        while (iter.hasNext()) {  \n\n            EventBean eventBean &#x3D; iter.next();  \n\n            System.out.println(eventBean.get(“id”) + “ “ + eventBean.get(“name”));  \n\n        }  \n\n    }  \n\n}\n\n\n  \n执行结果：\n\n@Hook(type&#x3D;HookType.SQLCOL, hook&#x3D;’example.MySQLColumnTypeConvertor’)select id, name from sql:test[‘select id, name from test1 where id&#x3D;${vari}’]  \n1 luonq\n\n  \n9.SQL Row POJO Conversion刚才说的列类型的转换以及列结果的转换，只是普通的转换。Esper还支持表的查询结果按行转换，比如说转换为POJO，而不像之前那样只能针对每一个字段结果单独进行转换。用法也是通过Hook注解来调用转换类。代码如下：\n\nimport java.sql.ResultSet;  \n\nimport java.sql.SQLException;  \n\nimport com.espertech.esper.client.hook.SQLOutputRowConversion;  \n\nimport com.espertech.esper.client.hook.SQLOutputRowTypeContext;  \n\nimport com.espertech.esper.client.hook.SQLOutputRowValueContext;  \n\npublic class MySQLOutputRowConvertor implements SQLOutputRowConversion {  \n\n    public Class getOutputRowType(SQLOutputRowTypeContext context) {  \n\n        return String.class;  \n\n    }  \n\n    public Object getOutputRow(SQLOutputRowValueContext context) {  \n\n        ResultSet result &#x3D; context.getResultSet();  \n\n        Object obj1 &#x3D; null;  \n\n        Object obj2 &#x3D; null;  \n\n        try {  \n\n            obj1 &#x3D; result.getObject(“id”);  \n\n            obj2 &#x3D; result.getObject(“name”);  \n\n        } catch (SQLException e) {  \n\n            e.printStackTrace();  \n\n        }  \n\n        return obj1 + “ and “ + obj2;  \n\n    }  \n\n}  \n\npackage example;  \n\nimport com.espertech.esper.client.Configuration;  \n\nimport com.espertech.esper.client.EPAdministrator;  \n\nimport com.espertech.esper.client.EPServiceProvider;  \n\nimport com.espertech.esper.client.EPServiceProviderManager;  \n\nimport com.espertech.esper.client.EPStatement;  \n\nimport com.espertech.esper.client.EventBean;  \n\nimport java.util.Iterator;  \n\npublic class SQLOutputRowConversionTest {  \n\n    public static void main(String[] args) throws InterruptedException {  \n\n        Configuration config &#x3D; new Configuration();  \n\n        config.configure(“esper.examples.cfg.xml”);  \n\n        config.addVariable(“vari”, Integer.class, 1);  \n\n        EPServiceProvider epService &#x3D; EPServiceProviderManager.getDefaultProvider(config);  \n\n        EPAdministrator admin &#x3D; epService.getEPAdministrator();  \n\n        String epl1 &#x3D; “@Hook(type&#x3D;HookType.SQLROW, hook&#x3D;’” + MySQLOutputRowConvertor.class.getName()  \n\n                + “‘)select * from sql:test[‘select id, name from test1 where id&#x3D;${vari}’]“;  \n\n        System.out.println(epl1);  \n\n        EPStatement state1 &#x3D; admin.createEPL(epl1);  \n\n        Iterator iter &#x3D; state1.iterator();  \n\n        while (iter.hasNext()) {  \n\n            EventBean eventBean &#x3D; iter.next();  \n\n            System.out.println(eventBean.getUnderlying());  \n\n        }  \n\n    }  \n\n}\n\n\n  \n执行结果：\n\n@Hook(type&#x3D;HookType.SQLROW, hook&#x3D;’example.MySQLOutputRowConvertor’)select * from sql:test[‘select id, name from test1 where id&#x3D;${vari}’]  \n1 and luonq\n\n  \n以上就是EPL和数据库交互的内容，针对普通的查询需求来说还是够用的，至于insert，update，delete我没有举例子，各位可以自己试试看可不可行。\n","slug":"BIGDATA/Esper学习之十：EPL语法（六）","date":"2023-06-09T09:30:55.000Z","categories_index":"name,BIGDATA","tags_index":"esper,sql,import","author_index":"dandeliono"},{"id":"c610892d4b9e5d360c3a54bc62d56b3a","title":"Esper学习之九：EPL语法（五）","content":"Esper学习之九：EPL语法（五）本篇的内容主要包括了Subquery（也就是子查询）和Join，内容不少，但是不难，基本上和sql差不太多。\n1.Subquery\nEPL里的Subquery和sql的类似，是否比sql的用法更多我不得而知，毕竟本人是sql菜鸟，只在where语句里用过子查询。废话不多说，先上几个Subquer的简单用法：\n子查询结果作为外部事件的属性\n\nselect assetId, (select zone from ZoneClosed.std:lastevent()) as lastClosed from RFIDEvent\n\n  \n上面的例子是说返回当前RFIDEvent的assetId属性值和最新ZoneClosed事件的zone属性值，且以lastClosed作为zone的别名。\n子查询关联外部事件的属性\n\nselect * from RfidEvent as RFID where ‘Dock 1’ &#x3D; (select name from Zones.std:unique(zoneId) where zoneId &#x3D; RFID.zoneId)\n\n  \n子查询语句中的where条件可以应用RFID的属性，即内部的zoneId&#x3D;RFID.zoneId\n\nselect zoneId, (select name from Zones.std:unique(zoneId) where zoneId &#x3D; RFID.zoneId) as name from RFIDEvent\n\n  \n关联外部事件属性的同时也可以作为外部事件的属性返回。\n子查询内部事件作为外部事件的属性\n\nselect (select * from MarketData.std:lastevent()) as md from SuperMarket\n\n  \n每进入一个SuperMarket事件就返回最新的MarketData事件作为属性返回，别名为md\n子查询中应用聚合函数\n\nselect * from MarketData where price &gt; (select max(price) from MarketData(symbol&#x3D;’GOOG’).std:lastevent())\n\n  \n子查询出得最大price和当前事件的price进行比较\n\nselect * from OrderEvent oe where qty &gt; (select sum(qty) from OrderEvent.win:time(1 hour) pd where pd.client &#x3D; oe.client)\n\n  \n子查询得出qty的总和和当前事件的qty进行比较\nFilter中使用子查询\n\nselect * from BarData(ticker&#x3D;’MSFT’, closePrice &lt; (select movAgv from SMA20Stream(ticker&#x3D;’MSFT’).std:lastevent()))\n\n  \n子查询返回的movAgv和外部事件的属性closePrice进行比较作为外部事件的一个filter\nPattern中使用子查询\n\nselect * from pattern [  \n  a&#x3D;A -&gt; b&#x3D;B(bvalue &#x3D; (select d_val from DNamedWindow as d where d.d_id &#x3D; b.b_id and d.d_id &#x3D; a.a_id))  \n]\n\n  \npattern的含义可先不深究，这里只要知道子查询可以用在pattern中就行了。\n**Expression中使用子查询**（什么是Expression？请看《Esper学习之五：EPL语法（一）》的第八点）\n\nexpression subq {  \n    (select max(quantity) as maxq, min(quantity) as minq from OrderEvent.win:time(1 min))  \n}  \nselect (quantity - minq) &#x2F; (subq().maxq  - subq().minq) as prorated from OrderEvent\n\n  \n以上就是子查询的几种简单用法，不过有几点注意事项是要各位悉知的：\n1.子查询的返回必须使用data window或者view来进行限制，控制子查询使用的事件数（data window和view相当于具有某种功能性的事件集合）\n2.子查询语句只能由select子句，from子句以及where子句组成，其他的均不支持，比如group by，limit等\n3.没有关联外部事件的子查询语句也可以使用聚合函数\n4.子查询语句中的select子句必须对所有属性使用聚合函数\n5.在使用子查询时，如果子查询的事件和外部事件类型一样，则事件到来时，先经过子查询语句的处理，然后再经过外部语句的处理。如果包含了多个子查询语句，则事件的处理顺序规则较为复杂，本人暂时没做研究。\n针对第4点可能说得有些不明白，特此举例说明下：\n\nclass Apple {  \n\n    private int price;  \n\n    private int size;  \n\n    public void setPrice(int price) {  \n\n        this.price &#x3D; price;  \n\n    }  \n\n    public void setSize(int size) {  \n\n        this.size &#x3D; size;  \n\n    }  \n\n    public int getPrice() {  \n\n        return price;  \n\n    }  \n\n    public int getSize() {  \n\n        return size;  \n\n    }  \n\n}  \n\nclass Fruit {  \n\n}  \n\npublic class Test {  \n\n    public static void main(String[] args) throws InterruptedException {  \n\n        EPServiceProvider epService &#x3D; EPServiceProviderManager.getDefaultProvider();  \n\n        EPAdministrator admin &#x3D; epService.getEPAdministrator();  \n\n        String epl1 &#x3D; “select (select sum(price), sum(size) from “ + Apple.class.getName() + “.std:lastevent()) from “ + Fruit.class.getName();  \n\n        admin.createEPL(epl1);  \n\n        System.out.println(“Create epl successfully!”);  \n\n    }  \n\n}\n\n\n  \n除了上面的语法外，有几个关键字也是需要大家注意的。\nexists\n这个exists和exist可不一样，虽说也是用在where子句中（不仅仅是where子句），但是exists前面是没有什么待比较属性的。语法如下：\n如果exists后面的subquery有查询结果，则返回true，如果没有查询结果就返回false。所以用在where子句中的示例如下：\n\nselect * from Fruit as F where exists (select * from Apple.std:lastevent() where acount &#x3D; F.fcount)\n\n  \n既然exists语法返回的时布尔值，那么可不可以用工在Filter中呢？经本人测试，确实可以用。示例如下：\n\nselect * from Fruit(exists (select * from Apple.win:keepall()))\n\n  \n不过在使用exists有两个注意点：\n1.exists后面的子查询语句一定要用圆括号括起来，遗漏的话会报语法错误。\n2.exists后面的子查询语句只能返回单列值。什么叫单列值？比如：Apple有price和size属性，那么select price和select size都是返回的单列（即一列），select price, size就是多列（即两列）。如果subquery中返回的大于一列，则会报multi-column错误。谨记！\nin &#x2F; not in\n用法和sql的一样，具体语法就不说了，直接看例子吧：\n\n&#x2F;&#x2F; 当前进入的Apple事件的aPrice存在于过去十秒内进入的Fruit的fPrice即可返回  \nselect * from Apple where aPrice in (select fPrice from Fruit.win:time(10 s))\n\n  \n注意：从语法上说in&#x2F;not in前面是expression，并没有规定只能是属性。比如：Apple有aPrice和aSize两个int属性，则where子句可以写成”aPrice+aSize in …..”（这里只是举例，不expression不表示任何意思）\nany &#x2F; some &#x2F; all\n除了用in或者not in判断属性值是否存在于子查询结果中，还可以使用any&#x2F;some&#x2F;all并配合一些比较符号与子查询结果进行比较。语法如下：\n\nexpression operator any&#x2F;some&#x2F;all (subquery)  \n&#x2F;&#x2F; operator包含：&gt;&#x3D;, !&#x3D;, &lt;&gt;, &lt;, &lt;&#x3D;, &gt;, &gt;&#x3D;\n\n  \n一个简单的例子：\n\n&#x2F;&#x2F; 如果当前Apple事件的aPrice小于前十个Fruit中的任何一个fPrice，则返回此事件  \n\nselect * from Apple where aPrice &lt; any (select fPrice from Fruit.win:length(10))  \n\n&#x2F;&#x2F; 如果当前Apple事件的aPrice小于前十个Fruit中的所有fPrice，则返回此事件  \n\nselect * from Apple where aPrice &lt; all (select fPrice from Fruit.win:length(10))\n\n\n  \nsome和any同义，所以用法也相同。既然一样为什么又要弄这么个关键字，我也没搞懂。。。\n子查询返回多列数据\n子查询可以返回单列数据，也可以返回多列数据。用法很简单，只需要“.”（点）就能找到每列的数据了。示例如下：\n\nselect *,  \n  (select bid, offer from MarketData.std:unique(symbol) as md where md.symbol &#x3D; oe.symbol) as bidoffer  \nfrom OrderEvent oe\n\n  \n如上所示，子查询语句中包含bid和offer两列，并且这个语句的结果用bidoffer作为别名，所以要想得到bid和offer的具体数据只需要bidoffer.bid和bidoffer.offer即可，简单吧。子查询返回多行数据和返回多列数据类似，只不过数据会以数组形式是返回。\n2.Join\nJoin在sql里是很常见的查询方法，EPL也同样支持，并且包含了full outer join &#x2F; left outer join &#x2F; right outer join &#x2F; inner join等。和sql基本无差别。\ninner join\n在没有任何关键字的修饰下，即为默认join方式，也就是inner join。必须等到所有join的事件都到了才可能输出，因为要是有where关联两个事件，得满足where条件了才能输出。例如：\n\nselect * from Orange.std:lastevent(), Banana.std:lastevent()\n\n  \n如果只有Orange到或者Banana到都不会输出内容。std:lastevent是一种data window。如果不加特殊修饰的话（特殊修饰？下面会告诉你），事件必须有data window或者view修饰，否则会出现语法错误。当然，不同的事件可以用不同的data window修饰，并没有规定要一样。\n评论里c77_cn有问到：\n为什么”select a.id, b.field from Event as a, method : Static Class . getMethod (a.id) as b”没有window或者view也报错？\n       后来我重读了一下官方文档，没有指明必须要window或者view。我这么写是因为我测时候的时候，错误里说明了需要window或者view（在没有unidirectional修饰的情况下，这个修饰就是刚才说的“特殊修饰”）。这个句子正确我猜测method方法是每到一个事件都会被调用，所以能够完成join的工作。如果不用method，在没有window或者view的情况下，两类不同的事件分先后进入引擎，那先进入的的事件不能暂存的话如何完成join呢？\nfull outer join\n上面看到的默认join方式是要求所有join的事件都必须到达引擎才会输出，并且join的事件之间通过where子句设置了条件判断，还得到达的两个事件满足条件了才能输出，而full outer join正好解决了这个问题，不管哪个事件到达，不管是否符合条件，都会输出。例如：\n\nselect * from Orange.std:lastevent() as o full outer join Banana.std:lastevent() as b on o.price &#x3D; b.price\n\n  \n输出结果有4种可能：\na.当只有Orange事件到达，没有满足join条件，会输出Orange事件，且Banana事件为null。\nb.当只有Banana事件到达，没有满足join条件，会输出Banana事件，且Orange事件为null。\nc.当两个事件都到达了，且没有满足join条件，即price不相等，则a，b情况各出现一次。\nd.当两个事件都到达了，且满足join条件，即price相等，即可输出满足条件的事件。\n所以说不管什么情况下，当前进入的事件都会输出，至于join的那个事件，满足即输出事件，不满足即输出null。\nleft outer join\nfull outer join输出了所进入的所有事件，不满足join条件的就输出null，而left outer join则规定关键字左边的事件可以即刻输出，而关键字右边的事件必须满足join条件才可输出。示例如下：\n\nselect * from Pink.std:lastevent() as pi left outer join Pear.std:lastevent() as pe on pi.price &#x3D; pe.price\n\n  \n因为Pink事件在left outer join的左边，所以他的输出不受join条件的限制，即事件到来该怎么输出怎么输出。但是Pear就不同，由于有join条件限制，即两个事件的price要相等，所以如果Pear事件到达的时候，如果没有满足条件的Pink事件，则Pear事件是不会输出的。（注意：输出null也算输出，这里是null都不会输出，即不触发listener）\nright outer join\n和left outer join相反，在关键字右边的事件不受join条件约束，而左边的事件必须满足join条件才可输出。具体例子我就不举了，大家可以写两个句子试试。\n此外，在使用以上4种join的时候，可以多种join混用。比如：\n\nselect * from Apple.std:lastevent() as a   \n     left outer join Banana.std:lastevent() as b on a.price &#x3D; b.price   \n     full outer join Orange.std:lastevent() as o on o.price &#x3D; a.price\n\n  \non后面的表达式是join的限制条件，且只能用“&#x3D;”，如果想用其他操作符，则必须放到where子句中，这点尤其要注意。多个限制条件只能用and连接，不能用逗号，且限制的事件也要一样。比如：\n\n&#x2F;&#x2F; a，b分别是两个事件的别名  \n\n&#x2F;&#x2F; 正确写法  \n\n……on a.price &#x3D; b.price and a.size &#x3D; b.size……  \n\n&#x2F;&#x2F; 错误写法1：不能用逗号连接  \n\n……on a.price &#x3D; b.price, a.size &#x3D; b.size……  \n\n&#x2F;&#x2F; 错误写法2：必须针对同样的事件进行限制（c是另一个事件的别名）  \n\n……on a.price &#x3D; b.price and a.size &#x3D; c.size……\n\n\n  \nUnidirectional Join\n之前说到，如果不加特殊修饰，则join的事件都需要data window或者view修饰，目的是为了暂存事件以便等待满足条件的事件并执行join。如果想让某个事件到来时直接触发join，不需要暂存，也就是不需要data window或者view修饰，则需要加上一个特殊关键字——unidirectional。先看一个简单的例子：\n\nselect * from Apple as a unidirectional, Banana.std:lastevent() as b where a.price &#x3D; b.price\n\n  \n上句的意思是：维持最新的Banana事件，直到一个和Banana的price相等的Apple事件到来时输出两者。\n       由于有unidirectional的修饰，表明Apple事件是即时出发join操作，也就是说进入此EPL的Apple事件是无状态的。所以当Apple事件到来时，如果没有price相等的Banana，则什么输出也没有，即使下一个Banana事件的price和之前来的Apple的price相等也不会有输出，因为那个Apple事件已经从这个句子的上下文中移除了。为了更好的看到效果，我给大家写了一给完整的例子：\n\nclass Orange {  \n\n    private int price;  \n\n    public void setPrice(int price) {  \n\n        this.price &#x3D; price;  \n\n    }  \n\n    public int getPrice() {  \n\n        return price;  \n\n    }  \n\n    @Override  \n\n    public String toString() {  \n\n        return “Orange price&#x3D;” + price;  \n\n    }  \n\n}  \n\nclass Banana {  \n\n    private int price;  \n\n    public int getPrice() {  \n\n        return price;  \n\n    }  \n\n    public void setPrice(int price) {  \n\n        this.price &#x3D; price;  \n\n    }  \n\n    @Override  \n\n    public String toString() {  \n\n        return “Banana price&#x3D;” + price;  \n\n    }  \n\n}  \n\nclass JoinUnidirectionalListener implements UpdateListener {  \n\n    public void update(EventBean[] newEvents, EventBean[] oldEvents) {  \n\n        if (newEvents !&#x3D; null) {  \n\n            System.out.println(newEvents[0].get(“o”) + “, “ + newEvents[0].get(“b”));  \n\n        }  \n\n    }  \n\n}  \n\npublic class JoinUnidirectionalTest {  \n\n    public static void main(String[] args) throws InterruptedException {  \n\n        EPServiceProvider epService &#x3D; EPServiceProviderManager.getDefaultProvider();  \n\n        EPAdministrator admin &#x3D; epService.getEPAdministrator();  \n\n        String epl1 &#x3D; “select * from “ + Orange.class.getName() + “ as o unidirectional, “ + Banana.class.getName()  \n\n                + “.std:lastevent() as b where o.price &#x3D; b.price”;  \n\n        EPStatement stat &#x3D; admin.createEPL(epl1);  \n\n        stat.addListener(new JoinUnidirectionalListener());  \n\n        EPRuntime runtime &#x3D; epService.getEPRuntime();  \n\n        Orange o1 &#x3D; new Orange();  \n\n        o1.setPrice(1);  \n\n        System.out.println(“Send Orange1”);  \n\n        runtime.sendEvent(o1);  \n\n        Banana b1 &#x3D; new Banana();  \n\n        b1.setPrice(1);  \n\n        System.out.println(“Send Banana1”);  \n\n        runtime.sendEvent(b1);  \n\n        Banana b2 &#x3D; new Banana();  \n\n        b2.setPrice(2);  \n\n        System.out.println(“Send Banana2”);  \n\n        runtime.sendEvent(b2);  \n\n        Orange o2 &#x3D; new Orange();  \n\n        o2.setPrice(2);  \n\n        System.out.println(“Send Orange2”);  \n\n        runtime.sendEvent(o2);  \n\n    }  \n\n}\n\n\n  \n执行结果：\n\nSend Orange1  \nSend Banana1  \nSend Banana2  \nSend Orange2  \nOrange price&#x3D;2, Banana price&#x3D;2\n\n  \n可以看到o1和b1的price相等，o2和b2的price相等。o1先于b1进入引擎，由于o1进入时发现没有满足条件的Banana事件，所以什么输出也没有。之后b1进入了，因为满足条件的o1已经移除了，所以也是没有输出。b2先于o2进入引擎，被引擎暂存了起来，然后o2进入后，立刻进行join条件判断，发现暂存的b2的price相等，所以触发了listener并输出满足条件的这两个对象。\nunidirectional使用很简单，但是也有其限制：\n1.在一个join句子中，unidirectional关键字只能用于一个事件流。\n2.用unidirectional修饰的事件流，不能通过esper的查询api查出来，因为该事件流是无状态的，不会暂存在引擎中，所以就没法查了。（关于查询api，后面的章节会详说）\n3.使用了unidirectional修饰的事件流就不能再用data window或者view修饰了，也就是说他们是互斥的。\n","slug":"BIGDATA/Esper学习之九：EPL语法（五）","date":"2023-06-09T09:25:57.000Z","categories_index":"price,BIGDATA","tags_index":"select,join,from","author_index":"dandeliono"},{"id":"0b518321d13efd904f78fbe39f8f7eba","title":"Esper学习之八：EPL语法（四）","content":"Esper学习之八：EPL语法（四）关于EPL，已经写了三篇了，预估计了一下，除了今天这篇，后面还有5篇左右。大家可别嫌多，官方的文档对EPL的讲解有将近140页，我已经尽量将废话都干掉了，再配合我附上的例子，看我的10篇文章比那140页英文文档肯定舒服多了吧。也请各位原谅我一周一篇的速度，毕竟我还要学习，生活，工作，一个都不能少。\n        今天讲解的内容包括三块：Order by，Limit，Insert into。大家会SQL的应该很熟悉这三个东西，前两个比较简单，Insert into会有一些差别，篇幅也相对多些。\n1.Order by\nEPL的Order by和SQL的几乎一模一样，作用都是对输出结果进行排序，但是也有一些需要注意的地方。语法如下：\n\norder by expression [asc | desc] [, expression [asc | desc]] [, …]\n\n  \nexpreession表示要排序的字段，asc表示升序排列（从小到大），desc表示降序排列（从大到小）。举个例子：\n\n&#x2F;&#x2F; 每进入5个事件输出一次，并且先按照name升序排列，再按照age降序排列。  \nselect * from User output every 5 events order by name, age desc\n\n  \n使用方法很简单，除了和SQL相似的特点外，还有他自己需要注意的几点：\na. 如果不特别说明是升序还是降序，默认情况下按照升序排列。\nb. 如果order by的子句中出现了聚合函数，那么该聚合函数必须出现在select的子句中。\nc. 出现在select中的expression或者在select中定义的expression，在order by中也有效。\nd. 如果order by所在的句子没有join或者没有group by，则排序结果幂等，否则为非幂等。\n2. Limit\nLimit在EPL中和在SQL中也基本一样，不过SQL中是用具体的数字来表示限制范围，而EPL可以是常量或者变量来表示限制范围。语法如下：\n\nlimit row_count [offset offset_count]\n\n  \nrow_count表示输出多少行，可以是一个整型常量，也可以是一个整型变量，以方便运行时修改。\noffset_count表示在当前结果集中跳过n行然后再输出，同样也可以是一个整型变量。如果不使用此参数，则表示跳过0行，即从第一行输出。举例如下：\n\n&#x2F;&#x2F; 输出结果集的第3行到第10行  \nselect uri, count() from WebEvent group by uri output snapshot every 1 minute order by count() desc limit 8 offset 2\n\n  \n除了以上的语法，limit还有一种简化的写法，实际上是参照SQL的标准。\n\nlimit offset_count[, row_count]\n\n  \n两个参数的含义和上面的一样，并且我们将上面的例子改写一下：\n\n&#x2F;&#x2F; 输出结果集的第3行到第10行  \nselect uri, count() from WebEvent group by uri output snapshot every 1 minute order by count() desc limit 2, 8\n\n  \n如果这个两个参数是负数会怎么样呢？\nrow_count为负数，则无限制输出，若为0，则不输出。当row_count是变量表示并且变量为null，则无限制输出。\noffset _count是不允许负数的，如果是变量表示，并且变量值为null或者负数，则EPL会把他假设为0。\n3. Insert into\n3.1 简单用法\n        EPL的Insert into和SQL的有比较大的区别。SQL是往一张表里插入数据，而EPL是把一个事件流的计算结果放入另一个事件流，然后可以对这个事件流进行别的计算。所以Insert into的一个好处就是可以将是事件流的计算结果不断级联，对于那种需要将上一个业务的结果数据放到下一个业务处理的场景再适合不过了。除此之外，Insert into还有合并多个计算结果的作用。到这里相信大家已经对他越来越好奇了，不急，咱们先来看看语法：\n\ninsert [istream | irstream | rstream] into event_stream_name [ (property_name [, property_name] ) ]\n\n  \nevent_stream_name定义了事件流的名称，在执行完insert的定义之后，我们可以使用select对这个事件流进行别的计算。\nistream | irstream | rstream表示该事件流允许另一个事件的输入&#x2F;输入和输出&#x2F;输出数据能够进入（解释好像很绕。。一会儿看例子就能明白了）\nproperty_name表示该事件流里包含的属性名称，多个属性名之间用逗号分割，并且用小括号括起来。\n上面的说明可能不是很好理解，咱们先看个例子：\n\n&#x2F;&#x2F; 将新进入的Asus事件传递到Computer，且Asus的id，size和Computer的cid，csize对应  \n\ninsert into Computer(cid,csize) select id,size from Asus  \n\n&#x2F;&#x2F; 第二种写法  \n\ninsert into Computer select id as cid, size as csize Asus\n\n\n  \n        从例子中可以看到，insert into需要配合select进行使用，以表明前一个事件流有哪些计算结果将进入insert into定义的事件流。并且在select中的字段要和insert里的事件流的属性要对应（这里指的对应是数据类型对应，而且属性数量也必须一样）。如果说insert定义的事件流名称在之前已经定义过（insert into中定义的除外），重名是不允许的。\n        我个人推荐第二种写法，通过as设置的别名即为insert定义的事件流的属性，这样可以避免属性的个数不一致的错误。\n刚才说了istream | irstream | rstream的用法，可能有点表述不清楚，这里看一个完整的例子。\n执行结果：\n\nSend Asus: id: 1, size: 1  \n\nSend Asus: id: 2, size: 1  \n\nSend Asus: id: 3, size: 3  \n\nInsert Asus: cid: 1  \n\nInsert Asus: cid: 2  \n\nSend Asus: id: 4, size: 4  \n\nSend Asus: id: 5, size: 3  \n\nInsert Asus: cid: 3  \n\nInsert Asus: cid: 4  \n\nSend Asus: id: 6, size: 4\n\n\n  \n        这个例子中，insertEPL表示当Asus事件从length为1的view中移除时，把移除的事件放入Computer。insertSelectEPL是对Computer的事件流进行计算，这里只是在每进入两个事件时才输出这两个事件的cid。而rstream在这里的表现，从执行结果中可以看到，在进入id为1 2 3的事件后，insertSelectEPL的监听器被触发，因为id为1和2的事件是在发送了Asus的id为2和3的事件之后被移除了，之后就进入了Computer，并满足了length&#x3D;2，因此在监听器里看到有id为1和2的事件进入了Computer。\n        如果不显示指定rstream，则insert into只允许istream的事件流进入Computer。如果指定为irstream，那么进入的和移除的Asus都会进入到Computer。\n上面的例子都是指定了insert into里事件流会有什么属性，如果不指定会是什么结果呢？请看例句：\n\ninsert into Computer select * from Asus\n\n  \n        很容易想到，这里实际上是把进入引擎的Asus事件都传递到Computer定义的事件流中，并且属性什么的完全和Asus一样，可以说是Asus的一个复制版本，只是名字不一样。也许有人觉得这么做没什么意思，直接计算Asus事件流不就可以了么，实际上在业务处理数据时，这种做法就可以屏蔽掉外部的数据来源，做到业务层上的隔离。\n        假设Asus中还包含其他的JavaBean，同样也可以将这个Bean的数据传递到另一个事件流中。例句如下：\n\n&#x2F;&#x2F; Lenovo中包含了thinkpad这个JavaBean  \ninsert into Computer select thinkpad.* from Lenovo\n\n  \n3.2 Merge Event Stream\n        insert into除了接收一个流的事件，同时也支持多个流的合并。通俗一点来说，合并的流数据要一致才可以合并。而且在第一次定义insert的事件流以后，别的事件流想要被合并就必须和之前定义的属性数量和数据类型对应。举例如下：\n\n&#x2F;&#x2F; 定义Computer并把Asus的数据输入  \n\ninsert into Computer(cid, csize) select aid,asize from Asus  \n\n&#x2F;&#x2F; 根据之前的Computer定义将Lenovo对应的属性输入  \n\ninsert into Computer(cid, csize) select lid,lsize from Lenovo\n\n\n  \n如果说select了多个事件流，但是你只想输入其中一个，应该像下面这样写：\n\ninsert into Computer select l.* from Asus as a, Lenovo as l\n\n  \n除此之外，EPL还支持调用函数转换事件后再输入insert into：\n\n&#x2F;&#x2F; 将Lenovo事件转换后输入Computer  \ninsert into Computer select Converter.convert(l) from Lenovo as l\n\n  \n注意，使用自定义函数一定要返回javabean，map，或者Object数组，且不能用as来为转换后的结果设置别名。\n3.3 Decorated Events\n之前所见到的不是将事件流整体输入insert，就是将事件流的部分属性输入insert。实际上可以将事件流整体和事件流属性组成的复杂表达式一起放入insert。示例如下：\n\ninsert into Computer select , sizeprice as sp from Asus  \n&#x2F;&#x2F; 第一个*表示Asus，size*price的*表示乘法，两者互不影响\n\n  \n如果说别的事件流想进入此insert，那么事件流属性一定要和第一个*表示的所有属性相同。\n3.4 Event Objects Instantiated by insert into\n        前面的所有例子中，对于insert into的事件结构都是在insert子句中配合select子句进行定义的。如果我们想用已经定义好的事件结构是否可以呢？答案是肯定的。但是如果事件是javabean，并且事先没有注册到引擎，则需要insert子句中写上类的全名。例如：\n\ninsert into test.computer.Computer …\n\n  \n当然，如果在使用之前有注册过，那么使用注册时的名称也是可以的。\n        因为事件结构是早就定义好的，所以在写select的时候就必须符合insert事件中的属性了，如果属性名称不一样需要使用as加上别名，一样的可以不用设置别名，且数据类型也要一一对应。例如：\n\n&#x2F;&#x2F; Computer中包含cid和csize属性  \ninsert into test.computer.Computer select aid as cid, asize as csize from Dell\n\n  \n但是这种用法在Computer存在包含了参数的构造方法时就显得没那么必须了。先看一个完整例子，你也许就会明白了。\n执行结果：\n\nAutoSize: 1, AutoName: car1  \nBenzSize: 1, BenzName: car1  \nAutoSize: 2, AutoName: car2  \nBenzSize: 2, BenzName: car2\n\n  \n这里的执行结果很容易理解，关键是carToAutoEpl和carToBenzEpl两个句子。\n        对于Auto的JavaBean，我们可以发现它包含一个有参数的构造函数且没有属性对应的set方法，在carToAutoEpl中，select的内容并没有和属性名称对应起来。这种写法确实是正确的，正因为Auto中定了含参的构造函数，才使得select可以写的更随意。但是一定要记住，构造函数里的参数顺序一定要和select中的属性的数据类型对应起来，如果这里把name和size互换位置，必定报错！\n        对于Benz的JavaBean，可以看到每个属性都有对应的set方法，而没有含参的构造函数，所以select中属性的名称需要as来设置别名。当然，像benzEpl2那种写法，同样可以避免select中设置别名。\n        一句话总结，如果JavaBean中有含参的构造函数，EPL中不需要显示写出属性名称。如果没有构造函数，那么必须包含set方法，且select中要写出具体的属性。这几种写法各有各的好处，大家使用时可针对具体的情况选择性使用。\n","slug":"BIGDATA/Esper学习之八：EPL语法（四）","date":"2023-06-08T18:08:42.000Z","categories_index":"insert,BIGDATA","tags_index":"select,Asus,into","author_index":"dandeliono"},{"id":"7e07be7bd570848d0be5a40e5365e5e4","title":"Esper学习之七：EPL语法（三）","content":"Esper学习之七：EPL语法（三）1.Aggregation\n和SQL一样，EPL也有Aggregation，即聚合函数。语法如下：\n\naggregate_function([all|distinct] expression)\n\n  \naggregate_function就是聚合函数的名字，比如avg，sum等。expression通常是事件流的某个属性，也可以是不同事件流的多个属性，或者是属性和常量、函数之间的运算。举例如下。\n\n&#x2F;&#x2F; 查询最新5秒的Apple的平均价格  \n\nselect avg(price) as aPrice from Apple.win:time(5 sec)  \n\n&#x2F;&#x2F; 查询最新10个Apple的价格总和的两倍  \n\nselect sum(price*2) as sPrice from Apple.win:length(10)  \n\n&#x2F;&#x2F; 查询最新10个Apple的价格，并用函数计算后再算平均值  \n\nselect avg(Compute.getResult(price)) from Apple.win:length(10)\n\n\n  \n函数只能是静态方法，普通方法不可用。即使是事件流里包含的静态方法，也必须用“类名.方法名”的方式进行引用。\n可以使用distinct关键字对expression加以约束，表示去掉expression产生的重复的值。默认情况下为all关键字，即所有的expression值都参与聚合运算。例如：\n\n&#x2F;&#x2F; 查询最新5秒的Apple的平均价格  \n\nselect avg(distinct price) as aPrice from Apple.win:time(5 sec)  \n\n&#x2F;&#x2F; 假如：5秒内进入了三个Apple事件，price分别为2,1,2。则针对该EPL的平均值为(2+1)&#x2F;2&#x3D;1.5。因为有distinct的修饰，所以第二个2不参与运算，事件总数即为2，而不是3。\n\n\n  \n以上就是聚合函数的使用方法，除此之外需要注意一下几点\n1.聚合函数能用于Select和Having，但是不能用于Where\n2.sum,avg,media,stddev,avedev只能计算数值，至于media，stddev和avedev代表什么意思，请自行百度。\n3.Esper会忽略expression为null不让参与聚合运算，但是count函数除外，即使是null也认为是一个事件。如果事件流集合中没有包含任何事件，或者包含的事件中用于聚合计算的expression都是null（比如收集5秒内进入的事件即为一个事件流集合），则所有聚合函数都返回null。\n2.Group by\nGroup by通常配合聚合函数使用。语法和SQL基本一样，产生的效果就是以某一个或者多个字段进行分组，然后使聚合函数作用于不同组的数据。简单语法如下：\n\ngroup by aggregate_free_expression [, aggregate_free_expression] [, …]\n\n  \n使用Group by要注意一下几点：\n1.Group by后面的内容不能包含聚合函数\n2.Group by后面的内容不能是之前select子句中聚合函数修饰的属性名\n3.通常情况要保证分组数量有限制，以防止内存溢出。但是如果分组分了很多，就需要使用@Hint加以控制。\n2.1.Group by基本用法\n针对上面的第三点，后面再说，先举几个例子说明下简单用法：\n\n&#x2F;&#x2F; 根据color和size来对10个Apple事件进行分组计算平均price  \nselect avg(price) as aPrice, color, size from Apple.win:length_batch(10) group by color,size\n\n  \n该句子遵从SQL的标准，如果某个事件的color和size和之前进入的事件的一样，则归为一组，否则新建一组，并计算平均price\n\n&#x2F;&#x2F; 根据size来对10个Apple事件进行分组计算平均price和color  \nselect avg(price) as aPrice, color, size from Apple.win:length_batch(10) group by size\n\n  \n可以发现，group by的对象只有size，而select中color不聚合，则生成的结果时，聚合函数会根据相同的size分组进行平均price的计算，但是color不是分组条件，所以color有多少个就有多少组，即使存在一样的color也不会影响分组数量（实际上就是不分组），但一定记住，聚合函数还是会根据分组条件计算其修饰的属性。\n\n&#x2F;&#x2F; 根据size来对10个Apple事件进行分组计算平均price和colorselect avg(price) as aPrice, color from Apple.win:length_batch(10) group by size\n\n  \n这一次select子句中没有包含分组的字段size，但是效果和上一个句子一样。Esper仍然会根据相同的size进行分组计算平均price，只不过计算结果中只有平均price和color，并且有十排结果。\n\n&#x2F;&#x2F; 根据size乘color来对10个Apple事件进行分组计算平均priceselect avg(price) as aPrice, size*color from Apple.win:length_batch(10) group by size*color\n\n  \ngroup by的对象只是一个值，以相同的值进行分组，所以上面和和普通的属性字段一样，计算一个值进行分组。如果group by后面的表达式值为null，则所有为null的事件都被分为一组进行计算。但是如果使用了count函数，则表达式为null的事件不会被计算在内。\n2.2.@Hint\n@Hint是Esper中注解的其中一个，如果不了解注解，可以先看看Esper学习之五：EPL语法（一）的第7节再继续阅读@Hint的内容。之前对@Hint一笔带过，那是因为它是专用于Group by的。我们平时使用Group by的时候，会遇到分组数量太多的情况。比如以时间单位进行分组，那么内存使用一定是一个大问题。因此@Hint为其设计了两个属性，用于限制Group by的生存时间，使虚拟机能及时回收内存。这两个属性分别为reclaim_group_aged和reclaim_group_freq\nreclaim_group_aged\n该属性后面跟着的是正整数，以秒为单位，表示在n秒内，若分组的数据没有进行更新，则分组数据被Esper回收。例如：\n\n&#x2F;&#x2F; 根据color对10秒内进入的Apple事件进行分组计算平均price,并且对5秒内没有数据更新的分组进行回收  \n@Hint(‘reclaim_group_aged&#x3D;5’)select avg(price) as aPrice, color from Apple.win:time(10 sec) group by color &#x2F;&#x2F;括号内可以使单引号也可以是双引号\n\n  \nreclaim_group_freq\n该属性后面跟着的是正整数，以秒为单位，表示每n秒清理一次分组，可清理的分组是reclaim_group_aged决定的，也就是说要使用该参数，就要配合reclaim_group_aged一起使用。可能不是很好理解，先看看例子：\n\n&#x2F;&#x2F; 根据color对10秒内进入的Apple事件进行分组计算平均price。对8秒内没有数据更新的分组进行回收,每2秒回收一次  \n@Hint(‘reclaim_group_aged&#x3D;8，reclaim_group_freq&#x3D;2’)select avg(price) as aPrice, color from Apple.win:time(10 sec) group by color\n\n  \n        如果不使用reclaim_group_freq属性，则默认值和reclaim_group_aged的值一样，对上面来说就是回收的条件为8秒内没有数据更新，且每8秒回收一次。这样的话有可能出现这么一种情况，上一个8秒的某个分组在下一个8秒还没到达时就已经持续8秒没有数据更新了（这句话会不会有点绕？），但是必须等到回收的时间点到达时才能回收这个分组。在分组产生很快的情况下，这样的回收不及时很可能会造成内存溢出。reclaim_group_freq正是为这种情况做准备，回收的频率高一些，在一定程度上能提高内存的使用率。\n        上面这两个属性的值除了可以使用正整数之外，也可以使用预先定义的变量或者常量\n3.Having\nHaving的用法和SQL一样，后面跟的是对聚合函数的计算结果进行过滤。Where子句不能包含聚合函数，所以就由Having来完成。示例如下：\n\n&#x2F;&#x2F; 根据size来对10个Apple事件进行分组计算平均price和color,并且排除平均price大于5的分组select avg(price) as aPrice, color from Apple.win:length_batch(10) group by size having avg(price) > 5\n\n  \n通常Having配合Group by使用，如果没有使用Group by，那么就只有一组。例如：\n\n&#x2F;&#x2F; 根据size来对10个Apple事件计算平均price和color,如果平均price大于5，则数据被排除掉select avg(price) as aPrice, color from Apple.win:length_batch(10) having avg(price) > 5\n\n  \nHaving后面可以跟多个判断式子，并且用and，or或者not进行连接。例如：\n\n&#x2F;&#x2F; 根据size来对10个Apple事件计算平均price和color,如果平均price大于5并且平均size小于3，则数据被排除掉select avg(price) as aPrice, color from Apple.win:length_batch(10) having avg(price) > 5 and avg(size) \n\n  \n4.Output\n4.1.基本语法\nOutput是EPL中非常有用的东西，用来控制Esper对事件流计算结果的输出时间和形式，可以以固定频率，也可以是某个时间点输出。简单语法如下：\n\noutput [after suppression_def]  \n[[all | first | last | snapshot] every time_period | output_rate events]\n\n  \nafter suppression_def是可选参数，表示先满足一定的条件再输出。\nall | first | last | snapshot表明输出结果的形式，默认值为all。\nevery output_rate表示输出频率，即每达到规定的频率就进行输出。time_period表示时间频率，相关语法在Esper学习之五：EPL语法（一）的第2节有说到。output_rate events表示事件数量。\n举例说明如下：\n\n&#x2F;&#x2F; 30分钟内，每进入一个OrderEvent，统计一次sum price，并且每60秒输出一次统计结果。  \nselect sum(price) from OrderEvent.win:time(30 min) output snapshot every 60 seconds\n\n  \n4.2.after\n之前在讲解Context的时候，有简单说到过after。关于Context，可参看Esper学习之四：Context。after在output里的使用也很简单，语法如下：\n\noutput after time_period | number events […]\n\n  \ntime_period表示时间段，number events表示事件数量。表示从EPL可用开始，经过一段时间或者接收到一定数量的事件再进行输出。例如：\n\n&#x2F;&#x2F; 统计20个Apple事件的sum price，并且在有5个Apple事件进入后才开始输出统计结果  \nselect sum(price) from Apple.win:length(20) output after 5 events\n\n  \n上面这个句子从第一个进入的事件进行统计，直到进入了5个事件以后才输出统计结果，之后每进入一个事件输出一次（这是win:length的特性）。但是要注意的是，after之后的时间长度和事件数量会影响之后的时间或者事件数量。什么意思？看个完整例子：\n\nclass Banana  \n\n{  \n\n    private int id;  \n\n    private int price;  \n\n    public int getId()  \n\n    {  \n\n        return id;  \n\n    }  \n\n    public void setId(int id)  \n\n    {  \n\n        this.id &#x3D; id;  \n\n    }  \n\n    public int getPrice()  \n\n    {  \n\n        return price;  \n\n    }  \n\n    public void setPrice(int price)  \n\n    {  \n\n        this.price &#x3D; price;  \n\n    }  \n\n    public String toString()  \n\n    {  \n\n        return “id: “ + id + “, price: “ + price;  \n\n    }  \n\n}  \n\nclass OutputAfterListener implements UpdateListener  \n\n{  \n\n    public void update(EventBean[] newEvents, EventBean[] oldEvents)  \n\n    {  \n\n        if (newEvents !&#x3D; null)  \n\n        {  \n\n            int price &#x3D; (Integer) newEvents[0].get(“sPrice”);  \n\n            System.out.println(“Banana’s sum price is “ + price);  \n\n        }  \n\n    }  \n\n}  \n\npublic class OutputAfterTest  \n\n{  \n\n    public static void main(String[] args) throws InterruptedException  \n\n    {  \n\n        EPServiceProvider epService &#x3D; EPServiceProviderManager.getDefaultProvider();  \n\n        EPAdministrator admin &#x3D; epService.getEPAdministrator();  \n\n        String banana &#x3D; Banana.class.getName();  \n\n        String epl &#x3D; “select sum(price) as sPrice from “ + banana + “.win:length(3) output after 1 events snapshot every 2 events”;  \n\n        EPStatement state &#x3D; admin.createEPL(epl);  \n\n        state.addListener(new OutputAfterListener());  \n\n        EPRuntime runtime &#x3D; epService.getEPRuntime();  \n\n        Banana b1 &#x3D; new Banana();  \n\n        b1.setId(1);  \n\n        b1.setPrice(6);  \n\n        System.out.println(“Send Banana Event: “ + b1);  \n\n        runtime.sendEvent(b1);  \n\n        Banana b2 &#x3D; new Banana();  \n\n        b2.setId(2);  \n\n        b2.setPrice(3);  \n\n        System.out.println(“Send Banana Event: “ + b2);  \n\n        runtime.sendEvent(b2);  \n\n        Banana b3 &#x3D; new Banana();  \n\n        b3.setId(3);  \n\n        b3.setPrice(1);  \n\n        System.out.println(“Send Banana Event: “ + b3);  \n\n        runtime.sendEvent(b3);  \n\n        Banana b4 &#x3D; new Banana();  \n\n        b4.setId(4);  \n\n        b4.setPrice(2);  \n\n        System.out.println(“Send Banana Event: “ + b4);  \n\n        runtime.sendEvent(b4);  \n\n        Banana b5 &#x3D; new Banana();  \n\n        b5.setId(5);  \n\n        b5.setPrice(4);  \n\n        System.out.println(“Send Banana Event: “ + b5);  \n\n        runtime.sendEvent(b5);  \n\n    }  \n\n}\n\n\n  \n执行结果：\n\nSend Banana Event: id: 1, price: 6  \nSend Banana Event: id: 2, price: 3  \nSend Banana Event: id: 3, price: 1  \nBanana’s sum price is 10  \nSend Banana Event: id: 4, price: 2  \nSend Banana Event: id: 5, price: 4  \nBanana’s sum price is 7\n\n  \n由此可见，after之后的every子句要等到after后面的表达式满足后才生效。所以第一个事件进入后，every 2 events生效，即等待两个事件进入后才输出结果。对于时间也是要等到after的子句满足后才开始计时。例如：\n\n&#x2F;&#x2F; 从EPL可用开始计时，经过1分钟后，每5秒输出一次当前100秒内的所有Banana的avg price（即：第一次输出在65秒时）  \nselect avg(price) from Banana.win:time(100 sec) after 1 min snapshot every 5 sec\n\n  \n4.3.first，last，all，snapshot\n每当达到输出时间点时，可以用这四个参数来控制输出内容。下面分别介绍并举例。\nfirst\n表示每一批可输出的内容中的第一个事件计算结果。比如：\n\nselect * from Fruit output first every 2 events\n\n  \n上面的句子表示每进入两个Fruit事件，输出这两个事件的第一个。last\n和first类似，表示每一批可输出的内容中的最后一个事件计算结果。比如：\n\nselect * from Fruit output last every 2 events\n\n  \n上面的句子表示每进入两个Fruit事件，输出这两个事件的第二个，也就是最后一个。\nsnapshot\n表示输出EPL所保持的所有事件计算结果，通常用来查看view或者window中现存的事件计算结果。比如：\n\nselect * from Fruit.win:time(5 sec) output snapshot every 2 events\n\n  \n上面的句子表示每进入两个事件输出5 sec内的所有事件，且不会讲这些事件从5 sec范围内移除\nall\n也是默认值。和snapshot类似，也是输出所有的事件，但是不同的是，snapshot相当于对计算结果拍了一张照片，把结果复制出来并输出，而all是把计算结果直接输出，不会复制。比如：\n\nselect * from Fruit.win:time(5 sec) output all every 2 events\n\n  \n上面的句子表示每进入两个事件输出5 sec内包含的所有事件，输出的事件不再保留于5 sec范围内。\n4.4.Crontab Output\noutput的另一个语法可以建立定时输出，关键字是at。语法如下：\n\noutput [after suppression_def]  \n[[all | first | last | snapshot] at  \n(minutes, hours, days of month, months, days of week [, seconds])]\n\n  \nminutes, hours, days of month, months, days of week [, seconds]这些都是时间单位，语法后面再细说。举个简单的例子：\n\n&#x2F;&#x2F; 在8点到17点这段时间内，每15分钟输出一次  \nselect * from Fruit output at (&#x2F;15,8:17,,,)\n\n  \n4.5.when\nOutput还可以使用when来实现达到某个固定条件再输出的效果，一般通过变量，用户自定义的 函数以及output内置的属性来实现。基本语法如下：\n\noutput [after suppression_def]  \n[[all | first | last | snapshot] when trigger_expression  \n[then set variable_name &#x3D; assign_expression [, variable_name &#x3D; assign_expression [,…]]]\n\n  \ntrigger_expression返回true或者false，表示输出或者不输出\nthen set variable_name&#x3D;assign_expression表示是当trigger_expression被触发时，可对变量重新赋值。完整例子如下：\n\nclass Pink  \n\n{  \n\n    private int id;  \n\n    private int price;  \n\n    public int getId()  \n\n    {  \n\n        return id;  \n\n    }  \n\n    public void setId(int id)  \n\n    {  \n\n        this.id &#x3D; id;  \n\n    }  \n\n    public int getPrice()  \n\n    {  \n\n        return price;  \n\n    }  \n\n    public void setPrice(int price)  \n\n    {  \n\n        this.price &#x3D; price;  \n\n    }  \n\n    public String toString()  \n\n    {  \n\n        return “id: “ + id + “, price: “ + price;  \n\n    }  \n\n}  \n\nclass OutputWhenListener implements UpdateListener  \n\n{  \n\n    public void update(EventBean[] newEvents, EventBean[] oldEvents)  \n\n    {  \n\n        if (newEvents !&#x3D; null)  \n\n        {  \n\n            for (int i &#x3D; 0; i &lt; newEvents.length; i++)  \n\n            {  \n\n                Pink pink &#x3D; (Pink) newEvents[i].getUnderlying();  \n\n                System.out.println(“Output Pink: “ + pink);  \n\n            }  \n\n        }  \n\n    }  \n\n}  \n\npublic class OutputWhenTest  \n\n{  \n\n    public static void main(String[] args) throws InterruptedException  \n\n    {  \n\n        EPServiceProvider epService &#x3D; EPServiceProviderManager.getDefaultProvider();  \n\n        EPAdministrator admin &#x3D; epService.getEPAdministrator();  \n\n        ConfigurationOperations config &#x3D; admin.getConfiguration();  \n\n        config.addVariable(“exceed”, boolean.class, false);  \n\n        String pink &#x3D; Pink.class.getName();  \n\n        String epl &#x3D; “select * from “ + pink + “ output when exceed then set exceed&#x3D;false”;  \n\n        EPStatement state &#x3D; admin.createEPL(epl);  \n\n        state.addListener(new OutputWhenListener());  \n\n        EPRuntime runtime &#x3D; epService.getEPRuntime();  \n\n        Random r &#x3D; new Random(47);  \n\n        for (int i &#x3D; 1; i &lt;&#x3D; 10; i++)  \n\n        {  \n\n            int price &#x3D; r.nextInt(10);  \n\n            Pink p &#x3D; new Pink();  \n\n            p.setId(i);  \n\n            p.setPrice(price);  \n\n            System.out.println(“Send Pink Event: “ + p);  \n\n            runtime.sendEvent(p);  \n\n            if (price &gt; 5)  \n\n            {  \n\n                runtime.setVariableValue(“exceed”, true);  \n\n                Thread.sleep(1000);  \n\n            }  \n\n        }  \n\n    }  \n\n}\n\n\n  \n执行结果：\n\nSend Pink Event: id: 1, price: 8  \nOutput Pink: id: 1, price: 8  \nSend Pink Event: id: 2, price: 5  \nSend Pink Event: id: 3, price: 3  \nSend Pink Event: id: 4, price: 1  \nSend Pink Event: id: 5, price: 1  \nSend Pink Event: id: 6, price: 9  \nOutput Pink: id: 2, price: 5  \nOutput Pink: id: 3, price: 3  \nOutput Pink: id: 4, price: 1  \nOutput Pink: id: 5, price: 1  \nOutput Pink: id: 6, price: 9  \nSend Pink Event: id: 7, price: 8  \nOutput Pink: id: 7, price: 8  \nSend Pink Event: id: 8, price: 0  \nSend Pink Event: id: 9, price: 2  \nSend Pink Event: id: 10, price: 7  \nOutput Pink: id: 8, price: 0  \nOutput Pink: id: 9, price: 2  \nOutput Pink: id: 10, price: 7\n\n  \n        从结果可以看出来。当price大于5的时候，设置exceed变量为true，即可输出之前进入的所有事件，then set子句将exceed设置为false，等待下一次exceed&#x3D;true时触发输出。由于输出线程是单独的线程，所以如果不sleep，结果可能会和这个不同。\n对于when关键字，Esper提供了一些内置的属性帮助我们实现更复杂的输出约束。如图所示：\n\n以上5个属性我就不多做解释了，使用方式是作为trigger_expression跟在when关键字的后面。例如：\n\n&#x2F;&#x2F; 进入的Apple事件总数达到5个时才输出，且不清零count_insert_total属性，继续累加事件总数  \n\nselect * from Apple output when count_insert_total&#x3D;5  \n\n&#x2F;&#x2F; 移除的Apple事件总数达到4个时才输出，并清零count_remove属性  \n\nselect * from Apple output when count_remove&#x3D;4\n\n\n  \n另外，在使用when的时候，有两点需要注意：\n1.当trigger_expression返回true时，Esper会输出从上一次输出之后到这次输出之间所有的insert stream和remove stream。\n2.若trigger_expression不断被触发并返回true时，则Esper最短的输出间隔为100毫秒。\n3.expression不能包含事件流的属性，聚合函数以及prev函数和prior函数\n4.6.Context Terminated\nOutput还针对Context专门设计了一个输出条件，即在Context终止时输出Context中的内容。关于Context，可以看看Esper学习之四：Context。具体语法如下：\n\noutput when terminated [and termination_expression]  \n[then set variable_name &#x3D; assign_expression [, variable_name &#x3D; assign_expression [,…]]]]\n\n  \nwhen terminated是关键字，之前可以通过and连接其他的式子一起使用。termination_expression是一个返回true或者false的表达式，同trigger_expression一样。举例如下：\n\n&#x2F;&#x2F; 在MyContext下，查询context的id并计算Apple的sum price，当Context结束且输入的事件总数大于10时，输出。然后设置FinishCompute变量为true  \n\ncontext MyContext select context.id, sum(price) from Apple output when terminated and count_insert_total &gt; 10 then set FinishCompute &#x3D; true  \n\n&#x2F;&#x2F; 在MyContext下，计算Apple的avg size，并每1分钟输出第一个进入的事件计算结果，当context结束时也输出一次计算结果  \n\ncontext MyContext select avg(size) from Apple output first every 1 min and when terminated\n\n\n  \n        Output和Aggregation，Group by一起使用时，first，last，all，snapshot四个关键字产生的效果会比较特别。建议各位自己看看Esper的官方文档的Appendix A，有相当完整的例子做说明，因为篇幅较长，所以我没有放在文章里进行讲解。另外针对first，last，all，snapshot四个关键字，只有使用snapshot是不会缓存计算结果。其他的关键字会缓存事件直到触发了输出条件才会释放，所以如果输入的数据量比较大，就要注意输出条件被触发前的内存使用量。\n        关于Output的内容比较多，使用起来也比较灵活。各位在使用的时候，也许会发现自己写的达不到预期的效果，本人在使用的时候也遇到过，所以还请各位耐心地多试几次。Group by和Aggregation和SQL的类似，所以使用起来很容易。\n","slug":"BIGDATA/Esper学习之七：EPL语法（三）","date":"2023-06-08T17:52:30.000Z","categories_index":"price,BIGDATA","tags_index":"Apple,select,color","author_index":"dandeliono"},{"id":"6e82ad6dc435102e8221e6c49dde0575","title":"Esper学习之六：EPL语法（二）","content":"Esper学习之六：EPL语法（二） 从上一篇开始说EPL的语法，主要是关于注解的。今天来说说比较常用的语法，Select Clause和From Clause。这个两个可以说是写EPL必备，要想得到事件流的处理结果，基本上就靠他们俩了（Pattern除外）。今天的内容比较简单，还请各位同学牢记，以免以后应用的时候花时间看文档或者我的文章。\nSelect Clause\n1.查询事件流的所有属性及特定属性\nEPL的select和SQL的select很相近，SQL用表示查询表的所有字段，而EPL用表示查询事件流的所有属性值。SQL查询某个字段名，直接在select后跟字段名就ok，EPL也是将要查询的属性名放在select之后。若查多个属性值，则用逗号分割。和SQL一样，EPL查询属性也可以设置别名。示例如下：\n\n&#x2F;&#x2F; EPL：查询完整的User对象  \n\nselect * from User  \n\n&#x2F;&#x2F; 获取User对象  \n\nUser u &#x3D; newEvent.getUnderlying();  \n\n&#x2F;&#x2F; EPL：查询User的name和id，id别名为i  \n\nselect name, id as i from User  \n\n&#x2F;&#x2F; 获取name和id  \n\nString name &#x3D; (String)newEvent.get(“name”);  \n\nint id &#x3D; (Integer)newEvent.get(“i”);\n\n\n  \n这里要注意，如果查询的是一个完整对象，需要调用getUnderlying()方法，而get方法是针对确定的属性名或者别名。另外*是不能设置别名的。\n2.表达式\n除了查询完整对象和特定属性，EPL还支持属性值的计算，以计算后的值作为结果返回，并且也能设置别名。这个计算的式子就是表达式。例如：\n\n&#x2F;&#x2F; 计算长方形的面积（长乘以宽）  \nselect length * width as area from Rectangle\n\n  \n除了简单的加减乘除，还可以利用事件流对象的某个方法。例如：\n\n&#x2F;&#x2F; 计算长方形的面积（长乘以宽）  \n\nselect r.getArea(r.length,r.width) as area from Rectangle as r  \n\nselect r.getArea() as area from Rectangle as r  \n\npublic class Rectangle  \n\n{  \n\n    private int length;  \n\n    private int width;  \n\n    public int getArea(int l, int w){  \n\n        return l*w;  \n\n    }  \n\n    public int getArea(){  \n\n        return length * width;  \n\n    }  \n\n}\n\n\n  \n如上所示，一个方法需要传参，另一个方法不需要，但是他会利用当前事件的length和width来计算面积。而且要注意的是事件流需要设置别名才能使用其方法，如：r.getArea()\n如果Rectangle类里没有计算面积的方法，但是提供了一个专门计算面积的静态方法，表达式也可以直接引用。不过要事先加载这个包含方法的类。例如：\n\npublic class ComputeArea{  \n\n    public static int getArea(int length, int width){  \n\n        return length*width;  \n\n    }  \n\n}  \n\nepService.getEPAdministrator().getConfiguration().addImport(ComputeArea.class);  \n\n&#x2F;&#x2F; 调用ComputeArea的getArea方法计算面积  \n\nselect ComputeArea.getArea(length,width) from Rectangle\n\n\n  \n注意一定要是静态方法，不然没有实例化是没法引用的。\n3.多事件流的查询\n和SQL类似，EPL也可以同时对多个事件流进行查询，即join，但是必须对每个事件流设置别名。例如：\n\n&#x2F;&#x2F; 当老师的id和学生的id相同时，查询学生的姓名和老师的姓名  \nselect s.name, t.name from Student.win:time(10) as s, Teacher.win:time(10) as t where s.id&#x3D;t.id\n\n  \n如果想查询Student或者Teacher，则EPL改写如下：\n\nselect s.* as st, t.* as tr from Student.win:time(10) as s, Teacher.win:time(10) as t where s.id&#x3D;t.id\n\n  \n如果想要查询的属性只有存在于一个事件，那么可以不用”别名.属性名”，但是最好还是带上别名，万一哪天另一个事件流多了一个一样的属性，那时候不需要修改EPL也可以使用。关于join，可看看这篇文章《Esper学习之九：EPL语法（五）》的join部分\n4.insert和remove事件流\nEsper对于事件流分输入和移出两种，分别对应监听器的两个参数newEvents和oldEvents，关于监听器的内容可参看《Esper学习之三：进程模型》。newEvents通常对应事件的计算结果，oldEvents可以理解过上一次计算结果。默认情况下，只有newEvents有值，oldEvents为null。如果需要查看oldEvents，则需要使用一个参数。例如：\n\nselect rstream * from User\n\n  \n如果使用了该参数，则会将上一次计算结果放入newEvents内，而不是oldEvents（以前我还以为这是一个bug，后面发现手册上官方明确就是newEvents，汗！）。并且无法获得当前的计算结果\n\nselect irstream * from User\n\n  \n如果使用了该参数，则会将当前的计算结果放入newEvents内，上一次的计算结果放入oldEvents内。\n\nselect istream * from User  \n&#x2F;&#x2F; 等同于  \nselect * from User\n\n  \n如果使用了该参数，则会将当前的计算结果放入newEvents内，并且无法获得上一次的计算结果。同时该参数也是默认参数，可不写。\n如果想修改默认参数，需要调用配置接口修改配置。\n5.Distinct\ndistinct的用法和SQL一样，放在需要修饰的属性或者*前即可。例如：\n\nselect distinct * from User.win:time(3 sec)\n\n  \n6.查询指定引擎的处理结果\n除了上述所说的一些特点外，select还可以针对某个引擎进行查询。因为引擎都有自己的URI，所以可以在select句子中增加URI标识来指定查询哪一个引擎的事件处理情况。例如：\n\n&#x2F;&#x2F; 引擎URI为Processor  \nselect Processor.MyEvent.myProperty from Processor.MyEvent\n\n  \nFrom Clause\n1.语法介绍\nFrom的语法不难，主要内容是针对事件流的处理。包括事件流过滤，事件流的维持等等。语法如下：\n\nfrom stream_def [as name] [unidirectional] [retain-union | retain-intersection] [, stream_def [as stream_name]] [, …]  \n\n&#x2F;&#x2F; 事件流  \n\nevent_stream_name [(filter_criteria)] [contained_selection] [.view_spec] [.view_spec] […]\n\n\n  \nunidirectional，retain-union，retain-intersection，contained_selection，view_spec这几个关键字因为涉及到view的知识，所以这里没法讲解。待学完view之后再来回顾这几个参数会很容易理解的。下面讲讲怎么过滤事件流\n2.事件流过滤\n2.1.事件属性过滤\n事件流过滤通常情况都是对其中某个或多个属性加以限制来达到过滤的目的。注意，过滤表达式是紧跟在事件流名称之后而不是别名之后。例如：\n\n&#x2F;&#x2F; 只有age大于10的User对象才可查询到name值  \n\nselect name from User(age&gt;10) as user  \n\n&#x2F;&#x2F; 当name&#x3D;“luonanqin”时，可获得其age值  \n\nselect age from User(name&#x3D;”luonanqin”)  \n\n&#x2F;&#x2F; 错误写法  \n\nselect name from User as user(age&gt;10)\n\n\n  \n过滤表达式写法多种多样，可以用符号，又或者使用and,or,between等逻辑语言。例如：\n\n&#x2F;&#x2F; 查询年龄大于15小于18的学生的姓名  \nselect name from Student(age between 15 and 18)  \n&#x2F;&#x2F; 等同于  \nselect name from Student(age &gt;&#x3D; 15 and age &lt;&#x3D; 18)  \n&#x2F;&#x2F; 等同于  \nselect name from Student(age &gt;&#x3D; 15, age &lt;&#x3D; 18)\n\n  \n看以看到，过滤表达式写法很多，并且多个表达式同时作用于一个事件流，用逗号连接即可。如果说满足其中一个条件即可，则需要用or连接。\n2.2.过滤范围\n刚才说到过滤表达式使用的符号很多，总结下来基本上有&lt;, &gt;, &lt;&#x3D;, &gt;&#x3D;, &#x3D;, !&#x3D;, between, in, not in, and, or, [ ], ( )。这里主要说下between，in，( )，[ ]\nbetween……and……\n和SQL的between……and……意思一样，是一个闭区间。比如说between 10 and 15，中文语义为10到15之间并包含10和15.\n( )\n表示一个开区间，语法为(low：high)。如(10:15)，表示10到15之间，并且不包含10和15\n[ ]\n表示一个闭区间，语法为[low：high]。如[10:15]，表示10到15之间，并且包含10和15\n( )和[ ]可以混合用。比如[10:15)或者(10:15]\nin\n配合( )和[ ]进行使用，表示值在某个范围内。比如：\n\nselect name from User(age in [10:15))\n\n  \n相应的，not in表示不在此范围内。\n以上都是针对数字的例子，in和not in同样可以作用于字符串。比如：\n\nselect age from User(name in (‘张三’, ‘李四’))\n\n  \n2.3 静态方法过滤\n除了上面说的这些符号以外，类似于select子句中使用的静态方法，过滤表达式中也可以使用，但是返回值必须为布尔值，不然会报错。例如：\n\npublic class IsZero  \n\n{  \n\n    public static boolean isZero(int sum)  \n\n    {  \n\n        return sum&#x3D;&#x3D;0;  \n\n    }  \n\n}  \n\nepService.getEPAdministrator().getConfiguration().addImport(IsZero.class);  \n\nselect name from User(IsZero.isZero(money))\n\n\n  \n事件流的过滤并不能弄得很复杂，他有一下几个限制：\n1. 要过滤的属性只能是数字和字符串。2. 过滤表达式中不能使用聚合函数。3. “prev”和“prior”函数不能用于过滤表达式（暂且不考虑这是什么）\n       Select和From的基础内容基本上就是上面所说的。当学过后面的章节之后，select和from可以写得很复杂，才能支持更为复杂的业务需求。特别是学过view和一些event function之后，变化就更加多样了。下一篇将讲解别的Clause，敬请期待。\n","slug":"BIGDATA/Esper学习之六：EPL语法（二）","date":"2023-06-08T17:50:28.000Z","categories_index":"select,BIGDATA","tags_index":"from,name,User","author_index":"dandeliono"},{"id":"c5c99cbcae7cadf842c021db56097c6a","title":"Esper学习之五：EPL语法（一）","content":"Esper学习之五：EPL语法（一）上篇说到了Esper的Context，要是不了解的同学请参看《Esper学习之四：Context》，看过的同学如果还是不理解的话可以给我评论，我将会尽可能的解答。之前有些同学问我Context和Group by有什么区别，其实如果只是很简单的用Context，那么确实没太大区别，无非是在Context下select可以不包含group by修饰的属性。但是Group by明显没有Context强大，很多复杂的分组Group by是没法做到的。不过在能达到同样效果的情况下，我还是建议使用Group by，毕竟Context的名字是不能重复的，而且在高并发的情况下Context会短时间锁住。至于原因，这已经是Esper的高级篇了，这里暂且不说。\n        今天开始讲解Esper的重中之重——EPL。EPL可以说是Esper的核心，要是不会将简单的业务需求转化为EPL，更别说复杂的EPL了，最后就等着被客户骂吧。接下来的很多篇都会围绕EPL讲解，大概会有十来篇吧，毕竟英文文档都有140页了，草草两篇根本就说不完。废话不多说，先简单介绍下什么是EPL，即使第一篇有说过，但是这里有必要细说一下。\n        EPL，全称Event Processing Language，是一种类似SQL的语言，包含了SELECT, FROM, WHERE, GROUP BY, HAVING 和 ORDER BY子句，同时用事件流代替了table作为数据源，并且能像SQL那样join，filtering和aggregation。所以如果各位有SQL基础的话，简单的EPL很容易掌握。除了select，EPL也有insert into，update，delete，不过含义和SQL并不是很接近。另外还有pattern和output子句，这两个是SQL所没有的。EPL还定义了一个叫view的东西，类似SQL的table，来决定哪些数据是可用的，Esper提供了十多个view，并且保证这些view可以被重复使用。而且用户还可以扩展view成为自定义view来满足需求。在view的基础上，EPL还提供了named window的定义，作用和view类似，但是更加灵活。。。还有很多东西，我再列举估计大家都要晕了，就先说说语法吧。\n1.EPL Syntax\n\n[annotations]  \n[expression_declarations]  \n[context context_name]  \n[insert into insert_into_def]  \nselect select_list  \nfrom stream_def [as name] [, stream_def [as name]] [,…]  \n[where search_conditions]  \n[group by grouping_expression_list]  \n[having grouping_search_conditions]  \n[output output_specification]  \n[order by order_by_expression_list]  \n[limit num_rows]\n\n  \n        基本上大部分的EPL都是按照这个格式来定义。看过之前几篇的同学应该对里面的某些内容熟悉，比如context context_name，select select_list from stream_def等等。其他的可以先不关心，后面会有详解。比如wher，group by都会有专门的篇幅进行描述。\n2.Time Periods\n这部分的内容说的是Esper中时间的表达形式。语法如下：\n\ntime-period : [year-part] [month-part] [week-part] [day-part] [hour-part]  \n[minute-part] [seconds-part] [milliseconds-part]  \nyear-part : (number|variable_name) (“years” | “year”)  \nmonth-part : (number|variable_name) (“months” | “month”)  \nweek-part : (number|variable_name) (“weeks” | “week”)  \nday-part : (number|variable_name) (“days” | “day”)  \nhour-part : (number|variable_name) (“hours” | “hour”)  \nminute-part : (number|variable_name) (“minutes” | “minute” | “min”)  \nseconds-part : (number|variable_name) (“seconds” | “second” | “sec”)  \nmilliseconds-part : (number|variable_name) (“milliseconds” | “millisecond” |  \n“msec”)\n\n  \n与时间相关的EPL基本都会用到上面列出的东西，举几个例子说明下：\n\n&#x2F;&#x2F; 计算过去的5分3秒中进入改语句的Fruit事件的平均price  \n\nselect avg(price) from Fruit.win:time(5 minute 3 sec)  \n\n&#x2F;&#x2F; 每一天输出一次用户的账户总额  \n\nselect sum(account) from User output every 1 day\n\n\n  \n用法比较简单，大家可以多试试。要注意的是，Esper规定每月的天数都是30天，所以对准确性要求高的业务，以月为单位进行计算会出现误差的。\n3.Comments\n注释基本上和Java差不多，只不过他没有&#x2F;** &#x2F;和&#x2F; &#x2F;之分，只有&#x2F; &#x2F;和&#x2F;&#x2F;，毕竟不需要生成文档，所以就没那个必要了。&#x2F;&#x2F;只能单行注释，而&#x2F; *&#x2F;可以多行注释。示例如下：\n\na.单行注释  \n\n&#x2F;&#x2F; This comment extends to the end of the line.  \n\n&#x2F;&#x2F; Two forward slashes with no whitespace between them begin such comments.  \n\nselect * from MyEvent  \n\nb.多行注释  \n\nselect * from OtherEvent   \n\n&#x2F;* this is a very  \n\n*important Event *&#x2F;  \n\nc.混合注释  \n\nselect field1 &#x2F;&#x2F; first comment  \n\n&#x2F;* second comment *&#x2F; field2 from MyEvent\n\n\n  \n4.Reserved Keywords\n        EPL里如果某个事件属性，或者事件流的名称和EPL的关键字一样，那么必须要以括起来才可用，在键盘上esc的下面，1的左边，叫什么忘记了，估计说出来也有很多人不知道。比如：\n\n&#x2F;&#x2F; insert和Order是关键字，这个EPL无效  \n\nselect insert from Order  \n\n&#x2F;&#x2F; `insert`和`Order`是属性名称和事件流名称，这个EPL有效  \n\nselect `insert` from `Order`\n\n\n  \n5.Escaping Strings\n在EPL中，字符串使用单引号或者双引号括起来的，那如果字符串里包含有单引号或者双引号怎么办呢。请看例子：\n\nselect * from OrderEvent(name&#x3D;’John’)  \n&#x2F;&#x2F; 等同于  \nselect * from OrderEvent(name&#x3D;”John”)\n\n  \n如果name&#x3D;John’s，则需要反斜杠进行转义。\n\nselect * from OrderEvent(name&#x3D;”John\\“s”)  \n&#x2F;&#x2F; 或者  \nselect * from OrderEvent(name&#x3D;’john\\‘s’)\n\n  \n除了使用反斜杠，还可以使用unicode来表示单引号和双引号。\n\nselect * from OrderEvent(name&#x3D;”John\\u0022s”)  \n&#x2F;&#x2F; 或者  \nselect * from OrderEvent(name&#x3D;’john\\u0027s’)\n\n  \n注意在Java编写EPL的时候，反斜杠和无含义的双引号还得转义，不然会和String的双引号冲突。比如\n\nepService.getEPAdministrator().createEPL(“select * from OrderEvent(name&#x3D;’John\\&#39;s’)”);  \n&#x2F;&#x2F; … and for double quotes…  \nepService.getEPAdministrator().createEPL(“select * from OrderEvent(name&#x3D;\\“Quote \\\\&quot;Hello\\\\&quot;\\“)”);\n\n  \n6.Data Types\n        EPL支持Java所有的数值数据类型，包括基本类型及其包装类，同时还支持java.math.BigInteger和java.math.BigDecimal，并且能自动转换数据类型不丢失精度（比如short转int，int转short则不行）。如果想在EPL内进行数据转换，可以使用cast函数。完整例子如下：\n\nimport com.espertech.esper.client.EPAdministrator;  \n\nimport com.espertech.esper.client.EPRuntime;  \n\nimport com.espertech.esper.client.EPServiceProvider;  \n\nimport com.espertech.esper.client.EPServiceProviderManager;  \n\nimport com.espertech.esper.client.EPStatement;  \n\nimport com.espertech.esper.client.EventBean;  \n\nimport com.espertech.esper.client.UpdateListener;  \n\nclass Banana  \n\n{  \n\n    private int price;  \n\n    public int getPrice()  \n\n    {  \n\n        return price;  \n\n    }  \n\n    public void setPrice(int price)  \n\n    {  \n\n        this.price &#x3D; price;  \n\n    }  \n\n}  \n\nclass CastDataTypeListener1 implements UpdateListener  \n\n{  \n\n    public void update(EventBean[] newEvents, EventBean[] oldEvents)  \n\n    {  \n\n        if (newEvents !&#x3D; null)  \n\n        {  \n\n            EventBean event &#x3D; newEvents[0];  \n\n            System.out.println(“Average Price: “ + event.get(“cast(avg(price), int)”) + “, DataType is “  \n\n                    + event.get(“cast(avg(price), int)”).getClass().getName());  \n\n        }  \n\n    }  \n\n}  \n\nclass CastDataTypeListener2 implements UpdateListener  \n\n{  \n\n    public void update(EventBean[] newEvents, EventBean[] oldEvents)  \n\n    {  \n\n        if (newEvents !&#x3D; null)  \n\n        {  \n\n            EventBean event &#x3D; newEvents[0];  \n\n            System.out.println(“Average Price: “ + event.get(“avg(price)”) + “, DataType is “ + event.get(“avg(price)”).getClass().getName());  \n\n        }  \n\n    }  \n\n}  \n\npublic class CastDataTypeTest  \n\n{  \n\n    public static void main(String[] args) throws InterruptedException  \n\n    {  \n\n        EPServiceProvider epService &#x3D; EPServiceProviderManager.getDefaultProvider();  \n\n        EPAdministrator admin &#x3D; epService.getEPAdministrator();  \n\n        String banana &#x3D; Banana.class.getName();  \n\n        String epl1 &#x3D; “select cast(avg(price),int) from “ + banana + “.win:length_batch(2)”;  \n\n        String epl2 &#x3D; “select avg(price) from “ + banana + “.win:length_batch(2)”;  \n\n        EPStatement state1 &#x3D; admin.createEPL(epl1);  \n\n        state1.addListener(new CastDataTypeListener1());  \n\n        EPStatement state2 &#x3D; admin.createEPL(epl2);  \n\n        state2.addListener(new CastDataTypeListener2());  \n\n        EPRuntime runtime &#x3D; epService.getEPRuntime();  \n\n        Banana b1 &#x3D; new Banana();  \n\n        b1.setPrice(1);  \n\n        runtime.sendEvent(b1);  \n\n        Banana b2 &#x3D; new Banana();  \n\n        b2.setPrice(2);  \n\n        runtime.sendEvent(b2);  \n\n    }  \n\n}\n\n\n  \n执行结果：\n\nAverage Price: 1, DataType is java.lang.Integer  \nAverage Price: 1.5, DataType is java.lang.Double\n\n  \n要提醒的是，如果某个数除以0，那么默认会返回正无穷大或者负无穷大，不过可以配置这个结果，比如用null来代替。\n7.Annotation\nEPL也可以写注解，种类不多，大部分简单而有效。不过有些注解内容较多，在这里只是简单介绍，以后会在具体的使用场景进行详细讲解。首先来了解下注解的语法。\n\n&#x2F;&#x2F; 不包含参数或者单个参数的注解  \n\n@annotation_name [(annotation_parameters)]  \n\n&#x2F;&#x2F; 包含多个属性名-值对的注解  \n\n@annotation_name (attribute_name &#x3D; attribute_value, [name&#x3D;value, …])  \n\n&#x2F;&#x2F; 多个注解联合使用  \n\n@annotation_name [(annotation_parameters)] [@annotation_name [(annotation_parameters)]] […]\n\n\n  \n下面讲解具体注解时，会结合语法进行说明。\na) @Name 指定EPL的名称，参数只有一个。例如：@Name(“MyEPL”)\nb) @Description 对EPL进行描述，参数只有一个。例如：@Description(“This is MyEPL”)\nc) @Tag 对EPL进行额外的说明，参数有两个，分别为Tag的名称和Tag的值，用逗号分隔。例如：@Tag(name&#x3D;”author”,value&#x3D;”luonanqin”)\nd) @Priority 指定EPL的优先级，参数只有一个，并且整数（可负可正）。例如：@Priority(10)\ne) @Drop 指定事件经过此EPL后不再参与其他的EPL计算，该注解无参数。\nf) @Hint 为EPL加上某些标记，让引擎对此EPL产生其他的操作，会改变EPL实例的内存占用，但通常不会改变输出。其参数固定，由Esper提供，之后的篇幅会穿插讲解这个注解的使用场景。\ng) @Audit EPL添加此注解后，可以额外输出EPL运行情况，有点类似日志的感觉（当然没有日志的功能全啦），具体使用场景在此先不提。\nh) @Hook 与SQL相关，这里暂且不说\ni) @EventRepresentation 这是用来指定EPL产生的计算结果事件包含的数据形式。参数只有一个，即array&#x3D;true或array&#x3D;false。false为默认值，代表数据形式为Map，若为true，则数据形式为数组。\n针对以上几个简单的注解，例子如下：\n\nimport com.espertech.esper.client.EPAdministrator;  \n\nimport com.espertech.esper.client.EPRuntime;  \n\nimport com.espertech.esper.client.EPServiceProvider;  \n\nimport com.espertech.esper.client.EPServiceProviderManager;  \n\nimport com.espertech.esper.client.EPStatement;  \n\nimport com.espertech.esper.client.EventBean;  \n\nimport com.espertech.esper.client.UpdateListener;  \n\nclass Apple  \n\n{  \n\n    private int price;  \n\n    public int getPrice()  \n\n    {  \n\n        return price;  \n\n    }  \n\n    public void setPrice(int price)  \n\n    {  \n\n        this.price &#x3D; price;  \n\n    }  \n\n}  \n\nclass SomeAnnotationListener implements UpdateListener  \n\n{  \n\n    public void update(EventBean[] newEvents, EventBean[] oldEvents)  \n\n    {  \n\n        if (newEvents !&#x3D; null)  \n\n        {  \n\n            EventBean event &#x3D; newEvents[0];  \n\n            System.out.println(“Sum Price: “ + event.get(“sum(price)”) + “, Event Type is “ + event.getEventType().getUnderlyingType());  \n\n        }  \n\n    }  \n\n}  \n\npublic class SomeAnnotationTest  \n\n{  \n\n    public static void main(String[] args) throws InterruptedException  \n\n    {  \n\n        EPServiceProvider epService &#x3D; EPServiceProviderManager.getDefaultProvider();  \n\n        EPAdministrator admin &#x3D; epService.getEPAdministrator();  \n\n        String apple &#x3D; Apple.class.getName();  \n\n        String epl1 &#x3D; “@Priority(10)@EventRepresentation(array&#x3D;true) select sum(price) from “ + apple + “.win:length_batch(2)”;  \n\n        String epl2 &#x3D; “@Name(\\“EPL2\\“)select sum(price) from “ + apple + “.win:length_batch(2)”;  \n\n        String epl3 &#x3D; “@Drop select sum(price) from “ + apple + “.win:length_batch(2)”;  \n\n        UpdateListener listenenr &#x3D; new SomeAnnotationListener();  \n\n        EPStatement state1 &#x3D; admin.createEPL(epl1);  \n\n        state1.addListener(listenenr);  \n\n        EPStatement state2 &#x3D; admin.createEPL(epl2);  \n\n        state2.addListener(listenenr);  \n\n        System.out.println(“epl2’s name is “ + state2.getName());  \n\n        EPStatement state3 &#x3D; admin.createEPL(epl3);  \n\n        state3.addListener(listenenr);  \n\n        EPRuntime runtime &#x3D; epService.getEPRuntime();  \n\n        Apple a1 &#x3D; new Apple();  \n\n        a1.setPrice(1);  \n\n        runtime.sendEvent(a1);  \n\n        Apple a2 &#x3D; new Apple();  \n\n        a2.setPrice(2);  \n\n        runtime.sendEvent(a2);  \n\n    }  \n\n}\n\n\n  \n执行结果：\n\nepl2’s name is EPL2  \nSum Price: 3, Event Type is class [Ljava.lang.Object;  \nSum Price: 3, Event Type is interface java.util.Map  \nSum Price: 3, Event Type is interface java.util.Map\n\n  \n可以发现，@Name和@EventRepresentation都起效果了，但是@Priority和@Drop没用，那是因为这两个是要配置才能生效的。以后讲Esper的Configuration的时候会说到。\n8.Expression\n        Expression类似自定义函数，通常用Lambda表达式来建立的（也有别的方法建立），而Lambda表达式就一个“ &#x3D;&gt; ”符号，表示“gose to”。符号的左边表示输入参数，符号右边表示计算过程，计算结果就是这个表达式的返回值，即Expression的返回值。语法如下：\n\nexpression expression_name { expression_body }\n\n  \nexpression是关键字，expression_name为expression的名称（唯一），expression_body是expression的具体内容。\n\nexpression_body: (input_param [,input_param [,…]]) &#x3D;&gt; expression\n\n  \ninput_param必须为事件流的别名，注意不是事件流名称。参数名写什么都可以（当然最好不用关键字），多个参数用逗号分隔，并用圆括号括起来。同时针对lamdba，举个例子一起说明下：\n\nexpression middle { x &#x3D;&gt; (x.max+x.min)&#x2F;2 } select middle(apple) from Apple as apple\n\n  \n1. x表示输入参数，而x.max和x.min都是x代表的事件流的属性，如果事件流没这个属性，expression的定义就是错误的。\n2. express的定义必须在使用它的句子之前完成。使用时直接写expression的名字和用圆括号包含要计算的参数即可。再次提醒，expression的参数只能是事件流别名，即apple，别名的定义就如上面那样，事件流之后跟着as，然后再跟别名。\n上面这个句子的执行结果，就是middle的计算结果，各位自己试试吧。\n对于多个参数的expression定义，例子如下：\n\nexpression sumage { (x,y) &#x3D;&gt; x.age+y.age } select sumage(me,you) from Me as me, You as you\n\n  \n要是两个age的数据类型不一样是什么结果呢？还是请各位自己试试。\n        expression_body除了可以用lambda表达式之外，还可以用聚合函数，变量，常量，子查询语句，甚至另一个表达式。子查询语句在没有in或者exist关键字的情况下，需要圆括号括起来。示例如下：\n\nexpression newsSubq(md) {  \n(select sentiment from NewsEvent.std:unique(symbol) where symbol &#x3D; md.symbol)  \n}  \nselect newsSubq(mdstream) from MarketDataEvent mdstream\n\n  \n针对变量和常量的示例如下：\n\nexpression twoPI { Math.PI * 2} select twoPI() from SampleEvent\n\n  \n        对于expression里用另一个expression，EPL不允许在一个句子里建立两个expression，所以就出现了Global-Expression。普通的expression只作用于定义它的epl，如上面所有的包含select子句的epl就是如此。Global-Expression的语法如下：\n\ncreate expression expression_name { expression_body }\n\n  \n和普通的expression相比，就是多了个create，不过他不能和别的子句放在一起，即他是单独执行的。例如：\n\nepService.getEPAdministrator().createEPL(“create expression avgPrice { x &#x3D;&gt; (x.fist+x.last)&#x2F;2 }”);\n\n  \n最后再举个例子说一下某个expression里用另一个expression。\n\ncreate expression avgPrice { x &#x3D;&gt; (x.fist+x.last)&#x2F;2 }  \n\nexpression bananaPrice{ x &#x3D;&gt; avgPrice(x) } select bananaPrice(b) from Banana as b\n\n\n  \nexpression_body里的空格只是为了看着清晰，有没有空格不会有影响。\n","slug":"BIGDATA/Esper学习之五：EPL语法（一）","date":"2023-06-08T17:50:09.000Z","categories_index":"EPL,BIGDATA","tags_index":"select,expression,name","author_index":"dandeliono"},{"id":"b92a10e317d46caa8257a30d6cf731e3","title":"Esper学习之四：Context","content":"Esper学习之四：Context Context是Esper里一个很有意思的概念，要是理解为上下文，我觉得有点不妥。以我的理解，Context就像一个框，把不同的事件按照框的规则框起来，并且有可能有多个框，而框与框之间不会互相影响。不知道各位在看完这篇文章后是否认同我的观点，我愿洗耳恭听。\n1.Context基本语法\n语法结构如下\n\ncreate context context_name partition [by] event_property [and event_property [and …]] from stream_def   \n[, event_property […] from stream_def] [, …]\n\n  \n说明：context_name为context的名字，并且唯一。如果重复，会说明已存在。\nevent_property为事件的属性名，多个属性名之间用and连接，也可以用逗号连接。\nstream_def为事件流的定义，简单的定义可以是一个事件的名称，比如之前定义了一个Map结构的事件为User，那么这里就可以写User。复杂的流定义后面会说到\n举个例子：\n\ncreate context NewUser partition by id and name from User  \n&#x2F;&#x2F; id和name是User的属性\n\n  \n如果context包含多个流，例子如下：\n\ncreate context Person partition by sid from Student, tid from Teacher  \n&#x2F;&#x2F; sid是Student的属性，tid是Teacher的属性\n\n  \n多个流一定要注意，每个流的中用于context的属性的数量要一样，数据类型也要一致。比如下面这几个就是错误的：\n\ncreate context Person partition by sid from Student, tname from Teacher  \n\n&#x2F;&#x2F; 错误：sid是int，tname是String，数据类型不一致  \n\ncreate context Person partition by sid from Student, tid,tname from Teacher  \n\n&#x2F;&#x2F; 错误：Student有一个属性，Teacher有两个属性，属性数量不一致  \n\ncreate context Person partition by sid,sname from Student, tname,tid from Teacher  \n\n&#x2F;&#x2F; 错误：sid对应tname，sname对应tid，并且sname和tname是String，sid和tid是int，属性数量一样，但是对应的数据类型不一致\n\n\n  \n实际上可以对进入context的事件增加过滤条件，不符合条件的就被过滤掉，就像下面这样：\n\ncreate context Person partition by sid from Student(age &gt; 20)  \n&#x2F;&#x2F; age大于20的Student事件才能建立或者进入context\n\n  \n       看了这么多，可能大家只是知道context的一些基本定义方法，但是不知道什么意思。其实很简单，partition by后面的属性，就是作为context的一个约束，比如说id，如果id相等的则进入同一个context里，如果id不同，那就新建一个context。好比根据id分组，id相同的会被分到一个组里，不同的会新建一个组并等待相同的进入。\n       如果parition by后面跟着同一个流的两个属性，那么必须两个属性值一样才能进入context。比如说A事件id&#x3D;1,name&#x3D;a，那么会以1和a两个值建立context，有点像数据库里的联合主键。然后B事件id&#x3D;1,name&#x3D;b，则又会新建一个context。接着C事件id&#x3D;1,name&#x3D;a，那么会进入A事件建立的context。\n       如果partition by后面跟着两个流的一个属性，那么两个属性值一样才能进入context。比如说Student事件sid&#x3D;1，那么会新建一个context，然后来了个Teacher事件tid&#x3D;1，则会进入sid&#x3D;1的那个context。多个流也一样，不用关心是什么事件，只用关心事件的属性值一样即可进入同一个context。\n要是说了这么多还是不懂，可以看看下面要讲的context自带属性也许就能明白一些了。\n2. Built-In Context Properties\nContext本身自带一些属性，最关键的是可以查看所创建的context的标识，并帮助我们理解context的语法。\n\n如上所示，name表示context的名称，这个是不会变的。id是每个context的唯一标识，从0开始。key1和keyN表示context定义时所选择的属性的值，1和N表示属性的位置。例如：\n\nEPL: create context Person partition by sid, sname from Student  \n&#x2F;&#x2F; key1为sid，key2为sname\n\n  \n为了说明对这几个属性的应用，我举了一个比较完整的例子。\n\nimport com.espertech.esper.client.EPAdministrator;  \n\nimport com.espertech.esper.client.EPRuntime;  \n\nimport com.espertech.esper.client.EPServiceProvider;  \n\nimport com.espertech.esper.client.EPServiceProviderManager;  \n\nimport com.espertech.esper.client.EPStatement;  \n\nimport com.espertech.esper.client.EventBean;  \n\nimport com.espertech.esper.client.UpdateListener;  \n\nclass ESB  \n\n{  \n\n    private int id;  \n\n    private int price;  \n\n    public int getId()  \n\n    {  \n\n        return id;  \n\n    }  \n\n    public void setId(int id)  \n\n    {  \n\n        this.id &#x3D; id;  \n\n    }  \n\n    public int getPrice()  \n\n    {  \n\n        return price;  \n\n    }  \n\n    public void setPrice(int price)  \n\n    {  \n\n        this.price &#x3D; price;  \n\n    }  \n\n}  \n\nclass ContextPropertiesListener2 implements UpdateListener  \n\n{  \n\n    public void update(EventBean[] newEvents, EventBean[] oldEvents)  \n\n    {  \n\n        if (newEvents !&#x3D; null)  \n\n        {  \n\n            EventBean event &#x3D; newEvents[0];  \n\n            System.out.println(“context.name “ + event.get(“name”) + “, context.id “ + event.get(“id”) + “, context.key1 “ + event.get(“key1”)  \n\n                    + “, context.key2 “ + event.get(“key2”));  \n\n        }  \n\n    }  \n\n}  \n\npublic class ContextPropertiesTest2  \n\n{  \n\n    public static void main(String[] args)  \n\n    {  \n\n        EPServiceProvider epService &#x3D; EPServiceProviderManager.getDefaultProvider();  \n\n        EPAdministrator admin &#x3D; epService.getEPAdministrator();  \n\n        EPRuntime runtime &#x3D; epService.getEPRuntime();  \n\n        String esb &#x3D; ESB.class.getName();  \n\n        String epl1 &#x3D; “create context esbtest partition by id,price from “ + esb;  \n\n        String epl2 &#x3D; “context esbtest select context.id,context.name,context.key1,context.key2 from “ + esb;  \n\n        admin.createEPL(epl1);  \n\n        EPStatement state &#x3D; admin.createEPL(epl2);  \n\n        state.addListener(new ContextPropertiesListener2());  \n\n        ESB e1 &#x3D; new ESB();  \n\n        e1.setId(1);  \n\n        e1.setPrice(20);  \n\n        System.out.println(“sendEvent: id&#x3D;1, price&#x3D;20”);  \n\n        runtime.sendEvent(e1);  \n\n        ESB e2 &#x3D; new ESB();  \n\n        e2.setId(2);  \n\n        e2.setPrice(30);  \n\n        System.out.println(“sendEvent: id&#x3D;2, price&#x3D;30”);  \n\n        runtime.sendEvent(e2);  \n\n        ESB e3 &#x3D; new ESB();  \n\n        e3.setId(1);  \n\n        e3.setPrice(20);  \n\n        System.out.println(“sendEvent: id&#x3D;1, price&#x3D;20”);  \n\n        runtime.sendEvent(e3);  \n\n        ESB e4 &#x3D; new ESB();  \n\n        e4.setId(4);  \n\n        e4.setPrice(20);  \n\n        System.out.println(“sendEvent: id&#x3D;4, price&#x3D;20”);  \n\n        runtime.sendEvent(e4);  \n\n    }  \n\n}\n\n\n  \n执行结果：\n\nsendEvent: id&#x3D;1, price&#x3D;20  \ncontext.name esbtest, context.id 0, context.key1 1, context.key2 20  \nsendEvent: id&#x3D;2, price&#x3D;30  \ncontext.name esbtest, context.id 1, context.key1 2, context.key2 30  \nsendEvent: id&#x3D;1, price&#x3D;20  \ncontext.name esbtest, context.id 0, context.key1 1, context.key2 20  \nsendEvent: id&#x3D;4, price&#x3D;20  \ncontext.name esbtest, context.id 2, context.key1 4, context.key2 20\n\n  \n      这个例子说得比较明白，针对不同的id和price，都会新建一个context，并context.id会从0开始增加作为其标识。如果id和price一样，事件就会进入之前已经存在的context，所以e3这个事件就会和e1一样存在于context.id&#x3D;0的context里面。\n      对于epl2这个句子，意思是在esbtest这个context限制下进行事件的计算，不过这个句子很简单，可以说没有什么计算，事件进入后就显示出来了。实际上写成什么样都可以，但是必须以context xxx开头（xxx表示context定义时的名字），比如说：\n\n&#x2F;&#x2F; context定义  \n\ncreate context esbtest2 partition by id from ESB  \n\n&#x2F;&#x2F; 每当5个id相同的ESB事件进入时，统计price的总和  \n\ncontext esbtest select sum(price) from ESB.win:length_batch(5)  \n\n&#x2F;&#x2F; 根据不同的id，统计3秒内进入的事件的平均price，且price必须大于10  \n\ncontext esbtest select avg(price) from ESB(price&gt;10).win:time(3 sec)\n\n\n  \n       也许你会发现为什么我写的句子都会带有”.win:length”或者”.win:time”，那是因为我要计算的都是一堆事件，所以必须用一定条件才能把事件聚集起来。当然并不是一个事件没法计算，只不过更多情况下计算都是以多个事件为基础的。关于这一点，学习到后面就会有更多的接触。\n3. Hash Context\n       前面介绍的Context语法是以事件属性来定义的，Esper提供了以Hash值为标准定义Context，通俗一点说就是提供事件属性参与hash值的计算，计算的值再对某个值（这是什么）是同余的则进入到同一个context中。详细语法如下：\n\ncreate context context_name coalesce [by]  \nhash_func_name(hash_func_param) from stream_def  \n[, hash_func_name(hash_func_param) from stream_def ]  \n[, …]  \ngranularity granularity_value  \n[preallocate]\n\n   \na). hash_func_name为hash函数的名称，Esper提供了CRC32或者使用Java的hashcode函数来计算hash值，分别为consistent_hash_crc32和hash_code。你也可以自己定义hash函数，不过这需要配置。\nb). hash_func_param为参与计算的属性列表，比如之前的sid或者tname什么的。\nc). stream_def就是事件类型，可以一个可以多个。不同于前面的Context语法要求，Hash Context不管有多个少属性作为基础来计算hash值，hash值都只有一个，并且为int型。所以就不用关心这些属性的个数以及数据类型了。\nd). granularity是必选参数，表示为最多能创建多少个context\ne). granularity_value就是那个用于取余的“某个值”，因为Esper为了防止内存溢出，就想出了取余这种办法来限制context创建的数量。也就是说context.id&#x3D;hash_func_name(hash_func_param)  % granularity_value。\nf). preallocate是一个可选参数，如果使用它，那么Esper会预分配空间来创建granularity_value数量的context。比如说granularity_value为1024，那么Esper会预创建1024个context。内存不大的话不建议使用这个参数。\nHash Context同样可以过滤事件，举个完整的例子：\n\n&#x2F;&#x2F; 以java的hashcode方法计算sid的值(sid必须大于5)，以CRC32算法计算tid的值，然后对10取余后的值来建立context  \ncreate context HashPerson coalesce by hash_code(sid) from Student(sid&gt;5), consistent_hash_crc32(tid) from Teacher granularity 10\n\n  \nHash Context也有Built-In Context Properties，只不过只有context.id和context.name了。用法和前面说的一样，这里就不列举了。\n小贴士：\n1.如果用于hash计算的属性比较多，那么就不建议使用CRC32算法了，因为他会把这些属性值先序列化字节数组以后才能计算hash值。hashcode方法相对它能快很多。\n2.如果使用preallocate参数，建议granularity_value不要超过1000\n3.如果granularity_value超过65536，引擎查找context会比较费劲，进而影响计算速度\n4. Category Context\nCategory Context相对之前的两类context要简单许多，也更容易理解。语法说明如下：\n\ncreate context context_name  \ngroup [by] group_expression as category_label  \n[, group [by] group_expression as category_label]  \n[, …]  \nfrom stream_def\n\n  \n       我相信基本上不用我说，大家都能理解。group_expression表示分组策略的表达式，category_label为策略定义一个名字，一个context可以有多个策略同时存在，但是特殊的是之能有一个stream_def。例如：\n\ncreate context CategoryByTemp  \ngroup temp &lt; 5 as cold, group temp between 5 and 85 as normal, group temp &gt; 85 as large  \nfrom Temperature\n\n  \nCategory Context也有它自带的属性。\n\nlabel指明进入的事件所处的group是什么。完整例子如下：\n\nimport com.espertech.esper.client.EPAdministrator;  \n\nimport com.espertech.esper.client.EPRuntime;  \n\nimport com.espertech.esper.client.EPServiceProvider;  \n\nimport com.espertech.esper.client.EPServiceProviderManager;  \n\nimport com.espertech.esper.client.EPStatement;  \n\nimport com.espertech.esper.client.EventBean;  \n\nimport com.espertech.esper.client.UpdateListener;  \n\nclass ESB3  \n\n{  \n\n    private int id;  \n\n    private int price;  \n\n    public int getId()  \n\n    {  \n\n        return id;  \n\n    }  \n\n    public void setId(int id)  \n\n    {  \n\n        this.id &#x3D; id;  \n\n    }  \n\n    public int getPrice()  \n\n    {  \n\n        return price;  \n\n    }  \n\n    public void setPrice(int price)  \n\n    {  \n\n        this.price &#x3D; price;  \n\n    }  \n\n}  \n\nclass ContextPropertiesListener4 implements UpdateListener  \n\n{  \n\n    public void update(EventBean[] newEvents, EventBean[] oldEvents)  \n\n    {  \n\n        if (newEvents !&#x3D; null)  \n\n        {  \n\n            EventBean event &#x3D; newEvents[0];  \n\n            System.out.println(“context.name “ + event.get(“name”) + “, context.id “ + event.get(“id”) + “, context.label “ + event.get(“label”));  \n\n        }  \n\n    }  \n\n}  \n\npublic class ContextPropertiesTest4  \n\n{  \n\n    public static void main(String[] args)  \n\n    {  \n\n        EPServiceProvider epService &#x3D; EPServiceProviderManager.getDefaultProvider();  \n\n        EPAdministrator admin &#x3D; epService.getEPAdministrator();  \n\n        EPRuntime runtime &#x3D; epService.getEPRuntime();  \n\n        String esb &#x3D; ESB3.class.getName();  \n\n        String epl1 &#x3D; “create context esbtest group by id&lt;0 as low, group by id&gt;0 and id&lt;10 as middle,group by id&gt;10 as high from “ + esb;  \n\n        String epl2 &#x3D; “context esbtest select context.id,context.name,context.label, price from “ + esb;  \n\n        admin.createEPL(epl1);  \n\n        EPStatement state &#x3D; admin.createEPL(epl2);  \n\n        state.addListener(new ContextPropertiesListener4());  \n\n        ESB3 e1 &#x3D; new ESB3();  \n\n        e1.setId(1);  \n\n        e1.setPrice(20);  \n\n        System.out.println(“sendEvent: id&#x3D;1, price&#x3D;20”);  \n\n        runtime.sendEvent(e1);  \n\n        ESB3 e2 &#x3D; new ESB3();  \n\n        e2.setId(0);  \n\n        e2.setPrice(30);  \n\n        System.out.println(“sendEvent: id&#x3D;0, price&#x3D;30”);  \n\n        runtime.sendEvent(e2);  \n\n        ESB3 e3 &#x3D; new ESB3();  \n\n        e3.setId(11);  \n\n        e3.setPrice(20);  \n\n        System.out.println(“sendEvent: id&#x3D;11, price&#x3D;20”);  \n\n        runtime.sendEvent(e3);  \n\n        ESB3 e4 &#x3D; new ESB3();  \n\n        e4.setId(-1);  \n\n        e4.setPrice(40);  \n\n        System.out.println(“sendEvent: id&#x3D;-1, price&#x3D;40”);  \n\n        runtime.sendEvent(e4);  \n\n    }  \n\n}\n\n\n  \n输出结果为：\n\nsendEvent: id&#x3D;1, price&#x3D;20  \ncontext.name esbtest, context.id 1, context.label middle  \nsendEvent: id&#x3D;0, price&#x3D;30  \nsendEvent: id&#x3D;11, price&#x3D;20  \ncontext.name esbtest, context.id 2, context.label high  \nsendEvent: id&#x3D;-1, price&#x3D;40  \ncontext.name esbtest, context.id 0, context.label low\n\n  \n可以发现，id&#x3D;0的事件，并没有触发监听器，那是因为context里的三个category没有包含id&#x3D;0的情况，所以这个事件就被排除掉了。\n5. Non-Overlapping Context\n这类Context有个特点，是由开始和结束两个条件构成context。语法如下：\n\ncreate context context_name start start_condition end end_condition\n\n  \n       这个context有两个条件做限制，形成一个约束范围。当开始条件和结束条件都没被触发时，引擎会观察事件的进入是否会触发开始条件。如果开始条件被触发了，那么就新建一个context，并且观察结束条件是否被触发。如果结束条件被触发，那么context结束，引擎继续观察开始条件何时被触发。所以说这类Context的另一个特点是，要么context存在并且只有一个，要么条件都没被触发，也就一个context都没有了。\nstart_condition和end_condition可以是时间，或者是事件类型。比如说：\n\ncreate context NineToFive start (0, 9, *, *, *) end (0, 17, *, *, *)  \n&#x2F;&#x2F;  9点到17点此context才可用（以引擎的时间为准）。如果事件进入的事件不在此范围内，则不受该context影响\n\n  \n我列了一个完整的例子，以某类事件开始，以某类事件结束\n\nimport com.espertech.esper.client.EPAdministrator;  \n\nimport com.espertech.esper.client.EPRuntime;  \n\nimport com.espertech.esper.client.EPServiceProvider;  \n\nimport com.espertech.esper.client.EPServiceProviderManager;  \n\nimport com.espertech.esper.client.EPStatement;  \n\nimport com.espertech.esper.client.EventBean;  \n\nimport com.espertech.esper.client.UpdateListener;  \n\nclass StartEvent  \n\n{  \n\n}  \n\nclass EndEvent  \n\n{  \n\n}  \n\nclass OtherEvent  \n\n{  \n\n    private int id;  \n\n    public int getId()  \n\n    {  \n\n        return id;  \n\n    }  \n\n    public void setId(int id)  \n\n    {  \n\n        this.id &#x3D; id;  \n\n    }  \n\n}  \n\nclass NoOverLappingContextTest3 implements UpdateListener  \n\n{  \n\n    public void update(EventBean[] newEvents, EventBean[] oldEvents)  \n\n    {  \n\n        if (newEvents !&#x3D; null)  \n\n        {  \n\n            EventBean event &#x3D; newEvents[0];  \n\n            System.out.println(“Class:” + event.getUnderlying().getClass().getName() + “, id:” + event.get(“id”));  \n\n        }  \n\n    }  \n\n}  \n\npublic class NoOverLappingContextTest  \n\n{  \n\n    public static void main(String[] args)  \n\n    {  \n\n        EPServiceProvider epService &#x3D; EPServiceProviderManager.getDefaultProvider();  \n\n        EPAdministrator admin &#x3D; epService.getEPAdministrator();  \n\n        EPRuntime runtime &#x3D; epService.getEPRuntime();  \n\n        String start &#x3D; StartEvent.class.getName();  \n\n        String end &#x3D; EndEvent.class.getName();  \n\n        String other &#x3D; OtherEvent.class.getName();  \n\n        String epl1 &#x3D; “create context NoOverLapping start “ + start + “ end “ + end;  \n\n        String epl2 &#x3D; “context NoOverLapping select * from “ + other;  \n\n        admin.createEPL(epl1);  \n\n        EPStatement state &#x3D; admin.createEPL(epl2);  \n\n        state.addListener(new NoOverLappingContextTest3());  \n\n        StartEvent s &#x3D; new StartEvent();  \n\n        System.out.println(“sendEvent: StartEvent”);  \n\n        runtime.sendEvent(s);  \n\n        OtherEvent o &#x3D; new OtherEvent();  \n\n        o.setId(2);  \n\n        System.out.println(“sendEvent: OtherEvent”);  \n\n        runtime.sendEvent(o);  \n\n        EndEvent e &#x3D; new EndEvent();  \n\n        System.out.println(“sendEvent: EndEvent”);  \n\n        runtime.sendEvent(e);  \n\n        OtherEvent o2 &#x3D; new OtherEvent();  \n\n        o2.setId(4);  \n\n        System.out.println(“sendEvent: OtherEvent”);  \n\n        runtime.sendEvent(o2);  \n\n    }  \n\n}\n\n\n  \n执行结果：\n\nsendEvent: StartEvent  \nsendEvent: OtherEvent  \nClass:blog.OtherEvent, id:2  \nsendEvent: EndEvent  \nsendEvent: OtherEvent\n\n  \n由此可以看出，在NoOverLapping这个Context下监控OtherEvent，必须是在StartEvent被触发才能监控到，所以在EndEvent发送后，再发送一个OtherEvent是不会触发Listener的。\n6. OverLapping\nOverLapping和NoOverLapping一样都有两个条件限制，但是区别在于OverLapping的初始条件可以被触发多次，并且只要被触发就会新建一个context，但是当终结条件被触发时，之前建立的所有context都会被销毁。他的语法也很简单：\n\ncreate context context_name initiated [by] initiating_condition terminated [by] terminating_condition\n\n  \ninitiating_condition和terminating_condition可以为事件类型，事件或者别的条件表达式。下面给出了一个完整的例子。\n\nimport com.espertech.esper.client.EPAdministrator;  \n\nimport com.espertech.esper.client.EPRuntime;  \n\nimport com.espertech.esper.client.EPServiceProvider;  \n\nimport com.espertech.esper.client.EPServiceProviderManager;  \n\nimport com.espertech.esper.client.EPStatement;  \n\nimport com.espertech.esper.client.EventBean;  \n\nimport com.espertech.esper.client.UpdateListener;  \n\nclass InitialEvent{}  \n\nclass TerminateEvent{}  \n\nclass SomeEvent  \n\n{  \n\n    private int id;  \n\n    public int getId()  \n\n    {  \n\n        return id;  \n\n    }  \n\n    public void setId(int id)  \n\n    {  \n\n        this.id &#x3D; id;  \n\n    }  \n\n}  \n\nclass OverLappingContextListener implements UpdateListener  \n\n{  \n\n    public void update(EventBean[] newEvents, EventBean[] oldEvents)  \n\n    {  \n\n        if (newEvents !&#x3D; null)  \n\n        {  \n\n            EventBean event &#x3D; newEvents[0];  \n\n            System.out.println(“context.id:” + event.get(“id”) + “, id:” + event.get(“id”));  \n\n        }  \n\n    }  \n\n}  \n\nclass OverLappingContextListener2 implements UpdateListener  \n\n{  \n\n    public void update(EventBean[] newEvents, EventBean[] oldEvents)  \n\n    {  \n\n        if (newEvents !&#x3D; null)  \n\n        {  \n\n            EventBean event &#x3D; newEvents[0];  \n\n            System.out.println(“Class:” + event.getUnderlying().getClass().getName() + “, id:” + event.get(“id”));  \n\n        }  \n\n    }  \n\n}  \n\npublic class OverLappingContextTest  \n\n{  \n\n    public static void main(String[] args)  \n\n    {  \n\n        EPServiceProvider epService &#x3D; EPServiceProviderManager.getDefaultProvider();  \n\n        EPAdministrator admin &#x3D; epService.getEPAdministrator();  \n\n        EPRuntime runtime &#x3D; epService.getEPRuntime();  \n\n        String initial &#x3D; InitialEvent.class.getName();  \n\n        String terminate &#x3D; TerminateEvent.class.getName();  \n\n        String some &#x3D; SomeEvent.class.getName();  \n\n        String epl1 &#x3D; “create context OverLapping initiated “ + initial + “ terminated “ + terminate;  \n\n        String epl2 &#x3D; “context OverLapping select context.id from “ + initial;  \n\n        String epl3 &#x3D; “context OverLapping select * from “ + some;  \n\n        admin.createEPL(epl1);  \n\n        EPStatement state &#x3D; admin.createEPL(epl2);  \n\n        state.addListener(new OverLappingContextListener());  \n\n        EPStatement state1 &#x3D; admin.createEPL(epl3);  \n\n        state1.addListener(new OverLappingContextListener2());  \n\n        InitialEvent i &#x3D; new InitialEvent();  \n\n        System.out.println(“sendEvent: InitialEvent”);  \n\n        runtime.sendEvent(i);  \n\n        SomeEvent s &#x3D; new SomeEvent();  \n\n        s.setId(2);  \n\n        System.out.println(“sendEvent: SomeEvent”);  \n\n        runtime.sendEvent(s);  \n\n        InitialEvent i2 &#x3D; new InitialEvent();  \n\n        System.out.println(“sendEvent: InitialEvent”);  \n\n        runtime.sendEvent(i2);  \n\n        TerminateEvent t &#x3D; new TerminateEvent();  \n\n        System.out.println(“sendEvent: TerminateEvent”);  \n\n        runtime.sendEvent(t);  \n\n        SomeEvent s2 &#x3D; new SomeEvent();  \n\n        s2.setId(4);  \n\n        System.out.println(“sendEvent: SomeEvent”);  \n\n        runtime.sendEvent(s2);  \n\n    }  \n\n}\n\n\n  \n执行结果：\n\nsendEvent: InitialEvent  \ncontext.id:0, id:0  \nsendEvent: SomeEvent  \nClass:blog.SomeEvent, id:2  \nsendEvent: InitialEvent  \ncontext.id:1, id:1  \ncontext.id:0, id:0  \nsendEvent: TerminateEvent  \nsendEvent: SomeEvent\n\n  \n从结果可以看得出来，每发送一个InitialEvent，都会新建一个context，以至于context.id&#x3D;0和1。并且当发送TerminateEvent后，再发送SomeEvent监听器也不会被触发了。\n另外，context.id是每一种Context都会有的自带属性，而且针对OverLapping，还增加了startTime和endTime两种属性，表明context的开始时间和结束时间。\n7. Context Condition\nContext Condition主要包含Filter，Pattern，Crontab以及Time Period\n**A).**Filter主要就是对属性值的过滤，比如：\n\ncreate context NewUser partition by id from User(id &gt; 10)\n\n  \n**B).**Pattern是复杂事件流的代表，比如说“A事件到达后跟着B事件到达”这是一个完整的Pattern。Pattern是Esper里面很特别的东西，并且用它描述复杂的事件流是最合适不过的了。这里暂且不展开说，后面会有专门好几篇来讲解Pattern。\n**C).**Crontab是定时任务，主要用于NoOverLapping，就像前面提到的(0, 9, *, *, *)，括号里的五项代表分，时，天，月，年。关于这个后面也会有讲解。\n**D).**Time Period在这里只有一种表达式，就是after time_period_expression。例如：after 1 minute，after 5 sec。结合Context的例子如下：\n\n&#x2F;&#x2F; 以0秒为时间初始点，新建一个context，于10秒后开始，1分钟后结束。下一个context从1分20秒开始  \ncreate context NonOverlap10SecFor1Min start after 10 seconds end after 1 minute\n\n  \n8. Context Nesting\nContext也可以嵌套，意义就是多个Context联合在一起组成一个大的Context，以满足复杂的限制需求。语法结构：\n\ncreate context context_name  \ncontext nested_context_name [as] nested_context_definition ,  \ncontext nested_context_name [as] nested_context_definition [, …]\n\n  \n举个例子：\n\ncreate context NineToFiveSegmented  \ncontext NineToFive start (0, 9, *, *, *) end (0, 17, *, *, *),  \ncontext SegmentedByUser partition by userId from User\n\n  \n应用和普通的Context没区别，在此就不举例了。另外针对嵌套Context，其自带的属性使用方式会有些变化。比如针对上面这个，若想查看NineToFive的startTime和SegmentedByUser的第一个属性值，要按照下面这样写：\n\ncontext NineToFiveSegmented select  \n context.NineToFive.startTime,  \n context.SegmentedByUser.key1  \n from User\n\n  \n9. Output When Context Partition Ends\n当Context销毁时，如果你想同时查看此时Context里的东西，那么Esper提供了一种办法来输出其内容。例如：\n\ncreate context OverLapping initiated InitialEvent terminated TerminateEvent  \ncontext OverLapping select * from User output snapshot when terminated\n\n  \n那么当终结事件发送到引擎后，会立刻输出OverLapping的快照。\n如果你想以固定的频率查看Context的内容，Esper也支持。例如：\n\ncontext OverLapping select * from User output snapshot every 2 minute &#x2F;&#x2F; 每两分钟输出OverLapping的事件\n\n  \n关于output表达式，后面也会有详解。\n       以上的内容算是包含了Context的所有方面，可能还有些细节需要各位自己去研读他的手册，并且多加练习。Esper的内容之多以至于我说了很多次“后面会专门讲解”，不过也确实是因为内容复杂，所以不得不先跳过这些。在学习到之后的内容以后，再回过头来理解Context可能会有另一番效果。\n","slug":"BIGDATA/Esper学习之四：Context","date":"2023-06-08T17:49:42.000Z","categories_index":"context,BIGDATA","tags_index":"sendEvent,Context,price","author_index":"dandeliono"},{"id":"9e7669ec94144638fab3876a099d0e10","title":"Esper学习之三：进程模型","content":"Esper学习之三：进程模型 之前对Esper所能处理的事件结构进行了概述，并结合了例子进行讲解，不清楚的同学请看Esper学习之二：事件类型。今天主要为大家解释一下Esper是怎么处理事件的，即Esper的进程模型。\n1.UpdateListener\nUpdaterListener是Esper提供的一个接口，用于监听某个EPL在引擎中的运行情况，即事件进入并产生结果后会通知UpdateListener。接口如下\n\npackage com.espertech.esper.client;  \n\nimport com.espertech.esper.client.EventBean;  \n\npublic interface UpdateListener  \n\n{  \n\n    public void update(EventBean[] newEvents, EventBean[] oldEvents);  \n\n}\n\n\n  \n接口很简单，就一个update方法，其中包括两个EventBean数组，至于两个参数的含义稍后再说。EventBean中有一个最常用的get方法，是用来得到EPL中某个字段的值。例如：\n\nEPL:select name from User  \n\n&#x2F;&#x2F;假设newEvents长度为一  \n\nnewEvents[0].get(“name”)能得到进入的User事件的name属性值  \n\nEPL:select count(*) from User.win:time(5 sec)  \n\n&#x2F;&#x2F;假设newEvents长度为一  \n\nnewEvents[0].get(“count(*))能得到5秒内进入引擎的User事件数量有多少\n\n\n  \nget方法最常用，此外还有getUnderlying等方法，以后会专门写一篇介绍EventBean的。\n2.Insert and Remove Stream\nInsert表示进入引擎，Remove表示移出引擎，事件在Esper中会因为某类EPL才会经历这两种状态。对应于UpdateListener接口就是newEvents和oldEvents，因为处于这两种状态的事件不一定只有一个，所以newEvents和oldEvents就是数组形式。举个例子说明下\n\n从此图可以看出，随着时间推移，每个进入到引擎的W事件都是newEvents，即Insert Stream。W后括号里的值为属性值，可忽略。\n有人可能要问了，为什么这里oldeEvents什么都没有。那是因为EPL的关系。看下面的例子\n\nEPL:select * from User.win:length(5)\n\n  \n\n注：win:length(5)是个view，详细的后面会专门讲解，这里先暂时理解为Esper开放一个空间并最多可同时存放5个事件（此空间其实就是大小为5的数组）\n       由图可知，length window可存放w1,w2等事件，在w6事件进入之前，每个事件进入都属于newEvents。直到w6进入后，length window不能容纳w1～w6的事件，必须把w1事件移出，即w1为oldEvents。length window就像一个队列，每当事件进入队列时，就会触发updateListener并告知有新事件进入。当队列满了，再进入一个新事件时，Esper会触发UpdateListener告知有新事件进入并且有旧事件移出，正如上图所示的w6和w1。\n实际上这个EPL触发监听器都只能看到newEvents，看不到oldEvents。如果想看到oldEvents，EPL要改写一下：\n\nEPL:select irstream * from User.win:length(5)\n\n  \n       默认情况下，Esper认为你只想让newEvents触发监听器，即istream(insert stream)。如果想让oldEvents触发监听器，那么为rstream(remove stream)。如果两个都想，那么为irstream。当然这个默认情况是可以配置的，以后会说到这个问题。\n       不过对于rstream，在我看来他有个bug，因为在运行时我发现，oldEvents触发监听器时，理论上应该是oldEvents这个参数有值，就算他没说明，按照常理推断也应该是oldEvents有值，但是实际上是newEvents有值，oldEvents为null。虽然说数据没有错，但是这个似乎不合常理。\n注：上面这段话是错误的，后来我看到第五章的时候，文档有明确说明当用rstream关键字的时候，过期事件是发到newEvents的，不会发到oldEvents，所以oldEvents是null。 只不过我觉得这样会给人怪怪的感觉。。（这段话回复了评论里的joy_91 ，感谢！）\n3.Filter and Where-Clause\nEPL有两种过滤事件的方式，一种是过滤事件进入view（可以把view理解为一个窗口），即Filter。另一种是让事件都进入view，但不触发UpdateListener，即Where子句。关于这两种语法后面会详细讲解，这里就只是简单介绍。\nFilter：\n\n&#x2F;&#x2F; Apple事件进入Esper，只有amount大于200的才能进入win:length，并且length长度为5  \nEPL:select * from Apple(amount&gt;200).win:length(5)\n\n  \n\n从图上可以看出，只有amount大于200，Esper才允许Apple事件进入view，并且作为一个newEvent触发UpdateListener\nWhere-Clause:\n\n&#x2F;&#x2F; Apple事件进入Esper并进入win:length(5)，但是只有amount大于200的才能触发UpdateListener  \nEPL:select * from Apple.win:length(5) where amount&gt;200\n\n  \n\n由图上可以看出，Apple事件先进入view，然后才被where子句过滤，以至于被过滤掉的事件不会作为newEvent触发UpdateListener\n其实单看两个EPL，就能发现一个过滤是在进入view前，一个过滤是在view后，所以大家在应用的时候要注意。PS:在我写这段的时候才发现以前认为这两种是一样的效果是错误滴- -！\n4.Aggregation and Grouping\n之前说过EPL是类SQL语法，所以也会有聚合和分组的功能。语法和SQL基本一样，下面给大家展示一下：\n\n&#x2F;&#x2F; 统计进入的5个Apple事件，amount的总数是多少  \n\nselect sum(amount) from Apple.win:length_batch(5)  \n\n&#x2F;&#x2F; 统计进入的5个Apple事件，amount的总数是多少，并按照price分组  \n\nselect price, sum(amount) from Apple.win:length_batch(5) group by price  \n\n&#x2F;&#x2F; 统计进入的5个Apple事件，amount的总数和name，并按照price分组  \n\nselect price, name, sum(amount) from Apple.win:length_batch(5) group by price\n\n\n  \n       最后一个和前一个的区别在于name也在统计的范围内，所以当name和price都一样的两个事件进入Esper，会有两个一模一样的事件作为newEvent触发UpdaterListener，即price，name，sum(amount)都一样。当然要是group by name, price的话，就只会有一个事件触发监听器了。\n","slug":"BIGDATA/Esper学习之三：进程模型","date":"2023-06-08T17:49:22.000Z","categories_index":"EPL,BIGDATA","tags_index":"Esper,length,newEvents","author_index":"dandeliono"},{"id":"ae77ccdae608040cb14f38fa6a0025f9","title":"Esper学习之二：事件类型","content":"Esper学习之二：事件类型Esper对事件有特殊的数据结构约定。能处理的事件结构有：POJO，java.util.Map，Object Array，XML\n1.POJO\n       对于POJO，Esper要求对每一个私有属性要有getter方法。Esper允许不必按照JavaBean规定的格式，但是getter方法是必须的。又或者可以在配置文件中配置可访问的方法来代替getter。简单示例如下\n\npublic class Person  \n\n{  \n\n    String name;  \n\n    int age;  \n\n    public String getName()  \n\n    {  \n\n        return name;  \n\n    }  \n\n    public int getAge()  \n\n    {  \n\n        return age;  \n\n    }  \n\n}\n\n\n  \nEsper同样也能支持复杂的数据类型以及嵌套。稍微复杂的Person如下\n\nimport java.util.List;  \n\nimport java.util.Map;  \n\npublic class Person  \n\n{  \n\n    String name;  \n\n    int age;  \n\n    List children;  \n\n    Map&lt;String, Integer&gt; phones;  \n\n    Address address;  \n\n    public String getName()  \n\n    {  \n\n        return name;  \n\n    }  \n\n    public int getAge()  \n\n    {  \n\n        return age;  \n\n    }  \n\n    public List getChildren()  \n\n    {  \n\n        return children;  \n\n    }  \n\n    public Map&lt;String, Integer&gt; getPhones()  \n\n    {  \n\n        return phones;  \n\n    }  \n\n    public Address getAddress()  \n\n    {  \n\n        return address;  \n\n    }  \n\n}  \n\nclass Child  \n\n{  \n\n    String name;  \n\n    int gender;  \n\n}  \n\nclass Address  \n\n{  \n\n    String road;  \n\n    String street;  \n\n    int houseNo;  \n\n}\n\n\n  \n如上所示，Esper能支持包含了集合类型和嵌套类的POJO，示例的EPL语句如下\n\n&#x2F;&#x2F; 当Person类型的事件中name为luonanqin时，Esper能得到对应的age,children和address  \nselect age,children,address from Person where name&#x3D;”luonanqin”\n\n   \n如果我不想要所有的child，而是想要第二个。并且我想得到家里的电话号码，那么Person需要改动一下\n\nimport java.util.List;  \n\nimport java.util.Map;  \n\npublic class Person  \n\n{  \n\n    String name;  \n\n    int age;  \n\n    List children;  \n\n    Map&lt;String, Integer&gt; phones;  \n\n    Address address;  \n\n    public String getName()  \n\n    {  \n\n        return name;  \n\n    }  \n\n    public int getAge()  \n\n    {  \n\n        return age;  \n\n    }  \n\n    public Child getChildren(int index)  \n\n    {  \n\n        return children.get(index);  \n\n    }  \n\n    public int getPhones(String name)  \n\n    {  \n\n        return phones.get(name);  \n\n    }  \n\n    public Address getAddress()  \n\n    {  \n\n        return address;  \n\n    }  \n\n}\n\n\n  \n对应的EPL如下\n\nselect children[1], phones(‘home’), address.road where Person where name&#x3D;”luonanqin”\n\n  \nEsper支持事件的更新，对此Esper要求提供对应的setter方法。Person需要再有点小该度。示例如下\n\nimport java.util.List;  \n\nimport java.util.Map;  \n\npublic class Person  \n\n{  \n\n    String name;  \n\n    int age;  \n\n    List children;  \n\n    Map&lt;String, Integer&gt; phones;  \n\n    Address address;  \n\n    public String getName()  \n\n    {  \n\n        return name;  \n\n    }  \n\n    public int getAge()  \n\n    {  \n\n        return age;  \n\n    }  \n\n    public Child getChildren(int index)  \n\n    {  \n\n        return children.get(index);  \n\n    }  \n\n    public void setPhones(String name, Integer number){  \n\n        phones.put(name, number);  \n\n    }  \n\n    public int getPhones(String name)  \n\n    {  \n\n        return phones.get(name);  \n\n    }  \n\n    public Address getAddress()  \n\n    {  \n\n        return address;  \n\n    }  \n\n}\n\n\n  \n对应的EPL如下\n\n&#x2F;&#x2F; 当Person类型的事件中name为luonanqin时，更新家里的电话  \nupdate Person set phones(‘home’) &#x3D; 123456789 where name&#x3D;”luonanqin”\n\n  \nEsper对POJO的支持基本上就是上面所说的，另外他还支持实现了多个接口类或者抽象类的POJO，使用方法和普通的POJO没什么区别，这里就不列举了。\n2.Map\nEsper支持原生Java Map结构的事件。相对于POJO来说，Map的结构更利于事件类型的热加载，毕竟不是class，所以不需要重启JVM。所以如果系统对重启比较敏感，建议使用Map来定义事件的结构。Map的结构很简单，主要分为事件定义名和事件属性列表。我们继续拿Person来讲解\n\nimport java.util.HashMap;  \n\nimport java.util.List;  \n\nimport java.util.Map;  \n\nimport com.espertech.esper.client.EPAdministrator;  \n\nimport com.espertech.esper.client.EPServiceProvider;  \n\nimport com.espertech.esper.client.EPServiceProviderManager;  \n\npublic class PersonMap  \n\n{  \n\n    public static void main(String[] args)  \n\n    {  \n\n        EPServiceProvider epService &#x3D; EPServiceProviderManager.getDefaultProvider();  \n\n        EPAdministrator admin &#x3D; epService.getEPAdministrator();  \n\n        Map&lt;String,Object&gt; person &#x3D; new HashMap&lt;String,Object&gt;();  \n\n        person.put(“name”, String.class);  \n\n        person.put(“age”, int.class);  \n\n        person.put(“children”, List.class);  \n\n        person.put(“phones”, Map.class);  \n\n        admin.getConfiguration().addEventType(“Person”, person);  \n\n    }  \n\n}\n\n\n  \n如上所示，Map结构的事件需要将属性名作为key，属性的数据类型作为value保存到Map中，然后再通过Esper的接口注册到Esper。其中addEventType的两个参数分别代表事件定义的名称和所定义的结构。\n对应的EPL和POJO的没有区别\n\n&#x2F;&#x2F; 当Person类型的事件中name为luonanqin时，Esper能得到对应的age,children  \nselect age,children from Person where name&#x3D;”luonanqin”\n\n   \nMap对于嵌套类的定义比较特别。如果嵌套的类是POJO，那就如上面所示。如果嵌套的还是Map，那么定义方式就需要改变。我们为Person加上Address，示例如下\n\nimport java.util.HashMap;  \n\nimport java.util.List;  \n\nimport java.util.Map;  \n\nimport com.espertech.esper.client.EPAdministrator;  \n\nimport com.espertech.esper.client.EPServiceProvider;  \n\nimport com.espertech.esper.client.EPServiceProviderManager;  \n\npublic class PersonMap  \n\n{  \n\n    public static void main(String[] args)  \n\n    {  \n\n        EPServiceProvider epService &#x3D; EPServiceProviderManager.getDefaultProvider();  \n\n        EPAdministrator admin &#x3D; epService.getEPAdministrator();  \n\n        Map&lt;String, Object&gt; address &#x3D; new HashMap&lt;String, Object&gt;();  \n\n        address.put(“road”, String.class);  \n\n        address.put(“street”, String.class);  \n\n        address.put(“houseNo”, int.class);  \n\n        Map&lt;String, Object&gt; person &#x3D; new HashMap&lt;String, Object&gt;();  \n\n        person.put(“name”, String.class);  \n\n        person.put(“age”, int.class);  \n\n        person.put(“children”, List.class);  \n\n        person.put(“phones”, Map.class);  \n\n        person.put(“address”, “Address”);  \n\n        admin.getConfiguration().addEventType(“Address”, address);  \n\n        admin.getConfiguration().addEventType(“Person”, person);  \n\n    }  \n\n}\n\n\n  \n如上所示，有两个关键点：\n1.Person在定义Address属性时，map的value不是Address.class，而是Address字符串，而这就代表引擎里的Address对应的Map结构定义\n2.事件定义注册必须是Address先于Person，因为Person用到了Address，而引擎是根据Address注册时用的名字去查找Address定义的，所以如果名字写错，引擎就找不到Address了\n如果Person有多个Address，则以数组方式定义Person的多个Address时，代码又变成下面的样子了\n\nperson.put(“addresses”, “Address[]“);\n\n  \n另外对于Map，Esper只支持增量更新，也就是说只能增加Map中的属性定义，而不能修改或者删除某个属性（实际上属性增多并不影响其处理性能，所以没有删除在我看来也没什么。至于修改，也只能是先注销再注册了）。我们为Person增加一个gender属性，示例如下\n\nimport java.util.Arrays;  \n\nimport java.util.HashMap;  \n\nimport java.util.List;  \n\nimport java.util.Map;  \n\nimport com.espertech.esper.client.EPAdministrator;  \n\nimport com.espertech.esper.client.EPServiceProvider;  \n\nimport com.espertech.esper.client.EPServiceProviderManager;  \n\nimport com.espertech.esper.client.EventType;  \n\npublic class PersonMap  \n\n{  \n\n    public static void main(String[] args)  \n\n    {  \n\n        EPServiceProvider epService &#x3D; EPServiceProviderManager.getDefaultProvider();  \n\n        EPAdministrator admin &#x3D; epService.getEPAdministrator();  \n\n        Map&lt;String, Object&gt; address &#x3D; new HashMap&lt;String, Object&gt;();  \n\n        address.put(“road”, String.class);  \n\n        address.put(“street”, String.class);  \n\n        address.put(“houseNo”, int.class);  \n\n        Map&lt;String, Object&gt; person &#x3D; new HashMap&lt;String, Object&gt;();  \n\n        person.put(“name”, String.class);  \n\n        person.put(“age”, int.class);  \n\n        person.put(“children”, List.class);  \n\n        person.put(“phones”, Map.class);  \n\n        person.put(“address”, “Address”);  \n\n        admin.getConfiguration().addEventType(“Address”, address);  \n\n        admin.getConfiguration().addEventType(“Person”, person);  \n\n        person.put(“gender”, int.class);  \n\n        admin.getConfiguration().updateMapEventType(“Person”, person);  \n\n        EventType event &#x3D; admin.getConfiguration().getEventType(“Person”);  \n\n        System.out.println(“Person props: “ + Arrays.asList(event.getPropertyNames()));  \n\n    }  \n\n}\n\n\n  \n3.Object Array\n对象数组和Map很像，基本没有差别。只是定义方式不一样，Esper同样也只支持增量更新。这里继续用Person为大家做例子\n\nimport java.util.Arrays;  \n\nimport java.util.Map;  \n\nimport com.espertech.esper.client.EPAdministrator;  \n\nimport com.espertech.esper.client.EPServiceProvider;  \n\nimport com.espertech.esper.client.EPServiceProviderManager;  \n\nimport com.espertech.esper.client.EventType;  \n\npublic class PersonArray  \n\n{  \n\n    public static void main(String[] args)  \n\n    {  \n\n        EPServiceProvider epService &#x3D; EPServiceProviderManager.getDefaultProvider();  \n\n        EPAdministrator admin &#x3D; epService.getEPAdministrator();  \n\n        String[] addressPropNames &#x3D; { “road”, “street”, “houseNo” };  \n\n        Object[] addressPropTypes &#x3D; { String.class, String.class, int.class };  \n\n        String[] childPropNames &#x3D; { “name”, “age” };  \n\n        Object[] childPropTypes &#x3D; { String.class, int.class };  \n\n        String[] personPropNames &#x3D; { “name”, “age”, “children”, “phones”, “address” };  \n\n        Object[] personPropTypes &#x3D; { String.class, int.class, “Child[]“, Map.class, “Address” };  \n\n        admin.getConfiguration().addEventType(“Address”, addressPropNames, addressPropTypes);  \n\n        admin.getConfiguration().addEventType(“Child”, childPropNames, childPropTypes);  \n\n        admin.getConfiguration().addEventType(“Person”, personPropNames, personPropTypes);  \n\n        admin.getConfiguration().updateObjectArrayEventType(“Person”, new String[] { “gender” }, new Object[] { int.class });  \n\n        EventType event &#x3D; admin.getConfiguration().getEventType(“Person”);  \n\n        System.out.println(“Person props: “ + Arrays.asList(event.getPropertyNames()));  \n\n    }  \n\n}\n\n\n  \n上面的例子包含了对象数组这种事件格式的所有特性，我就不多加解释了。\n4.XML\n待续\n","slug":"BIGDATA/Esper学习之二：事件类型","date":"2023-06-08T17:49:07.000Z","categories_index":"String,BIGDATA","tags_index":"class,Map,Person","author_index":"dandeliono"},{"id":"7959b0a0c2b89085e6f8a032821dbd4d","title":"Esper学习之一：Esper介绍","content":"Esper学习之一：Esper介绍CEP即Complex Event Process，中文意思就是“复杂事件处理”。听起来好像很复杂，实际上就是基于事件流进行数据处理，把要分析的数据抽象成事件，然后将数据发送到CEP引擎，引擎就会根据事件的输入和最初注册的处理模型，得到事件处理结果。\n       有人可能要问了，这和Hadoop有什么区别？可是本人不才，没学过Hadoop，虽然说赶上了这阵风，但是从很多人那了解以后，觉得不过就是个不是特别成熟的工具，然后各个公司要根据需求对Hadoop进行二次开发，就需要懂得源码的人。所以就没打算学了，一个工具而已，等到自己确实有空的时候再学也不迟。至于CEP和Hadoop的区别，应该是Esper和Hadoop的区别，我的理解是：Hadoop适合做事后分析，而Esper适合实时分析。Hadoop我确实不是很了解，如果有问题还希望大家指正。\n       CEP是一种标准，Esper只是对这个标准的一种开源实现。除了Esper，很多大公司也有类似的商业软件，比如IBM，Sybase等等，听说巨贵无比。CEP的一个重要特点就是他是一个内存计算工具和类SQL语句。内存计算可以说是一把双刃剑。好处自不必说，一个字：快！坏处也显而易见，数据有丢失的风险，而且还有容量的限制（实时计算其实并不受制于内存大小，而是得看如何对实时进行定义，也就是具体的业务来决定了）。所以如果业务不能容忍数据丢失，那么高可用方案就必须做好，不过Esper的高可用很不好做，后面我将会说到。\n       CEP的类SQL语句，可以理解为处理模型的定义与描述。这是运行在CEP引擎中的特殊语句，之所以叫他类SQL，是因为它和SQL确实很像，除了select，insert，delete，update，而且也有avg，count等函数。所以对于会SQL的人来说，他的语法结构大致还是能猜出一二的。在Esper中，这个句子叫做EPL，即Event Process Language。作为Esper的核心内容，对于它的讲解有三四百页的英文文档，所以之后我会慢慢向大家细细说明的。\n下面就简单写个列子给大家看看吧。场景是计算3个苹果的平均价格\n\npackage test;  \n\nimport com.espertech.esper.client.EPAdministrator;  \n\nimport com.espertech.esper.client.EPRuntime;  \n\nimport com.espertech.esper.client.EPServiceProvider;  \n\nimport com.espertech.esper.client.EPServiceProviderManager;  \n\nimport com.espertech.esper.client.EPStatement;  \n\nimport com.espertech.esper.client.EventBean;  \n\nimport com.espertech.esper.client.UpdateListener;  \n\nclass Apple  \n\n{  \n\n    private int id;  \n\n    private int price;  \n\n    public int getId()  \n\n    {  \n\n        return id;  \n\n    }  \n\n    public void setId(int id)  \n\n    {  \n\n        this.id &#x3D; id;  \n\n    }  \n\n    public int getPrice()  \n\n    {  \n\n        return price;  \n\n    }  \n\n    public void setPrice(int price)  \n\n    {  \n\n        this.price &#x3D; price;  \n\n    }  \n\n}  \n\nclass AppleListener implements UpdateListener  \n\n{  \n\n    public void update(EventBean[] newEvents, EventBean[] oldEvents)  \n\n    {  \n\n        if (newEvents !&#x3D; null)  \n\n        {  \n\n            Double avg &#x3D; (Double) newEvents[0].get(“avg(price)”);  \n\n            System.out.println(“Apple’s average price is “ + avg);  \n\n        }  \n\n    }  \n\n}  \n\npublic class Test {  \n\n    public static void main(String[] args) throws InterruptedException {  \n\n        EPServiceProvider epService &#x3D; EPServiceProviderManager.getDefaultProvider();  \n\n        EPAdministrator admin &#x3D; epService.getEPAdministrator();  \n\n        String product &#x3D; Apple.class.getName();  \n\n        String epl &#x3D; “select avg(price) from “ + product + “.win:length_batch(3)”;  \n\n        EPStatement state &#x3D; admin.createEPL(epl);  \n\n        state.addListener(new AppleListener());  \n\n        EPRuntime runtime &#x3D; epService.getEPRuntime();  \n\n        Apple apple1 &#x3D; new Apple();  \n\n        apple1.setId(1);  \n\n        apple1.setPrice(5);  \n\n        runtime.sendEvent(apple1);  \n\n        Apple apple2 &#x3D; new Apple();  \n\n        apple2.setId(2);  \n\n        apple2.setPrice(2);  \n\n        runtime.sendEvent(apple2);  \n\n        Apple apple3 &#x3D; new Apple();  \n\n        apple3.setId(3);  \n\n        apple3.setPrice(5);  \n\n        runtime.sendEvent(apple3);  \n\n    }  \n\n}\n\n\n  \n很简单的例子，虽然没有加注释，也应该很好懂吧。大家可以自己运行一下看看是什么结果。\nEsper的官网：http://esper.codehaus.org/ 里面有很多例子可以\n","slug":"BIGDATA/Esper学习之一：Esper介绍","date":"2023-06-08T17:48:32.000Z","categories_index":"apple,BIGDATA","tags_index":"Esper,esper,Apple","author_index":"dandeliono"},{"id":"1bb022f0a90cc114ad2400e87144dac4","title":"tcpdump在启用TCP卸载功能的网卡上报告 checksum error ·","content":"tcpdump在启用TCP卸载功能的网卡上报告”checksum error” ·在使用tcpdump对网卡进行抓包的时，很多时候会发现有cksum incorrect错误：\n12sudo tcpdump -i eth0 -nn -vv -e\n\n转包显示有很多incorrect数据包\n123456711:54:08.747448 00:16:3e:0f:02:76 &gt; 3c:8c:40:4e:dd:46, ethertype IPv4 (0x0800), length 70: (tos 0x0, ttl 128, id 12416, offset 0, flags [DF], proto: TCP (6), length: 56) 69.172.201.153.20648 &gt; 218.92.186.154.32233: P, cksum 0xeace (incorrect (-&gt; 0xd8b6), 649:665(16) ack 0 win 51111:54:08.747631 3c:8c:40:01:87:f5 &gt; 00:16:3e:0f:02:76, ethertype IPv4 (0x0800), length 60: (tos 0x0, ttl  54, id 43880, offset 0, flags [DF], proto: TCP (6), length: 40) 49.80.162.95.25453 &gt; 69.172.201.153.20648: ., cksum 0x08de (correct), 0:0(0) ack 643 win 6437911:54:08.747673 3c:8c:40:02:72:68 &gt; 00:16:3e:0f:02:76, ethertype IPv4 (0x0800), length 86: (tos 0x0, ttl  51, id 26279, offset 0, flags [DF], proto: TCP (6), length: 72) 49.81.57.179.18212 &gt; 69.172.201.153.5110: P, cksum 0xfc9e (correct), 1:33(32) ack 1 win 6393911:54:08.747685 3c:8c:40:01:87:f5 &gt; 00:16:3e:0f:02:76, ethertype IPv4 (0x0800), length 60: (tos 0x0, ttl  51, id 16050, offset 0, flags [DF], proto: TCP (6), length: 40) 219.139.32.98.13599 &gt; 69.172.201.153.20648: ., cksum 0xc58e (correct), 4:4(0) ack 643 win 6452311:54:08.747777 00:16:3e:0f:02:76 &gt; 3c:8c:40:4e:dd:46, ethertype IPv4 (0x0800), length 62: (tos 0x0, ttl 128, id 6219, offset 0, flags [DF], proto: TCP (6), length: 48) 69.172.201.153.20648 &gt; 219.139.32.98.13599: P, cksum 0x51bd (incorrect (-&gt; 0x5783), 643:651(8) ack 4 win 51111:54:08.748145 3c:8c:40:02:79:e8 &gt; 00:16:3e:0f:02:76, ethertype IPv4 (0x0800), length 60: (tos 0x0, ttl  54, id 16715, offset 0, flags [DF], proto: TCP (6), length: 40) 221.231.4.18.4134 &gt; 69.172.201.153.20648: ., cksum 0x78e2 (correct), 0:0(0) ack 643 win 65519\n\n\n\n\n\n\n\n\n\n\n上述案例中IP地址是伪造的，仅做示例\n参考 UDP &#x2F; TCP Checksum errors from tcpdump &amp; NIC Hardware Offloading\n当是使用tcpdump跟踪UDP或TCP数据流的时候，会看到大多数数据包显示checksum错误，这是因为网卡(NIC)启用了checksum offloading而tcpdump是从内核读取IP数据包，比NIC网卡芯片执行checksum要早。检查命令的方法是（假设检查DNS查询的数据包）\n12sudo tcpdump -i eth0 -vvv -nn udp dst port 53\n\n我模仿使用如下命令检查虚拟机69.172.201.153.123的UDP包，可以看到发出的UDP包的checksum都是错误的，从外面返回的UDP包则显示checksum正常\n12sudo tcpdump -i eth0 -vvv -nn udp and host 69.172.201.153\n\n可以看到\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748tcpdump: WARNING: eth0: no IPv4 address assignedtcpdump: listening on eth0, link-type EN10MB (Ethernet), capture size 96 bytes12:25:26.388718 IP (tos 0xc0, ttl  64, id 0, offset 0, flags [DF], proto: UDP (17), length: 76) 69.172.201.153.123 &gt; 58.216.105.122: [bad udp cksum 559b!] NTPv4, length 48    Client, Leap indicator:  (0), Stratum 3, poll 7s, precision -22    Root Delay: 0.030166, Root dispersion: 0.024185, Reference-ID: 110.75.186.249      Reference Timestamp:  3677372472.395108491 (2016/07/13 12:21:12)      Originator Timestamp: 3677372596.423037379 (2016/07/13 12:23:16)      Receive Timestamp:    3677372596.434980779 (2016/07/13 12:23:16)      Transmit Timestamp:   3677372726.389321297 (2016/07/13 12:25:26)        Originator - Receive Timestamp:  +0.011943411        Originator - Transmit Timestamp: +129.96628391712:25:26.412829 IP (tos 0x0, ttl  57, id 0, offset 0, flags [DF], proto: UDP (17), length: 76) 58.216.105.122 &gt; 69.172.201.153.123: [udp sum ok] NTPv4, length 48    Server, Leap indicator:  (0), Stratum 2, poll 7s, precision -24    Root Delay: 0.000106, Root dispersion: 0.001434, Reference-ID: 10.137.38.86      Reference Timestamp:  3677372712.335200518 (2016/07/13 12:25:12)      Originator Timestamp: 3677372726.389321297 (2016/07/13 12:25:26)      Receive Timestamp:    3677372726.401639968 (2016/07/13 12:25:26)      Transmit Timestamp:   3677372726.401653856 (2016/07/13 12:25:26)        Originator - Receive Timestamp:  +0.012318663        Originator - Transmit Timestamp: +0.01233254512:25:30.386709 IP (tos 0xc0, ttl  64, id 0, offset 0, flags [DF], proto: UDP (17), length: 76) 69.172.201.153.123 &gt; 110.75.186.248.123: [bad udp cksum 8059!] NTPv4, length 48    Client, Leap indicator:  (0), Stratum 3, poll 10s, precision -22    Root Delay: 0.030166, Root dispersion: 0.024246, Reference-ID: 110.75.186.249      Reference Timestamp:  3677372472.395108491 (2016/07/13 12:21:12)      Originator Timestamp: 0.000000000      Receive Timestamp:    0.000000000      Transmit Timestamp:   3677372730.387331545 (2016/07/13 12:25:30)        Originator - Receive Timestamp:  0.000000000        Originator - Transmit Timestamp: 3677372730.387331545 (2016/07/13 12:25:30)12:25:32.387721 IP (tos 0xc0, ttl  64, id 0, offset 0, flags [DF], proto: UDP (17), length: 76) 69.172.201.153.123 &gt; 115.28.122.198.123: [bad udp cksum 4b0a!] NTPv4, length 48    Client, Leap indicator:  (0), Stratum 3, poll 7s, precision -22    Root Delay: 0.030166, Root dispersion: 0.024276, Reference-ID: 110.75.186.249      Reference Timestamp:  3677372472.395108491 (2016/07/13 12:21:12)      Originator Timestamp: 3677372601.409083545 (2016/07/13 12:23:21)      Receive Timestamp:    3677372601.422362774 (2016/07/13 12:23:21)      Transmit Timestamp:   3677372732.388339668 (2016/07/13 12:25:32)        Originator - Receive Timestamp:  +0.013279223        Originator - Transmit Timestamp: +130.97925615312:25:32.419074 IP (tos 0x0, ttl  54, id 0, offset 0, flags [DF], proto: UDP (17), length: 76) 115.28.122.198.123 &gt; 69.172.201.153.123: [udp sum ok] NTPv4, length 48    Server, Leap indicator:  (0), Stratum 2, poll 7s, precision -24    Root Delay: 0.041046, Root dispersion: 0.002349, Reference-ID: 10.137.38.86      Reference Timestamp:  3677372656.633167505 (2016/07/13 12:24:16)      Originator Timestamp: 3677372732.388339668 (2016/07/13 12:25:32)      Receive Timestamp:    3677372732.406500339 (2016/07/13 12:25:32)      Transmit Timestamp:   3677372732.406535744 (2016/07/13 12:25:32)        Originator - Receive Timestamp:  +0.018160656        Originator - Transmit Timestamp: +0.018196072\n\n检查 eth0，发现原来tx-checksumming是开启的，而rx-checksumming则是关闭的\n12sudo ethtool -k eth0\n\n输出\n123456789Offload parameters for eth0:rx-checksumming: offtx-checksumming: onscatter-gather: ontcp segmentation offload: onudp fragmentation offload: offgeneric segmentation offload: ongeneric-receive-offload: off\n\n这台物理服务器上运行了虚拟化，在虚拟机中测试了dig @8.8.8.8 www.sina.com.cn，可以在物理服务器上抓包显示\n1234513:09:49.944926 IP (tos 0x0, ttl  64, id 48092, offset 0, flags [none], proto: UDP (17), length: 61) 69.172.201.153.37421 &gt; 8.8.8.8.53: [bad udp cksum b1aa!]  12774+ A? www.sina.com.cn. (33)13:09:50.945615 IP (tos 0x0, ttl  64, id 48093, offset 0, flags [none], proto: UDP (17), length: 61) 69.172.201.153.37421 &gt; 8.8.8.8.53: [bad udp cksum b1aa!]  12774+ A? www.sina.com.cn. (33)13:09:51.018821 IP (tos 0x0, ttl  42, id 23786, offset 0, flags [none], proto: UDP (17), length: 183) 8.8.8.8.53 &gt; 69.172.201.153.37421:  12774 q: A? www.sina.com.cn. 5/0/0 www.sina.com.cn. CNAME[|domain]13:09:51.092146 IP (tos 0x0, ttl  42, id 51428, offset 0, flags [none], proto: UDP (17), length: 183) 8.8.8.8.53 &gt; 69.172.201.153.37421:  12774 q: A? www.sina.com.cn. 5/0/0 www.sina.com.cn. CNAME[|domain]\n\n尝试在虚拟机中关闭TCO(注意，ethtool检查的时候参数是小写的-k，当ethtool设置参数的时候是大写的-K)\n12ethtool -K eth1 tx off\n\n1234567Actual changes:rx-checksumming: offtx-checksumming: offscatter-gather: offtcp-segmentation-offload: offudp-fragmentation-offload: off\n\n再次检查，发现rx和tx的checksum被同时关闭了\n12345678910111213ethtool -k eth1Features for eth1:rx-checksumming: offtx-checksumming: offscatter-gather: offtcp-segmentation-offload: offudp-fragmentation-offload: offgeneric-segmentation-offload: ongeneric-receive-offload: offlarge-receive-offload: offntuple-filters: offreceive-hashing: off\n\n\n\n\n\n\n\n\n\n\n奇怪：怎么tx off设置会同时关闭tx和rx的checksum?\n不过，验证下来发现，在虚拟机中关闭TCO（tx-checksumming和rx-checksumming）后，在NC上抓包显示，已经不再出现数据包发送的时候的checksum错误\n1234567891011121314151617181920212213:14:54.749620 IP (tos 0x0, ttl  64, id 48095, offset 0, flags [none], proto: UDP (17), length: 61) 69.172.201.153.53614 &gt; 8.8.8.8.53: [udp sum ok]  4027+ A? www.sina.com.cn. (33)13:14:54.964426 IP (tos 0x0, ttl  42, id 19522, offset 0, flags [none], proto: UDP (17), length: 183) 8.8.8.8.53 &gt; 69.172.201.153.53614:  4027 q: A? www.sina.com.cn. 5/0/0 www.sina.com.cn. CNAME[|domain]13:16:12.388133 IP (tos 0xc0, ttl  64, id 0, offset 0, flags [DF], proto: UDP (17), length: 76) 69.172.201.153.123 &gt; 58.216.105.122: [udp sum ok] NTPv4, length 48    Client, Leap indicator:  (0), Stratum 3, poll 9s, precision -22    Root Delay: 0.030258, Root dispersion: 0.028045, Reference-ID: 10.143.33.50      Reference Timestamp:  3677375656.402239203 (2016/07/13 13:14:16)      Originator Timestamp: 3677375243.399526447 (2016/07/13 13:07:23)      Receive Timestamp:    3677375243.411563843 (2016/07/13 13:07:23)      Transmit Timestamp:   3677375772.388326644 (2016/07/13 13:16:12)        Originator - Receive Timestamp:  +0.012037376        Originator - Transmit Timestamp: +528.98880016813:16:12.412254 IP (tos 0x0, ttl  57, id 0, offset 0, flags [DF], proto: UDP (17), length: 76) 58.216.105.122 &gt; 69.172.201.153.123: [udp sum ok] NTPv4, length 48    Server, Leap indicator:  (0), Stratum 2, poll 9s, precision -24    Root Delay: 0.000106, Root dispersion: 0.002014, Reference-ID: 10.137.38.86      Reference Timestamp:  3677375704.335250139 (2016/07/13 13:15:04)      Originator Timestamp: 3677375772.388326644 (2016/07/13 13:16:12)      Receive Timestamp:    3677375772.400358736 (2016/07/13 13:16:12)      Transmit Timestamp:   3677375772.400375515 (2016/07/13 13:16:12)        Originator - Receive Timestamp:  +0.012032079        Originator - Transmit Timestamp: +0.012048868\n\n再次恢复虚拟机的TCO\n12345[root@houyiecsayZ14464544641Z ~]# ethtool -K eth1 tx onActual changes:rx-checksumming: ontx-checksumming: on\n\n可以看到抓包显示依然出现了UDP发送包的chksum错误\n12313:18:47.851459 IP (tos 0x0, ttl  64, id 48096, offset 0, flags [none], proto: UDP (17), length: 61) 69.172.201.153.51360 &gt; 8.8.8.8.53: [bad udp cksum 256e!]  14335+ A? www.sina.com.cn. (33)13:18:47.983450 IP (tos 0x0, ttl  42, id 47800, offset 0, flags [none], proto: UDP (17), length: 183) 8.8.8.8.53 &gt; 69.172.201.153.51360:  14335 q: A? www.sina.com.cn. 5/0/0 www.sina.com.cn. CNAME[|domain]\n\n注意：为了避免性能问题，完成测试后需要恢复开启TCO功能\n原理简述Too many incorrect checksum errors in TCPDUMP提供了明确的解释：\n\n\n\n\n\n\n\n\n\n这个incorrect checksum是一个称为TCP checksum offloading的功能，这个发出的TCP数据包的checksum字段在通过操作系统时是没有预先计算但被设置成0，然后留给通过NIC网卡处理器的时候进行校验计算。\n在Wireshark FAQ提供了对数据包分析工具角度说明：\n\n\n\n\n\n\n\n\n\n如果发送给运行Wireshrk服务器的数据包显示有TCP checksum错误，可能是因为使用的网卡配置了TCP checksum offloading功能。这意味着TCP checksum功能通过网卡添加到了数据包中，而不是通过操作系统的TCP&#x2F;IP堆栈来添加。当在网卡接口上进行数据包捕捉时，通过主机发送的数据包是直接由操作系统在数据包捕捉接口上处理的，意味着此时数据包还没有添加一个TCP checksum。\n解决的唯一方法是禁止TCP checksum offloading，但是在一些操作系统不允许这么操作，并且会导致明显的网络性能下降。然而，可以关闭Wireshark的TCP checksum error报告功能，这样它就不会在数据包有错误TCP checksum时拒绝执行TCP重组。设置方法是在WireShark的Perferences中，在Protocols列表中选择TCP ，然后关闭Check the validity of the TCP checksum when possible。也可以在命令行使用参数-o tcp.check_checksum:false\n原理详解大多数现代操作系统都支持网络卸载（network offloading）功能，即部分网络处理由网卡完成而不是由CPU处理。这样可以释放系统资源以便能够处理更多的连接。不过对于数据包捕捉分析会带来一些较为奇怪的结果或者丢失一些流量。\nChecksum Offloading\n在支持checksum offloading的系统中，IP,TCP和UDP checksum可以在传输到网线之前由网卡NIC来完成。此时在Wirshark中会提示数据包错误[incorrect, should be xxxx (maybe caused by &quot;TCP checksum offload&quot;?)].（tcpdump也有同样提示cksum xxxx incorrect）。抓包工具Wireshark&#x2F;Tcpdump是在数据包被发送给网卡之前捕捉数据包的，此时它不会看到正确的checksum，因为此时尚未进行计算（因为checksum已经卸载到网卡，此时这个checksum字段会被填写为0）。这也就导致了抓包工具提示checksum错误的原因。\n要通过抓包工具来观察数据包是否真的存在checksum错误，可以暂时关闭网卡的checksum offload功能，验证完成后再恢复网卡的checksum offload（关闭TCO会有性能问题）\nSegmentation Offloading\n以下图示显示了TCP&#x2F;IP堆栈没有卸载的时候数据包，假设应用数据包(Application data)是7300字节，此时TCP将把应用数据包切分成5个分片（segments），这是因为以太网的最大传输单元（Maximum Transmission Unit, MTU）是1500字节，此外我们还要减去20字节的IP头部以及20字节的TCP头部，实际能够用于数据传输的TCP分片只有1460字节(也就是TCP最大分片大小，TCP Maximum Segment Size, MSS)。\n\n当使用了网卡提供的segmentation offloading功能，操作系统就不会对应用数据（application data）进行分片，而是直接将一个大的TCP&#x2F;IP包发送给驱动程序。此时TCP和IP头部实际上是一个临时头部。驱动程序会创建一个单一的巨大的以太网帧，此时这个巨大的以太网帧会被包捕捉工具（如Wireshark）捕捉到，然后再发送给物理网卡。最后，网卡才执行提以太网帧分片，此时网卡会把使用这个临时头部来创建真实的TCP&#x2F;IP以太网头部并生成5个以太网帧，最后这5个以太网帧才发送到网络中。\n\n上述过程，如果启用了segmentation offloading，则抓包工具wireshark只能捕捉到一个巨大的以太网帧（包含7300字节数据）\n\nLinux操作系统\n\n检查checksum offload\n12ethtool --show-offload ethX\n\n关闭checksum offload\n12ethtool --offload ethX rx off tx off\n\n或者使用命令\n12ethtool -K ethX rx off tx off\n\n在When is full packet capture NOT full packet capture?中整理提供了非常好的说明\n\n\n\nNIC offload功能列表\n说明\n\n\n\ntso\ntcp-segmentation-offload TCP分片卸载\n\n\ngso\ngenteric-segmentation-offload 通用分片卸载\n\n\ngro\ngeneric-receive-offload 通用接收卸载\n\n\n例如使用ethtool -k eth0可能可以看到如下输出\n12345678910Offload parameters for eth0:rx-checksumming: ontx-checksumming: onscatter-gather: ontcp-segmentation-offload: onudp-fragmentation-offload: offgeneric-segmentation-offload: ongeneric-receive-offload: onlarge-receive-offload: off\n\n关闭offload设置的方法案例\n123456789ethtool -K eth0 rx offethtool -K eth0 tx offethtool -K eth0 sg offethtool -K eth0 tso offethtool -K eth0 ufo offethtool -K eth0 gso offethtool -K eth0 gro offethtool -K eth0 lro off\n\n\n\n\n\n\n\n\n\n\n注意，上述设置是flying的，不是持久化设置。要持久化设置，需要修改网卡配置，例如RedHat操作系统的/etc/sysconfig/network-scripts/ifcfg-eth0配置\nBroadcom 早期的tg3的驱动可能在启用tso的时候会导致数据包不正确 - Take care if using an older Broadcom tg3 driver + TSO enabled at your ESXi host，在生产环境中也遇到过tg3启用tso之后导致内存分配错误\n\nWindows操作系统\n\n在虚拟化环境中，Windows虚拟机启用TCP Offloading可能会存在一些问题，可以通过下面的方法来禁用Windows虚拟机TCP Offloading（物理服务器设置方法相同）\n在Windows操作系统的控制面板（Control Pannel）中选择**Network Settings &gt; Change Adapter Settings，然后鼠标右击每个网卡，选择Networking菜单的Configure，然后点击Advanced**面板，此时可以看到TCP offload相关设置（下图是Citrix adapter虚拟网卡）\n\n关闭TCP offload相关的以下选项，然后点击**ok**\n12345IPv4 Checksum OffloadLarge Receive OffloadLarge Send OffloadTCP Checksum Offload\n\n\nWindows操作系统Chimney Offload操作方法\n\n在Windows操作系统中检查tcp设置\n12netsh int tcp show global\n\n关闭chimney offload\n12netsh int tcp set global chimney=disabled\n\n开启chimney offload\n12netsh int tcp set global chimney=enabled\n\nChimney offloading是一种通过网卡NIC卸载处理TCP连接的技术，在启用了chimney offloading的Windows操作系统，可以通过命令\n12netstat -t\n\n观察到如下输出\n1234567Active Connections  Proto  Local Address          Foreign Address        State           Offload State  TCP    127.0.0.1:52613        computer_name:52614       ESTABLISHED     InHost  TCP    192.168.1.103:52614        computer_name:52613       ESTABLISHED     Offloaded\n\n其中，第二行标记了Offloaded就是表示这个连接已经被网卡的Chimney offloading卸载了，这样系统可以承受更多的TCP连接。\n\nUDP &#x2F; TCP Checksum errors from tcpdump &amp; NIC Hardware Offloading\nSegmentation and Checksum Offloading: Turning Off with ethtool\nWireShark: CaptureSetup&#x2F;Offloading\nWhen is full packet capture NOT full packet capture?\nDisable TCP Offloading in Windows Server 2012\nInformation about the TCP Chimney Offload, Receive Side Scaling, and Network Direct Memory Access features in Windows Server 2008 微软官方文档，提供了TCP Chimney Offload（也就是TCP连接的卸载功能）以及接收滑动窗口，网络直接内存访问的相关设置，对于Windows Server 2008操作系统的参考非常详尽\n\n","slug":"LINUX/tcpdump在启用TCP卸载功能的网卡上报告 checksum error ·","date":"2023-06-08T12:39:56.000Z","categories_index":"TCP,LINUX","tags_index":"checksum,offloading,offload","author_index":"dandeliono"},{"id":"56ab2e24b6f8563e1b3aef14738d978d","title":"MySql Lock wait timeout exceeded","content":"MySql Lock wait timeout exceededMysql造成锁的情况有很多，下面我们就列举一些情况：\n\n执行DML操作没有commit，再执行删除操作就会锁表。\n在同一事务内先后对同一条数据进行插入和更新操作。\n表索引设计不当，导致数据库出现死锁。\n长事物，阻塞DDL，继而阻塞所有同表的后续操作。\n\n但是要区分的是Lock wait timeout exceeded与Dead Lock是不一样。\n\nLock wait timeout exceeded：后提交的事务等待前面处理的事务释放锁，但是在等待的时候超过了mysql的锁等待时间，就会引发这个异常。\nDead Lock：两个事务互相等待对方释放相同资源的锁，从而造成的死循环，就会引发这个异常。\n\n还有一个要注意的是innodb_lock_wait_timeout与lock_wait_timeout也是不一样的。\n\ninnodb_lock_wait_timeout：innodb的dml操作的行级锁的等待时间\nlock_wait_timeout：数据结构ddl操作的锁的等待时间\n\n如何查看innodb_lock_wait_timeout的具体值？\n12SHOW VARIABLES LIKE &#x27;innodb_lock_wait_timeout&#x27;\n\n如何修改innode lock wait timeout的值？\n参数修改的范围有Session和Global，并且支持动态修改，可以有两种方法修改：\n方法一：\n通过下面语句修改\n123set innodb_lock_wait_timeout=100;set global innodb_lock_wait_timeout=100;\n\nps. 注意global的修改对当前线程是不生效的，只有建立新的连接才生效。\n方法二：\n修改参数文件/etc/my.cnf innodb_lock_wait_timeout = 50\nps. innodb_lock_wait_timeout指的是事务等待获取资源等待的最长时间，超过这个时间还未分配到资源则会返回应用失败； 当锁等待超过设置时间的时候，就会报如下的错误；ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction。其参数的时间单位是秒，最小可设置为1s(一般不会设置得这么小)，最大可设置1073741824秒，默认安装时这个值是50s(默认参数设置)。\n下面介绍在遇到这类问题该如何处理\n\n数据更新或新增后数据经常自动回滚。\n\n表操作总报 Lock wait timeout exceeded 并长时间无反应\n\n应急方法：show full processlist; kill掉出现问题的进程。 ps.有的时候通过processlist是看不出哪里有锁等待的，当两个事务都在commit阶段是无法体现在processlist上\n\n根治方法：select * from innodb_trx;查看有是哪些事务占据了表资源。 ps.通过这个办法就需要对innodb有一些了解才好处理\n\n\n说起来很简单找到它杀掉它就搞定了，但是实际上并没有想象的这么简单，当问题出现要分析问题的原因，通过原因定位业务代码可能某些地方实现的有问题，从而来避免今后遇到同样的问题。\nMysql的InnoDB存储引擎是支持事务的，事务开启后没有被主动Commit。导致该资源被长期占用，其他事务在抢占该资源时，因上一个事务的锁而导致抢占失败！因此出现 Lock wait timeout exceeded\n下面几张表是innodb的事务和锁的信息表，理解这些表就能很好的定位问题。\ninnodb_trx ## 当前运行的所有事务 innodb_locks ## 当前出现的锁 innodb_lock_waits ## 锁等待的对应关系\n下面对 innodb_trx 表的每个字段进行解释：\n1234567891011121314151617181920212223trx_id：事务ID。trx_state：事务状态，有以下几种状态：RUNNING、LOCK WAIT、ROLLING BACK 和 COMMITTING。trx_started：事务开始时间。trx_requested_lock_id：事务当前正在等待锁的标识，可以和 INNODB_LOCKS 表 JOIN 以得到更多详细信息。trx_wait_started：事务开始等待的时间。trx_weight：事务的权重。trx_mysql_thread_id：事务线程 ID，可以和 PROCESSLIST 表 JOIN。trx_query：事务正在执行的 SQL 语句。trx_operation_state：事务当前操作状态。trx_tables_in_use：当前事务执行的 SQL 中使用的表的个数。trx_tables_locked：当前执行 SQL 的行锁数量。trx_lock_structs：事务保留的锁数量。trx_lock_memory_bytes：事务锁住的内存大小，单位为 BYTES。trx_rows_locked：事务锁住的记录数。包含标记为 DELETED，并且已经保存到磁盘但对事务不可见的行。trx_rows_modified：事务更改的行数。trx_concurrency_tickets：事务并发票数。trx_isolation_level：当前事务的隔离级别。trx_unique_checks：是否打开唯一性检查的标识。trx_foreign_key_checks：是否打开外键检查的标识。trx_last_foreign_key_error：最后一次的外键错误信息。trx_adaptive_hash_latched：自适应散列索引是否被当前事务锁住的标识。trx_adaptive_hash_timeout：是否立刻放弃为自适应散列索引搜索 LATCH 的标识。\n\n下面对 innodb_locks 表的每个字段进行解释：\n1234567891011lock_id：锁 ID。lock_trx_id：拥有锁的事务 ID。可以和 INNODB_TRX 表 JOIN 得到事务的详细信息。lock_mode：锁的模式。有如下锁类型：行级锁包括：S、X、IS、IX，分别代表：共享锁、排它锁、意向共享锁、意向排它锁。表级锁包括：S_GAP、X_GAP、IS_GAP、IX_GAP 和 AUTO_INC，分别代表共享间隙锁、排它间隙锁、意向共享间隙锁、意向排它间隙锁和自动递增锁。lock_type：锁的类型。RECORD 代表行级锁，TABLE 代表表级锁。lock_table：被锁定的或者包含锁定记录的表的名称。lock_index：当 LOCK_TYPE=’RECORD’ 时，表示索引的名称；否则为 NULL。lock_space：当 LOCK_TYPE=’RECORD’ 时，表示锁定行的表空间 ID；否则为 NULL。lock_page：当 LOCK_TYPE=’RECORD’ 时，表示锁定行的页号；否则为 NULL。lock_rec：当 LOCK_TYPE=’RECORD’ 时，表示一堆页面中锁定行的数量，亦即被锁定的记录号；否则为 NULL。lock_data：当 LOCK_TYPE=’RECORD’ 时，表示锁定行的主键；否则为NULL。\n\n下面对 innodb_lock_waits 表的每个字段进行解释：\n12345requesting_trx_id：请求事务的 ID。requested_lock_id：事务所等待的锁定的 ID。可以和 INNODB_LOCKS 表 JOIN。blocking_trx_id：阻塞事务的 ID。blocking_lock_id：某一事务的锁的 ID，该事务阻塞了另一事务的运行。可以和 INNODB_LOCKS 表 JOIN。\n\n\n直接查看 innodb_lock_waits 表\n\n12SELECT * FROM innodb_lock_waits;\n\n\ninnodb_locks 表和 innodb_lock_waits 表结合：\n\n12SELECT * FROM innodb_locks WHERE lock_trx_id IN (SELECT blocking_trx_id FROM innodb_lock_waits);\n\n\ninnodb_locks 表 JOIN innodb_lock_waits 表:\n\n12SELECT innodb_locks.* FROM innodb_locks JOIN innodb_lock_waits ON (innodb_locks.lock_trx_id = innodb_lock_waits.blocking_trx_id);\n\n\n查询 innodb_trx 表:\n\n12SELECT trx_id, trx_requested_lock_id, trx_mysql_thread_id, trx_query FROM innodb_trx WHERE trx_state = &#x27;LOCK WAIT&#x27;;\n\n\ntrx_mysql_thread_id 即kill掉事务线程 ID\n\n123SHOW ENGINE INNODB STATUS ;SHOW PROCESSLIST ;\n\n从上述方法中得到了相关信息，我们可以得到发生锁等待的线程 ID，然后将其 KILL 掉。 KILL 掉发生锁等待的线程。\n12kill ID;\n","slug":"MIDDLEWARE/MySql Lock wait timeout exceeded","date":"2023-05-17T15:04:27.000Z","categories_index":"innodb,MIDDLEWARE","tags_index":"wait,timeout,lock","author_index":"dandeliono"},{"id":"923091e5afde124d5b66314a0b825fb8","title":"Shiro中多Realm下抛出异常不准确","content":"Shiro中多Realm下抛出异常不准确如果你的项目中存在多个Realm，当你在Realm中判断账号异常抛出了一个UnknownAccountException异常时，到达FormAuthenticationFilter.onLoginFailure方法中的异常信息为：\n1Authentication token of type [class org.apache.shiro.authc.UsernamePasswordToken] could not be authenticated by any configured realms.  Please ensure that at least one realm can authenticate these tokens.\n\n查看**ModularRealmAuthenticator.doMultiRealmAuthentication方法源码：\n123456789101112131415161718192021222324252627282930313233343536373839404142protected AuthenticationInfo doMultiRealmAuthentication(Collection&lt;Realm&gt; realms, AuthenticationToken token) &#123;        AuthenticationStrategy strategy = getAuthenticationStrategy();        AuthenticationInfo aggregate = strategy.beforeAllAttempts(realms, token);        if (log.isTraceEnabled()) &#123;            log.trace(&quot;Iterating through &#123;&#125; realms for PAM authentication&quot;, realms.size());        &#125;        for (Realm realm : realms) &#123;            aggregate = strategy.beforeAttempt(realm, token, aggregate);            if (realm.supports(token)) &#123;                log.trace(&quot;Attempting to authenticate token [&#123;&#125;] using realm [&#123;&#125;]&quot;, token, realm);                AuthenticationInfo info = null;                Throwable t = null;                try &#123;                    info = realm.getAuthenticationInfo(token);                &#125; catch (Throwable throwable) &#123;                    t = throwable;                    if (log.isDebugEnabled()) &#123;                        String msg = &quot;Realm [&quot; + realm + &quot;] threw an exception during a multi-realm authentication attempt:&quot;;                        log.debug(msg, t);                    &#125;                &#125;                aggregate = strategy.afterAttempt(realm, token, info, aggregate, t);            &#125; else &#123;                log.debug(&quot;Realm [&#123;&#125;] does not support token &#123;&#125;.  Skipping realm.&quot;, realm, token);            &#125;        &#125;        aggregate = strategy.afterAllAttempts(token, aggregate);        return aggregate;    &#125;\n\n可见，代码并没有保存并抛出某个Realm具体抛出的异常，而是在遍历完Realm后基于策略抛出了一个AuthenticationException异常。所以要解决这个问题，只需要重构doMultiRealmAuthentication方法，以下为我重构的代码：\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657@Override    protected AuthenticationInfo doMultiRealmAuthentication(Collection&lt;Realm&gt; realms, AuthenticationToken token) &#123;        AuthenticationStrategy strategy = getAuthenticationStrategy();        AuthenticationInfo aggregate = strategy.beforeAllAttempts(realms, token);        if (log.isTraceEnabled()) &#123;            log.trace(&quot;Iterating through &#123;&#125; realms for PAM authentication&quot;, realms.size());        &#125;        AuthenticationException authException = null;        for (Realm realm : realms) &#123;            aggregate = strategy.beforeAttempt(realm, token, aggregate);            if (realm.supports(token)) &#123;                log.trace(&quot;Attempting to authenticate token [&#123;&#125;] using realm [&#123;&#125;]&quot;, token, realm);                AuthenticationInfo info = null;                try &#123;                    info = realm.getAuthenticationInfo(token);                &#125; catch (Throwable throwable) &#123;                                        if (throwable instanceof AuthenticationException) &#123;                        authException = (AuthenticationException) throwable;                    &#125; else &#123;                        authException = new AuthenticationException(&quot;账号登录异常&quot;, throwable);                    &#125;                    if (log.isDebugEnabled()) &#123;                        String msg = &quot;Realm [&quot; + realm + &quot;] threw an exception during a multi-realm authentication attempt:&quot;;                        log.debug(msg, throwable);                    &#125;                &#125;                aggregate = strategy.afterAttempt(realm, token, info, aggregate, authException);            &#125; else &#123;                log.debug(&quot;Realm [&#123;&#125;] does not support token &#123;&#125;.  Skipping realm.&quot;, realm, token);            &#125;        &#125;                if (authException != null) &#123;            throw authException;        &#125;        aggregate = strategy.afterAllAttempts(token, aggregate);        return aggregate;    &#125;\n\n该重构基于只要一个realm验证成功就算成功，并且每个realm支持的token类型不一样，否则异常会出现覆盖情况，读者根据自己的需求修改，并且最后注入到WebSecurityManager注入重构的类：\n12securityManager.setAuthenticator(new MultiRealmAuthenticator());\n\n根据异常更准确的返回信息，可以重构FormAuthenticationFilter.onLoginFailure方法，该方法会在登录异常后执行，例如以下为我的登录异常处理：\n12345678910111213141516171819202122232425262728293031private final static Map&lt;Class, String&gt; exceptionMessageMap;    static &#123;        exceptionMessageMap = new HashMap&lt;&gt;();        exceptionMessageMap.put(IncorrectCredentialsException.class, &quot;账号密码不正确&quot;);        exceptionMessageMap.put(ExpiredCredentialsException.class, &quot;账号密码过期&quot;);        exceptionMessageMap.put(CredentialsException.class, &quot;账号密码异常&quot;);        exceptionMessageMap.put(ConcurrentAccessException.class, &quot;无法同时多个用户登录&quot;);        exceptionMessageMap.put(UnknownAccountException.class, &quot;账号不存在&quot;);        exceptionMessageMap.put(ExcessiveAttemptsException.class, &quot;账号验证次数超过限制&quot;);        exceptionMessageMap.put(LockedAccountException.class, &quot;账号被锁定&quot;);        exceptionMessageMap.put(DisabledAccountException.class, &quot;账号被禁用&quot;);        exceptionMessageMap.put(AccountException.class, &quot;账号异常&quot;);        exceptionMessageMap.put(UnsupportedTokenException.class, &quot;不支持当前TOKEN&quot;);    &#125;    protected boolean onLoginFailure(AuthenticationToken token, AuthenticationException e, ServletRequest request, ServletResponse response) &#123;        String errorMsg = exceptionMessageMap.get(e.getClass());        if (errorMsg == null) &#123;            errorMsg = e.getMessage();        &#125;        if (WebUtil.isAjaxRequest((HttpServletRequest) request)) &#123;            WebUtil.sendJsonByCors((HttpServletResponse) response, R.fail(HttpCode.UNAUTHORIZED, errorMsg));            return false;        &#125; else &#123;            setFailureAttribute(request, e);            request.setAttribute(ERROR_KEY_LOGIN_FAIL_MESSAGE, errorMsg);            return true;        &#125;    &#125;\n","slug":"JAVA/Shiro中多Realm下抛出异常不准确","date":"2023-05-12T15:13:14.000Z","categories_index":"Realm,JAVA","tags_index":"FormAuthenticationFilter,onLoginFailure,doMultiRealmAuthentication","author_index":"dandeliono"},{"id":"38926a48bdd2f6821bf4e05b53cee11b","title":"flink cep，使用groovy脚本及表达式求值","content":"flink cep，使用groovy脚本及表达式求值使用表达式求值和groovy脚本来生成flink cep Pattern。减少代码量，一份代码可通过传script脚本的方式生成不同的pattern。1.source\n12SingleOutputStreamOperator&lt;MetricEvent&gt; metricEvent = dataStreamSource               .flatMap(new ParseMetricEventFunction()).returns(MetricEvent.class); \n\n2.pattern\n123456789101112131415Pattern p1 = ScriptEngine.getPattern(                &quot;  import org.apache.flink.cep.nfa.aftermatch.AfterMatchSkipStrategy\\n&quot; +                        &quot;import org.apache.flink.cep.pattern.Pattern\\n&quot; +                        &quot;import test.cep.function.AviatorCondition \\n&quot; +                        &quot;where1 = new AviatorCondition(&quot; +                        &quot;   \\&quot;getT(tags,\\\\\\&quot;cluster_name\\\\\\&quot;)==\\\\\\&quot;terminus-x\\\\\\&quot;&amp;&amp;getF(fields,\\\\\\&quot;load5\\\\\\&quot;)&gt;15 \\&quot;&quot; +                        &quot;        )\\n&quot; +                        &quot;def get()&#123; &quot; +                        &quot;      return Pattern.begin(\\&quot;start\\&quot;, AfterMatchSkipStrategy.noSkip())\\n&quot; +                        &quot;        .where(where1)&quot; +                        &quot;&#125;&quot;,                &quot;get&quot;); \n\n3.result\n12345678910111213PatternStream pStream2 = CEP.pattern(metricEvent.keyBy(metricEvent1 -&gt; metricEvent1.getName() + Joiner.on(&quot;,&quot;).join(metricEvent1.getTags().values())), p);       SingleOutputStreamOperator filter2 = pStream2.select(new PatternSelectFunction&lt;MetricEvent, String&gt;() &#123;           @Override           public String select(Map&lt;String, List&lt;MetricEvent&gt;&gt; pattern) throws Exception &#123;               return &quot;-----------------------------&quot; + pattern.toString();           &#125;       &#125;);       filter2.print();       StateBackend memory = new MemoryStateBackend(10 * 5 * 1024 * 1024, true);       env.setStateBackend(memory);       env.execute(&quot;----flink cep alert ----&quot;); \n\n4.model\n1234567891011@Datapublic class MetricEvent &#123;    private String name;    private long timestamp;    private Map&lt;String, Object&gt; fields = new HashMap&lt;&gt;();    private Map&lt;String, String&gt; tags = new HashMap&lt;&gt;();&#125; \n\n5.ScriptEngine\n1234567891011121314151617181920import org.apache.flink.cep.pattern.Pattern;import javax.script.Bindings;import javax.script.Invocable;import javax.script.ScriptEngineManager;import javax.script.ScriptException;import java.util.Date;public class ScriptEngine &#123;   public static Pattern getPattern(String text,String name) throws ScriptException, NoSuchMethodException &#123;       ScriptEngineManager factory = new ScriptEngineManager();       javax.script.ScriptEngine engine =  factory.getEngineByName(&quot;groovy&quot;);       System.out.println(engine.toString());       assert engine != null;       engine.eval(text);       Pattern pattern = (Pattern)((Invocable)engine).invokeFunction(name);       return pattern;   &#125;&#125; \n\n6.表达式相关GetTagMapFunction\n123456789101112public class GetTagMapFunction extends AbstractFunction &#123;    @Override    public String getName() &#123;        return &quot;getF&quot;;    &#125;    @Override    public AviatorDouble call(Map&lt;String, Object&gt; env, AviatorObject args1, AviatorObject args2) &#123;        Map&lt;String, Object&gt; map = (Map&lt;String, Object&gt;) FunctionUtils.getJavaObject(args1, env);        String field = FunctionUtils.getStringValue(args2, env);        return new AviatorDouble((Double) map.get(field));    &#125;&#125; \n\nGetFieldMapFunction\n1234567891011121314public class GetFieldMapFunction extends AbstractFunction &#123;    @Override    public String getName() &#123;        return &quot;getT&quot;;    &#125;    @Override    public AviatorString call(Map&lt;String, Object&gt; env, AviatorObject args1, AviatorObject args2) &#123;        Map&lt;String, String&gt; map = (Map&lt;String, String&gt;) FunctionUtils.getJavaObject(args1, env);        String field = FunctionUtils.getStringValue(args2, env);        return new AviatorString(map.get(field));    &#125;&#125; \n\n7.表达式Condition,注册表达式\n12345678910111213141516171819202122232425public class AviatorCondition extends SimpleCondition&lt;MetricEvent&gt; implements Serializable &#123;    private static Logger logger = LoggerFactory.getLogger(AviatorCondition.class);    private String script;    static &#123;        AviatorEvaluator.addFunction(new GetFieldMapFunction());        AviatorEvaluator.addFunction(new GetTagMapFunction());    &#125;    public AviatorCondition(String script) &#123;        this.script = script;    &#125;    @Override    public boolean filter(MetricEvent event) throws Exception &#123;        Map&lt;String, Object&gt; env = new HashMap&lt;String, Object&gt;();        env.put(&quot;event&quot;, event);        env.put(&quot;fields&quot;, event.getFields());        env.put(&quot;tags&quot;, event.getTags());        Boolean result = false;        try &#123;            result = (Boolean) AviatorEvaluator.execute(script, env);        &#125; catch (Exception e) &#123;            logger.error(&quot;execute script with event error,script:&#123;&#125;,event:&#123;&#125;,error;&#123;&#125;&quot;, script, event, e);        &#125;        return result;    &#125;&#125; \n\n8.依赖\n123456789101112131415161718192021222324252627282930313233343536373839404142&lt;dependency&gt;           &lt;groupId&gt;com.googlecode.aviator&lt;/groupId&gt;           &lt;artifactId&gt;aviator&lt;/artifactId&gt;           &lt;version&gt;2.3.3&lt;/version&gt;       &lt;/dependency&gt;       &lt;dependency&gt;           &lt;groupId&gt;org.codehaus.groovy&lt;/groupId&gt;           &lt;artifactId&gt;groovy&lt;/artifactId&gt;           &lt;version&gt;2.4.7&lt;/version&gt;       &lt;/dependency&gt;       &lt;dependency&gt;           &lt;groupId&gt;org.codehaus.groovy&lt;/groupId&gt;           &lt;artifactId&gt;groovy-jsr223&lt;/artifactId&gt;           &lt;version&gt;2.4.7&lt;/version&gt;       &lt;/dependency&gt;       &lt;dependency&gt;           &lt;groupId&gt;org.codehaus.groovy&lt;/groupId&gt;           &lt;artifactId&gt;groovy&lt;/artifactId&gt;           &lt;version&gt;2.4.7&lt;/version&gt;       &lt;/dependency&gt;             &lt;dependency&gt;           &lt;groupId&gt;com.alibaba&lt;/groupId&gt;           &lt;artifactId&gt;fastjson&lt;/artifactId&gt;           &lt;version&gt;1.2.51&lt;/version&gt;       &lt;/dependency&gt;       &lt;dependency&gt;           &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;           &lt;artifactId&gt;flink-cep-scala_2.11&lt;/artifactId&gt;           &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;       &lt;/dependency&gt;       &lt;dependency&gt;           &lt;groupId&gt;mysql&lt;/groupId&gt;           &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;           &lt;version&gt;5.1.34&lt;/version&gt;       &lt;/dependency&gt;       &lt;dependency&gt;           &lt;groupId&gt;com.google.guava&lt;/groupId&gt;           &lt;artifactId&gt;guava&lt;/artifactId&gt;           &lt;version&gt;26.0-jre&lt;/version&gt;       &lt;/dependency&gt; \n","slug":"BIGDATA/flink cep，使用groovy脚本及表达式求值","date":"2023-05-10T17:02:17.000Z","categories_index":"flink,BIGDATA","tags_index":"cep,groovy,pattern","author_index":"dandeliono"},{"id":"f2f3d8a736517daef67fee74c45d8b64","title":"Apache HttpClient使用不当导致的请求超时问题排查","content":"Apache HttpClient使用不当导致的请求超时问题排查Apache HttpClient使用不当导致的请求超时问题排查\n近期负责的线上应用出现调用失败的问题，排查后发现是HttpClient相关的问题，特此记录一下。\n因为涉及线上数据安全隐私，模拟生产的应用写了demo，模拟一定的并发请求复现该问题。\n1 问题介绍收到线上告警通知：外部服务调用某应用A大量报错，平均响应时常很长，经常有请求超时的情况。\n1 初步排查1.1 查连接情况外部请求是http请求，所以可以查一下连接的情况。对应 java 进程的连接数：\n具体的连接情况：\n\n123[root@sky ~]400\n\n应用的 8080 端口有400个连接。\n1.2 查线程情况平均响应时常比较长，一般是线程长时间在等待某个步骤完成。查一下应用的线程执行情况：\n执行命令 jstack 8433 | less\n\njstack的日志中有很多类似上面图中的线程，处于 TIME_WAITING状态，而下面的堆栈中，HttpClientUtils.httpGet()是使用 Apache HttpClient 工具封装的 http 请求函数，PoolHttpClientConnectionManager.leaseConnection() 看上去是在等待获取连接。大概分析得出：有很多线程在执行过程中，等待HttpClient获取连接。\n1.3 查应用异常日志查一下应用的日志，寻找线索，在日志中看到许多下面的异常日志：\n1234567891011121314151617182022-07-24 21:47:44.765 ERROR 82381 --- [pool-2-thread-7] com.skyme.TestController                 : error:org.apache.http.conn.ConnectionPoolTimeoutException: Timeout waiting for connection from pool\tat org.apache.http.impl.conn.PoolingHttpClientConnectionManager.leaseConnection(PoolingHttpClientConnectionManager.java:286) ~[httpclient-4.5.2.jar:4.5.2]\tat org.apache.http.impl.conn.PoolingHttpClientConnectionManager$1.get(PoolingHttpClientConnectionManager.java:263) ~[httpclient-4.5.2.jar:4.5.2]\tat org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:190) ~[httpclient-4.5.2.jar:4.5.2]\tat org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:184) ~[httpclient-4.5.2.jar:4.5.2]\tat org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:88) ~[httpclient-4.5.2.jar:4.5.2]\tat org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:110) ~[httpclient-4.5.2.jar:4.5.2]\tat org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:184) ~[httpclient-4.5.2.jar:4.5.2]\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82) ~[httpclient-4.5.2.jar:4.5.2]\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:107) ~[httpclient-4.5.2.jar:4.5.2]\tat com.skyme.HttpClientUtils.httpGet(HttpClientUtils.java:31) ~[classes/:na]\tat com.skyme.TestController.lambda$batchHttpCall$0(TestController.java:47) ~[classes/:na]\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_73]\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ~[na:1.8.0_73]\tat java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_73]\n\n1.4 初步结论外部有大量请求，很多请求的处理线程在等待 HttpClient 获取连接，问题应该在 HttpClient 。\n2 进一步排查2.1 分析 HttpClient日志里两个关键信息：\n1.是HttpClient内部报错\n123org.apache.http.conn.ConnectionPoolTimeoutException: Timeout waiting for connection from pool\tat org.apache.http.impl.conn.PoolingHttpClientConnectionManager.leaseConnection(PoolingHttpClientConnectionManager.java:286) ~[httpclient-4.5.2.jar:4.5.2]\n\n对应的源码org.apache.http.impl.conn.PoolingHttpClientConnectionManager.leaseConnection(PoolingHttpClientConnectionManager.java:286)：\n123456789101112131415161718192021protected HttpClientConnection leaseConnection(        final Future&lt;CPoolEntry&gt; future,        final long timeout,        final TimeUnit tunit) throws InterruptedException, ExecutionException, ConnectionPoolTimeoutException &#123;    final CPoolEntry entry;    try &#123;        entry = future.get(timeout, tunit);         if (entry == null || future.isCancelled()) &#123;            throw new InterruptedException();        &#125;        Asserts.check(entry.getConnection() != null, &quot;Pool entry with no connection&quot;);        if (this.log.isDebugEnabled()) &#123;            this.log.debug(&quot;Connection leased: &quot; + format(entry) + formatStats(entry.getRoute()));        &#125;        return CPoolProxy.newProxy(entry);    &#125; catch (final TimeoutException ex) &#123;                throw new ConnectionPoolTimeoutException(&quot;Timeout waiting for connection from pool&quot;);    &#125;&#125;    \n\n抛出ConnectionPoolTimeoutException是因为触发了TimeoutException异常，而TimeoutException异常对应于entry = future.get(timeout, tunit);\nPoolingHttpClientConnectionManager.leaseConnection 是为了获取连接，分析一下其执行流程，摘录一下网上博客（segmentfault.com&#x2F;a&#x2F;119000001… 获取connection时序图：\n\n重点的步骤：\n1.如果route下有空闲连接，则直接返回空闲连接\n2.如果没有空闲连接，且该connPerRoute及池中conn数均没有达到最大值，的conn数没有达到最大值，则创建连接，并放入池中，并返回该连接\n3.如果没有空闲连接，且达到了maxRoute或maxPoolsize，则阻塞等待，等待的时常，便是entry = future.get(timeout, tunit);中的 timeout 时长。如果指定的时间没有 lease 到 connection ，则 entry = future.get(timeout, tunit); 会抛出 TimeoutException 异常。\n一般 pool 设计都是和上面的类似，我们继续分析一下 httpclient pool 的结构 ：\n图片摘录自 (www.pudn.com/news/628f83…) \nPoolEntry&lt;HttpRoute, OperatedClientConnection&gt;: 路由和连接的对应关系。\nrouteToPool: 可以多个（图中仅示例两个），图中各队列大小动态变化，并不相等。\nmaxTotal: 限制的是外层 httpConnPool 中 leased 集合和 available 队列的总和的大小，httpConnPool 的 leased 和 available 的大小没有单独限制。\nmaxPerRoute: 限制的是每个 routeToPool 中 leased 集合和 available 队列的总和的大小。\n一步步跟踪源码entry = future.get(timeout, tunit);，定位到核心调用处 org.apache.http.pool.AbstractConnPool#getPoolEntryBlocking() ，对应源码\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127private E getPoolEntryBlocking(        final T route, final Object state,        final long timeout, final TimeUnit tunit,        final PoolEntryFuture&lt;E&gt; future)            throws IOException, InterruptedException, TimeoutException &#123;    Date deadline = null;     if (timeout &gt; 0) &#123;         deadline = new Date            (System.currentTimeMillis() + tunit.toMillis(timeout));    &#125;        this.lock.lock();    try &#123;                final RouteSpecificPool&lt;T, C, E&gt; pool = getPool(route);        E entry = null;        while (entry == null) &#123;            Asserts.check(!this.isShutDown, &quot;Connection pool shut down&quot;);            for (;;) &#123;                entry = pool.getFree(state);                if (entry == null) &#123;                    break;                &#125;                                if (entry.isExpired(System.currentTimeMillis())) &#123;                    entry.close();                &#125; else if (this.validateAfterInactivity &gt; 0) &#123;                    if (entry.getUpdated() + this.validateAfterInactivity &lt;= System.currentTimeMillis()) &#123;                        if (!validate(entry)) &#123;                            entry.close();                        &#125;                    &#125;                &#125;                if (entry.isClosed()) &#123;                    this.available.remove(entry);                    pool.free(entry, false);                &#125; else &#123;                    break;                &#125;            &#125;                        if (entry != null) &#123;                this.available.remove(entry);                this.leased.add(entry);                onReuse(entry);                return entry;            &#125;                        final int maxPerRoute = getMax(route);                                                final int excess = Math.max(0, pool.getAllocatedCount() + 1 - maxPerRoute);            if (excess &gt; 0) &#123;                for (int i = 0; i &lt; excess; i++) &#123;                    final E lastUsed = pool.getLastUsed();                    if (lastUsed == null) &#123;                        break;                    &#125;                    lastUsed.close();                    this.available.remove(lastUsed);                    pool.remove(lastUsed);                &#125;            &#125;                        if (pool.getAllocatedCount() &lt; maxPerRoute) &#123;                final int totalUsed = this.leased.size();                                final int freeCapacity = Math.max(this.maxTotal - totalUsed, 0);                if (freeCapacity &gt; 0) &#123;                    final int totalAvailable = this.available.size();                                        if (totalAvailable &gt; freeCapacity - 1) &#123;                        if (!this.available.isEmpty()) &#123;                            final E lastUsed = this.available.removeLast();                            lastUsed.close();                            final RouteSpecificPool&lt;T, C, E&gt; otherpool = getPool(lastUsed.getRoute());                            otherpool.remove(lastUsed);                        &#125;                    &#125;                                        final C conn = this.connFactory.create(route);                                        entry = pool.add(conn);                                        this.leased.add(entry);                                        return entry;                &#125;            &#125;                        boolean success = false;            try &#123;                                pool.queue(future);                                this.pending.add(future);                                success = future.await(deadline);            &#125; finally &#123;                                                                                                pool.unqueue(future);                this.pending.remove(future);            &#125;                                    if (!success &amp;&amp; (deadline != null) &amp;&amp;                (deadline.getTime() &lt;= System.currentTimeMillis())) &#123;                break;            &#125;        &#125;                throw new TimeoutException(&quot;Timeout waiting for connection&quot;);    &#125; finally &#123;                this.lock.unlock();    &#125;&#125;\n\n综合分析：\n\n通过线程池获取连接要通过 ReetrantLock 加锁，保证线程安全\n不论是大连接池还是小连接池，当超过数量的时候，都要通过LRU释放一些连接\n如果拿到了可用连接，则返回给上层使用\n如果没有拿到可用连接，HttpClient 会判断当前 route 连接池是否已经超过了最大数量，没有到上限就会新建一个连接，并放入池中\n如果到达了上限，就排队等待，等到了信号量，就重新获得一次，等待不到就抛 TimeoutException\n\n思考池类的设计，不管是连接池，线程池，一般不够用的场景，都是最大数设置的不够大，不足以应对并发获取连接、线程等？\n2.2 确认HttpClient 实际情况上面的都是猜想，最终问题定位还是要摸清实际情况。\n应用里是如何创建 HttpClient 实例下面是应用中创建 HttpClient 实例的地方，我们从变量 HTTP_CLIENT开始定位。\n123456789private static final CloseableHttpClient HTTP_CLIENT = HttpClients        .custom()        .setDefaultRequestConfig(                RequestConfig.custom()                .setCookieSpec(CookieSpecs.IGNORE_COOKIES)                .setConnectionRequestTimeout(30000)                .build()        ).build();\n\n上面代码可知：\n1.没有设置 maxPerRoute、maxTotal这类的参数。\n2.从连接池获取连接的超时时间的参数 ConnectionRequestTimeout ，设置为30秒。\narthas 定位实际数据因为不是本地环境，不能方便地 Debug，这时可以用 arthas 来查进行内存中 HttpClient 的实际数据。\n在arthas中执行：getstatic com.skyme.HttpClientUtils HTTP_CLIENT -x 4，结果如下：\n\nrouteToPool中，route [https&#x2F;&#x2F;github.com] 的使用情况是：leased为2，avaibable为0，pending为198。\nleased + avaibable 结果为2，怎么这么少？难道是 maxPerRoute 参数设置的过小？\n在arthas结果中我们找到了 maxPerRoute 相关的配置，maxPerRoute 默认值只有2，因为我们创建 HttpClient 实例时，没有设置maxPerRoute的数值，所以每个route 的 maxPerRoute 为2， 这和上面的 leased + avaibable 的结果对应上了。此外，还有一个重要参数 maxToal 为20。\n\n大致结论：默认参数太小到这里，问题基本定位清楚了。因为应用没有显示地设置 maxPerRoute 和 maxToal ，所以取了默认值，默认值比较小，maxPerRoute 为2， maxToal 为20， 导致并发获取连接时，连接不够用，导致了获取连接的线程一直在等待，等待时间为我们创建 HttpClien 实例时自定义的 30 秒，等待时间过长，导致了外部请求超时。\n1 临时办法可以使用 arthas 的 vmtool 命令将内存中的 defaultMaxPerRoute 、maxTotal 参数修改大一些，该操作过于风骚，一般不符合生产环境的运维操作规范。但是测试环境或者紧急情况可以尝试使用。\n1234[arthas@8433]$ vmtool --action getInstances --className org.apache.http.impl.client.CloseableHttpClient --express &#x27;instances[0].connManager.pool.defaultMaxPerRoute=100&#x27;@Integer[100][arthas@8433]$ vmtool --action getInstances --className org.apache.http.impl.client.CloseableHttpClient --express &#x27;instances[0].connManager.pool.maxTotal=500&#x27;\n\n修改后，连接池中 leased 明显增多，pending 减少。\n\n一段时间后，pending为0，说明没有阻塞，连接数已足够使用。\n\n注意：\n1.该修改不需要应用重启，但应用重启后，内存的设置将销毁，依然还会存在问题。\n2.非常规操作，谨慎执行。\n3.vmtool 命令在 arthas 3.5.1 版本之后才有。\n2 长期办法修改代码，配置合理的最大连接数maxPerRoute、maxTotal、ConnectionRequestTimeout。\n当前 HttpClient 版本为 4.5.2，其他版本可能代码有所差异。\n123456789101112131415161718192021private final static PoolingHttpClientConnectionManager POOLING_HTTP_CLIENT_CONNECTION_MANAGER        = new PoolingHttpClientConnectionManager();static &#123;        POOLING_HTTP_CLIENT_CONNECTION_MANAGER.setDefaultMaxPerRoute(100);        POOLING_HTTP_CLIENT_CONNECTION_MANAGER.setMaxTotal(500);&#125;private static final CloseableHttpClient HTTP_CLIENT = HttpClients    .custom()    .setConnectionManager(POOLING_HTTP_CLIENT_CONNECTION_MANAGER)    .setDefaultRequestConfig(            RequestConfig.custom()            .setCookieSpec(CookieSpecs.IGNORE_COOKIES)                        .setConnectionRequestTimeout(10000)            .build()    ).build();\n\ndefaultMaxPerRoute 设置为100，maxTotal 设置为500，并将 connectionRequestTimeout 从30秒减少到10秒，没有拿到连接快速失败，防止过多的线程阻塞导致挤压。\n1.问题原因是 HttpClient 使用时没有对参数足够了解，对于连接池的框架，最好设置最大数，获取连接的等待超时时间，以及回收的周期，连接是否可用的校验等等参数。其他池类的框架同理，例如线程池。\n2.通过源码、日志、线程、连接等多方面进行分析，结论会更准确。\n3.arthas 是神器，可以多尝试。\n","slug":"JAVA/Apache HttpClient使用不当导致的请求超时问题排查","date":"2023-05-09T10:27:18.000Z","categories_index":"HttpClient,JAVA","tags_index":"https,com,fbpfcp","author_index":"dandeliono"},{"id":"de6c24354e140e6b852bef14e54f1711","title":"Kafka Consumer 重置 Offset","content":"Kafka Consumer 重置 Offset在Kafka Version为0.11.0.0之后，Consumer的Offset信息不再默认保存在Zookeeper上，而是选择用Topic的形式保存下来。在命令行中可以使用kafka-consumer-groups的脚本实现Offset的相关操作。\n更新Offset的三个维度： \n\nTopic的作用域\n\n重置策略\n\n执行方案\n\n–all-topics：为consumer group下所有topic的所有分区调整位移）\n\n–topic t1 –topic t2：为指定的若干个topic的所有分区调整位移\n\n–topic t1:0,1,2：为指定的topic分区调整位移\n\n–to-earliest：把位移调整到分区当前最小位移\n\n–to-latest：把位移调整到分区当前最新位移\n\n–to-current：把位移调整到分区当前位移\n\n–to-offset ： 把位移调整到指定位移处\n\n–shift-by N： 把位移调整到当前位移 + N处，注意N可以是负数，表示向前移动\n\n–to-datetime ：把位移调整到大于给定时间的最早位移处，datetime格式是yyyy-MM-ddTHH:mm:ss.xxx，比如2017-08-04T00:00:00.000\n\n–by-duration ：把位移调整到距离当前时间指定间隔的位移处，duration格式是PnDTnHnMnS，比如PT0H5M0S\n\n–from-file ：从CSV文件中读取调整策略\n\n\n什么参数都不加：只是打印出位移调整方案，不具体执行\n\n–execute：执行真正的位移调整\n–export：把位移调整方案按照CSV格式打印，方便用户成csv文件，供后续直接使用\n\n\nconsumer group状态必须是inactive的，即不能是处于正在工作中的状态\n\n不加执行方案，默认是只做打印操作\n\n更新到当前group最初的offset位置\n12bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group test-group --reset-offsets --all-topics --to-earliest --execute\n\n更新到指定的offset位置\n12bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group test-group --reset-offsets --all-topics --to-offset 500000 --execute\n\n更新到当前offset位置（解决offset的异常）\n12bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group test-group --reset-offsets --all-topics --to-current --execute\n\noffset位置按设置的值进行位移\n12bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group test-group --reset-offsets --all-topics --shift-by -100000 --execute\n\noffset设置到指定时刻开始\n12bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group test-group --reset-offsets --all-topics --to-datetime 2017-08-04T14:30:00.000\n\n","slug":"MIDDLEWARE/Kafka Consumer 重置 Offset","date":"2023-05-06T18:33:12.000Z","categories_index":"offset,MIDDLEWARE","tags_index":"consumer,topic,Offset","author_index":"dandeliono"},{"id":"8ebea8637205b0f6626f1c002b32980b","title":"Flux数据脚本语言","content":"Flux数据脚本语言官方文档：Flux data scripting language | InfluxDB Enterprise 1.9 Documentation\nFlux 被设计成可用、可读、灵活、可组合、可测试、可贡献和可共享。它的语法很大程度上受到 2018 年最流行的脚本语言 Javascript 的启发，并采用函数式方法进行数据探索和处理。\n以下示例说明了从存储桶（类似于 InfluxQL 数据库）中提取过去 5 分钟的数据，通过 cpu 测量值和 cpu&#x3D;cpu-total 标签过滤该数据，以 1 分钟间隔对数据进行窗口化，并计算平均值每个窗口的：\n1234from(bucket: &quot;telegraf/autogen&quot;)    |&gt; range(start: -1h)    |&gt; filter(fn: (r) =&gt; r._measurement == &quot;cpu&quot; and r.cpu == &quot;cpu-total&quot;)    |&gt; aggregateWindow(every: 1m, fn: mean)\n\nFlux 与 InfluxDB v1.8+ 打包在一起，不需要任何额外的安装，但是默认情况下它是禁用的，需要启用。\n通过在 influxdb.conf 的 [http] 部分下将 Flux-enabled 选项设置为 true 来启用 Flux：\n12345influxdb.conf：        [http]            flux-enabled = true \n\n当 InfluxDB 启动时，Flux 守护进程也会启动，并且可以使用 Flux 查询数据。\n有多种方法可以使用 InfluxDB Enterprise 和 Chronograf v1.8+ 执行 Flux 查询。\n3.1、Chronograf的数据浏览器Chronograf v1.8+ 在其 Data Explorer 中支持 Flux。 Flux 查询可以在 Chronograf 用户界面中构建、执行和可视化。\n3.2、Influx CLI要使用 InfluxDB Enterprise 1.9+ influx CLI 启动交互式 Flux read-eval-print-loop (REPL)，请使用以下标志运行 influx 命令：\n1234567891011121314-type=flux-path-prefix=/api/v2/query如果在您的 InfluxDB 实例上启用了身份验证，请使用 -username 标志来提供您的 InfluxDB 用户名和 -password 标志来提供您的密码。- 无授权：influx -type=flux -path-prefix=/api/v2/query- 启用身份验证：influx -type=flux \\  -path-prefix=/api/v2/query \\  -username myuser \\  -password PasSw0rd\n\n任何 Flux 查询都可以在 REPL 中执行。\n3.2.1、通过参数提交Flux查询Flux 查询也可以使用 influx CLI 的 -type&#x3D;flux 选项和 -execute 参数作为参数传递给 Flux REPL。随附的字符串作为 Flux 查询执行，结果在终端中输出。\n1234567891011- 无授权influx -type=flux \\  -path-prefix=/api/v2/query \\  -execute &#x27;&lt;flux query&gt;&#x27;- 启用身份验证influx -type=flux \\  -path-prefix=/api/v2/query \\  -username myuser \\  -password PasSw0rd \\  -execute &#x27;&lt;flux query&gt;&#x27;\n\n3.2.2、通过STDIN提交Flux查询Flux 查询通过 STDIN 输入到 influx CLI 中。查询结果在您的终端中输出。\n12345678- 无授权echo &#x27;&lt;flux query&gt;&#x27; | influx -type=flux -path-prefix=/api/v2/query- 启用身份验证echo &#x27;&lt;flux query&gt;&#x27; | influx -type=flux \\  -path-prefix=/api/v2/query \\  -username myuser \\  -password PasSw0rd\n\n3.3、Flux数据库APIFlux 可用于通过 InfluxDB 的 &#x2F;api&#x2F;v2&#x2F;query 端点查询 InfluxDB。查询的数据以带注释的 CSV 格式返回。\n在您的请求中，设置以下内容：\n123Accept header to application/csvContent-type header to application/vnd.flux如果在您的 InfluxDB 实例上启用了身份验证，则 授权头为 Token &lt;username&gt;:&lt;password&gt;\n\n这允许您以纯文本形式发布 Flux 查询并接收带注释的 CSV 响应。\n下面是一个使用 Flux 查询 InfluxDB 的 curl 命令示例：\n123456curl -XPOST localhost:8086/api/v2/query -sS \\  -H &#x27;Accept:application/csv&#x27; \\  -H &#x27;Content-type:application/vnd.flux&#x27; \\  -d &#x27;from(bucket:&quot;telegraf&quot;)          |&gt; range(start:-5m)          |&gt; filter(fn:(r) =&gt; r._measurement == &quot;cpu&quot;)&#x27;\n\nFlux 是 InfluxData 的新功能数据脚本语言，专为查询、分析和处理数据而设计。这个多部分的入门指南介绍了与 Flux 相关的重要概念。它涵盖了使用 Flux 从 InfluxDB 查询时间序列数据，并介绍了 Flux 语法和函数。\n4.1、你需要什么\nInfluxDB v1.8：Flux v0.65 内置于 InfluxDB v1.8 中，可用于查询存储在 InfluxDB 中的数据。\nChronograf v1.8+：不是必需的，但强烈推荐。 Chronograf v1.8 的 Data Explorer 提供了用于编写 Flux 脚本和可视化结果的用户界面 (UI)。 Chronograf v1.8+ 中的仪表板也支持 Flux 查询。\n\n4.2、关键概念Flux 引入了重要的新概念，您在开始时应该理解。\n4.2.1、Buckets（桶）Flux 引入了“buckets”，这是 InfluxDB 的一种新的数据存储概念。存储桶是具有保留策略的存储数据的命名位置。它类似于 InfluxDB v1.x “数据库”，但它是数据库和保留策略的组合。使用多个保留策略时，每个保留策略都被视为其自己的存储桶。\nFlux 的 from() 函数，它定义了一个 InfluxDB 数据源，需要一个桶参数。将 Flux 与 InfluxDB v1.x 一起使用时，请使用以下存储桶命名约定，它将数据库名称和保留策略组合成一个存储桶名称：\n123456InfluxDB v1.x存储桶命名约定：        from(bucket:&quot;&lt;database&gt;/&lt;retention-policy&gt;&quot;)        from(bucket:&quot;telegraf/autogen&quot;)\n\n4.2.2、Pipe-forward operator（管道转发运算符）Flux 广泛使用管道转发运算符 (|&gt;) 将操作链接在一起。在每个函数或操作之后，Flux 返回一个包含数据的表或表集合。管道转发操作符将这些表通过管道传送到下一个函数或操作中，在这些函数或操作中进一步处理或操作它们。\n4.2.3、Tables（表）Flux 构造表中的所有数据。当数据从数据源流式传输时，Flux 将其格式化为带注释的逗号分隔值 (CSV)，表示表格。然后函数操作或处理它们并输出新表。这使得将函数链接在一起以构建复杂的查询变得容易。\nGroup keys（组键）：每个表都有一个描述表内容的组键。它是一个列列表，表中的每一行都将具有相同的值。每行中具有唯一值的列不属于组键。随着函数处理和转换数据，每个函数都会修改输出表的组键。了解函数如何修改表和组键是正确调整数据以获得所需输出的关键。\n12- 实例组键：    [_start, _stop, _field, _measurement, host]\n\n请注意，_time 和 _value 被排除在示例组键之外，因为它们对于每一行都是唯一的。\n4.3、使用Flux的工具您有多种选择来编写和运行 Flux 查询，但在您开始时，我们建议使用以下方法：Chronograf的数据浏览器。Chronograf 的数据资源管理器可以轻松编写您的第一个 Flux 脚本并可视化结果。要使用 Chronograf 的 Flux UI，请打开数据资源管理器并在图表占位符上方的源下拉列表右侧，选择 Flux 作为源类型。这将提供模式、脚本和函数窗格。 Schema 窗格允许您浏览数据。脚本窗格是您编写 Flux 脚本的地方。 Functions 窗格提供了 Flux 查询中可用的函数列表。\n4.4、使用Flux查询InfluxDB本指南介绍了使用 Flux 从 InfluxDB 查询数据的基础知识。如果您还没有，请确保安装 InfluxDB v1.8+，启用 Flux，并选择用于编写 Flux 查询的工具。可以使用执行Flux查询中描述的任何方法执行以下查询。请务必为每种方法提供您的 InfluxDB Enterprise 授权凭证。\n每个 Flux 查询都需要以下内容：① 数据源；② 一个时间范围；③ 数据过滤器。\n4.4.1、定义你的数据源Flux 的 from() 函数定义了一个 InfluxDB 数据源。它需要一个桶参数。对于此示例，使用 telegraf&#x2F;autogen，这是 TICK 堆栈提供的默认数据库和保留策略的组合。\n4.4.2、指定时间范围Flux 在查询时间序列数据时需要时间范围。 “无界”查询非常耗费资源，作为一种保护措施，Flux 不会查询没有指定范围的数据库。使用管道转发运算符 (|&gt;) 将数据从数据源通过管道传输到 range() 函数，该函数指定查询的时间范围。它接受两个属性：开始和停止。范围可以是使用负持续时间的相对范围或使用时间戳的绝对范围。相对范围是相对于“现在”的。\n12345678910111213141516- 示例：相对时间范围        from(bucket:&quot;telegraf/autogen&quot;)        |&gt; range(start: -1h)        from(bucket:&quot;telegraf/autogen&quot;)        |&gt; range(start: -1h, stop: -10m)- 示例：绝对时间范围    from(bucket:&quot;telegraf/autogen&quot;)        |&gt; range(start: 2018-11-05T23:30:00Z, stop: 2018-11-06T00:00:00Z)对于本指南，使用相对时间范围 -15m 将查询结果限制为最近 15 分钟的数据：    from(bucket:&quot;telegraf/autogen&quot;)        |&gt; range(start: -15m) \n\n4.4.3、过滤您的数据将您的范围数据传递给 filter() 函数，以根据数据属性或列缩小结果范围。 filter() 函数有一个参数 fn，它需要一个匿名函数，该函数具有基于列或属性过滤数据的逻辑。Flux 的匿名函数语法与 Javascript 非常相似。记录或行作为记录 (r) 传递给 filter() 函数。匿名函数获取记录并评估它以查看它是否与定义的过滤器匹配。使用 AND 关系运算符链接多个过滤器。\n12345678910 (r) =&gt; (r.recordProperty comparisonOperator comparisonExpression)(r) =&gt; (r._measurement == &quot;cpu&quot;)(r) =&gt; (r._measurement == &quot;cpu&quot;) and (r._field != &quot;usage_system&quot; )对于此示例，按 cpu 测量值、usage_system 字段和 cpu-total 标记值进行过滤：from(bucket: &quot;telegraf/autogen&quot;)    |&gt; range(start: -15m)    |&gt; filter(fn: (r) =&gt; r._measurement == &quot;cpu&quot; and r._field == &quot;usage_system&quot; and r.cpu == &quot;cpu-total&quot;)\n\n4.4.4、输出您的查询数据使用 Flux 的 yield() 函数将过滤后的表作为查询结果输出。\n1234from(bucket: &quot;telegraf/autogen&quot;)    |&gt; range(start: -15m)    |&gt; filter(fn: (r) =&gt; r._measurement == &quot;cpu&quot; and r._field == &quot;usage_system&quot; and r.cpu == &quot;cpu-total&quot;)    |&gt; yield()\n\nChronograf 和 influx CLI 在每个脚本的末尾自动假设一个 yield() 函数，以便输出和可视化数据。最佳实践是包含一个 yield() 函数，但这并不总是必要的。\n4.5、使用Flux转换数据从 InfluxDB 查询数据时，您通常需要以某种方式转换该数据。常见的示例是将数据聚合为平均值、对数据进行下采样等。本指南演示了如何使用 Flux 函数来转换您的数据。它逐步创建了一个 Flux 脚本，该脚本将数据划分为时间窗口，平均每个窗口中的 _values，并将平均值作为新表输出。了解数据的“形状”如何通过这些操作发生变化非常重要。\n4.5.1、查询数据使用之前 InfluxDB 指南中的 Query data 中内置的查询，但更新范围以从最后一小时提取数据：\n123from(bucket: &quot;telegraf/autogen&quot;)    |&gt; range(start: -1h)    |&gt; filter(fn: (r) =&gt; r._measurement == &quot;cpu&quot; and r._field == &quot;usage_system&quot; and r.cpu == &quot;cpu-total&quot;)\n\n4.5.2、Flux函数Flux 提供了许多执行特定操作、转换和任务的函数。您还可以在 Flux 查询中创建自定义函数。 Flux 标准库文档中详细介绍了函数。转换从 InfluxDB 查询的数据时使用的一种常见函数类型是聚合函数。聚合函数采用表中的一组 _values，聚合它们，并将它们转换为新值。\n此示例使用 mean() 函数对时间窗口内的值进行平均。\n4.5.3、窗口化您的数据Flux 的 window() 函数根据时间值对记录进行分区。使用 every 参数定义每个窗口的持续时间。every 支持所有有效的持续时间单位，包括日历月 (1mo) 和年 (1y)。\n对于此示例，以五分钟为间隔 (5m) 的窗口数据。\n1234from(bucket: &quot;telegraf/autogen&quot;)    |&gt; range(start: -1h)    |&gt; filter(fn: (r) =&gt; r._measurement == &quot;cpu&quot; and r._field == &quot;usage_system&quot; and r.cpu == &quot;cpu-total&quot;)    |&gt; window(every: 5m)\n\n随着数据被收集到时间窗口中，每个窗口都作为自己的表格输出。可视化时，每个表都分配有唯一的颜色。\n4.5.4、聚合窗口数据Flux 聚合函数获取每个表中的 _values 并以某种方式聚合它们。使用 mean() 函数对每个表的 _values 进行平均。\n12345from(bucket: &quot;telegraf/autogen&quot;)    |&gt; range(start: -1h)    |&gt; filter(fn: (r) =&gt; r._measurement == &quot;cpu&quot; and r._field == &quot;usage_system&quot; and r.cpu == &quot;cpu-total&quot;)    |&gt; window(every: 5m)    |&gt; mean()\n\n由于每个窗口中的行都被聚合，它们的输出表只包含一个具有聚合值的行。窗口化表格仍然是独立的，并且在可视化时将显示为单个未连接的点。\n4.5.5、将时间添加到您的聚合中当值被聚合时，结果表没有 _time 列，因为用于聚合的记录都有不同的时间戳。聚合函数不会推断应该将什么时间用于聚合值。因此 _time 列被删除。在下一个操作中需要一个 _time 列。要添加一个，请使用 duplicate() 函数将 _stop 列复制为每个窗口表的 _time 列。\n123456from(bucket: &quot;telegraf/autogen&quot;)    |&gt; range(start: -1h)    |&gt; filter(fn: (r) =&gt; r._measurement == &quot;cpu&quot; and r._field == &quot;usage_system&quot; and r.cpu == &quot;cpu-total&quot;)    |&gt; window(every: 5m)    |&gt; mean()    |&gt; duplicate(column: &quot;_stop&quot;, as: &quot;_time&quot;)\n\n4.5.6、展开聚合表使用带有 every: inf 参数的 window() 函数将所有点聚集到一个单一的无限窗口中。\n1234567from(bucket: &quot;telegraf/autogen&quot;)    |&gt; range(start: -1h)    |&gt; filter(fn: (r) =&gt; r._measurement == &quot;cpu&quot; and r._field == &quot;usage_system&quot; and r.cpu == &quot;cpu-total&quot;)    |&gt; window(every: 5m)    |&gt; mean()    |&gt; duplicate(column: &quot;_stop&quot;, as: &quot;_time&quot;)    |&gt; window(every: inf)\n\n一旦取消分组并组合成一个表，聚合数据点将在您的可视化中显示为连接。\n4.5.7、辅助函数这似乎只是为了构建一个聚合数据的查询而进行的大量编码，但是通过该过程有助于了解数据在通过每个函数时如何改变“形状”。Flux 提供（并允许您创建）抽象许多这些步骤的“帮助”函数。本指南中执行的相同操作可以使用 aggregateWindow() 函数完成。\n1234from(bucket: &quot;telegraf/autogen&quot;)    |&gt; range(start: -1h)    |&gt; filter(fn: (r) =&gt; r._measurement == &quot;cpu&quot; and r._field == &quot;usage_system&quot; and r.cpu == &quot;cpu-total&quot;)    |&gt; aggregateWindow(every: 5m, fn: mean)\n\n4.6、Flux语法基础Flux的核心是一种专门为处理数据而设计的脚本语言。本指南介绍了一些简单的表达式以及如何在 Flux 中处理它们。\n4.6.1、简单的表达式12&gt; 1 + 12\n\n4.6.2、变量1234567891011&gt; s = &quot;this is a string&quot;&gt; i = 1 &gt; f = 2.0 键入变量的名称以打印其值：&gt; sthis is a string&gt; i1&gt; f2\n\n4.6.3、记录Flux 还支持记录。记录中的每个值都可以是不同的数据类型。\n1&gt; o = &#123;name:&quot;Jim&quot;, age: 42, &quot;favorite color&quot;: &quot;red&quot;&#125;\n\n使用点表示法访问记录的属性：\n1234&gt; o.nameJim&gt; o.age42\n\n或括号表示法：\n123456&gt; o[&quot;name&quot;]Jim&gt; o[&quot;age&quot;]42&gt; o[&quot;favorite color&quot;]red\n\n使用括号表示法来引用属性键中具有特殊或空白字符的记录属性。\n4.6.4、列表Flux 支持列表。列表值必须是同一类型。\n1234&gt; n = 4&gt; l = [1,2,3,n]&gt; l[1, 2, 3, 4]\n\n4.6.5、函数Flux 使用函数来完成大部分繁重的工作。下面是一个对数字 n 求平方的简单函数。\n123&gt; square = (n) =&gt; n * n&gt; square(n:3)9\n\nFlux 不支持位置参数或参数。调用函数时必须始终命名参数。\n4.6.6、管道转发运算符Flux 广泛使用管道转发运算符 (|&gt;) 将操作链接在一起。在每个函数或操作之后，Flux 返回一个包含数据的表或表集合。 pipe-forward 操作符将这些表通过管道传送到下一个函数中，在该函数中进一步处理或操作它们。\n1data |&gt; someFunction() |&gt; anotherFunction()\n\n4.7、基本语法的实际应用如果您已经阅读过其他入门指南，这可能看起来很熟悉。 Flux 的语法受到 Javascript 和其他函数式脚本语言的启发。当您开始将这些基本原则应用于实际用例（例如创建数据流变量、自定义函数等）时，Flux 的强大功能及其查询和处理数据的能力将变得显而易见。\n下面的示例提供了每个输入命令的多行和单行版本。 Flux 中的回车不是必需的，但有助于提高可读性。单行和多行命令都可以复制并粘贴到以 Flux 模式运行的 influx CLI 中。\n4.7.1、多行输入定义数据流变量：Flux 中变量赋值的一个常见用例是为一个或多个输入数据流创建变量。\n123456789timeRange = -1hcpuUsageUser = from(bucket: &quot;telegraf/autogen&quot;)    |&gt; range(start: timeRange)    |&gt; filter(fn: (r) =&gt; r._measurement == &quot;cpu&quot; and r._field == &quot;usage_user&quot; and r.cpu == &quot;cpu-total&quot;)memUsagePercent = from(bucket: &quot;telegraf/autogen&quot;)    |&gt; range(start: timeRange)    |&gt; filter(fn: (r) =&gt; r._measurement == &quot;mem&quot; and r._field == &quot;used_percent&quot;)\n\n这些变量可以在其他函数中使用，例如 join()，同时保持语法最小和灵活。\n定义自定义函数：创建一个函数，该函数返回输入流中具有最高 _value 的 N 行。为此，请将输入流（表）和要返回的结果数 (n) 传递给自定义函数。然后使用 Flux 的 sort() 和 limit() 函数在数据集中查找前 n 个结果。\n123topN = (tables=&lt;-, n) =&gt; tables    |&gt; sort(desc: true)    |&gt; limit(n: n)\n\n使用这个新的自定义函数 topN 和上面定义的 cpuUsageUser 数据流变量，找到前五个数据点并产生结果。\n123cpuUsageUser    |&gt; topN(n: 5)    |&gt; yield()\n\n此查询将返回过去一小时内用户 CPU 使用率最高的五个数据点。\n4.7.2、单行输入定义数据流变量：Flux 中变量赋值的一个常见用例是为多个过滤的输入数据流创建变量。\n123456789timeRange = -1hcpuUsageUser = from(bucket: &quot;telegraf/autogen&quot;)    |&gt; range(start: timeRange)    |&gt; filter(fn: (r) =&gt; r._measurement == &quot;cpu&quot; and r._field == &quot;usage_user&quot; and r.cpu == &quot;cpu-total&quot;)memUsagePercent = from(bucket: &quot;telegraf/autogen&quot;)    |&gt; range(start: timeRange)    |&gt; filter(fn: (r) =&gt; r._measurement == &quot;mem&quot; and r._field == &quot;used_percent&quot;)\n\n这些变量可以在其他函数中使用，例如 join()，同时保持语法最小和灵活。\n定义自定义函数：让我们创建一个函数，它返回输入数据流中具有最高 _value 的 N 行。为此，请将输入流（表）和要返回的结果数 (n) 传递给自定义函数。然后使用 Flux 的 sort() 和 limit() 函数在数据集中查找前 n 个结果。\n1topN = (tables=&lt;-, n) =&gt; tables |&gt; sort(desc: true) |&gt; limit(n: n)\n\n使用上面定义的 cpuUsageUser 数据流变量，使用自定义 topN 函数找到前五个数据点并产生结果。此查询将返回过去一小时内用户 CPU 使用率最高的五个数据点。\n4.8、使用Flux查询数据以下指南中提供的许多示例都使用数据变量，它表示按度量和字段过滤数据的基本查询。数据定义为：\n123data = from(bucket: &quot;db/rp&quot;)    |&gt; range(start: -1h)    |&gt; filter(fn: (r) =&gt; r._measurement == &quot;example-measurement&quot; and r._field == &quot;example-field&quot;)\n\n4.8.1、查询字段和标签使用 filter() 函数根据字段、标签或任何其他列值查询数据。 filter() 执行类似于 InfluxQL 和其他类似 SQL 的查询语言中的 SELECT 语句和 WHERE 子句的操作。\n1234567from(bucket: &quot;db/rp&quot;)  |&gt; range(start: -1h)  |&gt; filter(fn: (r) =&gt;      r._measurement == &quot;example-measurement&quot; and      r._field == &quot;example-field&quot; and      r.tag == &quot;example-tag&quot;  )\n\nfilter()函数：filter() 有一个 fn 参数，它需要一个判定函数，一个由一个或多个判定表达式组成的匿名函数。判定函数评估每个输入行。评估结果为 true 的行包含在输出数据中。评估结果为 false 的行将从输出数据中排除。\n1|&gt; filter(fn: (r) =&gt; r._measurement == &quot;example-measurement&quot; )\n\nfn 判定函数需要一个 r 参数，它将每一行表示为 filter() 迭代输入数据。行记录中的键值对表示列及其值。使用点表示法或方括号表示法来引用判定函数中的特定列值。使用逻辑运算符将多个判定表达式链接在一起。\n12345 r = &#123;foo: &quot;bar&quot;, baz: &quot;quz&quot;&#125;(r) =&gt; r.foo == &quot;bar&quot; and r[&quot;baz&quot;] == &quot;quz&quot;(r) =&gt; true and true\n\n按字段和标签过滤：from()、range() 和 filter() 的组合代表了最基本的 Flux 查询：\n\n使用 from() 定义您的存储桶。\n使用 range() 按时间限制查询结果。\n使用 filter() 来确定要输出的数据行。\n\n1234from(bucket: &quot;db/rp&quot;)    |&gt; range(start: -1h)    |&gt; filter(fn: (r) =&gt; r._measurement == &quot;example-measurement&quot; and r.tag == &quot;example-tag&quot;)    |&gt; filter(fn: (r) =&gt; r._field == &quot;example-field&quot;)\n\n4.8.2、group()：对InfluxDB中的数据进行分组使用 Flux，您可以按查询数据集中的任何列对数据进行分组。 “分组”将数据分区到表中，其中每一行为指定的列共享一个公共值。本指南介绍了 Flux 中的数据分组，并提供了数据在流程中如何形成的示例。\ngroup keys（组键）：每个表都有一个组键——一个列列表，表中的每一行都具有相同的值。\n示例组键：\n1[_start, _stop, _field, _measurement, host]\n\nFlux 中对数据进行分组，本质上就是定义输出表的组键。了解修改组键如何塑造输出数据是成功将数据分组和转换为所需输出的关键。\ngroup()函数：Flux 的 group() 函数定义了输出表的组键。使用 group() 函数对特定列中具有共同值的数据进行分组。group()函数具有以下参数：\n\ncolumns：在分组操作中包含或排除的列列表（取决于模式）。\nmode：用于定义组和结果组键的方法。可能的值包括 by 和 except。\n\n12345data    |&gt; group(columns: [&quot;cpu&quot;, &quot;host&quot;], mode: &quot;by&quot;)产生的组键：    [cpu, host]\n\n输入：\n\n\n\n_time\nhost\n_value\n\n\n\n2020-01-01T00:01:00Z\nhost1\n1.0\n\n\n2020-01-01T00:01:00Z\nhost2\n2.0\n\n\n2020-01-01T00:02:00Z\nhost1\n1.0\n\n\n2020-01-01T00:02:00Z\nhost2\n3.0\n\n\n输出：\n\n\n\n_time\nhost\n_value\n\n\n\n2020-01-01T00:01:00Z\nhost1\n1.0\n\n\n2020-01-01T00:02:00Z\nhost1\n1.0\n\n\n\n\n\n_time\nhost\n_value\n\n\n\n2020-01-01T00:01:00Z\nhost2\n2.0\n\n\n2020-01-01T00:02:00Z\nhost2\n3.0\n\n\n为了说明分组的工作原理，定义一个从 db&#x2F;rp 存储桶查询系统 CPU 使用率的 dataSet 变量。过滤 cpu 标记，使其仅返回每个编号的 CPU 内核的结果。\n系统操作为所有编号的 CPU 内核使用的 CPU。它使用正则表达式来过滤仅编号的核心。\n1234dataSet = from(bucket: &quot;db/rp&quot;)    |&gt; range(start: -2m)    |&gt; filter(fn: (r) =&gt; r._field == &quot;usage_system&quot; and r.cpu =~ /cpu[0-9*]/)    |&gt; drop(columns: [&quot;host&quot;])\n\n此示例从返回的数据中删除主机列，因为仅跟踪单个主机的 CPU 数据，并且它简化了输出表。如果监控多个主机，请不要删除主机列。请注意，每个表都会输出组键：表：键：。\n123456789101112131415161718192021222324252627282930313233Table: keys: [_start, _stop, _field, _measurement, cpu]                   _start:time                      _stop:time           _field:string     _measurement:string              cpu:string                      _time:time                  _value:float2018-11-05T21:34:00.000000000Z  2018-11-05T21:36:00.000000000Z            usage_system                     cpu                    cpu0  2018-11-05T21:34:00.000000000Z             7.8921078921078922018-11-05T21:34:00.000000000Z  2018-11-05T21:36:00.000000000Z            usage_system                     cpu                    cpu0  2018-11-05T21:34:10.000000000Z                           7.22018-11-05T21:34:00.000000000Z  2018-11-05T21:36:00.000000000Z            usage_system                     cpu                    cpu0  2018-11-05T21:34:20.000000000Z                           7.42018-11-05T21:34:00.000000000Z  2018-11-05T21:36:00.000000000Z            usage_system                     cpu                    cpu0  2018-11-05T21:34:30.000000000Z                           5.52018-11-05T21:34:00.000000000Z  2018-11-05T21:36:00.000000000Z            usage_system                     cpu                    cpu0  2018-11-05T21:34:40.000000000Z                           7.42018-11-05T21:34:00.000000000Z  2018-11-05T21:36:00.000000000Z            usage_system                     cpu                    cpu0  2018-11-05T21:34:50.000000000Z                           7.52018-11-05T21:34:00.000000000Z  2018-11-05T21:36:00.000000000Z            usage_system                     cpu                    cpu0  2018-11-05T21:35:00.000000000Z                          10.32018-11-05T21:34:00.000000000Z  2018-11-05T21:36:00.000000000Z            usage_system                     cpu                    cpu0  2018-11-05T21:35:10.000000000Z                           9.22018-11-05T21:34:00.000000000Z  2018-11-05T21:36:00.000000000Z            usage_system                     cpu                    cpu0  2018-11-05T21:35:20.000000000Z                           8.42018-11-05T21:34:00.000000000Z  2018-11-05T21:36:00.000000000Z            usage_system                     cpu                    cpu0  2018-11-05T21:35:30.000000000Z                           8.52018-11-05T21:34:00.000000000Z  2018-11-05T21:36:00.000000000Z            usage_system                     cpu                    cpu0  2018-11-05T21:35:40.000000000Z                           8.62018-11-05T21:34:00.000000000Z  2018-11-05T21:36:00.000000000Z            usage_system                     cpu                    cpu0  2018-11-05T21:35:50.000000000Z                          10.22018-11-05T21:34:00.000000000Z  2018-11-05T21:36:00.000000000Z            usage_system                     cpu                    cpu0  2018-11-05T21:36:00.000000000Z                          10.6Table: keys: [_start, _stop, _field, _measurement, cpu]                   _start:time                      _stop:time           _field:string     _measurement:string              cpu:string                      _time:time                  _value:float2018-11-05T21:34:00.000000000Z  2018-11-05T21:36:00.000000000Z            usage_system                     cpu                    cpu1  2018-11-05T21:34:00.000000000Z            0.79920079920079922018-11-05T21:34:00.000000000Z  2018-11-05T21:36:00.000000000Z            usage_system                     cpu                    cpu1  2018-11-05T21:34:10.000000000Z                           0.72018-11-05T21:34:00.000000000Z  2018-11-05T21:36:00.000000000Z            usage_system                     cpu                    cpu1  2018-11-05T21:34:20.000000000Z                           0.72018-11-05T21:34:00.000000000Z  2018-11-05T21:36:00.000000000Z            usage_system                     cpu                    cpu1  2018-11-05T21:34:30.000000000Z                           0.42018-11-05T21:34:00.000000000Z  2018-11-05T21:36:00.000000000Z            usage_system                     cpu                    cpu1  2018-11-05T21:34:40.000000000Z                           0.72018-11-05T21:34:00.000000000Z  2018-11-05T21:36:00.000000000Z            usage_system                     cpu                    cpu1  2018-11-05T21:34:50.000000000Z                           0.72018-11-05T21:34:00.000000000Z  2018-11-05T21:36:00.000000000Z            usage_system                     cpu                    cpu1  2018-11-05T21:35:00.000000000Z                           1.42018-11-05T21:34:00.000000000Z  2018-11-05T21:36:00.000000000Z            usage_system                     cpu                    cpu1  2018-11-05T21:35:10.000000000Z                           1.22018-11-05T21:34:00.000000000Z  2018-11-05T21:36:00.000000000Z            usage_system                     cpu                    cpu1  2018-11-05T21:35:20.000000000Z                           0.82018-11-05T21:34:00.000000000Z  2018-11-05T21:36:00.000000000Z            usage_system                     cpu                    cpu1  2018-11-05T21:35:30.000000000Z            0.89910089910089912018-11-05T21:34:00.000000000Z  2018-11-05T21:36:00.000000000Z            usage_system                     cpu                    cpu1  2018-11-05T21:35:40.000000000Z            0.80080080080080082018-11-05T21:34:00.000000000Z  2018-11-05T21:36:00.000000000Z            usage_system                     cpu                    cpu1  2018-11-05T21:35:50.000000000Z             0.9990009990009992018-11-05T21:34:00.000000000Z  2018-11-05T21:36:00.000000000Z            usage_system                     cpu                    cpu1  2018-11-05T21:36:00.000000000Z            1.1022044088176353\n\n按 cpu 列对数据集流进行分组：\n12dataSet    |&gt; group(columns: [&quot;cpu&quot;])\n\n这实际上不会改变数据的结构，因为它已经在组键中有 cpu，因此按 cpu 分组。但是，请注意它确实更改了组键：\n123456789101112131415161718192021222324252627282930313233Table: keys: [cpu]            cpu:string                      _stop:time                      _time:time                  _value:float           _field:string     _measurement:string                     _start:time                  cpu0  2018-11-05T21:36:00.000000000Z  2018-11-05T21:34:00.000000000Z             7.892107892107892            usage_system                     cpu  2018-11-05T21:34:00.000000000Z                  cpu0  2018-11-05T21:36:00.000000000Z  2018-11-05T21:34:10.000000000Z                           7.2            usage_system                     cpu  2018-11-05T21:34:00.000000000Z                  cpu0  2018-11-05T21:36:00.000000000Z  2018-11-05T21:34:20.000000000Z                           7.4            usage_system                     cpu  2018-11-05T21:34:00.000000000Z                  cpu0  2018-11-05T21:36:00.000000000Z  2018-11-05T21:34:30.000000000Z                           5.5            usage_system                     cpu  2018-11-05T21:34:00.000000000Z                  cpu0  2018-11-05T21:36:00.000000000Z  2018-11-05T21:34:40.000000000Z                           7.4            usage_system                     cpu  2018-11-05T21:34:00.000000000Z                  cpu0  2018-11-05T21:36:00.000000000Z  2018-11-05T21:34:50.000000000Z                           7.5            usage_system                     cpu  2018-11-05T21:34:00.000000000Z                  cpu0  2018-11-05T21:36:00.000000000Z  2018-11-05T21:35:00.000000000Z                          10.3            usage_system                     cpu  2018-11-05T21:34:00.000000000Z                  cpu0  2018-11-05T21:36:00.000000000Z  2018-11-05T21:35:10.000000000Z                           9.2            usage_system                     cpu  2018-11-05T21:34:00.000000000Z                  cpu0  2018-11-05T21:36:00.000000000Z  2018-11-05T21:35:20.000000000Z                           8.4            usage_system                     cpu  2018-11-05T21:34:00.000000000Z                  cpu0  2018-11-05T21:36:00.000000000Z  2018-11-05T21:35:30.000000000Z                           8.5            usage_system                     cpu  2018-11-05T21:34:00.000000000Z                  cpu0  2018-11-05T21:36:00.000000000Z  2018-11-05T21:35:40.000000000Z                           8.6            usage_system                     cpu  2018-11-05T21:34:00.000000000Z                  cpu0  2018-11-05T21:36:00.000000000Z  2018-11-05T21:35:50.000000000Z                          10.2            usage_system                     cpu  2018-11-05T21:34:00.000000000Z                  cpu0  2018-11-05T21:36:00.000000000Z  2018-11-05T21:36:00.000000000Z                          10.6            usage_system                     cpu  2018-11-05T21:34:00.000000000ZTable: keys: [cpu]            cpu:string                      _stop:time                      _time:time                  _value:float           _field:string     _measurement:string                     _start:time                  cpu1  2018-11-05T21:36:00.000000000Z  2018-11-05T21:34:00.000000000Z            0.7992007992007992            usage_system                     cpu  2018-11-05T21:34:00.000000000Z                  cpu1  2018-11-05T21:36:00.000000000Z  2018-11-05T21:34:10.000000000Z                           0.7            usage_system                     cpu  2018-11-05T21:34:00.000000000Z                  cpu1  2018-11-05T21:36:00.000000000Z  2018-11-05T21:34:20.000000000Z                           0.7            usage_system                     cpu  2018-11-05T21:34:00.000000000Z                  cpu1  2018-11-05T21:36:00.000000000Z  2018-11-05T21:34:30.000000000Z                           0.4            usage_system                     cpu  2018-11-05T21:34:00.000000000Z                  cpu1  2018-11-05T21:36:00.000000000Z  2018-11-05T21:34:40.000000000Z                           0.7            usage_system                     cpu  2018-11-05T21:34:00.000000000Z                  cpu1  2018-11-05T21:36:00.000000000Z  2018-11-05T21:34:50.000000000Z                           0.7            usage_system                     cpu  2018-11-05T21:34:00.000000000Z                  cpu1  2018-11-05T21:36:00.000000000Z  2018-11-05T21:35:00.000000000Z                           1.4            usage_system                     cpu  2018-11-05T21:34:00.000000000Z                  cpu1  2018-11-05T21:36:00.000000000Z  2018-11-05T21:35:10.000000000Z                           1.2            usage_system                     cpu  2018-11-05T21:34:00.000000000Z                  cpu1  2018-11-05T21:36:00.000000000Z  2018-11-05T21:35:20.000000000Z                           0.8            usage_system                     cpu  2018-11-05T21:34:00.000000000Z                  cpu1  2018-11-05T21:36:00.000000000Z  2018-11-05T21:35:30.000000000Z            0.8991008991008991            usage_system                     cpu  2018-11-05T21:34:00.000000000Z                  cpu1  2018-11-05T21:36:00.000000000Z  2018-11-05T21:35:40.000000000Z            0.8008008008008008            usage_system                     cpu  2018-11-05T21:34:00.000000000Z                  cpu1  2018-11-05T21:36:00.000000000Z  2018-11-05T21:35:50.000000000Z             0.999000999000999            usage_system                     cpu  2018-11-05T21:34:00.000000000Z                  cpu1  2018-11-05T21:36:00.000000000Z  2018-11-05T21:36:00.000000000Z            1.1022044088176353            usage_system                     cpu  2018-11-05T21:34:00.000000000Z\n\n可视化保持不变。\n\n按 _time 列对数据进行分组很好地说明了分组如何更改数据的结构：\n12dataSet    |&gt; group(columns: [&quot;_time&quot;])\n\n当按 _time 分组时，共享一个公共 _time 值的所有记录将被分组到单独的表中。所以每个输出表代表一个时间点。\n123456789101112131415Table: keys: [_time]                    _time:time                     _start:time                      _stop:time                  _value:float           _field:string     _measurement:string              cpu:string2018-11-05T21:34:00.000000000Z  2018-11-05T21:34:00.000000000Z  2018-11-05T21:36:00.000000000Z             7.892107892107892            usage_system                     cpu                    cpu02018-11-05T21:34:00.000000000Z  2018-11-05T21:34:00.000000000Z  2018-11-05T21:36:00.000000000Z            0.7992007992007992            usage_system                     cpu                    cpu12018-11-05T21:34:00.000000000Z  2018-11-05T21:34:00.000000000Z  2018-11-05T21:36:00.000000000Z                           4.1            usage_system                     cpu                    cpu22018-11-05T21:34:00.000000000Z  2018-11-05T21:34:00.000000000Z  2018-11-05T21:36:00.000000000Z            0.5005005005005005            usage_system                     cpu                    cpu3Table: keys: [_time]                    _time:time                     _start:time                      _stop:time                  _value:float           _field:string     _measurement:string              cpu:string2018-11-05T21:34:10.000000000Z  2018-11-05T21:34:00.000000000Z  2018-11-05T21:36:00.000000000Z                           7.2            usage_system                     cpu                    cpu02018-11-05T21:34:10.000000000Z  2018-11-05T21:34:00.000000000Z  2018-11-05T21:36:00.000000000Z                           0.7            usage_system                     cpu                    cpu12018-11-05T21:34:10.000000000Z  2018-11-05T21:34:00.000000000Z  2018-11-05T21:36:00.000000000Z                           3.6            usage_system                     cpu                    cpu22018-11-05T21:34:10.000000000Z  2018-11-05T21:34:00.000000000Z  2018-11-05T21:36:00.000000000Z                           0.5            usage_system                     cpu                    cpu3\n\n因为每个时间戳都被构造为一个单独的表，所以在可视化时，共享相同时间戳的所有点看起来都是连接的。\n\n通过一些进一步的处理，您可以计算每个时间点所有 CPU 的平均 CPU 使用率，并将它们分组到一个表中，但我们不会在此示例中涉及。如果您有兴趣自己运行和可视化它，查询将如下所示：\n1234dataSet    |&gt; group(columns: [&quot;_time&quot;])    |&gt; mean()    |&gt; group(columns: [&quot;_value&quot;, &quot;_time&quot;], mode: &quot;except&quot;)\n\n按 cpu 和 _time 列分组：\n12dataSet    |&gt; group(columns: [&quot;cpu&quot;, &quot;_time&quot;])\n\n这会为每个唯一的 cpu 和 _time 组合输出一个表：\n123456789Table: keys: [_time, cpu]                    _time:time              cpu:string                      _stop:time                  _value:float           _field:string     _measurement:string                     _start:time2018-11-05T21:34:00.000000000Z                    cpu0  2018-11-05T21:36:00.000000000Z             7.892107892107892            usage_system                     cpu  2018-11-05T21:34:00.000000000ZTable: keys: [_time, cpu]                    _time:time              cpu:string                      _stop:time                  _value:float           _field:string     _measurement:string                     _start:time2018-11-05T21:34:00.000000000Z                    cpu1  2018-11-05T21:36:00.000000000Z            0.7992007992007992            usage_system                     cpu  2018-11-05T21:34:00.000000000Z\n\n可视化时，表格显示为单独的、未连接的点。\n\n按 cpu 和 _time 分组很好地说明了分组的工作原理。\n4.8.3、sort() &amp; limit()：使用Flux对数据进行排序和限制使用 sort() 函数按特定列对每个表中的记录进行排序，使用 limit() 函数将输出表中的记录数限制为固定数 n。\n123data    |&gt; sort(columns: [&quot;host&quot;, &quot;_value&quot;])    |&gt; limit(n: 4)\n\n输入：\n\n\n\n_time\nhost\n_value\n\n\n\n2020-01-01T00:01:00Z\nA\n1.0\n\n\n2020-01-01T00:02:00Z\nB\n1.2\n\n\n2020-01-01T00:03:00Z\nA\n1.8\n\n\n2020-01-01T00:04:00Z\nB\n0.9\n\n\n2020-01-01T00:05:00Z\nB\n1.4\n\n\n2020-01-01T00:06:00Z\nB\n2.0\n\n\n输出：\n\n\n\n_time\nhost\n_value\n\n\n\n2020-01-01T00:03:00Z\nA\n1.8\n\n\n2020-01-01T00:01:00Z\nA\n1.0\n\n\n2020-01-01T00:06:00Z\nB\n2.0\n\n\n2020-01-01T00:05:00Z\nB\n1.4\n\n\n以下示例首先按区域排序系统正常运行时间region，然后是主机host，然后是值_value。\n1234from(bucket: &quot;db/rp&quot;)    |&gt; range(start: -12h)    |&gt; filter(fn: (r) =&gt; r._measurement == &quot;system&quot; and r._field == &quot;uptime&quot;)    |&gt; sort(columns: [&quot;region&quot;, &quot;host&quot;, &quot;_value&quot;])\n\nlimit() 函数将输出表中的记录数限制为固定数 n。以下示例最多显示过去一小时的 10 条记录。\n123from(bucket:&quot;db/rp&quot;)    |&gt; range(start:-1h)    |&gt; limit(n:10)\n\n您可以同时使用 sort() 和 limit() 来显示前 N 条记录。下面的示例返回首先按区域排序的 10 个最高系统正常运行时间值，然后是主机，然后是值。\n12345from(bucket: &quot;db/rp&quot;)    |&gt; range(start: -12h)    |&gt; filter(fn: (r) =&gt; r._measurement == &quot;system&quot; and r._field == &quot;uptime&quot;)    |&gt; sort(columns: [&quot;region&quot;, &quot;host&quot;, &quot;_value&quot;])    |&gt; limit(n: 10)\n\n您现在已经创建了一个对数据进行排序和限制的 Flux 查询。 Flux 还提供了 top() 和 bottom() 函数来同时执行这两个函数。\n4.8.4、Window &amp; aggregate使用时间序列数据执行的常见操作是将数据分组到时间窗口或“窗口化”数据，然后将窗口化值聚合成新值。本指南介绍了使用 Flux 对数据进行窗口化和聚合，并演示了数据在该过程中是如何形成的。\n以下示例深入介绍了窗口化和聚合数据所需的步骤。 aggregateWindow() 函数为您执行这些操作，但了解数据在此过程中的形成方式有助于成功创建所需的输出。\n12data    |&gt; aggregateWindow(every: 20m, fn: mean)\n\n输入：\n\n\n\n_time\n_value\n\n\n\n\n\n\n\n2020-01-01T00:00:00Z\n | 250 || \n2020-01-01T00:04:00Z\n | 160 || \n2020-01-01T00:12:00Z\n | 150 || \n2020-01-01T00:19:00Z\n | 220 || \n2020-01-01T00:32:00Z\n | 200 || \n2020-01-01T00:51:00Z\n | 290 || \n2020-01-01T01:00:00Z\n | 340 |\n输出：\n\n\n\n_time\n_value\n\n\n\n2020-01-01T00:20:00Z\n195\n\n\n2020-01-01T00:40:00Z\n200\n\n\n2020-01-01T01:00:00Z\n290\n\n\n2020-01-01T01:20:00Z\n340\n\n\n为本指南的目的，定义一个代表您的基础数据集的变量。以下示例查询主机的内存使用情况。\n1234dataSet = from(bucket: &quot;db/rp&quot;)    |&gt; range(start: -5m)    |&gt; filter(fn: (r) =&gt; r._measurement == &quot;mem&quot; and r._field == &quot;used_percent&quot;)    |&gt; drop(columns: [&quot;host&quot;])\n\n此示例从返回的数据中删除主机列，因为仅跟踪单个主机的内存数据并且它简化了输出表。删除主机列是可选的，如果监视多个主机上的内存，则不建议这样做。dataSet 现在可用于表示您的基础数据，类似于以下内容：\n12345678910111213Table: keys: [_start, _stop, _field, _measurement]                   _start:time                      _stop:time           _field:string     _measurement:string                      _time:time                  _value:float2018-11-03T17:50:00.000000000Z  2018-11-03T17:55:00.000000000Z            used_percent                     mem  2018-11-03T17:50:00.000000000Z             71.116113662719732018-11-03T17:50:00.000000000Z  2018-11-03T17:55:00.000000000Z            used_percent                     mem  2018-11-03T17:50:10.000000000Z             67.396306991577152018-11-03T17:50:00.000000000Z  2018-11-03T17:55:00.000000000Z            used_percent                     mem  2018-11-03T17:50:20.000000000Z             64.166665077209472018-11-03T17:50:00.000000000Z  2018-11-03T17:55:00.000000000Z            used_percent                     mem  2018-11-03T17:50:30.000000000Z             64.199519157409672018-11-03T17:50:00.000000000Z  2018-11-03T17:55:00.000000000Z            used_percent                     mem  2018-11-03T17:50:40.000000000Z              64.21227455139162018-11-03T17:50:00.000000000Z  2018-11-03T17:55:00.000000000Z            used_percent                     mem  2018-11-03T17:50:50.000000000Z             64.222097396850592018-11-03T17:50:00.000000000Z  2018-11-03T17:55:00.000000000Z            used_percent                     mem  2018-11-03T17:51:00.000000000Z              64.63365554809572018-11-03T17:50:00.000000000Z  2018-11-03T17:55:00.000000000Z            used_percent                     mem  2018-11-03T17:51:10.000000000Z             64.165163040161132018-11-03T17:50:00.000000000Z  2018-11-03T17:55:00.000000000Z            used_percent                     mem  2018-11-03T17:51:20.000000000Z             64.183497428894042018-11-03T17:50:00.000000000Z  2018-11-03T17:55:00.000000000Z            used_percent                     mem  2018-11-03T17:51:30.000000000Z             64.20474052429199\n\n使用 window() 函数根据时间范围对数据进行分组。与 window() 一起传递的最常见的参数是 every ，它定义了窗口之间的持续时间。every 参数支持所有有效的持续时间单位，包括日历月 (1mo) 和年 (1y)。其他参数可用，但对于此示例，将基础数据集窗口化为一分钟窗口。\n12dataSet    |&gt; window(every: 1m)\n\n12345678910111213141516171819Table: keys: [_start, _stop, _field, _measurement]                   _start:time                      _stop:time           _field:string     _measurement:string                      _time:time                  _value:float2018-11-03T17:50:00.000000000Z  2018-11-03T17:51:00.000000000Z            used_percent                     mem  2018-11-03T17:50:00.000000000Z             71.116113662719732018-11-03T17:50:00.000000000Z  2018-11-03T17:51:00.000000000Z            used_percent                     mem  2018-11-03T17:50:10.000000000Z             67.396306991577152018-11-03T17:50:00.000000000Z  2018-11-03T17:51:00.000000000Z            used_percent                     mem  2018-11-03T17:50:20.000000000Z             64.166665077209472018-11-03T17:50:00.000000000Z  2018-11-03T17:51:00.000000000Z            used_percent                     mem  2018-11-03T17:50:30.000000000Z             64.199519157409672018-11-03T17:50:00.000000000Z  2018-11-03T17:51:00.000000000Z            used_percent                     mem  2018-11-03T17:50:40.000000000Z              64.21227455139162018-11-03T17:50:00.000000000Z  2018-11-03T17:51:00.000000000Z            used_percent                     mem  2018-11-03T17:50:50.000000000Z             64.22209739685059Table: keys: [_start, _stop, _field, _measurement]                   _start:time                      _stop:time           _field:string     _measurement:string                      _time:time                  _value:float2018-11-03T17:51:00.000000000Z  2018-11-03T17:52:00.000000000Z            used_percent                     mem  2018-11-03T17:51:00.000000000Z              64.63365554809572018-11-03T17:51:00.000000000Z  2018-11-03T17:52:00.000000000Z            used_percent                     mem  2018-11-03T17:51:10.000000000Z             64.165163040161132018-11-03T17:51:00.000000000Z  2018-11-03T17:52:00.000000000Z            used_percent                     mem  2018-11-03T17:51:20.000000000Z             64.183497428894042018-11-03T17:51:00.000000000Z  2018-11-03T17:52:00.000000000Z            used_percent                     mem  2018-11-03T17:51:30.000000000Z             64.204740524291992018-11-03T17:51:00.000000000Z  2018-11-03T17:52:00.000000000Z            used_percent                     mem  2018-11-03T17:51:40.000000000Z             68.650627136230472018-11-03T17:51:00.000000000Z  2018-11-03T17:52:00.000000000Z            used_percent                     mem  2018-11-03T17:51:50.000000000Z             67.20139980316162\n\n在 InfluxDB UI 中可视化时，每个窗口表都以不同的颜色显示。\n\n聚合函数获取表中所有行的值并使用它们来执行聚合操作，结果在单行表中作为新值输出。由于窗口化数据被拆分为单独的表，聚合操作分别针对每个表运行并输出仅包含聚合值的新表。\n对于此示例，使用 mean() 函数输出每个窗口的平均值：\n123dataSet    |&gt; window(every: 1m)    |&gt; mean()\n\n123456789Table: keys: [_start, _stop, _field, _measurement]                   _start:time                      _stop:time           _field:string     _measurement:string                  _value:float2018-11-03T17:50:00.000000000Z  2018-11-03T17:51:00.000000000Z            used_percent                     mem             65.88549613952637Table: keys: [_start, _stop, _field, _measurement]                   _start:time                      _stop:time           _field:string     _measurement:string                  _value:float2018-11-03T17:51:00.000000000Z  2018-11-03T17:52:00.000000000Z            used_percent                     mem             65.50651391347249\n\n因为每个数据点都包含在自己的表中，所以在可视化时，它们会显示为单个未连接的点。\n\n请注意 _time 列不在聚合输出表中。因为每个表中的记录都聚合在一起，它们的时间戳不再适用，并且该列从组键和表中删除。另请注意 _start 和 _stop 列仍然存在。这些代表时间窗口的下限和上限。许多 Flux 函数依赖于 _time 列。要在聚合函数之后进一步处理您的数据，您需要重新添加 _time。使用 duplicate() 函数将 _start 或 _stop 列复制为新的 _time 列。\n1234dataSet    |&gt; window(every: 1m)    |&gt; mean()    |&gt; duplicate(column: &quot;_stop&quot;, as: &quot;_time&quot;)\n\n123456789Table: keys: [_start, _stop, _field, _measurement]                   _start:time                      _stop:time           _field:string     _measurement:string                      _time:time                  _value:float2018-11-03T17:50:00.000000000Z  2018-11-03T17:51:00.000000000Z            used_percent                     mem  2018-11-03T17:51:00.000000000Z             65.88549613952637Table: keys: [_start, _stop, _field, _measurement]                   _start:time                      _stop:time           _field:string     _measurement:string                      _time:time                  _value:float2018-11-03T17:51:00.000000000Z  2018-11-03T17:52:00.000000000Z            used_percent                     mem  2018-11-03T17:52:00.000000000Z             65.50651391347249\n\n将汇总值保存在单独的表中通常不是您想要的数据格式。使用 window() 函数将数据“展开”到单个无限 (inf) 窗口中。\n12345dataSet    |&gt; window(every: 1m)    |&gt; mean()    |&gt; duplicate(column: &quot;_stop&quot;, as: &quot;_time&quot;)    |&gt; window(every: inf)\n\n窗口化需要一个 _time 列，这就是为什么需要在聚合后重新创建 _time 列的原因。\n12345678910未加窗的输出列表：Table: keys: [_start, _stop, _field, _measurement]                   _start:time                      _stop:time           _field:string     _measurement:string                      _time:time                  _value:float2018-11-03T17:50:00.000000000Z  2018-11-03T17:55:00.000000000Z            used_percent                     mem  2018-11-03T17:51:00.000000000Z             65.885496139526372018-11-03T17:50:00.000000000Z  2018-11-03T17:55:00.000000000Z            used_percent                     mem  2018-11-03T17:52:00.000000000Z             65.506513913472492018-11-03T17:50:00.000000000Z  2018-11-03T17:55:00.000000000Z            used_percent                     mem  2018-11-03T17:53:00.000000000Z             65.307195981343582018-11-03T17:50:00.000000000Z  2018-11-03T17:55:00.000000000Z            used_percent                     mem  2018-11-03T17:54:00.000000000Z             64.393309752146412018-11-03T17:50:00.000000000Z  2018-11-03T17:55:00.000000000Z            used_percent                     mem  2018-11-03T17:55:00.000000000Z             64.493862787882492018-11-03T17:50:00.000000000Z  2018-11-03T17:55:00.000000000Z            used_percent                     mem  2018-11-03T17:55:00.000000000Z             64.49816226959229\n\n通过单个表中的聚合值，可视化中的数据点被连接起来。\n\n您现在已经创建了一个窗口和聚合数据的 Flux 查询。本指南中概述的数据转换过程应用于所有聚合操作。Flux 还提供了 aggregateWindow() 函数，它为您执行所有这些单独的函数。以下 Flux 查询将返回相同的结果：\n12dataSet    |&gt; aggregateWindow(every: 1m, fn: mean)\n\n4.8.5、用数学运算转换数据Flux 支持数据转换中的数学表达式。本文介绍如何使用 Flux 算术运算符“映射”数据并使用数学运算转换值。\n基本数学运算：Flux 数学运算中的操作数必须是相同的数据类型。例如，整数不能用于浮点运算。否则，您将收到类似于以下内容的错误：\n1Error: type error: float != int\n\n要将操作数转换为相同类型，请使用类型转换函数或手动格式化操作数。操作数数据类型确定输出数据类型。例如：\n12345678100 100.0 &gt; 20 / 82&gt; 20.0 / 8.02.5\n\n使用 map() 函数重新映射列值并应用数学运算。\n12data    |&gt; map(fn: (r) =&gt; (&#123; r with _value: r._value * r._value &#125;))\n\n输入：\n\n\n\n_time\n_value\n\n\n\n2020-01-01T00:01:00Z\n2\n\n\n2020-01-01T00:02:00Z\n4\n\n\n2020-01-01T00:03:00Z\n3\n\n\n2020-01-01T00:04:00Z\n5\n\n\n输出：\nFlux 允许您创建使用数学运算的自定义函数。查看以下示例。\n1234567- 自定义乘法函数：multiply = (x, y) =&gt; x * ymultiply(x: 10, y: 12)- 自定义百分比函数percent = (sample, total) =&gt; (sample / total) * 100.0percent(sample: 20.0, total: 80.0) \n\n要转换输入流中的多个值，您的函数需要：\n\n处理管道转发数据。\n计算所需的每个操作数都存在于每一行中（参见下面的 Pivot vs join）。\n使用 map() 函数遍历每一行。\n\n下面的示例 multiplyByX() 函数包括：\n\n表示输入数据流 (&lt;-) 的表参数。\n一个 x 参数，它是 _value 列中的值相乘的数字。\n遍历输入流中每一行的 map() 函数。它使用 with 运算符来保留每行中的现有列。它还将 _value 列乘以 x。\n\n12345multiplyByX = (x, tables=&lt;-) =&gt; tables    |&gt; map(fn: (r) =&gt; (&#123;r with _value: r._value * x&#125;))data    |&gt; multiplyByX(x: 10)\n\n例：将Byte转换为GB。要将活动内存从字节转换为千兆字节 (GB)，请将内存测量中的活动字段除以 1,073,741,824。map() 函数遍历管道转发数据中的每一行，并通过将原始 _value 除以 1073741824 来定义新的 _value。\n1234from(bucket: &quot;db/rp&quot;)    |&gt; range(start: -10m)    |&gt; filter(fn: (r) =&gt; r._measurement == &quot;mem&quot; and r._field == &quot;active&quot;)    |&gt; map(fn: (r) =&gt; (&#123;r with _value: r._value / 1073741824&#125;))\n\n您可以将相同的计算转换为函数：\n12345bytesToGB = (tables=&lt;-) =&gt; tables    |&gt; map(fn: (r) =&gt; (&#123;r with _value: r._value / 1073741824&#125;))data    |&gt; bytesToGB()\n\n因为原始度量（字节）是整数，所以操作的输出是整数并且不包括部分 GB。要计算部分 GB，请使用 float() 函数将 _value 列及其值转换为浮点数，并将除法运算中的分母格式化为浮点数。\n12bytesToGB = (tables=&lt;-) =&gt; tables    |&gt; map(fn: (r) =&gt; (&#123;r with _value: float(v: r._value) / 1073741824.0&#125;))\n\n要计算百分比，请使用简单除法，然后将结果乘以 100。\n12&gt; 1.0 / 4.0 * 100.025.0\n\n**pivot() &amp; join()**：要在 Flux 中查询和使用数学运算中的值，操作数值必须存在于单行中。 pivot() 和 join() 都会这样做，但两者之间有重要区别：\n\npivot性能更高：pivot() 读取和操作单个数据流。 join() 需要两个数据流，读取和组合两个数据流的开销可能很大，尤其是对于较大的数据集。\n对多个数据源使用join()：查询来自不同存储桶或数据源的数据时使用 join()。\n\n将字段透视到列中以进行数学运算：\n123data    |&gt; pivot(rowKey: [&quot;_time&quot;], columnKey: [&quot;_field&quot;], valueColumn: &quot;_value&quot;)    |&gt; map(fn: (r) =&gt; (&#123;r with _value: (r.field1 + r.field2) / r.field3 * 100.0&#125;))\n\n加入多个数据源进行数学运算：\n12345678910111213141516171819import &quot;sql&quot;import &quot;influxdata/influxdb/secrets&quot;pgUser = secrets.get(key: &quot;POSTGRES_USER&quot;)pgPass = secrets.get(key: &quot;POSTGRES_PASSWORD&quot;)pgHost = secrets.get(key: &quot;POSTGRES_HOST&quot;)t1 = sql.from(    driverName: &quot;postgres&quot;,    dataSourceName: &quot;postgresql://$&#123;pgUser&#125;:$&#123;pgPass&#125;@$&#123;pgHost&#125;&quot;,    query: &quot;SELECT id, name, available FROM exampleTable&quot;,)t2 = from(bucket: &quot;db/rp&quot;)    |&gt; range(start: -1h)    |&gt; filter(fn: (r) =&gt; r._measurement == &quot;example-measurement&quot; and r._field == &quot;example-field&quot;)join(tables: &#123;t1: t1, t2: t2&#125;, on: [&quot;id&quot;])    |&gt; map(fn: (r) =&gt; (&#123;r with _value: r._value_t2 / r.available_t1 * 100.0&#125;))\n\n4.8.6、计算百分比从查询数据计算百分比是时间序列数据的常见用例。要计算Flux中的百分比，操作数必须在每一行中。使用 map() 重新映射行中的值并计算百分比。\n计算百分比： \n\n使用 from()、range() 和 filter() 来查询操作数。\n使用 pivot() 或 join() 将操作数值对齐到行中。\n使用 map() 将分子操作数值除以分母操作数值并乘以 100。\n\n以下示例使用 pivot() 将操作数对​​齐到行中，因为 pivot() 在大多数情况下都有效，并且比 join() 性能更高。\n123456789data  |&gt; pivot(rowKey:[&quot;_time&quot;], columnKey: [&quot;_field&quot;], valueColumn: &quot;_value&quot;)  |&gt; map(      fn: (r) =&gt; (&#123;          _time: r._time,          _field: &quot;used_percent&quot;,          _value: float(v: r.used) / float(v: r.total) * 100.0,      &#125;),  )\n\n输入：\n\n\n\n_time_时间\n_field_场地\n_value_价值\n\n\n\n2020-01-01T00:00:00Z2020-01-01T00:00:00Z\nused用过的\n2.5\n\n\n2020-01-01T00:00:10Z2020-01-01T00:00:10Z\nused用过的\n3.1\n\n\n2020-01-01T00:00:20Z2020-01-01T00:00:20Z\nused用过的\n4.2\n\n\n\n\n\n_time_时间\n_field_场地\n_value_价值\n\n\n\n2020-01-01T00:00:00Z2020-01-01T00:00:00Z\ntotal全部的\n8.0\n\n\n2020-01-01T00:00:10Z2020-01-01T00:00:10Z\ntotal全部的\n8.0\n\n\n2020-01-01T00:00:20Z2020-01-01T00:00:20Z\ntotal全部的\n8.0\n\n\n输出：\n\n\n\n_time_时间\n_field_场地\n_value_价值\n\n\n\n2020-01-01T00:00:00Z2020-01-01T00:00:00Z\nused_percentused_percent\n31.25\n\n\n2020-01-01T00:00:10Z2020-01-01T00:00:10Z\nused_percentused_percent\n38.75\n\n\n2020-01-01T00:00:20Z2020-01-01T00:00:20Z\nused_percentused_percent\n52.50\n\n\n示例1：以下示例从 gpu-monitor 存储桶中查询数据并计算一段时间内使用的 GPU 内存百分比。数据包括以下内容：\n\ngpu 测量；\nmem_used 字段：使用的 GPU 内存（以字节为单位）\nmem_total 字段：以字节为单位的总 GPU 内存\n\n查询mem_used和mem_total字段：\n123from(bucket: &quot;gpu-monitor&quot;)    |&gt; range(start: 2020-01-01T00:00:00Z)    |&gt; filter(fn: (r) =&gt; r._measurement == &quot;gpu&quot; and r._field =~ /mem_/)\n\n返回以下两个表流：\n\n\n\n_time\n_measurement\n_field\n_value\n\n\n\n2020-01-01T00:00:00Z\ngpu\nmem_used\n2517924577\n\n\n2020-01-01T00:00:10Z\ngpu\nmem_used\n2695091978\n\n\n2020-01-01T00:00:20Z\ngpu\nmem_used\n2576980377\n\n\n2020-01-01T00:00:30Z\ngpu\nmem_used\n3006477107\n\n\n2020-01-01T00:00:40Z\ngpu\nmem_used\n3543348019\n\n\n2020-01-01T00:00:50Z\ngpu\nmem_used\n4402341478\n\n\n\n\n\n_time\n_measurement\n_field\n_value\n\n\n\n2020-01-01T00:00:00Z\ngpu\nmem_total\n8589934592\n\n\n2020-01-01T00:00:10Z\ngpu\nmem_total\n8589934592\n\n\n2020-01-01T00:00:20Z\ngpu\nmem_total\n8589934592\n\n\n2020-01-01T00:00:30Z\ngpu\nmem_total\n8589934592\n\n\n2020-01-01T00:00:40Z\ngpu\nmem_total\n8589934592\n\n\n2020-01-01T00:00:50Z\ngpu\nmem_total\n8589934592\n\n\n使用 pivot() 将 mem_used 和 mem_total 字段转换为列。输出包括 mem_used 和 mem_total 列，其中包含每个对应 _time 的值。\n1|&gt; pivot(rowKey:[&quot;_time&quot;], columnKey: [&quot;_field&quot;], valueColumn: &quot;_value&quot;)\n\n\n\n\n_time\n_measurement\nmem_used\nmem_total\n\n\n\n2020-01-01T00:00:00Z\ngpu\n2517924577\n8589934592\n\n\n2020-01-01T00:00:10Z\ngpu\n2695091978\n8589934592\n\n\n2020-01-01T00:00:20Z\ngpu\n2576980377\n8589934592\n\n\n2020-01-01T00:00:30Z\ngpu\n3006477107\n8589934592\n\n\n2020-01-01T00:00:40Z\ngpu\n3543348019\n8589934592\n\n\n2020-01-01T00:00:50Z\ngpu\n4402341478\n8589934592\n\n\n现在每一行都包含计算百分比所需的值。使用 map() 重新映射每行中的值。将 mem_used 除以 mem_total 并乘以 100 以返回百分比。（要返回包含小数点的精确浮点百分比值，下面的示例将整数字段值转换为浮点数并乘以浮点值 (100.0)。）\n12345678|&gt; map(       fn: (r) =&gt; (&#123;           _time: r._time,           _measurement: r._measurement,           _field: &quot;mem_used_percent&quot;,           _value: float(v: r.mem_used) / float(v: r.mem_total) * 100.0       &#125;)   )\n\n查询结果：\n\n\n\n_time\n_measurement\n_field\n_value\n\n\n\n2020-01-01T00:00:00Z\ngpu\nmem_used_percent\n29.31\n\n\n2020-01-01T00:00:10Z\ngpu\nmem_used_percent\n31.37\n\n\n2020-01-01T00:00:20Z\ngpu\nmem_used_percent\n30.00\n\n\n2020-01-01T00:00:30Z\ngpu\nmem_used_percent\n35.00\n\n\n2020-01-01T00:00:40Z\ngpu\nmem_used_percent\n41.25\n\n\n2020-01-01T00:00:50Z\ngpu\nmem_used_percent\n51.25\n\n\n完整查询：\n123456789101112from(bucket: &quot;gpu-monitor&quot;)    |&gt; range(start: 2020-01-01T00:00:00Z)    |&gt; filter(fn: (r) =&gt; r._measurement == &quot;gpu&quot; and r._field =~ /mem_/ )    |&gt; pivot(rowKey:[&quot;_time&quot;], columnKey: [&quot;_field&quot;], valueColumn: &quot;_value&quot;)    |&gt; map(        fn: (r) =&gt; (&#123;            _time: r._time,            _measurement: r._measurement,            _field: &quot;mem_used_percent&quot;,            _value: float(v: r.mem_used) / float(v: r.mem_total) * 100.0        &#125;)    )\n\n示例2：使用多个字段计算百分比。\n123456from(bucket: &quot;db/rp&quot;)    |&gt; range(start: -1h)    |&gt; filter(fn: (r) =&gt; r._measurement == &quot;example-measurement&quot;)    |&gt; filter(fn: (r) =&gt; r._field == &quot;used_system&quot; or r._field == &quot;used_user&quot; or r._field == &quot;total&quot;)    |&gt; pivot(rowKey: [&quot;_time&quot;], columnKey: [&quot;_field&quot;], valueColumn: &quot;_value&quot;)    |&gt; map(fn: (r) =&gt; (&#123;r with _value: float(v: r.used_system + r.used_user) / float(v: r.total) * 100.0&#125;))\n\n示例3：使用多个测量计算百分比：\n\n确保测量值在同一个桶中。\n使用 filter() 包含来自两个测量的数据。\n使用 group() 取消分组数据并返回单个表。\n使用 pivot() 将字段旋转到列中。\n使用 map() 重新映射行并执行百分比计算。\n\n123456from(bucket: &quot;db/rp&quot;)    |&gt; range(start: -1h)    |&gt; filter(fn: (r) =&gt; (r._measurement == &quot;m1&quot; or r._measurement == &quot;m2&quot;) and (r._field == &quot;field1&quot; or r._field == &quot;field2&quot;))    |&gt; group()    |&gt; pivot(rowKey: [&quot;_time&quot;], columnKey: [&quot;_field&quot;], valueColumn: &quot;_value&quot;)    |&gt; map(fn: (r) =&gt; (&#123;r with _value: r.field1 / r.field2 * 100.0&#125;))\n\n示例4：使用多个数据源计算百分比。\n12345678910111213141516171819import &quot;sql&quot;import &quot;influxdata/influxdb/secrets&quot;pgUser = secrets.get(key: &quot;POSTGRES_USER&quot;)pgPass = secrets.get(key: &quot;POSTGRES_PASSWORD&quot;)pgHost = secrets.get(key: &quot;POSTGRES_HOST&quot;)t1 = sql.from(    driverName: &quot;postgres&quot;,    dataSourceName: &quot;postgresql://$&#123;pgUser&#125;:$&#123;pgPass&#125;@$&#123;pgHost&#125;&quot;,    query: &quot;SELECT id, name, available FROM exampleTable&quot;,)t2 = from(bucket: &quot;db/rp&quot;)    |&gt; range(start: -1h)    |&gt; filter(fn: (r) =&gt; r._measurement == &quot;example-measurement&quot; and r._field == &quot;example-field&quot;)join(tables: &#123;t1: t1, t2: t2&#125;, on: [&quot;id&quot;])    |&gt; map(fn: (r) =&gt; (&#123;r with _value: r._value_t2 / r.available_t1 * 100.0&#125;))\n\n4.8.7、increase()：跟踪表中多个列的增加当跟踪随时间推移或定期重置的计数器值的变化时，此功能特别有用。increase() 返回表中行之间非负差异的累积总和。\n12data    |&gt; increase()\n\n输入：\n\n\n\n_time\n_value\n\n\n\n2020-01-01T00:01:00Z\n1\n\n\n2020-01-01T00:02:00Z\n2\n\n\n2020-01-01T00:03:00Z\n8\n\n\n2020-01-01T00:04:00Z\n10\n\n\n2020-01-01T00:05:00Z\n0\n\n\n2020-01-01T00:06:00Z\n4\n\n\n输出：\n\n\n\n_time\n_value\n\n\n\n2020-01-01T00:02:00Z\n1\n\n\n2020-01-01T00:03:00Z\n7\n\n\n2020-01-01T00:04:00Z\n9\n\n\n2020-01-01T00:05:00Z\n9\n\n\n2020-01-01T00:06:00Z\n13\n\n\n4.8.8、movingAverage() &amp; timedMovingAverage() ：移动平均值使用movingAverage() 或timedMovingAverage() 函数返回数据的移动平均值：\n\nmovingAverage()：对于表中的每一行，movingAverage() 返回当前值和先前值的平均值，其中 n 是用于计算平均值的值的总数。\n\ntimedMovingAverage()：返回当前值和上一周期（持续时间）中所有行值的平均值。它以每个参数定义的频率返回移动平均值。下图中的每种颜色代表用于计算平均值的时间段以及返回表示平均值的点的时间。如果every &#x3D; 30m 且period &#x3D; 1h：\n\n\n\n12data    |&gt; movingAverage(n: 3)\n\n输入：\n\n\n\n_time\n_value\n\n\n\n2020-01-01T00:01:00Z\n1.0\n\n\n2020-01-01T00:02:00Z\n1.2\n\n\n2020-01-01T00:03:00Z\n1.8\n\n\n2020-01-01T00:04:00Z\n0.9\n\n\n2020-01-01T00:05:00Z\n1.4\n\n\n2020-01-01T00:06:00Z\n2.0\n\n\n输出：\n\n\n\n_time\n_value\n\n\n\n2020-01-01T00:03:00Z\n1.33\n\n\n2020-01-01T00:04:00Z\n1.30\n\n\n2020-01-01T00:05:00Z\n1.36\n\n\n2020-01-01T00:06:00Z\n1.43\n\n\n12data    |&gt; timedMovingAverage(every: 2m, period: 4m)\n\n输入：\n\n\n\n_time\n_value\n\n\n\n2020-01-01T00:01:00Z\n1.0\n\n\n2020-01-01T00:02:00Z\n1.2\n\n\n2020-01-01T00:03:00Z\n1.8\n\n\n2020-01-01T00:04:00Z\n0.9\n\n\n2020-01-01T00:05:00Z\n1.4\n\n\n2020-01-01T00:06:00Z\n2.0\n\n\n输出：\n\n\n\n_time\n_value\n\n\n\n2020-01-01T00:02:00Z\n1.000\n\n\n2020-01-01T00:04:00Z\n1.333\n\n\n2020-01-01T00:06:00Z\n1.325\n\n\n2020-01-01T00:06:00Z\n1.150\n\n\n4.8.9、计算变化率使用 derivative() 函数计算后续值之间的变化率，或使用 aggregate.rate() 函数计算每个时间窗口的平均变化率。如果点之间的时间发生变化，这些函数会将点标准化为一个共同的时间间隔，从而使值易于比较。\nderivative()：计算后续非空值之间每单位时间的变化率。默认情况下，derivative() 仅返回正导数值，并将负值替换为 null。计算值作为浮点数返回。\n12data    |&gt; derivative(unit: 1m, nonNegative: true)\n\n输入：\n\n\n\n_time\n_value\n\n\n\n2020-01-01T00:00:00Z\n250\n\n\n2020-01-01T00:04:00Z\n160\n\n\n2020-01-01T00:12:00Z\n150\n\n\n2020-01-01T00:19:00Z\n220\n\n\n2020-01-01T00:32:00Z\n200\n\n\n2020-01-01T00:51:00Z\n290\n\n\n2020-01-01T01:00:00Z\n340\n\n\n输出：\n\n\n\n_time\n_value\n\n\n\n2020-01-01T00:04:00Z\n\n\n\n2020-01-01T00:12:00Z\n\n\n\n2020-01-01T00:19:00Z\n10.0\n\n\n2020-01-01T00:32:00Z\n\n\n\n2020-01-01T00:51:00Z\n4.74\n\n\n2020-01-01T01:00:00Z\n5.56\n\n\n要返回负导数值，请将 nonNegative 参数设置为 false。\n1|&gt; derivative(unit: 1m, nonNegative: false)\n\n输入：\n\n\n\n_time\n_value\n\n\n\n2020-01-01T00:00:00Z\n250\n\n\n2020-01-01T00:04:00Z\n160\n\n\n2020-01-01T00:12:00Z\n150\n\n\n2020-01-01T00:19:00Z\n220\n\n\n2020-01-01T00:32:00Z\n200\n\n\n2020-01-01T00:51:00Z\n290\n\n\n2020-01-01T01:00:00Z\n340\n\n\n输出：\n\n\n\n_time\n_value\n\n\n\n2020-01-01T00:04:00Z\n-22.5\n\n\n2020-01-01T00:12:00Z\n-1.25\n\n\n2020-01-01T00:19:00Z\n10.0\n\n\n2020-01-01T00:32:00Z\n-1.54\n\n\n2020-01-01T00:51:00Z\n4.74\n\n\n2020-01-01T01:00:00Z\n5.56\n\n\n使用 aggregate.rate() 函数计算每个时间窗口的平均变化率。aggregate.rate() 返回由 every 定义的时间间隔的每单位平均变化率（作为浮点数）。负值替换为 null。aggregate.rate() 不支持 nonNegative: false。\n1234import &quot;experimental/aggregate&quot;data    |&gt; aggregate.rate(every: 20m, unit: 1m)\n\n输入：\n\n\n\n_time\n_value\n\n\n\n2020-01-01T00:00:00Z\n250\n\n\n2020-01-01T00:04:00Z\n160\n\n\n2020-01-01T00:12:00Z\n150\n\n\n2020-01-01T00:19:00Z\n220\n\n\n2020-01-01T00:32:00Z\n200\n\n\n2020-01-01T00:51:00Z\n290\n\n\n2020-01-01T01:00:00Z\n340\n\n\n输出：\n\n\n\n_time\n_value\n\n\n\n2020-01-01T00:20:00Z\n\n\n\n2020-01-01T00:40:00Z\n10.0\n\n\n2020-01-01T01:00:00Z\n4.74\n\n\n2020-01-01T01:20:00Z\n5.56\n\n\n4.8.10、使用Flux创建直方图直方图为您的数据分布提供了宝贵的洞察力。本指南介绍如何使用 Flux 的 histogram() 函数将数据转换为累积直方图。\nhistogram()函数：通过计算“bins”列表的数据频率来近似数据集的累积分布。 bin 只是数据点所在的范围。所有小于或等于边界的数据点都计入 bin。在直方图输出中，添加了一列 (le)，表示每个 bin 的上限。 Bin 计数是累积的。\nFlux 提供了两个辅助函数来生成直方图 的bins。每个都生成并输出一个浮点数组，设计用于 histogram() 函数的 bins 参数。\n\nlinearBins()：生成线性分隔的浮点数列表。&#96;&#96;&#96;linearBins(start: 0.0, width: 10.0, count: 10) 123    *   logarithmicBins()：生成一个以指数方式分隔的浮点数列表。```    logarithmicBins(start: 1.0, factor: 2.0, count: 10, infinty: true) \n\n示例1：\n1234567data    |&gt; histogram(        column: &quot;_value&quot;,        upperBoundColumn: &quot;le&quot;,        countColumn: &quot;_value&quot;,        bins: [100.0, 200.0, 300.0, 400.0],    )\n\n输入：\n\n\n\n_time\n_value\n\n\n\n2020-01-01T00:00:00Z\n250.0\n\n\n2020-01-01T00:01:00Z\n160.0\n\n\n2020-01-01T00:02:00Z\n150.0\n\n\n2020-01-01T00:03:00Z\n220.0\n\n\n2020-01-01T00:04:00Z\n200.0\n\n\n2020-01-01T00:05:00Z\n290.0\n\n\n2020-01-01T01:00:00Z\n340.0\n\n\n输出：\n\n\n\nle\n_value\n\n\n\n100.0\n0.0\n\n\n200.0\n3.0\n\n\n300.0\n6.0\n\n\n400.0\n7.0\n\n\n示例2：使用线性bin生成直方图。\n1234from(bucket: &quot;telegraf/autogen&quot;)    |&gt; range(start: -5m)    |&gt; filter(fn: (r) =&gt; r._measurement == &quot;mem&quot; and r._field == &quot;used_percent&quot;)    |&gt; histogram(bins: linearBins(start: 65.5, width: 0.5, count: 20, infinity: false))\n\n输出表：\n1234567891011121314151617181920212223Table: keys: [_start, _stop, _field, _measurement, host]                   _start:time                      _stop:time           _field:string     _measurement:string               host:string                      le:float                  _value:float2018-11-07T22:19:58.423658000Z  2018-11-07T22:24:58.423658000Z            used_percent                     mem  Scotts-MacBook-Pro.local                          65.5                             52018-11-07T22:19:58.423658000Z  2018-11-07T22:24:58.423658000Z            used_percent                     mem  Scotts-MacBook-Pro.local                            66                             62018-11-07T22:19:58.423658000Z  2018-11-07T22:24:58.423658000Z            used_percent                     mem  Scotts-MacBook-Pro.local                          66.5                             82018-11-07T22:19:58.423658000Z  2018-11-07T22:24:58.423658000Z            used_percent                     mem  Scotts-MacBook-Pro.local                            67                             92018-11-07T22:19:58.423658000Z  2018-11-07T22:24:58.423658000Z            used_percent                     mem  Scotts-MacBook-Pro.local                          67.5                             92018-11-07T22:19:58.423658000Z  2018-11-07T22:24:58.423658000Z            used_percent                     mem  Scotts-MacBook-Pro.local                            68                            102018-11-07T22:19:58.423658000Z  2018-11-07T22:24:58.423658000Z            used_percent                     mem  Scotts-MacBook-Pro.local                          68.5                            122018-11-07T22:19:58.423658000Z  2018-11-07T22:24:58.423658000Z            used_percent                     mem  Scotts-MacBook-Pro.local                            69                            122018-11-07T22:19:58.423658000Z  2018-11-07T22:24:58.423658000Z            used_percent                     mem  Scotts-MacBook-Pro.local                          69.5                            152018-11-07T22:19:58.423658000Z  2018-11-07T22:24:58.423658000Z            used_percent                     mem  Scotts-MacBook-Pro.local                            70                            232018-11-07T22:19:58.423658000Z  2018-11-07T22:24:58.423658000Z            used_percent                     mem  Scotts-MacBook-Pro.local                          70.5                            302018-11-07T22:19:58.423658000Z  2018-11-07T22:24:58.423658000Z            used_percent                     mem  Scotts-MacBook-Pro.local                            71                            302018-11-07T22:19:58.423658000Z  2018-11-07T22:24:58.423658000Z            used_percent                     mem  Scotts-MacBook-Pro.local                          71.5                            302018-11-07T22:19:58.423658000Z  2018-11-07T22:24:58.423658000Z            used_percent                     mem  Scotts-MacBook-Pro.local                            72                            302018-11-07T22:19:58.423658000Z  2018-11-07T22:24:58.423658000Z            used_percent                     mem  Scotts-MacBook-Pro.local                          72.5                            302018-11-07T22:19:58.423658000Z  2018-11-07T22:24:58.423658000Z            used_percent                     mem  Scotts-MacBook-Pro.local                            73                            302018-11-07T22:19:58.423658000Z  2018-11-07T22:24:58.423658000Z            used_percent                     mem  Scotts-MacBook-Pro.local                          73.5                            302018-11-07T22:19:58.423658000Z  2018-11-07T22:24:58.423658000Z            used_percent                     mem  Scotts-MacBook-Pro.local                            74                            302018-11-07T22:19:58.423658000Z  2018-11-07T22:24:58.423658000Z            used_percent                     mem  Scotts-MacBook-Pro.local                          74.5                            302018-11-07T22:19:58.423658000Z  2018-11-07T22:24:58.423658000Z            used_percent                     mem  Scotts-MacBook-Pro.local                            75                            30\n\n示例3：使用对数bin生成直方图。\n1234from(bucket: &quot;telegraf/autogen&quot;)    |&gt; range(start: -5m)    |&gt; filter(fn: (r) =&gt; r._measurement == &quot;mem&quot; and r._field == &quot;used_percent&quot;)    |&gt; histogram(bins: logarithmicBins(start: 0.5, factor: 2.0, count: 10, infinity: false))\n\n输出表：\n12345678910111213Table: keys: [_start, _stop, _field, _measurement, host]                   _start:time                      _stop:time           _field:string     _measurement:string               host:string                      le:float                  _value:float2018-11-07T22:23:36.860664000Z  2018-11-07T22:28:36.860664000Z            used_percent                     mem  Scotts-MacBook-Pro.local                           0.5                             02018-11-07T22:23:36.860664000Z  2018-11-07T22:28:36.860664000Z            used_percent                     mem  Scotts-MacBook-Pro.local                             1                             02018-11-07T22:23:36.860664000Z  2018-11-07T22:28:36.860664000Z            used_percent                     mem  Scotts-MacBook-Pro.local                             2                             02018-11-07T22:23:36.860664000Z  2018-11-07T22:28:36.860664000Z            used_percent                     mem  Scotts-MacBook-Pro.local                             4                             02018-11-07T22:23:36.860664000Z  2018-11-07T22:28:36.860664000Z            used_percent                     mem  Scotts-MacBook-Pro.local                             8                             02018-11-07T22:23:36.860664000Z  2018-11-07T22:28:36.860664000Z            used_percent                     mem  Scotts-MacBook-Pro.local                            16                             02018-11-07T22:23:36.860664000Z  2018-11-07T22:28:36.860664000Z            used_percent                     mem  Scotts-MacBook-Pro.local                            32                             02018-11-07T22:23:36.860664000Z  2018-11-07T22:28:36.860664000Z            used_percent                     mem  Scotts-MacBook-Pro.local                            64                             22018-11-07T22:23:36.860664000Z  2018-11-07T22:28:36.860664000Z            used_percent                     mem  Scotts-MacBook-Pro.local                           128                            302018-11-07T22:23:36.860664000Z  2018-11-07T22:28:36.860664000Z            used_percent                     mem  Scotts-MacBook-Pro.local                           256                            30\n\n4.8.11、填充使用 fill() 函数将空值替换为：\n\n前一个非空值：要使用前一个非空值填充空值，请将 usePrevious 参数设置为 true。&#96;&#96;&#96;data|&gt; fill(usePrevious: true)1234    *   一个指定的值。```    data        |&gt; fill(value: 0.0)\n\nfill() 函数不会填充空的时间窗口。它只替换现有数据中的空值。填充空的时间窗口需要时间插值（参见 influxdata&#x2F;flux#2428）。\n12data    |&gt; fill(usePrevious: true)\n\n输入：\n\n\n\n_time\n_value\n\n\n\n2020-01-01T00:01:00Z\nnull\n\n\n2020-01-01T00:02:00Z\n0.8\n\n\n2020-01-01T00:03:00Z\nnull\n\n\n2020-01-01T00:04:00Z\nnull\n\n\n2020-01-01T00:05:00Z\n1.4\n\n\n输出：\n\n\n\n_time\n_value\n\n\n\n2020-01-01T00:01:00Z\nnull\n\n\n2020-01-01T00:02:00Z\n0.8\n\n\n2020-01-01T00:03:00Z\n0.8\n\n\n2020-01-01T00:04:00Z\n0.8\n\n\n2020-01-01T00:05:00Z\n1.4\n\n\n12data    |&gt; fill(value: 0.0)\n\n输入：\n\n\n\n_time\n_value\n\n\n\n2020-01-01T00:01:00Z\nnull\n\n\n2020-01-01T00:02:00Z\n0.8\n\n\n2020-01-01T00:03:00Z\nnull\n\n\n2020-01-01T00:04:00Z\nnull\n\n\n2020-01-01T00:05:00Z\n1.4\n\n\n输出：\n\n\n\n_time\n_value\n\n\n\n2020-01-01T00:01:00Z\n0.0\n\n\n2020-01-01T00:02:00Z\n0.8\n\n\n2020-01-01T00:03:00Z\n0.0\n\n\n2020-01-01T00:04:00Z\n0.0\n\n\n2020-01-01T00:05:00Z\n1.4\n\n\n4.8.12、查找中值使用 median() 函数返回一个表示输入数据的 0.5 分位数（第 50 个百分位数）或中位数的值。\n选择以下方法之一来计算中位数：\n\nestimate_tdigest（估计）：（默认）一种聚合方法，它使用 t-digest 数据结构来计算大型数据源的准确 0.5 分位数估计。输出表由包含计算中位数的单行组成。使用默认方法“estimate_tdigest”返回表中包含表中第 50 个百分位数据的值的所有行。&#96;&#96;&#96;data|&gt; median()\n1234567891011        | _time | _value |    | --- | --- |    | 2020-01-01T00:01:00Z | 1.0 |    | 2020-01-01T00:02:00Z | 1.0 |    | 2020-01-01T00:03:00Z | 2.0 |    | 2020-01-01T00:04:00Z | 3.0 |    *   exact\\_mean（精确平均）：一种聚合方法，取最接近 0.5 分位数的两个点的平均值。输出表由包含计算中位数的单行组成。使用exact\\_mean 方法返回每个输入表的单行，其中包含最接近表中数据的数学中位数的两个值的平均值。```    data        |&gt; median(method: &quot;exact_mean&quot;)\n\n\n\n\n_time\n_value\n\n\n\n2020-01-01T00:01:00Z\n1.0\n\n\n2020-01-01T00:02:00Z\n1.0\n\n\n2020-01-01T00:03:00Z\n2.0\n\n\n2020-01-01T00:04:00Z\n3.0\n\n\n\nexact_selector（精确选择器）：一种选择器方法，它返回至少 50% 的点小于的数据点。输出表由包含计算中位数的单行组成。&#96;&#96;&#96;data|&gt; median(method: “exact_selector”)\n123456789101112131415        | _time | _value |    | --- | --- |    | 2020-01-01T00:01:00Z | 1.0 |    | 2020-01-01T00:02:00Z | 1.0 |    | 2020-01-01T00:03:00Z | 2.0 |    | 2020-01-01T00:04:00Z | 3.0 |        | _time | _value |    | --- | --- |    | 2020-01-01T00:02:00Z | 1.0 |    将median() 和 aggregateWindow() 一起使用：aggregateWindow() 将数据分割成时间窗口，将每个窗口中的数据聚合成一个点，然后去除基于时间的分割。它主要用于对数据进行下采样。要在 aggregateWindow() 中指定中值计算方法，请使用完整的函数语法：\ndata|&gt; aggregateWindow(every: 5m, fn: (tables&#x3D;&lt;-, column) &#x3D;&gt; tables |&gt; median(method: “exact_selector”))\n1234567891011121314151617181920212223242526272829303132333435363738394041### 4.8.13、查找百分位数和分位数使用 quantile() 函数返回输入数据的 q 分位数或百分位数内的所有值。百分位数和分位数非常相似，只是用于计算返回值的数字不同。百分位数使用 0 到 100 之间的数字计算。分位数使用 0.0 和 1.0 之间的数字计算。例如，0.5 分位数与第 50 个百分位数相同。选择以下方法之一来计算分位数：*   estimate_tdigest（估计）：（默认）一种聚合方法，它使用 t-digest 数据结构来计算大型数据源的分位数估计。输出表由包含计算的分位数的单行组成。如果计算 0.5 分位数或第 50 个百分位数：        | _time | _value |    | --- | --- |    | 2020-01-01T00:01:00Z | 1.0 |    | 2020-01-01T00:02:00Z | 1.0 |    | 2020-01-01T00:03:00Z | 2.0 |    | 2020-01-01T00:04:00Z | 3.0 |    *   exact_mean（精确平均）：一种聚合方法，取最接近分位数的两个点的平均值。输出表由包含计算的分位数的单行组成。如果计算 0.5 分位数或第 50 个百分位数：        | _time | _value |    | --- | --- |    | 2020-01-01T00:01:00Z | 1.0 |    | 2020-01-01T00:02:00Z | 1.0 |    | 2020-01-01T00:03:00Z | 2.0 |    | 2020-01-01T00:04:00Z | 3.0 |    *   exact_selector（精确选择器）：一个选择器方法，它返回至少 q 个点小于的数据点。输出表由包含计算的分位数的单行组成。如果计算 0.5 分位数或第 50 个百分位数：        | _time | _value |    | --- | --- |    | 2020-01-01T00:01:00Z | 1.0 |    | 2020-01-01T00:02:00Z | 1.0 |    | 2020-01-01T00:03:00Z | 2.0 |    | 2020-01-01T00:04:00Z | 3.0 |        | _time | _value |    | --- | --- |    | 2020-01-01T00:02:00Z | 1.0 |    使用默认方法“estimate_tdigest”返回表中包含表中第 99 个百分位数据的值的所有行：\ndata|&gt; quantile(q: 0.99)\n123使用exact_mean 方法返回每个输入表的单行，其中包含最接近表中数据的数学分位数的两个值的平均值。例如，要计算 0.99 分位数：\ndata|&gt; quantile(q: 0.99, method: “exact_mean”)\n123使用exact_selector 方法为每个输入表返回一行，其中包含表中q * 100% 的值小于的值。例如，要计算 0.99 分位数：\ndata|&gt; quantile(q: 0.99, method: “exact_selector”)\n123使用 quantile() 和 aggregateWindow()：aggregateWindow() 将数据分割成时间窗口，将每个窗口中的数据聚合成一个点，然后去除基于时间的分割。它主要用于对数据进行下采样。要在 aggregateWindow() 中指定分位数计算方法，请使用完整的函数语法：\ndata|&gt; aggregateWindow(every: 5m,fn: (tables&#x3D;&lt;-, column) &#x3D;&gt; tables    |&gt; quantile(q: 0.99, method: “exact_selector”),)\n1234567891011### 4.8.14、Join（加入）：使用Flux连接数据join() 函数将两个或多个输入流（其值在一组公共列上相等）合并为单个输出流。 Flux 允许您连接两个数据流之间共有的任何列，并为交叉测量连接和跨测量数学等操作打开了大门。为了说明连接操作，使用 Telegraf 捕获并存储在 InfluxDB 中的数据 - 内存使用和进程。在本指南中，我们将连接两个数据流，一个代表内存使用量，另一个代表正在运行的进程总数，然后计算每个正在运行的进程的平均内存使用量。**为了执行连接，您必须有两个数据流**。为每个数据流分配一个变量。*   定义一个 memUsed 变量，用于过滤 mem 度量和 used 字段。这将返回使用的内存量（以字节为单位）。```    memUsed = from(bucket: &quot;db/rp&quot;)        |&gt; range(start: -5m)        |&gt; filter(fn: (r) =&gt; r._measurement == &quot;mem&quot; and r._field == &quot;used&quot;)\n\n123456789101112131415161718Table: keys: [_start, _stop, _field, _measurement, host]                   _start:time                      _stop:time           _field:string     _measurement:string               host:string                      _time:time                  _value:int2018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z                    used                     mem               host1.local  2018-11-06T05:50:00.000000000Z                 109563330562018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z                    used                     mem               host1.local  2018-11-06T05:50:10.000000000Z                 110140088322018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z                    used                     mem               host1.local  2018-11-06T05:50:20.000000000Z                 113734287362018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z                    used                     mem               host1.local  2018-11-06T05:50:30.000000000Z                 110014218242018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z                    used                     mem               host1.local  2018-11-06T05:50:40.000000000Z                 109858529282018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z                    used                     mem               host1.local  2018-11-06T05:50:50.000000000Z                 109922795522018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z                    used                     mem               host1.local  2018-11-06T05:51:00.000000000Z                 110535680002018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z                    used                     mem               host1.local  2018-11-06T05:51:10.000000000Z                 110922424322018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z                    used                     mem               host1.local  2018-11-06T05:51:20.000000000Z                 116127744002018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z                    used                     mem               host1.local  2018-11-06T05:51:30.000000000Z                 111319613442018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z                    used                     mem               host1.local  2018-11-06T05:51:40.000000000Z                 111248056322018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z                    used                     mem               host1.local  2018-11-06T05:51:50.000000000Z                 113324646402018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z                    used                     mem               host1.local  2018-11-06T05:52:00.000000000Z                 111769231362018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z                    used                     mem               host1.local  2018-11-06T05:52:10.000000000Z                 111810682882018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z                    used                     mem               host1.local  2018-11-06T05:52:20.000000000Z                 11182579712\n\n定义一个 procTotal 变量，用于过滤流程测量和总计字段。这将返回正在运行的进程数。&#96;&#96;&#96;procTotal &#x3D; from(bucket: “db&#x2F;rp”)|&gt; range(start: -5m)|&gt; filter(fn: (r) &#x3D;&gt; r._measurement &#x3D;&#x3D; “processes” and r._field &#x3D;&#x3D; “total”)\n1    \nTable: keys: [_start, _stop, _field, _measurement, host]               _start:time                      _stop:time           _field:string     _measurement:string               host:string                      _time:time                  _value:int2018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z                   total               processes               host1.local  2018-11-06T05:50:00.000000000Z                         4702018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z                   total               processes               host1.local  2018-11-06T05:50:10.000000000Z                         4702018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z                   total               processes               host1.local  2018-11-06T05:50:20.000000000Z                         4712018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z                   total               processes               host1.local  2018-11-06T05:50:30.000000000Z                         4702018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z                   total               processes               host1.local  2018-11-06T05:50:40.000000000Z                         4692018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z                   total               processes               host1.local  2018-11-06T05:50:50.000000000Z                         4712018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z                   total               processes               host1.local  2018-11-06T05:51:00.000000000Z                         4702018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z                   total               processes               host1.local  2018-11-06T05:51:10.000000000Z                         4702018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z                   total               processes               host1.local  2018-11-06T05:51:20.000000000Z                         4702018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z                   total               processes               host1.local  2018-11-06T05:51:30.000000000Z                         470\n1234567    加入两个数据流：定义了两个数据流后，使用 join() 函数将它们连接在一起。 join() 需要两个参数：*   tables（表）：一个表的映射，用于连接它们将被别名的键。在下面的示例中，mem 是 memUsed 的别名，proc 是 procTotal 的别名。*   on：一个字符串数组，定义表将在其上连接的列。两个表都必须具有此列表中指定的所有列。\n\njoin(tables: {mem: memUsed, proc: procTotal}, on: [“_time”, “_stop”, “_start”, “host”])\n1\nTable: keys: [_field_mem, _field_proc, _measurement_mem, _measurement_proc, _start, _stop, host]     _field_mem:string      _field_proc:string  _measurement_mem:string  _measurement_proc:string                     _start:time                      _stop:time               host:string                      _time:time              _value_mem:int             _value_proc:int\n              used                   total                      mem                 processes  2018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z  Scotts-MacBook-Pro.local  2018-11-06T05:50:00.000000000Z                 10956333056                         470\n              used                   total                      mem                 processes  2018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z  Scotts-MacBook-Pro.local  2018-11-06T05:50:10.000000000Z                 11014008832                         470\n              used                   total                      mem                 processes  2018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z  Scotts-MacBook-Pro.local  2018-11-06T05:50:20.000000000Z                 11373428736                         471\n              used                   total                      mem                 processes  2018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z  Scotts-MacBook-Pro.local  2018-11-06T05:50:30.000000000Z                 11001421824                         470\n              used                   total                      mem                 processes  2018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z  Scotts-MacBook-Pro.local  2018-11-06T05:50:40.000000000Z                 10985852928                         469\n              used                   total                      mem                 processes  2018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z  Scotts-MacBook-Pro.local  2018-11-06T05:50:50.000000000Z                 10992279552                         471\n              used                   total                      mem                 processes  2018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z  Scotts-MacBook-Pro.local  2018-11-06T05:51:00.000000000Z                 11053568000                         470\n              used                   total                      mem                 processes  2018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z  Scotts-MacBook-Pro.local  2018-11-06T05:51:10.000000000Z                 11092242432                         470\n              used                   total                      mem                 processes  2018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z  Scotts-MacBook-Pro.local  2018-11-06T05:51:20.000000000Z                 11612774400                         470\n              used                   total                      mem                 processes  2018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z  Scotts-MacBook-Pro.local  2018-11-06T05:51:30.000000000Z                 11131961344                         470\n              used                   total                      mem                 processes  2018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z  Scotts-MacBook-Pro.local  2018-11-06T05:51:40.000000000Z                 11124805632                         469\n              used                   total                      mem                 processes  2018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z  Scotts-MacBook-Pro.local  2018-11-06T05:51:50.000000000Z                 11332464640                         471\n              used                   total                      mem                 processes  2018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z  Scotts-MacBook-Pro.local  2018-11-06T05:52:00.000000000Z \n\n123456789101112请注意，输出表包括以下列，这些列为具有两个输入表唯一值的列。*   \\_field\\_mem：*   \\_field\\_proc：*   \\_measurement\\_mem：*   \\_measurement\\_proc：*   \\_value\\_mem：*   \\_value\\_proc：**计算并创建一个新表**：将两个数据流连接到一个表中，使用 map() 函数通过将现有的 \\_time 列映射到新的 \\_time 列并将 \\_value\\_mem 除以 \\_value\\_proc 并将其映射到新的 _value 列来构建新表。\njoin(tables: {mem: memUsed, proc: procTotal}, on: [“_time”, “_stop”, “_start”, “host”])    |&gt; map(fn: (r) &#x3D;&gt; ({_time: r._time, _value: r._value_mem &#x2F; r._value_proc}))\n1\nTable: keys: [_field_mem, _field_proc, _measurement_mem, _measurement_proc, _start, _stop, host]     _field_mem:string      _field_proc:string  _measurement_mem:string  _measurement_proc:string                     _start:time                      _stop:time               host:string                      _time:time                  _value:int\n              used                   total                      mem                 processes  2018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z  Scotts-MacBook-Pro.local  2018-11-06T05:50:00.000000000Z                    23311346\n              used                   total                      mem                 processes  2018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z  Scotts-MacBook-Pro.local  2018-11-06T05:50:10.000000000Z                    23434061\n              used                   total                      mem                 processes  2018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z  Scotts-MacBook-Pro.local  2018-11-06T05:50:20.000000000Z                    24147407\n              used                   total                      mem                 processes  2018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z  Scotts-MacBook-Pro.local  2018-11-06T05:50:30.000000000Z                    23407280\n              used                   total                      mem                 processes  2018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z  Scotts-MacBook-Pro.local  2018-11-06T05:50:40.000000000Z                    23423993\n              used                   total                      mem                 processes  2018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z  Scotts-MacBook-Pro.local  2018-11-06T05:50:50.000000000Z                    23338173\n              used                   total                      mem                 processes  2018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z  Scotts-MacBook-Pro.local  2018-11-06T05:51:00.000000000Z                    23518229\n              used                   total                      mem                 processes  2018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z  Scotts-MacBook-Pro.local  2018-11-06T05:51:10.000000000Z                    23600515\n              used                   total                      mem                 processes  2018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z  Scotts-MacBook-Pro.local  2018-11-06T05:51:20.000000000Z                    24708030\n              used                   total                      mem                 processes  2018-11-06T05:50:00.000000000Z  2018-11-06T05:55:00.000000000Z  Scotts-MacBook-Pro.local  2018-11-06T05:51:30.000000000Z \n\n12345此表表示每个正在运行的进程的平均内存量（以字节为单位）。**现实世界的例子**：以下函数通过连接来自 httpd 的字段和写入测量值来计算写入 InfluxDB 集群的批量大小，以便比较 pointReq 和 writeReq。结果按集群 ID 分组，因此您可以跨集群进行比较。\nbatchSize &#x3D; (cluster_id, start&#x3D;-1m, interval&#x3D;10s) &#x3D;&gt; {    httpd &#x3D; from(bucket: “telegraf”)        |&gt; range(start: start)        |&gt; filter(fn: (r) &#x3D;&gt; r._measurement &#x3D;&#x3D; “influxdb_httpd” and r._field &#x3D;&#x3D; “writeReq” and r.cluster_id &#x3D;&#x3D; cluster_id)        |&gt; aggregateWindow(every: interval, fn: mean)        |&gt; derivative(nonNegative: true, unit: 60s)\nwrite = from(bucket: &quot;telegraf&quot;)\n    |&gt; range(start: start)\n    |&gt; filter(fn: (r) =&gt; r._measurement == &quot;influxdb_write&quot; and r._field == &quot;pointReq&quot; and r.cluster_id == cluster_id)\n    |&gt; aggregateWindow(every: interval, fn: max)\n    |&gt; derivative(nonNegative: true, unit: 60s)\n\nreturn join(tables: &#123;httpd: httpd, write: write&#125;, on: [&quot;_time&quot;, &quot;_stop&quot;, &quot;_start&quot;, &quot;host&quot;])\n    |&gt; map(fn: (r) =&gt; (&#123;_time: r._time, _value: r._value_httpd / r._value_write&#125;))\n    |&gt; group(columns: cluster_id)\n\n}\nbatchSize(cluster_id: “enter cluster id here”)\n12345### 4.8.15、查询累计总和使用cumulativeSum() 函数计算值的运行总计。 cumulativeSum 对后续记录的值求和，并返回使用总和更新的每一行。\ndata    |&gt; cumulativeSum()\n123456789101112131415161718192021输入：| _time | _value || --- | --- || 2020-01-01T00:01:00Z | 1 || 2020-01-01T00:02:00Z | 2 || 2020-01-01T00:03:00Z | 1 || 2020-01-01T00:04:00Z | 3 |输出：| _time | _value || --- | --- || 2020-01-01T00:01:00Z | 1 || 2020-01-01T00:02:00Z | 3 || 2020-01-01T00:03:00Z | 4 || 2020-01-01T00:04:00Z | 7 |**将 cumulativeSum() 与 aggregateWindow()一起使用**：aggregateWindow() 将数据分段为时间窗口，将每个窗口中的数据聚合为一个点，然后删除基于时间的分段。它主要用于对数据进行下采样。aggregateWindow() 需要一个聚合函数，它为每个时间窗口返回一行。要将cumulativeSum() 与aggregateWindow 一起使用，请在aggregateWindow() 中使用sum，然后使用cumulativeSum() 计算聚合值的总和。\ndata    |&gt; aggregateWindow(every: 5m, fn: sum)    |&gt; cumulativeSum()\n12345### 4.8.16、first() &amp; last()：查询第1个和最后一个值使用 first() 或 last() 函数返回输入表中的第一个或最后一条记录。\ndata    |&gt; first()\n12345678910111213141516输入：| _time | _value || --- | --- || 2020-01-01T00:01:00Z | 1.0 || 2020-01-01T00:02:00Z | 1.0 || 2020-01-01T00:03:00Z | 2.0 || 2020-01-01T00:04:00Z | 3.0 |输出：| _time | _value || --- | --- || 2020-01-01T00:01:00Z | 1.0 |\ndata    |&gt; last()\n123456789101112131415161718输入：| _time | _value || --- | --- || 2020-01-01T00:01:00Z | 1.0 || 2020-01-01T00:02:00Z | 1.0 || 2020-01-01T00:03:00Z | 2.0 || 2020-01-01T00:04:00Z | 3.0 |输出：| _time | _value || --- | --- || 2020-01-01T00:04:00Z | 3.0 |将 first() 或 last() 与 aggregateWindow()一起使用：使用 first() 和 last() 与 aggregateWindow() 选择基于时间的组中的第一条或最后一条记录。 aggregateWindow() 将数据分割成时间窗口，使用聚合或选择器函数将每个窗口中的数据聚合成一个点，然后删除基于时间的分割。\n|&gt; aggregateWindow(    every: 1h,    fn: first,)\n123456789101112131415161718192021222324输入：| _time | _value || --- | --- || 2020-01-01T00:00:00Z | 10 || 2020-01-01T00:00:15Z | 12 || 2020-01-01T00:00:45Z | 9 || 2020-01-01T00:01:05Z | 9 || 2020-01-01T00:01:10Z | 15 || 2020-01-01T00:02:30Z | 11 |输出：| _time | _value || --- | --- || 2020-01-01T00:00:59Z | 10 || 2020-01-01T00:01:59Z | 9 || 2020-01-01T00:02:59Z | 11 |### 4.8.17、exists 运算符使用 Flux exists 运算符检查记录是否包含键或该键的值是否为空。\np &#x3D; {firstName: “John”, lastName: “Doe”, age: 42}\nexists p.firstName\nexists p.height \n12345将exists与行函数（filter()、map()、reduce()）一起使用来检查行是否包含列或该列的值是否为空。**1、过滤空值：** \nfrom(bucket: “db&#x2F;rp”)    |&gt; range(start: -5m)    |&gt; filter(fn: (r) &#x3D;&gt; exists r._value)\n123**2、基于存在映射值：** \nfrom(bucket: “default”)    |&gt; range(start: -30s)    |&gt; map(        fn: (r) &#x3D;&gt; ({r with            human_readable: if exists r._value then                “${r._field} is ${string(v: r._value)}.”            else                “${r._field} has no value.”,        }),    )\n123**3、忽略自定义聚合函数中的值**\ncustomSumProduct &#x3D; (tables&#x3D;&lt;-) &#x3D;&gt; tables    |&gt; reduce(        identity: {sum: 0.0, product: 1.0},        fn: (r, accumulator) &#x3D;&gt; ({r with            sum: if exists r._value then                r._value + accumulator.sum            else                accumulator.sum,            product: if exists r._value then                r.value * accumulator.product            else                accumulator.product,        }),    )\n12345678910111213141516171819### 4.8.18、用Flux提取标量值使用 Flux 流和表函数从 Flux 查询输出中提取标量值。例如，这使您可以使用查询结果动态设置变量。从输出中提取标量值：*   提取表。*   从表中提取一列或从表中提取一行。目前的限制：*   InfluxDB 用户界面 (UI) 当前不支持原始标量输出。使用 map() 将标量值添加到输出数据。*   Flux REPL 目前不支持 Flux 流和表函数（也称为“动态查询”）。请参阅#15321。**1、tableFind()：提取表**Flux 将查询结果格式化为表流。要从表流中提取标量值，您必须首先提取单个表。如果查询结果只包含一个表，它仍然被格式化为表流。您仍然必须从流中提取该表。使用 tableFind() 提取组键值与 fn 判别函数匹配的第一个表。判别函数需要一个key记录，它表示每个表的组键。\nsampleData    |&gt; tableFind(fn: (key) &#x3D;&gt; key._field &#x3D;&#x3D; “temp” and key.location &#x3D;&#x3D; “sfo”)\n12345678910111213141516上面的示例返回一个表：| _time | location | _field | _value || --- | --- | --- | --- || 2019-11-01T12:00:00Z | sfo | temp | 65.1 || 2019-11-01T13:00:00Z | sfo | temp | 66.2 || 2019-11-01T14:00:00Z | sfo | temp | 66.3 || 2019-11-01T15:00:00Z | sfo | temp | 66.8 |提取正确的表：Flux 函数不保证表顺序，并且 tableFind() 仅返回与 fn 判别匹配的第一个表。要提取包含您实际需要的数据的表，请在您的判别函数中非常具体，或者过滤和转换您的数据以最小化通过管道转发到 tableFind() 的表的数量。**2、getColumn()：从表中提取一列**使用 getColumn() 函数从提取的表中的特定列输出值数组。\nsampleData    |&gt; tableFind(fn: (key) &#x3D;&gt; key._field &#x3D;&#x3D; “temp” and key.location &#x3D;&#x3D; “sfo”)    |&gt; getColumn(column: “_value”) \n123**使用提取的列值**：使用变量来存储值数组。在下面的示例中，SFOTemps 表示值数组。引用数组中的特定索引（从 0 开始的整数）以返回该索引处的值。\nSFOTemps &#x3D; sampleData    |&gt; tableFind(fn: (key) &#x3D;&gt; key._field &#x3D;&#x3D; “temp” and key.location &#x3D;&#x3D; “sfo”)    |&gt; getColumn(column: “_value”)\nSFOTemps\nSFOTemps[0]\nSFOTemps[2] \n12345**3、getRecord()：从表中提取一行**使用 getRecord() 函数从提取的表中的单行输出数据。使用 idx 参数指定要输出的行的索引。该函数为每列输出一个包含键值对的记录\nsampleData    |&gt; tableFind(fn: (key) &#x3D;&gt; key._field &#x3D;&#x3D; “temp” and key.location &#x3D;&#x3D; “sfo”)    |&gt; getRecord(idx: 0) \n123**使用提取的行记录**：使用变量来存储提取的行记录。在下面的示例中， tempInfo 表示提取的行。使用点表示法来引用记录中的键。\ntempInfo &#x3D; sampleData    |&gt; tableFind(fn: (key) &#x3D;&gt; key._field &#x3D;&#x3D; “temp” and key.location &#x3D;&#x3D; “sfo”)    |&gt; getRecord(idx: 0)\ntempInfo\ntempInfo._time\ntempInfo.location \n1234567**4、辅助函数示例**创建自定义辅助函数以从查询输出中提取标量值。提取标量字段值：\n getFieldValue &#x3D; (tables&#x3D;&lt;-, field) &#x3D;&gt; {    extract &#x3D; tables        |&gt; tableFind(fn: (key) &#x3D;&gt; key._field &#x3D;&#x3D; field)        |&gt; getColumn(column: “_value”)\nreturn extract[0]\n\n}\nlastJFKTemp &#x3D; sampleData    |&gt; filter(fn: (r) &#x3D;&gt; r.location &#x3D;&#x3D; “kjfk”)    |&gt; last()    |&gt; getFieldValue(field: “temp”)\nlastJFKTemp \n123提取标量行数据：\n getRow &#x3D; (tables&#x3D;&lt;-, field, idx&#x3D;0) &#x3D;&gt; {    extract &#x3D; tables        |&gt; tableFind(fn: (key) &#x3D;&gt; true)        |&gt; getRecord(idx: idx)\nreturn extract\n\n}\nlastReported &#x3D; sampleData    |&gt; last()    |&gt; getRow(field: “temp”)\n“The last location to report was ${lastReported.location}.The temperature was ${string(v: lastReported._value)}°F.” \n12345**5、样本数据**以下示例数据集代表从三个位置收集的虚构温度指标。它采用带注释的 CSV 格式，并使用 csv.from() 函数导入到 Flux 查询中。将以下内容放在查询的开头以使用示例数据：\nimport “csv”\nsampleData &#x3D; csv.from(csv: “#datatype,string,long,dateTime:RFC3339,string,string,double#group,false,true,false,true,true,false#default,,,,,,,result,table,_time,location,_field,_value,,0,2019-11-01T12:00:00Z,sfo,temp,65.1,,0,2019-11-01T13:00:00Z,sfo,temp,66.2,,0,2019-11-01T14:00:00Z,sfo,temp,66.3,,0,2019-11-01T15:00:00Z,sfo,temp,66.8,,1,2019-11-01T12:00:00Z,kjfk,temp,69.4,,1,2019-11-01T13:00:00Z,kjfk,temp,69.9,,1,2019-11-01T14:00:00Z,kjfk,temp,71.0,,1,2019-11-01T15:00:00Z,kjfk,temp,71.2,,2,2019-11-01T12:00:00Z,kord,temp,46.4,,2,2019-11-01T13:00:00Z,kord,temp,46.3,,2,2019-11-01T14:00:00Z,kord,temp,42.7,,2,2019-11-01T15:00:00Z,kord,temp,38.9“)\n123456789### 4.8.19、操作时间戳存储在 InfluxDB 中的每个点都有一个关联的时间戳。使用 Flux 处理和操作时间戳以满足您的需求。**1、转换时间戳格式***   Unix纳秒转换为RFC3339：使用 time() 函数将 Unix 纳秒时间戳转换为 RFC3339 时间戳。```    time(v: 1568808000000000000) \n\nRFC3339转换为Unix纳秒：&#96;&#96;&#96;uint(v: 2019-09-18T12:00:00.000000000Z) 12345678910    **2、计算2个时间戳之间的持续时间**Flux 不支持使用时间类型值的数学运算。要计算两个时间戳之间的持续时间：*   使用 uint() 函数将每个时间戳转换为 Unix 纳秒时间戳。*   该 Unix 纳秒时间戳减去另一个。*   使用 duration() 函数将结果转换为持续时间。\ntime1 &#x3D; uint(v: 2019-09-17T21:12:05Z)time2 &#x3D; uint(v: 2019-09-18T22:16:35Z)\n\nduration(v: time2 - time1) \n1234567Flux 不支持持续时间列类型。要将持续时间存储在列中，请使用 string() 函数将持续时间转换为字符串。**3、检索当前时间***   当前UTC时间：使用 now() 函数以 RFC3339 格式返回当前 UTC 时间。now() 在运行时被缓存，因此 Flux 脚本中的所有 now() 实例都返回相同的值。```    now()\n\n当前系统时间：导入system包，使用system.time()函数以RFC3339格式返回宿主机当前系统时间。system.time() 返回执行的时间，因此 Flux 脚本中的每个 system.time() 实例都返回一个唯一值。&#96;&#96;&#96;import “system”\nsystem.time()\n123456    **4、标准化不规则时间戳**要标准化不规则时间戳，请使用 truncateTimeColumn() 函数将所有 _time 值截断为指定单位。这在点应按时间对齐的 join() 和 pivot() 操作中很有用，但时间戳略有不同。\n\ndata    |&gt; truncateTimeColumn(unit: 1m)\n1234567891011121314151617181920212223242526输入：| _time | _value || --- | --- || 2020-01-01T00:00:49Z | 2.0 || 2020-01-01T00:01:01Z | 1.9 || 2020-01-01T00:03:22Z | 1.8 || 2020-01-01T00:04:04Z | 1.9 || 2020-01-01T00:05:38Z | 2.1 |输出：| _time | _value || --- | --- || 2020-01-01T00:00:00Z | 2.0 || 2020-01-01T00:01:00Z | 1.9 || 2020-01-01T00:03:00Z | 1.8 || 2020-01-01T00:04:00Z | 1.9 || 2020-01-01T00:05:00Z | 2.1 |**5、一起使用时间戳（timestamp）和持续时间（duration）***   为时间戳添加持续时间：Experimental.addDuration() 函数将持续时间添加到指定时间并返回结果时间。通过使用experimental.addDuration()，您接受实验功能的风险。```    import &quot;experimental&quot;    experimental.addDuration(d: 6h, to: 2019-09-16T12:00:00Z) \n\n从时间戳中减去持续时间：Experimental.subDuration() 函数从指定时间中减去一个持续时间并返回结果时间。通过使用 experimental.subDuration()，您接受实验功能的风险。&#96;&#96;&#96;import “experimental”experimental.subDuration(d: 6h, from: 2019-09-16T12:00:00Z) 12345678910111213141516171819    ### 4.8.20、监控状态Flux 可帮助您监控指标和事件中的状态：*   找出一个状态持续多久；*   计算连续状态的数量；*   检测状态变化。**1、找出一个状态持续多久**使用 stateDuration() 函数计算列值保持相同值（或状态）的时间。包括以下信息：*   要搜索的列：任何标签键、标签值、字段键、字段值或度量。*   值：要在指定列中搜索的值（或状态）。*   状态持续时间列：用于存储状态持续时间的新列——指定值持续的时间长度。*   Unit：用于增加状态持续时间的时间单位（1s（默认）、1m、1h）。\ndata|&gt; stateDuration(fn: (r) &#x3D;&gt; r._column_to_search &#x3D;&#x3D; “value_to_search_for”,column: “state_duration”,unit: 1s,)12345678使用 stateDuration() 搜索每个点的指定值：*   对于评估为 true 的第一个点，状态持续时间设置为 0。对于评估为 true 的每个连续点，状态持续时间增加每个连续点之间的时间间隔（以指定单位）。*   如果状态为假，则状态持续时间重置为 -1。使用stateDuration()的示例查询：以下查询在过去 5 分钟内搜索门存储桶以查找门已关闭的秒数：\nfrom(bucket: “doors”)|&gt; range(start: -5m)|&gt; stateDuration(fn: (r) &#x3D;&gt; r._value &#x3D;&#x3D; “closed”,column: “door_closed”,unit: 1s,)12345在此示例中，door\\_closed 是状态持续时间列。如果您每分钟将数据写入门存储桶，则状态持续时间会在 \\_value 关闭的每个连续点增加 60 秒。如果 _value 未关闭，则状态持续时间重置为 0。上面示例查询的结果可能如下所示（为简单起见，我们省略了测量、标签和字段列）：\n_time                   _value        door_closed2019-10-26T17:39:16Z    closed        02019-10-26T17:40:16Z    closed        602019-10-26T17:41:16Z    closed        1202019-10-26T17:42:16Z    open          -12019-10-26T17:43:16Z    closed        02019-10-26T17:44:27Z    closed        60123456789**2、计算连续状态的数量**使用 stateCount() 函数并包含以下信息：*   要搜索的列：任何标签键、标签值、字段键、字段值或度量。*   值：在指定列中搜索。*   状态计数列：用于存储状态计数的新列─指定值存在的连续记录数。\ndata|&gt; stateCount(fn: (r) &#x3D;&gt; r._column_to_search &#x3D;&#x3D; “value_to_search_for”,column: “state_count”)12345678使用stateCount()在每个点中搜索指定值：*   对于评估为 true 的第一个点，状态计数设置为 1。对于评估为 true 的每个连续点，状态计数增加 1。*   如果状态为假，则状态计数重置为 -1。使用stateCount()的示例查询：以下查询在过去 5 分钟内搜索门存储桶，并计算已关闭的点数作为它们的 _value：\nfrom(bucket: “doors”)|&gt; range(start: -5m)|&gt; stateCount(fn: (r) &#x3D;&gt; r._value &#x3D;&#x3D; “closed”, column: “door_closed”)12345此示例将状态计数存储在 door\\_closed 列中。如果您每分钟将数据写入门存储桶，则状态计数会在 \\_value 关闭的每个连续点增加 1。如果 _value 未关闭，则状态计数重置为 -1。上面示例查询的结果可能如下所示（为简单起见，我们省略了测量、标签和字段列）：\n_time                   _value        door_closed2019-10-26T17:39:16Z    closed        12019-10-26T17:40:16Z    closed        22019-10-26T17:41:16Z    closed        32019-10-26T17:42:16Z    open          -12019-10-26T17:43:16Z    closed        12019-10-26T17:44:27Z    closed        2123机器状态计数的示例查询：以下查询每分钟检查一次机器状态（空闲、已分配或忙碌）。 InfluxDB 在过去一小时内搜索服务器存储桶并计算机器状态为空闲、已分配或忙碌的记录。\nfrom(bucket: “servers”)|&gt; range(start: -1h)|&gt; filter(fn: (r) &#x3D;&gt; r.machine_state &#x3D;&#x3D; “idle” or r.machine_state &#x3D;&#x3D; “assigned” or r.machine_state &#x3D;&#x3D; “busy”)|&gt; stateCount(fn: (r) &#x3D;&gt; r.machine_state &#x3D;&#x3D; “busy”, column: “_count”)|&gt; stateCount(fn: (r) &#x3D;&gt; r.machine_state &#x3D;&#x3D; “assigned”, column: “_count”)|&gt; stateCount(fn: (r) &#x3D;&gt; r.machine_state &#x3D;&#x3D; “idle”, column: “_count”)123456789101112131415161718### 4.8.21、查询SQL数据源Flux sql 包提供了处理 SQL 数据源的函数。 sql.from() 让您可以查询 PostgreSQL、MySQL 和 SQLite 等 SQL 数据源，并将结果用于 InfluxDB 仪表板、任务和其他操作。**1、查询SQL数据源**查询 SQL 数据源：1.  在 Flux 查询中导入 sql 包2.  使用 sql.from() 函数指定用于从 SQL 数据源查询数据的驱动程序、数据源名称 (DSN) 和查询：```    import &quot;sql&quot;        sql.from(        driverName: &quot;mysql&quot;,        dataSourceName: &quot;user:password@tcp(localhost:3306)/db&quot;,        query: &quot;SELECT * FROM example_table&quot;,    )\n\n2、将SQL数据与InfluxDB中的数据连接起来\n从 InfluxDB 查询 SQL 数据源的主要好处之一是能够使用存储在 InfluxDB 之外的数据来丰富查询结果。使用下面的空气传感器示例数据，以下查询将存储在 InfluxDB 中的空气传感器指标与存储在 PostgreSQL 中的传感器信息连接起来。连接的数据允许您根据未存储在 InfluxDB 中的传感器信息查询和过滤结果。\n12345678910111213 import &quot;sql&quot;sensorInfo = sql.from(    driverName: &quot;postgres&quot;,    dataSourceName: &quot;postgresql://localhost?sslmode=disable&quot;,    query: &quot;SELECT * FROM sensors&quot;,)sensorMetrics = from(bucket: &quot;telegraf/autogen&quot;)    |&gt; range(start: -1h)    |&gt; filter(fn: (r) =&gt; r._measurement == &quot;airSensors&quot;)join(tables: &#123;metric: sensorMetrics, info: sensorInfo&#125;, on: [&quot;sensor_id&quot;])\n\n3、样本传感器数据\n样本数据生成器和样本传感器信息模拟一组传感器，用于测量整个建筑物房间内的温度、湿度和一氧化碳。每个收集的数据点都存储在 InfluxDB 中，带有一个 sensor_id 标签，用于标识它来自的特定传感器。示例传感器信息存储在 PostgreSQL 中。\n样本数据包括：\n\n从每个传感器收集的模拟数据并存储在 InfluxDB 中的 airSensors 测量中：① temperature；② humidity；③ co。\n有关存储在 PostgreSQL 中的传感器表中的每个传感器的信息：① sensor_id；② location；③ model_number；④ last_inspected。\n\n》导入并生成示例传感器数据：\n1）下载并运行示例数据生成器\n\nair-sensor-data.rb 是一个生成空气传感器数据并将数据存储在 InfluxDB 中的脚本。要使用 air-sensor-data.rb：\n\n创建一个数据库来存储数据。\n\n下载示例数据生成器。此工具需要 Ruby。\n\n赋予 air-sensor-data.rb 可执行权限：&#96;&#96;&#96;chmod +x air-sensor-data.rb\n123    5.  启动启动器。指定您的数据库。```    ./air-sensor-data.rb -d database-name\n\n生成器开始向 InfluxDB 写入数据，并将一直持续到停止。使用 ctrl-c 停止生成器。注意：使用 –help 标志查看其他配置选项。\n\n查询您的目标数据库以确保生成的数据写入成功。生成器不会从写入请求中捕获错误，因此即使数据没有成功写入 InfluxDB，它也会继续运行。&#96;&#96;&#96;from(bucket: “database-name&#x2F;autogen”)   |&gt; range(start: -1m)   |&gt; filter(fn: (r) &#x3D;&gt; r._measurement &#x3D;&#x3D; “airSensors”)\n12345678910111213    2）导入样本传感器信息1.  下载并安装 PostgreSQL。2.  下载示例传感器信息 CSV。3.  使用 PostgreSQL 客户端（psql 或 GUI）创建传感器表：```    CREATE TABLE sensors (      sensor_id character varying(50),      location character varying(50),      model_number character varying(50),      last_inspected date    );\n\n导入下载的 CSV 样本数据。将 FROM 文件路径更新为下载的 CSV 样本数据的路径。&#96;&#96;&#96;COPY sensors(sensor_id,location,model_number,last_inspected)FROM ‘&#x2F;path&#x2F;to&#x2F;sample-sensor-info.csv’ DELIMITER ‘,’ CSV HEADER;\n123    5.  查询表以确保数据正确导入：```    SELECT * FROM sensors;\n\n4.8.22、条件逻辑Flux 提供了 if、then 和 else 条件表达式，允许强大而灵活的 Flux 查询。本指南介绍如何使用 Flux 条件表达式来查询和转换数据。 Flux 从左到右评估语句，一旦条件匹配就停止评估。\n条件表达式语法：\n123 if &lt;condition&gt; then &lt;action&gt; else &lt;alternative-action&gt;if color == &quot;green&quot; then &quot;008000&quot; else &quot;ffffff&quot;\n\n条件表达式在以下情况下最有用：\n\n定义变量时。\n使用一次对单行操作的函数时（filter()、map()、reduce()）。\n\n1、评估条件表达式\nFlux 按顺序评估语句，并在条件匹配时停止评估。例如，给定以下语句：\n12345678if r._value &gt; 95.0000001 and r._value &lt;= 100.0 then    &quot;critical&quot;else if r._value &gt; 85.0000001 and r._value &lt;= 95.0 then    &quot;warning&quot;else if r._value &gt; 70.0000001 and r._value &lt;= 85.0 then    &quot;high&quot;else    &quot;normal&quot;\n\n当 r._value 为 96 时，输出为“critical”并且不评估剩余条件。\n2、例子\n1）有条件地设置变量的值：下面的示例根据 dueDate 变量与 now() 的关系设置过期变量：\n12dueDate = 2019-05-01T00:00:00Zoverdue = if dueDate &lt; now() then true else false\n\n2）创建条件过滤器：以下示例使用示例度量变量来更改查询过滤数据的方式。 metric 有三个可能的值：Memeory、CPU、Disk。\n1234567891011121314metric = &quot;Memory&quot;from(bucket: &quot;telegraf/autogen&quot;)    |&gt; range(start: -1h)    |&gt; filter(        fn: (r) =&gt; if v.metric == &quot;Memory&quot; then            r._measurement == &quot;mem&quot; and r._field == &quot;used_percent&quot;        else if v.metric == &quot;CPU&quot; then            r._measurement == &quot;cpu&quot; and r._field == &quot;usage_user&quot;        else if v.metric == &quot;Disk&quot; then            r._measurement == &quot;disk&quot; and r._field == &quot;used_percent&quot;        else            r._measurement != &quot;&quot;,    )\n\n3）使用map()有条件地转换列值：以下示例使用 map() 函数有条件地转换列值。它将级别列设置为基于 _value 列的特定字符串。\n123456789101112131415161718from(bucket: &quot;telegraf/autogen&quot;)    |&gt; range(start: -5m)    |&gt; filter(fn: (r) =&gt; r._measurement == &quot;mem&quot; and r._field == &quot;used_percent&quot;)    |&gt; map(        fn: (r) =&gt; (&#123;                        r with                        level: if r._value &gt;= 95.0000001 and r._value &lt;= 100.0 then                &quot;critical&quot;            else if r._value &gt;= 85.0000001 and r._value &lt;= 95.0 then                &quot;warning&quot;            else if r._value &gt;= 70.0000001 and r._value &lt;= 85.0 then                &quot;high&quot;            else                &quot;normal&quot;,        &#125;),    )\n\n4、使用reduce()有条件地增加一个计数：以下示例使用 aggregateWindow() 和 reduce() 函数计算每五分钟窗口中超过定义阈值的记录数。\n1234567891011121314151617181920212223threshold = 65.0from(bucket: &quot;telegraf/autogen&quot;)    |&gt; range(start: -1h)    |&gt; filter(fn: (r) =&gt; r._measurement == &quot;mem&quot; and r._field == &quot;used_percent&quot;)        |&gt; aggregateWindow(        every: 5m,                        fn: (column, tables=&lt;-) =&gt; tables            |&gt; reduce(                identity: &#123;above_threshold_count: 0.0&#125;,                fn: (r, accumulator) =&gt; (&#123;                                                            above_threshold_count: if r._value &gt;= threshold then                        accumulator.above_threshold_count + 1.0                    else                        accumulator.above_threshold_count + 0.0,                &#125;),            ),    )\n\n4.8.23、正则表达式在匹配大量数据集合中的模式时，正则表达式（正则表达式）非常强大。使用 Flux，正则表达式主要用于判别函数中的评估逻辑，例如过滤行、删除和保留列、状态检测等。本指南展示了如何在 Flux 脚本中使用正则表达式。\n1、Go 正则表达式语法\nFlux 使用 Go 的 regexp 包进行正则表达式搜索。\n2、正则表达式运算符\nFlux 提供了两个用于正则表达式的比较运算符。\n\n&#x3D;~：当左边的表达式匹配右边的正则表达式时，它的计算结果为真。\n!~：当左边的表达式与右边的正则表达式不匹配时，它的计算结果为真。\n\n3、Flux中的正则表达式\n在 Flux 脚本中使用正则表达式匹配时，用 &#x2F; 将正则表达式括起来。以下是基本的正则表达式比较语法：\n12expression =~ /regex/expression !~ /regex/\n\n4、例子\n1）使用正则表达式按标签值过滤：以下示例按 cpu 标签过滤记录。它只保留 cpu 为 cpu0、cpu1 或 cpu2 的记录。\n123from(bucket: &quot;db/rp&quot;)    |&gt; range(start: -15m)    |&gt; filter(fn: (r) =&gt; r._measurement == &quot;cpu&quot; and r._field == &quot;usage_user&quot; and r.cpu =~ /cpu[0-2]/)\n\n2）使用正则表达式按字段键过滤：以下示例排除了字段键中没有 _percent 的记录。\n123from(bucket: &quot;db/rp&quot;)    |&gt; range(start: -15m)    |&gt; filter(fn: (r) =&gt; r._measurement == &quot;mem&quot; and r._field =~ /_percent/)\n\n3）删除与正则表达式匹配的列：以下示例删除名称不带 _ 的列。\n1234from(bucket: &quot;db/rp&quot;)    |&gt; range(start: -15m)    |&gt; filter(fn: (r) =&gt; r._measurement == &quot;mem&quot;)    |&gt; drop(fn: (column) =&gt; column !~ /_.*/)\n\n例子：\n12data    |&gt; filter(fn: (r) =&gt; r.tag =~ /^foo[1-3]/)\n\n输入：\n\n\n\n_time\ntag\n_value\n\n\n\n2020-01-01T00:01:00Z\nfoo1\n1.0\n\n\n2020-01-01T00:02:00Z\nfoo5\n1.2\n\n\n2020-01-01T00:03:00Z\nbar3\n1.8\n\n\n2020-01-01T00:04:00Z\nfoo3\n0.9\n\n\n2020-01-01T00:05:00Z\nfoo2\n1.4\n\n\n2020-01-01T00:06:00Z\nbar1\n2.0\n\n\n输出：\n\n\n\n_time\ntag\n_value\n\n\n\n2020-01-01T00:01:00Z\nfoo1\n1.0\n\n\n2020-01-01T00:04:00Z\nfoo3\n0.9\n\n\n2020-01-01T00:05:00Z\nfoo2\n1.4\n\n\n4.8.24、地理时态数据使用 Flux Geo 包过滤地理时态数据并按地理位置或轨迹分组。Geo 软件包是实验性的，可能随时更改。使用它，即表示您同意实验功能的风险。\n要使用地理时态数据：\n\n导入experimental&#x2F;geo包。&#96;&#96;&#96;import “experimental&#x2F;geo”123456    2.  加载地时数据。有关示例地时数据，请参见下文。3.  执行以下一项或多项操作：① 形状数据与Geo包一起使用；② 按区域过滤数据（使用严格或非严格过滤器）；③ 按区域或按轨道对数据进行分组。**1、形状数据与Geo包一起使用**：Flux Geo 包中的函数需要 lat 和 lon 字段以及 s2\\_cell\\_id 标签。重命名纬度和经度字段并生成 S2 小区 ID 令牌。\nimport “experimental&#x2F;geo”\n\nsampleGeoData  |&gt; geo.shapeData(latField: “latitude”, lonField: “longitude”, level: 10)\n123**2、按区域过滤时空数据**：使用 geo.filterRows 函数按箱形、圆形或多边形地理区域过滤地理时态数据。\nimport “experimental&#x2F;geo”\nsampleGeoData  |&gt; geo.filterRows(    region: {lat: 30.04, lon: 31.23, radius: 200.0},    strict: true  )\n123**3、分组地时数据**：使用 geo.groupByArea() 按区域对地理时态数据进行分组，使用 geo.asTracks() 将数据分组为轨迹或路线。\nimport “experimental&#x2F;geo”\nsampleGeoData  |&gt; geo.groupByArea(newColumn: “geoArea”, level: 5)  |&gt; geo.asTracks(groupBy: [“id”],orderBy: [“_time”])\n12345》样本数据：本节中的许多示例都使用了一个 sampleGeoData 变量，该变量表示地理时态数据的样本集。 GitHub 上的 Bird Migration Sample Data 提供了满足 Flux Geo 包要求的示例地理时态数据。**1）加载带注释的CSV样本数据**：使用实验性 csv.from() 函数从 GitHub 加载样本鸟迁徙注释 CSV 数据：\nimport experimental/csv\nsampleGeoData &#x3D; csv.from(  url: “https://github.com/influxdata/influxdb2-sample-data/blob/master/bird-migration-data/bird-migration.csv“)\n12345csv.from(url: ...) 每次执行查询时都会下载示例数据 (~1.3 MB)。如果带宽是一个问题，使用 to() 函数将数据写入存储桶，然后使用 from() 查询存储桶。**2）使用线路协议将样本数据写入InfluxDB**：使用 curl 和 influx write 命令将鸟类迁徙线路协议写入 InfluxDB。将 db/rp 替换为您的目标存储桶：\ncurl https:influx write -b db&#x2F;rp @.&#x2F;tmp-datarm -f .&#x2F;tmp-data\n123使用 Flux 查询鸟类迁徙数据并将其分配给 sampleGeoData 变量：\nsampleGeoData &#x3D; from(bucket: “db&#x2F;rp”)    |&gt; range(start: 2019-01-01T00:00:00Z, stop: 2019-12-31T23:59:59Z)    |&gt; filter(fn: (r) &#x3D;&gt; r._measurement &#x3D;&#x3D; “migration”)\n1234567891011121314151617181920212223242526272829303132333435363738394041425.1、使用pushdowns启动查询-------------------pushdowns是将数据操作推送到底层数据源而不是对内存中的数据进行操作的函数或函数组合。使用pushdowns启动查询以提高查询性能。一旦非pushdowns函数运行，Flux 会将数据拉入内存并在那里运行所有后续操作。### 5.1.1、下推功能和功能组合InfluxDB Enterprise 1.9+ 支持以下下推。| Functions | Supported || --- | --- || **count()** | √ || **drop()** | √ || **duplicate()** | √ || **filter()** * | √ || **fill()** | √ || **first()** | √ || **group()** | √ || **keep()** | √ || **last()** | √ || **max()** | √ || **mean()** | √ || **min()** | √ || **range()** | √ || **rename()** | √ || **sum()** | √ || **window()** | √ || _Function combinations_ |   || **window()** |\\&gt; **count()** | √ || **window()** |\\&gt; **first()** | √ || **window()** |\\&gt; **last()** | √ || **window()** |\\&gt; **max()** | √ || **window()** |\\&gt; **min()** | √ || **window()** |\\&gt; **sum()** | √ |\\* filter() 仅在所有参数值为静态时下推。请参阅避免内联处理过滤器。在查询开始时使用下推函数和函数组合。一旦非下推函数运行，Flux 会将数据拉入内存并在那里运行所有后续操作。下推使用中的函数：\nfrom(bucket: “db&#x2F;rp”)    |&gt; range(start: -1h)    |&gt; filter(fn: (r) &#x3D;&gt; r.sensor &#x3D;&#x3D; “abc123”)    |&gt; group(columns: [“_field”, “host”])    |&gt; aggregateWindow(every: 5m, fn: max)    |&gt; filter(fn: (r) &#x3D;&gt; r._value &gt;&#x3D; 90.0)     \n|&gt; top(n: 10) \n\n123456785.2、避免内联过滤处理器-------------避免使用数学运算或内联字符串操作来定义数据过滤器。内联处理过滤器值可防止 filter() 将其操作下推到底层数据源，因此前一个函数返回的数据会加载到内存中。这通常会导致严重的性能损失。例如，以下查询使用 Chronograf 仪表板模板变量和字符串连接来定义要过滤的区域。因为 filter() 使用内联字符串连接，所以它无法将其操作推送到底层数据源并将从 range() 返回的所有数据加载到内存中。\nfrom(bucket: “db&#x2F;rp”)    |&gt; range(start: -1h)    |&gt; filter(fn: (r) &#x3D;&gt; r.region &#x3D;&#x3D; v.provider + v.region)\n123要动态设置过滤器并保持 filter() 函数的下推能力，请使用变量在 filter() 之外定义过滤器值：\nregion &#x3D; v.provider + v.region\nfrom(bucket: “db&#x2F;rp”)    |&gt; range(start: -1h)    |&gt; filter(fn: (r) &#x3D;&gt; r.region &#x3D;&#x3D; region)\n1234567891011121314151617181920212223242526275.3、避免短窗口持续时间-------------开窗（根据时间间隔对数据进行分组）通常用于聚合和下采样数据。通过避免较短的窗口持续时间来提高性能。更多的窗口需要更多的计算能力来评估每行应该分配到哪个窗口。合理的窗口时长取决于查询的总时间范围。5.4、谨慎使用“繁重”的函数---------------以下函数比其他函数使用更多的内存或 CPU。在使用它们之前，请考虑它们在您的数据处理中的必要性：*   map()；*   reduce()；*   join()；*   union()；*   pivot()；5.5、尽可能使用set()而不是map()----------------------set()、experimental.set() 和 map 都可以设置数据中的列值，但是 set 函数比 map() 具有性能优势。使用以下准则来确定使用哪个：*   如果将列值设置为预定义的静态值，请使用 set() 或 experimental.set()。*   如果使用现有行数据动态设置列值，请使用 map()。**1、将列值设置为静态值**：以下查询在功能上相同，但使用 set() 比使用 map() 更高效。\ndata    |&gt; map(fn: (r) &#x3D;&gt; ({ r with foo: “bar” }))\ndata    |&gt; set(key: “foo”, value: “bar”)\n123**2、使用现有行数据动态设置列值**\ndata    |&gt; map(fn: (r) &#x3D;&gt; ({ r with foo: r.bar }))\n123456789101112131415165.6、平衡时间范围和数据精度---------------为确保查询是高性能的，请平衡数据的时间范围和精度。例如，如果您查询每秒存储的数据并请求六个月的数据，则每个系列的结果将包括约 1550 万个点。根据 filter()(cardinality) 之后返回的序列数，这很快就会变成数十亿个点。 Flux 必须将这些点存储在内存中以生成响应。使用下推优化内存中存储的点数。5.7、使用Flux分析器测量查询性能-------------------使用 Flux Profiler 包测量查询性能并将性能指标附加到查询输出中。以下Flux分析器可用：*   query：提供有关整个 Flux 脚本执行的统计信息。*   operator：提供有关查询中每个操作的统计信息。导入分析器包并使用 profile.enabledProfilers 选项启用分析器。\nimport “profiler”option profiler.enabledProfilers &#x3D; [“query”, “operator”] \n12345678910Flux 是 InfluxQL 和其他类似 SQL 的查询语言的替代品，用于查询和分析数据。 Flux 使用函数式语言模式，使其非常强大、灵活，并且能够克服 InfluxQL 的许多限制。本文概述了使用 Flux 而不是 InfluxQL 可能执行的许多任务，并提供有关 Flux 和 InfluxQL 奇偶校验的信息。6.1、可以使用Flux的情形---------------### 6.1.1、Joins（连接）InfluxQL 从不支持连接。它们可以使用 TICKscript 来完成，但即使是 TICKscript 的连接能力也是有限的。 Flux 的 join() 函数允许您连接来自任何存储桶、任何度量和任何列的数据，只要每个数据集都包含要连接它们的列。这为真正强大和有用的操作打开了大门。\ndataStream1 &#x3D; from(bucket: “bucket1”)    |&gt; range(start: -1h)    |&gt; filter(fn: (r) &#x3D;&gt; r._measurement &#x3D;&#x3D; “network” and r._field &#x3D;&#x3D; “bytes-transferred”)\ndataStream2 &#x3D; from(bucket: “bucket1”)    |&gt; range(start: -1h)    |&gt; filter(fn: (r) &#x3D;&gt; r._measurement &#x3D;&#x3D; “httpd” and r._field &#x3D;&#x3D; “requests-per-sec”)\njoin(tables: {d1: dataStream1, d2: dataStream2}, on: [“_time”, “_stop”, “_start”, “host”])\n12345### 6.1.2、跨测量数学能够执行交叉测量连接还允许您使用来自单独测量的数据运行计算——这是 InfluxData 社区高度要求的功能。下面的示例从单独的测量、mem和processes中获取两个数据流，将它们连接起来，然后计算每个正在运行的进程使用的平均内存量：\n memUsed &#x3D; from(bucket: “telegraf&#x2F;autogen”)    |&gt; range(start: -1h)    |&gt; filter(fn: (r) &#x3D;&gt; r._measurement &#x3D;&#x3D; “mem” and r._field &#x3D;&#x3D; “used”)\nprocTotal &#x3D; from(bucket: “telegraf&#x2F;autogen”)    |&gt; range(start: -1h)    |&gt; filter(fn: (r) &#x3D;&gt; r._measurement &#x3D;&#x3D; “processes” and r._field &#x3D;&#x3D; “total”)\njoin(tables: {mem: memUsed, proc: procTotal}, on: [“_time”, “_stop”, “_start”, “host”])    |&gt; map(fn: (r) &#x3D;&gt; ({_time: r._time, _value: r._value_mem &#x2F; r._value_proc &#x2F; 1000000}))\n12345### 6.1.3、按标签排序InfluxQL 的排序功能非常有限，只允许您使用 ORDER BY 时间子句控制时间的排序顺序。 Flux 的 sort() 函数根据列列表对记录进行排序。根据列类型，记录按字典顺序、数字顺序或时间顺序排序。\nfrom(bucket: “telegraf&#x2F;autogen”)    |&gt; range(start: -12h)    |&gt; filter(fn: (r) &#x3D;&gt; r._measurement &#x3D;&#x3D; “system” and r._field &#x3D;&#x3D; “uptime”)    |&gt; sort(columns: [“region”, “host”, “_value”])\n12345### 6.1.4、按任意列分组InfluxQL 允许您按标签或按时间间隔分组，但仅此而已。 Flux 允许您按数据集中的任何列进行分组，包括 _value。使用 Flux group() 函数定义对数据进行分组的列。\nfrom(bucket:”telegraf&#x2F;autogen”)    |&gt; range(start:-12h)    |&gt; filter(fn: (r) &#x3D;&gt; r._measurement &#x3D;&#x3D; “system” and r._field &#x3D;&#x3D; “uptime” )    |&gt; group(columns:[“host”, “_value”])\n12345### 6.1.5、按日历月和年的窗口InfluxQL 不支持按日历月和年的窗口数据，因为它们的长度不同。 Flux 支持日历月和年的持续时间单位（1mo、1y），并允许您按日历月和年窗口和聚合数据。\nfrom(bucket:”telegraf&#x2F;autogen”)    |&gt; range(start:-1y)    |&gt; filter(fn: (r) &#x3D;&gt; r._measurement &#x3D;&#x3D; “mem” and r._field &#x3D;&#x3D; “used_percent” )    |&gt; aggregateWindow(every: 1mo, fn: mean)\n12345### 6.1.6、使用多个数据源InfluxQL 只能查询存储在 InfluxDB 中的数据。 Flux 可以从 CSV、PostgreSQL、MySQL、Google BigTable 等其他数据源查询数据。将该数据与 InfluxDB 中的数据相结合，以丰富查询结果。\nimport “csv”import “sql”\ncsvData &#x3D; csv.from(csv: rawCSV)sqlData &#x3D; sql.from(    driverName: “postgres”,    dataSourceName: “postgresql:&#x2F;&#x2F;user:password@localhost”,    query: “SELECT * FROM example_table”,)data &#x3D; from(bucket: “telegraf&#x2F;autogen”)    |&gt; range(start: -24h)    |&gt; filter(fn: (r) &#x3D;&gt; r._measurement &#x3D;&#x3D; “sensor”)\nauxData &#x3D; join(tables: {csv: csvData, sql: sqlData}, on: [“sensor_id”])enrichedData &#x3D; join(tables: {data: data, aux: auxData}, on: [“sensor_id”])\nenrichedData    |&gt; yield(name: “enriched_data”)\n12345### 6.1.7、类DatePart查询InfluxQL 不支持仅在一天中的指定时间返回结果的类似 DatePart 的查询。 Flux hourSelection 函数仅返回时间值在指定小时范围内的数据。\nfrom(bucket: “telegraf&#x2F;autogen”)    |&gt; range(start: -1h)    |&gt; filter(fn: (r) &#x3D;&gt; r._measurement &#x3D;&#x3D; “cpu” and r.cpu &#x3D;&#x3D; “cpu-total”)    |&gt; hourSelection(start: 9, stop: 17)\n12345### 6.1.8、PivotInfluxQL 从未支持透视数据表。 Flux pivot() 函数提供了通过指定 rowKey、columnKey 和 valueColumn 参数来透视数据表的能力。\nfrom(bucket: “telegraf&#x2F;autogen”)    |&gt; range(start: -1h)    |&gt; filter(fn: (r) &#x3D;&gt; r._measurement &#x3D;&#x3D; “cpu” and r.cpu &#x3D;&#x3D; “cpu-total”)    |&gt; pivot(rowKey: [“_time”], columnKey: [“_field”], valueColumn: “_value”)\n12345### 6.1.9、直方图生成直方图的能力一直是 InfluxQL 非常需要的功能，但从未得到支持。 Flux 的 histogram() 函数使用输入数据生成累积直方图，并支持未来的其他直方图类型。\nfrom(bucket: “telegraf&#x2F;autogen”)    |&gt; range(start: -1h)    |&gt; filter(fn: (r) &#x3D;&gt; r._measurement &#x3D;&#x3D; “mem” and r._field &#x3D;&#x3D; “used_percent”)    |&gt; histogram(buckets: [10, 20, 30, 40, 50, 60, 70, 80, 90, 100,])\n1234567### 6.1.10、协方差Flux 提供了简单的协方差计算函数。 covariance() 函数计算两列之间的协方差，cov() 函数计算两个数据流之间的协方差。**1、两列之间的协方差**\nfrom(bucket: “telegraf&#x2F;autogen”)    |&gt; range(start: -5m)    |&gt; covariance(columns: [“x”, “y”])\n123**2、两个数据流之间的协方差**\ntable1 &#x3D; from(bucket: “telegraf&#x2F;autogen”)    |&gt; range(start: -15m)    |&gt; filter(fn: (r) &#x3D;&gt; r._measurement &#x3D;&#x3D; “measurement_1”)\ntable2 &#x3D; from(bucket: “telegraf&#x2F;autogen”)    |&gt; range(start: -15m)    |&gt; filter(fn: (r) &#x3D;&gt; r._measurement &#x3D;&#x3D; “measurement_2”)\ncov(x: table1, y: table2, on: [“_time”, “_field”])\n1234567### 6.1.11、将布尔值转换为整数InfluxQL 支持类型转换，但仅适用于数字数据类型（浮点数到整数，反之亦然）。 Flux 类型转换函数为类型转换提供了更广泛的支持，并允许您执行一些长期请求的操作，例如将布尔值转换为整数。**1、将布尔字段值转换为整数**\nfrom(bucket: “telegraf&#x2F;autogen”)    |&gt; range(start: -1h)    |&gt; filter(fn: (r) &#x3D;&gt; r._measurement &#x3D;&#x3D; “m” and r._field &#x3D;&#x3D; “bool_field”)    |&gt; toInt()\n12345### 6.1.12、字符串操作和数据整形InfluxQL 在查询数据时不支持字符串操作。 Flux Strings 包是对字符串数据进行操作的函数集合。当与 map() 函数结合使用时，字符串包中的函数允许进行字符串清理和规范化等操作。\nimport “strings”\nfrom(bucket: “telegraf&#x2F;autogen”)    |&gt; range(start: -1h)    |&gt; filter(fn: (r) &#x3D;&gt; r._measurement &#x3D;&#x3D; “weather” and r._field &#x3D;&#x3D; “temp”)    |&gt; map(        fn: (r) &#x3D;&gt; ({            r with            location: strings.toTitle(v: r.location),            sensor: strings.replaceAll(v: r.sensor, t: “ “, u: “-“),            status: strings.substring(v: r.status, start: 0, end: 8),        })    )\n123### 6.1.13、使用地时数据\nimport “experimental&#x2F;geo”\nfrom(bucket: “geo&#x2F;autogen”)    |&gt; range(start: -1w)    |&gt; filter(fn: (r) &#x3D;&gt; r._measurement &#x3D;&#x3D; “taxi”)    |&gt; geo.shapeData(latField: “latitude”, lonField: “longitude”, level: 20)    |&gt; geo.filterRows(region: {lat: 40.69335938, lon: -73.30078125, radius: 20.0}, strict: true)    |&gt; geo.asTracks(groupBy: [“fare-id”])\n\n6.2、使用 InfluxQL 和 Flux parity\n-----------------------------\n\nFlux 正在努力实现与 InfluxQL 的完全平等，并为此添加了新功能。下表显示了 InfluxQL 语句、子句和函数及其等效的 Flux 函数。\n\n*to()函数只写入InfluxDB 2.0\n\n","slug":"MIDDLEWARE/Flux数据脚本语言","date":"2023-04-26T20:04:53.000Z","categories_index":"Flux,MIDDLEWARE","tags_index":"使用,time,value","author_index":"dandeliono"},{"id":"6f7565fb2acf3311af859a2ea4b76698","title":"SpringBoot3.x原生镜像-Native Image尝鲜","content":"SpringBoot3.x原生镜像-Native Image尝鲜前提#Spring团队致力于为Spring应用程序提供原生映像支持已经有一段时间了。在SpringBoo2.x的Spring Native实验项目中酝酿了3年多之后，随着Spring Framework 6和Spring Boot 3的发布，对应的项目就是Spring Native，原生镜像支持将会发布GA版本（换言之就是，Native Image相关支持会在Spring Boot 3的GA版本中一起发布）。\n\n\n\n\n\n\n\n\n\n前面这一段简介摘抄自参考资料中的《Native Support in Spring Boot 3.0.0-M5》\n笔者在写这篇文章（2022-10-28）前后SpringBoot尚未发布3.x GA，版本3.0.0-M5+算是GA前相对稳定的版本，这里选用当前3.x的最新非GA版本3.0.0-RC1进行调研。\n\n\n什么是Native Image#Native Image，这里直译为原生镜像或者本地镜像，是一种提前将（Java）代码编译为二进制文件（原生可执行文件，native executable）的技术。原生可执行文件只包含运行时所需要的代码，即应用程序类、标准库类、语言运行时和来自JDK的静态链接的原生代码（也就是**这样的二进制文件可以直接运行，不需要额外安装JDK**）。由原生镜像生成的可执行文件有几个重要的优点：\n\n使用Java虚拟机所需资源的一小部分，因此运行成本更低\n启动时间大幅度下降，以毫秒为单位\n不需要进行预热即可提供最佳性能\n可以打包到轻量级容器映像中以便快速有效地部署\n减少了攻击面（这个和网络安全相关）\n\nSpring Boot 3中使用GraalVM方案提供Native Image支持。\n安装GraalVM#在https://www.graalvm.org/downloads - Download GraalVM页面中下载对应操作系统的GraalVM：\n\n\n笔者开发环境使用的操作系统是Windows10，下载和选用下图中的安装包：\n\n\n解压完成后配置一下JAVA_HOME、GRAALVM_HOME并且把GRAALVM_HOME\\bin添加到PATH中。完成后可以执行一下java -version进行验证：\n\n\n\n\n\n\n\n\n\n\n\n如果已经安装了其他版本的JDK，先暂时全局替换为GraalVM，也就是JAVA_HOME、GRAALVM_HOME同时配置为GraalVM的解压目录，因为目前看来这样做才能正常打包原生镜像\n确定GraalVM版本无误，到此安装完成。另外，需要配置好了Maven，建议重新安装一个3.6.x+版本的Maven并且把MAVEN_HOME\\bin添加到PATH中。\n编写应用程序#新建一个命名为spring-boot-native-image的Maven项目或者模块，选用刚才下载好的GraalVM：\n\n\n项目的POM文件引入下面几组依赖：\n\nspring的快照repository，因为需要下载RC1版本依赖，暂时不能从中央仓库拉取\nspring-boot-starter-parent，定义版本为RC1\nnative-maven-plugin插件，用于原生镜像打包\nspring-boot-starter-web，用于构建一个简单的web项目\n\n1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768 &lt;parent&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;    &lt;version&gt;3.0.0-RC1&lt;/version&gt;&lt;/parent&gt;&lt;repositories&gt;    &lt;repository&gt;        &lt;id&gt;spring-snapshots&lt;/id&gt;        &lt;url&gt;https://repo.spring.io/snapshot&lt;/url&gt;        &lt;snapshots&gt;            &lt;enabled&gt;true&lt;/enabled&gt;        &lt;/snapshots&gt;    &lt;/repository&gt;    &lt;repository&gt;        &lt;id&gt;spring-milestones&lt;/id&gt;        &lt;url&gt;https://repo.spring.io/milestone&lt;/url&gt;    &lt;/repository&gt;&lt;/repositories&gt;&lt;pluginRepositories&gt;    &lt;pluginRepository&gt;        &lt;id&gt;spring-snapshots&lt;/id&gt;        &lt;url&gt;https://repo.spring.io/snapshot&lt;/url&gt;    &lt;/pluginRepository&gt;    &lt;pluginRepository&gt;        &lt;id&gt;spring-milestones&lt;/id&gt;        &lt;url&gt;https://repo.spring.io/milestone&lt;/url&gt;    &lt;/pluginRepository&gt;&lt;/pluginRepositories&gt;&lt;dependencies&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;        &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;    &lt;/dependency&gt;&lt;/dependencies&gt;&lt;plugins&gt;    &lt;plugin&gt;        &lt;groupId&gt;org.graalvm.buildtools&lt;/groupId&gt;        &lt;artifactId&gt;native-maven-plugin&lt;/artifactId&gt;        &lt;version&gt;0.9.16&lt;/version&gt;        &lt;extensions&gt;true&lt;/extensions&gt;        &lt;executions&gt;            &lt;execution&gt;                &lt;id&gt;build-native&lt;/id&gt;                &lt;goals&gt;                    &lt;goal&gt;compile-no-fork&lt;/goal&gt;                &lt;/goals&gt;                &lt;phase&gt;package&lt;/phase&gt;            &lt;/execution&gt;            &lt;execution&gt;                &lt;id&gt;test-native&lt;/id&gt;                &lt;goals&gt;                    &lt;goal&gt;test&lt;/goal&gt;                &lt;/goals&gt;                &lt;phase&gt;test&lt;/phase&gt;            &lt;/execution&gt;        &lt;/executions&gt;        &lt;configuration&gt;            &lt;mainClass&gt;cn.vlts.NativeApplication&lt;/mainClass&gt;            &lt;imageName&gt;native-app&lt;/imageName&gt;            &lt;buildArgs&gt;                &lt;buildArg&gt;--verbose&lt;/buildArg&gt;            &lt;/buildArgs&gt;        &lt;/configuration&gt;    &lt;/plugin&gt;&lt;/plugins&gt; \n\n最终的POM文件看起来如下：\n\n\n项目中只有一个启动类cn.vlts.NativeApplication，编写了main方法和一个用于集成测试的控制器方法：\n12345678910111213@RestController@SpringBootApplicationpublic class NativeApplication &#123;    public static void main(String[] args) &#123;        SpringApplication.run(NativeApplication.class, args);    &#125;    @GetMapping(path = &quot;/hello&quot;)    public ResponseEntity&lt;String&gt; hello() &#123;        return ResponseEntity.ok(&quot;world&quot;);    &#125;&#125; \n\n打包和调试#完成项目配置和代码编写后，执行下面的Maven命令进行打包：\n1mvn package -Pnative \n\n打包过程可能会遇到下面的问题：\n\n最有可能的问题：Default native-compiler executable &#39;cl.exe&#39; not found via environment variable PATH\n\n解决方案在Stackoverflow对应的问题回答中找到：\n\n\n其实就是在Window操作系统开发环境下基于GraalVM构建原生镜像依赖Microsoft Visual C++ (MSVC)，建议安装MSVC 2017 15.5.5+，可以安装Visual Studio (2019)并且安装对应的MSVC。\n因为很早之前笔者在调试Rust时候已经安装过Visual Studio 2019用于其debug工具链，这里无须进行安装。在安装Visual Studio勾选MSVC vXXX的组件进行安装即可，然后需要把对应的MSVC工具的bin目录添加到PATH中（这个目录一般是VS_HOME\\VC\\Tools\\MSVC\\版本号\\bin\\Hostx64\\x64）：\n\n\n\n其次可能遇到的问题：打包过程出现stdio.h库文件报错或者找不到主类Main entry point class &#39;app.jar&#39; not found x.y.Application\n\n其实还是因为MSVC的问题，在GraalVM文档中有提示如下：\n\n\n简单来说就是必须在Visual Studio自带的命令行工具x64 Native Tools Command Prompt中执行native image相关命令，这个命令行工具初始化如下：\n\n\n在x64 Native Tools Command Prompt中先进入目标项目根目录，然后执行mvn -Pnative package：\n\n\n最终看到BUILD SUCCESS字眼，项目的target目录下可以看到一个.exe和一个.jar文件，而.exe文件就是前面一直提到的可执行的二进制文件：\n\n\n直接运行它：\n\n\n可以看到这个文件运行完全不依赖外部Java虚拟机，并且启动速度极快（600毫秒左右），可以用POSTMAN等工具测试程序接口：\n\n\n到此可以验证程序功能正常。\n小结#SpringBoot3.x原生镜像正式发布后会是SpringBoot在云原生领域的一个巨大进步，让我们拭目以待。但是就目前来看，常用的Windows开发环境下想要尝试native image技术需要解决比较多的问题，Linux和Unix平台下尚未验证，希望后面的版本迭代能够降低使用难度并且支持一个命令多平台打包的功能。\n参考资料：\n\nNative Support in Spring Boot 3.0.0-M5\nGraalVM - Quick Start Guide\n\n","slug":"JAVA/SpringBoot3.x原生镜像-Native Image尝鲜","date":"2023-04-26T16:52:21.000Z","categories_index":"native,JAVA","tags_index":"https,spring,boot","author_index":"dandeliono"},{"id":"09d76d6e509e739e24346e7394d4915f","title":"如何在Ubuntu 20.04设置DNS","content":"如何在Ubuntu 20.04设置DNSDNS域名系统是网络基础架构的核心部分，提供了一种将域名转换为IP地址的方法。\n您可以将DNS视为互联网的电话簿。注意这篇文章不是在Ubuntu 20.04配置自己的DNS服务器。而是给客户端配置DNS的IP地址。\n连接到Internet的每个设备均由其IP地址作为唯一标识。当您在浏览器中键入要访问的网站时，其域名必须转换为相应的IP地址。\n什么是 DNS与DNS工作方式操作系统首先检查其hosts文件中是否存在相应的域，如果域名中不存在hosts文件，它将查询配置的DNS域名服务器以解析指定的域名。\n域名解析后，系统将缓存请求并保留域名和相应IP地址的记录。DNS域名服务器为其他设备执行DNS解释的服务器。\n通常，DNS服务器地址由您的ISP分配。但是，这些DNS服务器可能运行缓慢或未定期更新，这可能会导致在查询域名时解析为错误的IP地址。\n或者出现延迟，有时您可能根本无法解析所需的域名的IP地址。有很多免费的公共DNS服务器。\n这些DNS域名服务器快速，并定期更新。这是一些最受欢迎的公共DNS服务器。Google（8.8.8.8，8.8.4.4），Cloudflare（1.1.1.1，1.0.0.1）。\nOpenDNS（208.67.222.222，208.67.220.220），Level3（209.244.0.3，209.244.0.4）。\n如果要为连接到本地网络的所有设备设置DNS域名服务器，那么最简单和建议的方法是在家庭路由器进行更改。\nUbuntu 20.04 查看 DNS在Linux 查看正在使用DNS服务器IP地址方法有很多，可以使用[dig](https://www.myfreax.com/ru-he-zai-linuxshi-yong-dnscha-xun-gong-ju-dig)命令或者查看systemd-resolve服务的状态。\nsystemd-resolve -status命令打印很多信息。但我们可以使用grep命令来过滤DNS Servers字符串。\n对于dig命令，如果只需要查看当前使用DNS地址，请运行命令dig myfreax.com | grep SERVER。\n12dig myfreax.com | grep SERVER sudo systemd-resolve --status | grep &#x27;DNS Servers&#x27; -A2\n\n1;; SERVER: 8.8.8.8#53(8.8.8.8)\n\ndig myfreax.com | grep SERVER \n12DNS Servers: 1.1.1.1             1.0.0.1\n\nsudo systemd-resolve –status | grep ‘DNS Servers’ -A2\nUbuntu 桌面设置DNS在Ubuntu 20.04桌面环境设置DNS地址非常简单，不需要任何技术知识。首先打开设置窗口。\n如果您连接到WiFi网络，请单击Wi-FI标签。否则，如果您有有线连接，请单击网络选项卡。\n选择要为其设置DNS的连接，然后单击齿轮图标以打开网络管理器。选择IPv4设置选项卡。\n禁用自动切换开关，然后输入DNS服务器的IP地址，以逗号分隔。我们将使用Google DNS服务器。\n\nUbunut 20.04 设置 DNS地址\n点击应用按钮以保存更改。更改立即生效，除非您的系统或应用程序缓存了DNS记录。\n如果要切换回原来的设置，请打开网络管理器，转到IPv4设置并启用自动切换开关。\nUbuntu 20.04服务器设置DNS过去，每当您想在Linux中配置DNS地址时，只需打开/etc/resolv.conf文件，编辑记录，保存文件就可以。\n现在/etc/resolv.conf文件仍然存在，但它是由systemd-resolved服务控制的符号链接，不应手动对其进行编辑。\nsystemd-resolved是为本地服务和应用程序提供DNS名称解析的服务，可以使用Netplan进行配置，Netplan是Ubuntu 20.04的默认网络管理工具。\nNetplan配置文件存储在/etc/netplan目录。您可能会在此目录中找到一个或两个YAML文件。\n各个安装程序的文件名可能不同。通常，该文件名为01-netcfg.yaml或50-cloud-init.yaml，但在您的系统中，可能会有所不同。\n这些文件使您可以配置网络接口，我们通常称为网卡，包括IP地址，网关，DNS域名服务器等。\n要设置DNS，请使用你喜欢的文本编辑器编辑文件，在本教程中我们将使用vim编辑文件。\n1sudo vim /etc/netplan/01-network-manager-all.yaml\n\n12345678network:  version: 2  renderer: NetworkManager  ethernets:    ens3:      dhcp4: true      nameservers:        addresses: [8.8.8.8, 8.8.4.4]\n\n&#x2F;etc&#x2F;netplan&#x2F;01-network-manager-all.yaml\n注意，你必须修改本教程中接口名称ens3为你的计算机接口名称。然后使用您的首选DNS更改nameservers的IP地址。\n例如如果您想使用Cloudflare的DNS服务器，则可以将nameservers的addresses行更改为。\nDNS服务器IP地址必须用逗号分隔，如果该nameservers字段不存在，请将nameservers添加到接口名称字段下。\n12nameservers:    addresses: [1.1.1.1, 1.0.0.1]\n\n编辑Yaml文件时，请确保您遵循YAML代码缩进标准。如果配置中存在语法错误，Netplan将无法解析该文件。完成后，保存文件并退出vim。\n然后运行命令sudo netplan apply应用更改。Netplan将为systemd-resolved的服务生成配置文件&#x2F;run&#x2F;systemd&#x2F;resolve&#x2F;resolv.conf。\n此外，还有一些应用程序依然使用&#x2F;etc&#x2F;resolv.conf的配置文件的DNS地址进行域名的解释，因此你还需要修改&#x2F;etc&#x2F;resolv.conf文件。\n要修改此文件我们只需要将&#x2F;run&#x2F;systemd&#x2F;resolve&#x2F;resolv.conf软链接到&#x2F;etc&#x2F;resolv.con即可。\n运行命令sudo ln -sf /run/systemd/resolve/resolv.conf /etc/resolv.conf。\n12sudo netplan applysudo ln -sf /run/systemd/resolve/resolv.conf /etc/resolv.conf \n\nUbuntu 20.04 验证DNS设置要验证是否正确设置DNS地址，请运行命令systemd-resolve --status | grep &#39;DNS Servers&#39; -A2或者命令dig www.myfreax.com。\nsystemd-resolve -status命令打印很多信息。我们使用grep命令来过滤DNS Servers字符串。\n1sudo systemd-resolve --status | grep &#x27;DNS Servers&#x27; -A2\n\n12DNS Servers: 1.1.1.1             1.0.0.1\n\n这里需要注意的是，如果你的Ubuntu 20.04带有桌面环境，请一定使用桌面环境的网络设置来修改DNS的IP地址。\n如果你想使用命令的方式修改DNS地址，可能会不如你所愿。当你在终端运行命令dig www.myfreax.com的时候。\n你将会在dig命令的结果看到应答的服务器永远是;; SERVER: 127.0.0.1#53(127.0.0.1)或者是你的网关地址;; SERVER: 192.168.1.1#53(192.168.1.1)。\n1dig www.myfreax.com\n\n12345678910111213141516171819; &lt;&lt;&gt;&gt; DiG 9.16.1-Ubuntu &lt;&lt;&gt;&gt; www.myfreax.com;; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 451;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 512;; QUESTION SECTION:;www.myfreax.com.\t\tIN\tA;; ANSWER SECTION:www.myfreax.com.\t300\tIN\tA\t43.457.68.101;; Query time: 119 msec;; SERVER: 8.8.8.8#53(8.8.8.8);; WHEN: Thu Dec 01 19:14:33 HKT 2022;; MSG SIZE  rcvd: 60\n\n结论Netplan是Ubuntu 20.04的默认网络管理工具，替代之前的Ubuntu版本中用于配置网络的/etc/resolv.conf和/etc/network/interfaces配置文件。\n","slug":"LINUX/如何在Ubuntu 20.04设置DNS","date":"2023-04-25T15:22:25.000Z","categories_index":"DNS,LINUX","tags_index":"https,com,myfreax","author_index":"dandeliono"},{"id":"9af4a9fff87dbbddf6cd710f2420d952","title":"influxdb内存消耗分析及性能优化—2","content":"influxdb内存消耗分析及性能优化—2\n由于业务场景需求，在生产环境服务器(32core64G)搭建了基于golang开发的influx时序数据库v1.8版本 ，经过持续一周的运行之后(每天写入约100G数据)，发现服务器内存消耗95%以上，并偶现SWAP报警:\n\n\n\n\n\n\n\n\n\n(swap使用率)[交换内存使用率][79.10744][server_alarm]\n使用top命令查看当前服务器状态:\n1top - 16:06:48 up 31 days,  1:03,  4 users,  load average: 0.01, 0.11, 0.30Tasks:   1 total,   0 running,   1 sleeping,   0 stopped,   0 zombie%Cpu(s): 45.0 us,  3.0 sy,  0.0 ni, 37.9 id, 43.1 wa,  0.0 hi,  0.0 si,  0.0 stKiB Mem : 65433636 total,   274968 free, 63048048 used,  2110620 buff/cacheKiB Swap: 32833532 total, 30839420 free,  1994112 used.  1776336 avail Mem   PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                                                       32309 root      20   0  0.411t 0.058t 177080 S   1534 95.3   6926:00 influxd\n\ninflux进程物理内存占用58G, 内存使用率95.3%；且当前wa为43.1，说明磁盘IO非常繁忙。于是便思考:\n\n为什么进程内存消耗那么高？\n为什么磁盘io那么忙？\n\n(1) 使用influx客户端, 查看influx服务的 runtime状态:\n1&gt; ./influx -host 10.x.xx.xx -port xx -username &#x27;x&#x27; -password &#x27;x&#x27; -execute &quot;show stats&quot;                      name: runtimeAlloc       Frees     HeapAlloc   HeapIdle    HeapInUse   HeapObjects HeapReleased HeapSys     Lookups Mallocs   NumGC NumGoroutine PauseTotalNs   Sys         TotalAlloc-----       -----        ---------       --------      ---------          ----------- ------------ -------                    ------- -------      ----- ------------ ------------                   ---         ----------16389315856 363815829 16389315856 51905806336 16609361920 254434947   44391612416  68515168256 0       618250776 2336  24           15652952340  71222090601 45846325521880 name: databasetags: database=iot_cloudnumMeasurements numSeries--------------- ---------3               20927158\n\n发现当前influxd进程 HeapIdle约51G, HeapInUse约16G, HeapReleased约44G, 当前series数量为2092万左右.随之而来的疑惑:\n为什么进程RES实际占用58G, 而当前进程runtime堆占用内存仅有23G ???\n\n\n\n\n\n\n\n\n\ninfluxdb_v1.8基于go1.13编译，参考runtime相关参数的注释:\n(https://github.com/golang/go/blob/release-branch.go1.13/src/runtime/mstats.go#L245)，\n按照go的内存分配空间布局规则，可以根据如下计算方式估计go的当前堆内存:\n(HeapIdle)51-(HeapReleased)44+(HeapInUse)16 &#x3D; 23 G\n(2) 为了确认是否存在内存泄漏，进一步查看进程的内存块详细数据:\n1#当前influxd进程id为32309#1.pmap命令查看进程内存块分配&gt; pmap -x 32309 | less32309:   /etc/influxdb/usr/bin/influxd -config /etc/influxdb/influxdb.confAddress           Kbytes     RSS   Dirty Mode  Mapping0000000000400000   14928    2284       0 r-x-- influxd0000000001294000   31092    3664       0 r---- influxd00000000030f1000    4668    4360     368 rw--- influxd0000000003580000     180      96      96 rw---   [ anon ]0000000004ead000     132       0       0 rw---   [ anon ]000000c000000000 66912256 59789960 51476676 rw---   [ anon ] #堆内存00007f80e6469000    4232    1440    1440 rw---   [ anon ]00007f80e6913000 1886000 1872676 1872672 rw---   [ anon ]00007f8159ae5000  232264  230124  230124 rw---   [ anon ]00007f8167dc3000  172360  168804  168804 rw---   [ anon ]00007f817261c000  111564  107452  107452 rw---   [ anon ] #2.查看更详细的每一块内存分配#命令:cat /proc/pid/smaps#如下发现进程堆内存地址空间为:c000000000-cff4000000&gt; cat /proc/32309/smaps | lessc000000000-cff4000000 rw-p 00000000 00:00 0Size:           66912256 kBRss:            59789960 kBPss:            59789960 kBShared_Clean:          0 kBShared_Dirty:          0 kBPrivate_Clean:   8313284 kBPrivate_Dirty:  51476676 kBReferenced:     51368732 kBAnonymous:      59789960 kBAnonHugePages:   5994496 kBSwap:            1055452 kBKernelPageSize:        4 kBMMUPageSize:           4 kBLocked:                0 kBVmFlags: rd wr mr mp me ac sd #3.使用gdb打印堆栈#输入程序地址空间0xc000000000 0xcff4000000&gt; gdb -p 32309&gt;&gt;&gt; dump binary memory ./meminfo.log 0xc000000000 0xcff4000000&gt;&gt;&gt; bt    #查看内存调用栈backtrace&gt;&gt;&gt; q     #退出 #4.查看内存内容meminfo.loghexdump -C ./meminfo.log | less #查看内存块数据\n\n通过内存块调用栈 bt命令及导出的 meminfo.log文件，并没有发现内存泄漏的导向。\n(3) 使用go pprof查看进程累计内存分配 alloc-space:\n1&gt; go tool pprof -alloc_space http://x.x.x.x:xx/debug/pprof/heapFetching profile over HTTP from http://x.x.x.x:xx/debug/pprof/heapSaved profile in /home/yushaolong/pprof/pprof.influxd.alloc_objects.alloc_space.inuse_objects.inuse_space.004.pb.gzFile: influxdType: alloc_spaceTime: Oct 9, 2020 at 3:59pm (CST)Entering interactive mode (type &quot;help&quot; for commands, &quot;o&quot; for options)(pprof) topShowing nodes accounting for 42527GB, 99.59% of 42700.66GB totalDropped 443 nodes (cum &lt;= 213.50GB)      flat  flat%   sum%        cum   cum%   42527GB 99.59% 99.59%    42527GB 99.59%  github.com/influxdata/influxdb/tsdb/index/inmem.(*Index).DropSeriesGlobal /go/src/github.com/influxdata/influxdb/tsdb/index/inmem/inmem.go         0     0% 99.59% 42528.02GB 99.60%  github.com/influxdata/influxdb/services/retention.(*Service).Open.func1 /go/src/github.com/influxdata/influxdb/services/retention/service.go         0     0% 99.59% 42527.94GB 99.60%  github.com/influxdata/influxdb/tsdb.(*Store).DeleteShard.func3 /go/src/github.com/influxdata/influxdb/tsdb/store.go(pprof) list DropSeriesGlobalTotal: 42700.66GBROUTINE ======================== github.com/influxdata/influxdb/tsdb/index/inmem.(*Index).DropSeriesGlobal in /go/src/github.com/influxdata/influxdb/tsdb/index/inmem/inmem.go   42527GB    42527GB (flat, cum) 99.59% of Total         .          .    792:   &#125;         .          .    793:         .          .    794:   i.mu.Lock()         .          .    795:   defer i.mu.Unlock()         .          .    796:   42527GB    42527GB    797:   k := string(key)         .          .    798:   series := i.series[k]         .          .    799:   if series == nil &#123;         .          .    800:           return nil         .          .    801:   &#125;         .          .    802:(pprof)\n\n发现进程在删除series(influx索引)时, 累计消耗了42T的内存空间。说明进程在series删除时消耗了大量的内存堆(https://github.com/influxdata/influxdb/issues/10453)，所以占用内存会在此时持续飙高，但这些内存应该会被GC掉？重新看一下runtime及系统内存分配，发现了一些端倪:\n| 进程RES | HeapIdle | HeapReleased | HeapInUse || 58G | 51G | 44G | 16G |\n目前influxd进程持有的有效内存为 51-44+16&#x3D;23G, 而系统进程RES为58G。猜想存在58-23&#x3D;35g的内存，进程标记不再使用，当然系统也没有进行回收。 \n(4) 使用 memtester 工具验证猜想:\n1# 内存测试工具 memtester# 使用文档:https://www.cnblogs.com/xiayi/p/9640619.html# 向操作系统申请 30G内存&gt; /usr/local/bin/memtester 30G 1memtester version 4.5.0 (64-bit)Copyright (C) 2001-2020 Charles Cazabon.Licensed under the GNU General Public License version 2 (only). pagesize is 4096pagesizemask is 0xfffffffffffff000want 20480MB (21474836480 bytes)got  20480MB (21474836480 bytes), trying mlock ...locked.Loop 1/1:  Stuck Address       : setting   1\n\n向操作系统申请30G内存后，使用命令查看内存状态:\n1top - 16:24:13 up 31 days,  1:21,  5 users,  load average: 1.28, 1.21, 0.70Tasks: 386 total,   2 running, 384 sleeping,   0 stopped,   0 zombie%Cpu(s):  3.1 us,  0.0 sy,  0.0 ni, 96.8 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 stKiB Mem : 65433636 total,   820012 free, 63739976 used,   873648 buff/cacheKiB Swap: 32833532 total, 30837852 free,  1995680 used.  1174164 avail Mem   PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND            32309 root      20   0  0.411t   0.029t   8764 S   0.0 48.1   6946:31 influxd14921 root      20   0  30.004g  0.029t    464 R  99.7 48.1   3:25.59 memtester\n\n此时，influxd占用29G内存，memtester占用29G内存。\n果然influxd释放的内存，此时才被系统重新回收, 翻阅了go的资料，找到了原因\n(https://colobu.com/2019/08/28/go-memory-leak-i-dont-think-so/):\n\n\n\n\n\n\n\n\n\n一直以来 go 的 runtime 在释放内存返回到内核时，在 Linux 上使用的是 MADV_DONTNEED，虽然效率比较低，但是会让 RSS（resident set size 常驻内存集）数量下降得很快。不过在 go 1.12 里专门针对这个做了优化，runtime 在释放内存时，使用了更加高效的 MADV_FREE 而不是之前的 MADV_DONTNEED。这样带来的好处是，一次 GC 后的内存分配延迟得以改善，runtime 也会更加积极地将释放的内存归还给操作系统，以应对大块内存分配无法重用已存在的堆空间的问题。不过也会带来一个副作用：RSS 不会立刻下降，而是要等到系统有内存压力了，才会延迟下降。为了避免像这样一些靠判断 RSS 大小的自动化测试因此出问题，也提供了一个 GODEBUG&#x3D;madvdontneed&#x3D;1 参数可以强制 runtime 继续使用 MADV_DONTNEED。\n原来是由于go内部优化而使进程内存没有立即释放，至此解答了内存高消耗的疑惑。\n使用 iostat 命令查看磁盘io状态:\n1#系统命令iostat&gt; iostat -x 1 3 #每秒打印1次，打印3次磁盘状态#示例第二次状态avg-cpu:  %user   %nice %system %iowait  %steal   %idle           7.98    0.00    3.80   10.33    0.00   77.89 Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %utilsda              26.00  1480.00 2544.00 1692.00 28800.00 33576.00    29.45     4.18    0.98    1.59    0.06   0.23  98.70dm-0              0.00     0.00  316.00    0.00  1384.00     0.00     8.76     0.42    1.36    1.36    0.00   0.88  27.90dm-4              0.00     0.00 2198.00 3163.00 27192.00 33576.00    22.67     3.48    0.64    1.48    0.06   0.18  97.50\n\n按照 linux进程io磁盘性能分析 ,得知influxd进程写盘的Device为dm-4 ，指标分析如下:\n\n当前磁盘iops为5361&#x2F;s (r + w)\n每秒io读取约27M&#x2F;s, 写入约33M&#x2F;s\nio队列中，有3.48个堆积 (avgqu-sz)\n每次io等待 0.47ms (await), 处理耗时0.17ms (svctm)\n%util为97.5%，接近100%，说明I&#x2F;O请求太多，I&#x2F;O系统已经满负荷\n\n发现influx进程对磁盘的io消耗过大。\n通过以上分析，可以得到:\n\ninflux使用 inmem 引擎时(默认)，在retention policy时会消耗过高的内存\n使用 GODEBUG&#x3D;madvdontneed&#x3D;1 可以让go程序尽快释放内存\ninflux磁盘的iops过高，应该从(增大内存buffer&#x2F;增加批量写落盘)方面进行优化\n\n因此对配置文件influxdb.conf做了如下优化:\n1# 详细配置说明见官方文档# https://docs.influxdata.com/influxdb/v1.8/administration/config/#data-settings [data]  #说明: wal预写日志log,用于事务一致性  #默认为0，每次写入都落盘。  #修改为1s, 根据业务场景，不保证强一致性,可采用异步刷盘  #[优化点]:用于减轻磁盘io压力  wal-fsync-delay = &quot;1s&quot;     #说明: influx索引  #默认为inmem,创建内存型索引,在delete retention会消耗过高内存  #修改为tsi1, 注意重建tsi1索引(https://blog.csdn.net/wzy_168/article/details/107043840)  #[优化点]:降低删除保留策略时的内存消耗  index-version = &quot;tsi1&quot;     #说明: 压缩TSM数据,一次落盘的吞吐量  #默认48m  #修改为64m  #[优化点]:增大写入量，减轻io压力  compact-throughput = &quot;64m&quot;\n\n修改配置之后，执行如下命令启动influx进程:\n1env GODEBUG=madvdontneed=1 /usr/bin/influxd  -config  /usr/bin/influxdb.conf\n\ninfluxd进程重新运行一周之后，再次观察系统状态:\n(1) 内存消耗约占55%左右:\n\n(2) 磁盘iops约为200左右，util占用6.2%。\n1avg-cpu:  %user   %nice %system %iowait  %steal   %idle          7.21    0.00    1.00   0.16    0.00   91.64 Device:         rrqm/s   wrqm/s     r/s     w/s       rkB/s    wkB/s      avgrq-sz   avgqu-sz   await   r_await   w_await  svctm   %utilsda                  0.00    39.00       5.00  161.00    20.00   28036.00   338.02     0.25           1.51     10.40      1.24         0.37    6.10dm-4              0.00     0.00         5.00  189.00    20.00   28036.00   289.24     0.26           1.32     10.60      1.07         0.32    6.20\n\n发现进程运行符合预期,问题得到初步解决。\n","slug":"MIDDLEWARE/influxdb内存消耗分析及性能优化—2","date":"2023-04-19T16:40:55.000Z","categories_index":"influx,MIDDLEWARE","tags_index":"使用,influxd,runtime","author_index":"dandeliono"},{"id":"6e2f3443cc73a783063825b9fb8c089c","title":"influxdb内存消耗分析及性能优化【追踪篇】","content":"influxdb内存消耗分析及性能优化【追踪篇】由于业务场景需求，在生产环境服务器(32core64G)搭建了基于golang开发的influx时序数据库v1.8版本 ，经过持续一周的运行之后(每天写入约100G数据)，发现服务器内存消耗95%以上，并偶现SWAP报警:\n\n\n\n\n\n\n\n\n\n(swap使用率)[交换内存使用率][79.10744][server_alarm]\n使用top命令查看当前服务器状态:\n1top - 16:06:48 up 31 days,  1:03,  4 users,  load average: 0.01, 0.11, 0.30Tasks:   1 total,   0 running,   1 sleeping,   0 stopped,   0 zombie%Cpu(s): 45.0 us,  3.0 sy,  0.0 ni, 37.9 id, 43.1 wa,  0.0 hi,  0.0 si,  0.0 stKiB Mem : 65433636 total,   274968 free, 63048048 used,  2110620 buff/cacheKiB Swap: 32833532 total, 30839420 free,  1994112 used.  1776336 avail Mem   PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                                                       32309 root      20   0  0.411t 0.058t 177080 S   1534 95.3   6926:00 influxd\n\ninflux进程物理内存占用58G, 内存使用率95.3%；且当前wa为43.1，说明磁盘IO非常繁忙。于是便思考:\n\n为什么进程内存消耗那么高？\n为什么磁盘io那么忙？\n\n(1) 使用influx客户端, 查看influx服务的 runtime状态:\n1&gt; ./influx -host 10.x.xx.xx -port xx -username &#x27;x&#x27; -password &#x27;x&#x27; -execute &quot;show stats&quot;                      name: runtimeAlloc       Frees     HeapAlloc   HeapIdle    HeapInUse   HeapObjects HeapReleased HeapSys     Lookups Mallocs   NumGC NumGoroutine PauseTotalNs   Sys         TotalAlloc-----       -----        ---------       --------      ---------          ----------- ------------ -------                    ------- -------      ----- ------------ ------------                   ---         ----------16389315856 363815829 16389315856 51905806336 16609361920 254434947   44391612416  68515168256 0       618250776 2336  24           15652952340  71222090601 45846325521880 name: databasetags: database=iot_cloudnumMeasurements numSeries--------------- ---------3               20927158\n\n发现当前influxd进程 HeapIdle约51G, HeapInUse约16G, HeapReleased约44G, 当前series数量为2092万左右.随之而来的疑惑:\n为什么进程RES实际占用58G, 而当前进程runtime堆占用内存仅有23G ???\n\n\n\n\n\n\n\n\n\ninfluxdb_v1.8基于go1.13编译，参考runtime相关参数的注释:\n(https://github.com/golang/go/blob/release-branch.go1.13/src/runtime/mstats.go#L245)，\n按照go的内存分配空间布局规则，可以根据如下计算方式估计go的当前堆内存:\n(HeapIdle)51-(HeapReleased)44+(HeapInUse)16 &#x3D; 23 G\n(2) 为了确认是否存在内存泄漏，进一步查看进程的内存块详细数据:\n1#当前influxd进程id为32309#1.pmap命令查看进程内存块分配&gt; pmap -x 32309 | less32309:   /etc/influxdb/usr/bin/influxd -config /etc/influxdb/influxdb.confAddress           Kbytes     RSS   Dirty Mode  Mapping0000000000400000   14928    2284       0 r-x-- influxd0000000001294000   31092    3664       0 r---- influxd00000000030f1000    4668    4360     368 rw--- influxd0000000003580000     180      96      96 rw---   [ anon ]0000000004ead000     132       0       0 rw---   [ anon ]000000c000000000 66912256 59789960 51476676 rw---   [ anon ] #堆内存00007f80e6469000    4232    1440    1440 rw---   [ anon ]00007f80e6913000 1886000 1872676 1872672 rw---   [ anon ]00007f8159ae5000  232264  230124  230124 rw---   [ anon ]00007f8167dc3000  172360  168804  168804 rw---   [ anon ]00007f817261c000  111564  107452  107452 rw---   [ anon ] #2.查看更详细的每一块内存分配#命令:cat /proc/pid/smaps#如下发现进程堆内存地址空间为:c000000000-cff4000000&gt; cat /proc/32309/smaps | lessc000000000-cff4000000 rw-p 00000000 00:00 0Size:           66912256 kBRss:            59789960 kBPss:            59789960 kBShared_Clean:          0 kBShared_Dirty:          0 kBPrivate_Clean:   8313284 kBPrivate_Dirty:  51476676 kBReferenced:     51368732 kBAnonymous:      59789960 kBAnonHugePages:   5994496 kBSwap:            1055452 kBKernelPageSize:        4 kBMMUPageSize:           4 kBLocked:                0 kBVmFlags: rd wr mr mp me ac sd #3.使用gdb打印堆栈#输入程序地址空间0xc000000000 0xcff4000000&gt; gdb -p 32309&gt;&gt;&gt; dump binary memory ./meminfo.log 0xc000000000 0xcff4000000&gt;&gt;&gt; bt    #查看内存调用栈backtrace&gt;&gt;&gt; q     #退出 #4.查看内存内容meminfo.loghexdump -C ./meminfo.log | less #查看内存块数据\n\n通过内存块调用栈 bt命令及导出的 meminfo.log文件，并没有发现内存泄漏的导向。\n(3) 使用go pprof查看进程累计内存分配 alloc-space:\n1&gt; go tool pprof -alloc_space http://x.x.x.x:xx/debug/pprof/heapFetching profile over HTTP from http://x.x.x.x:xx/debug/pprof/heapSaved profile in /home/yushaolong/pprof/pprof.influxd.alloc_objects.alloc_space.inuse_objects.inuse_space.004.pb.gzFile: influxdType: alloc_spaceTime: Oct 9, 2020 at 3:59pm (CST)Entering interactive mode (type &quot;help&quot; for commands, &quot;o&quot; for options)(pprof) topShowing nodes accounting for 42527GB, 99.59% of 42700.66GB totalDropped 443 nodes (cum &lt;= 213.50GB)      flat  flat%   sum%        cum   cum%   42527GB 99.59% 99.59%    42527GB 99.59%  github.com/influxdata/influxdb/tsdb/index/inmem.(*Index).DropSeriesGlobal /go/src/github.com/influxdata/influxdb/tsdb/index/inmem/inmem.go         0     0% 99.59% 42528.02GB 99.60%  github.com/influxdata/influxdb/services/retention.(*Service).Open.func1 /go/src/github.com/influxdata/influxdb/services/retention/service.go         0     0% 99.59% 42527.94GB 99.60%  github.com/influxdata/influxdb/tsdb.(*Store).DeleteShard.func3 /go/src/github.com/influxdata/influxdb/tsdb/store.go(pprof) list DropSeriesGlobalTotal: 42700.66GBROUTINE ======================== github.com/influxdata/influxdb/tsdb/index/inmem.(*Index).DropSeriesGlobal in /go/src/github.com/influxdata/influxdb/tsdb/index/inmem/inmem.go   42527GB    42527GB (flat, cum) 99.59% of Total         .          .    792:   &#125;         .          .    793:         .          .    794:   i.mu.Lock()         .          .    795:   defer i.mu.Unlock()         .          .    796:   42527GB    42527GB    797:   k := string(key)         .          .    798:   series := i.series[k]         .          .    799:   if series == nil &#123;         .          .    800:           return nil         .          .    801:   &#125;         .          .    802:(pprof)\n\n发现进程在删除series(influx索引)时, 累计消耗了42T的内存空间。说明进程在series删除时消耗了大量的内存堆(https://github.com/influxdata/influxdb/issues/10453)，所以占用内存会在此时持续飙高，但这些内存应该会被GC掉？重新看一下runtime及系统内存分配，发现了一些端倪:\n| 进程RES | HeapIdle | HeapReleased | HeapInUse || 58G | 51G | 44G | 16G |\n目前influxd进程持有的有效内存为 51-44+16&#x3D;23G, 而系统进程RES为58G。猜想存在58-23&#x3D;35g的内存，进程标记不再使用，当然系统也没有进行回收。 \n(4) 使用 memtester 工具验证猜想:\n1# 内存测试工具 memtester# 使用文档:https://www.cnblogs.com/xiayi/p/9640619.html# 向操作系统申请 30G内存&gt; /usr/local/bin/memtester 30G 1memtester version 4.5.0 (64-bit)Copyright (C) 2001-2020 Charles Cazabon.Licensed under the GNU General Public License version 2 (only). pagesize is 4096pagesizemask is 0xfffffffffffff000want 20480MB (21474836480 bytes)got  20480MB (21474836480 bytes), trying mlock ...locked.Loop 1/1:  Stuck Address       : setting   1\n\n向操作系统申请30G内存后，使用命令查看内存状态:\n1top - 16:24:13 up 31 days,  1:21,  5 users,  load average: 1.28, 1.21, 0.70Tasks: 386 total,   2 running, 384 sleeping,   0 stopped,   0 zombie%Cpu(s):  3.1 us,  0.0 sy,  0.0 ni, 96.8 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 stKiB Mem : 65433636 total,   820012 free, 63739976 used,   873648 buff/cacheKiB Swap: 32833532 total, 30837852 free,  1995680 used.  1174164 avail Mem   PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND            32309 root      20   0  0.411t   0.029t   8764 S   0.0 48.1   6946:31 influxd14921 root      20   0  30.004g  0.029t    464 R  99.7 48.1   3:25.59 memtester\n\n此时，influxd占用29G内存，memtester占用29G内存。\n果然influxd释放的内存，此时才被系统重新回收, 翻阅了go的资料，找到了原因\n(https://colobu.com/2019/08/28/go-memory-leak-i-dont-think-so/):\n\n\n\n\n\n\n\n\n\n一直以来 go 的 runtime 在释放内存返回到内核时，在 Linux 上使用的是 MADV_DONTNEED，虽然效率比较低，但是会让 RSS（resident set size 常驻内存集）数量下降得很快。不过在 go 1.12 里专门针对这个做了优化，runtime 在释放内存时，使用了更加高效的 MADV_FREE 而不是之前的 MADV_DONTNEED。这样带来的好处是，一次 GC 后的内存分配延迟得以改善，runtime 也会更加积极地将释放的内存归还给操作系统，以应对大块内存分配无法重用已存在的堆空间的问题。不过也会带来一个副作用：RSS 不会立刻下降，而是要等到系统有内存压力了，才会延迟下降。为了避免像这样一些靠判断 RSS 大小的自动化测试因此出问题，也提供了一个 GODEBUG&#x3D;madvdontneed&#x3D;1 参数可以强制 runtime 继续使用 MADV_DONTNEED。\n原来是由于go内部优化而使进程内存没有立即释放，至此解答了内存高消耗的疑惑。\n使用 iostat 命令查看磁盘io状态:\n1#系统命令iostat&gt; iostat -x 1 3 #每秒打印1次，打印3次磁盘状态#示例第二次状态avg-cpu:  %user   %nice %system %iowait  %steal   %idle           7.98    0.00    3.80   10.33    0.00   77.89 Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %utilsda              26.00  1480.00 2544.00 1692.00 28800.00 33576.00    29.45     4.18    0.98    1.59    0.06   0.23  98.70dm-0              0.00     0.00  316.00    0.00  1384.00     0.00     8.76     0.42    1.36    1.36    0.00   0.88  27.90dm-4              0.00     0.00 2198.00 3163.00 27192.00 33576.00    22.67     3.48    0.64    1.48    0.06   0.18  97.50\n\n按照 linux进程io磁盘性能分析 ,得知influxd进程写盘的Device为dm-4 ，指标分析如下:\n\n当前磁盘iops为5361&#x2F;s (r + w)\n每秒io读取约27M&#x2F;s, 写入约33M&#x2F;s\nio队列中，有3.48个堆积 (avgqu-sz)\n每次io等待 0.47ms (await), 处理耗时0.17ms (svctm)\n%util为97.5%，接近100%，说明I&#x2F;O请求太多，I&#x2F;O系统已经满负荷\n\n发现influx进程对磁盘的io消耗过大。\n通过以上分析，可以得到:\n\ninflux使用 inmem 引擎时(默认)，在retention policy时会消耗过高的内存\n使用 GODEBUG&#x3D;madvdontneed&#x3D;1 可以让go程序尽快释放内存\ninflux磁盘的iops过高，应该从(增大内存buffer&#x2F;增加批量写落盘)方面进行优化\n\n因此对配置文件influxdb.conf做了如下优化:\n1# 详细配置说明见官方文档# https://docs.influxdata.com/influxdb/v1.8/administration/config/#data-settings [data]  #说明: wal预写日志log,用于事务一致性  #默认为0，每次写入都落盘。  #修改为1s, 根据业务场景，不保证强一致性,可采用异步刷盘  #[优化点]:用于减轻磁盘io压力  wal-fsync-delay = &quot;1s&quot;     #说明: influx索引  #默认为inmem,创建内存型索引,在delete retention会消耗过高内存  #修改为tsi1, 注意重建tsi1索引(https://blog.csdn.net/wzy_168/article/details/107043840)  #[优化点]:降低删除保留策略时的内存消耗  index-version = &quot;tsi1&quot;     #说明: 压缩TSM数据,一次落盘的吞吐量  #默认48m  #修改为64m  #[优化点]:增大写入量，减轻io压力  compact-throughput = &quot;64m&quot;\n\n修改配置之后，执行如下命令启动influx进程:\n1env GODEBUG=madvdontneed=1 /usr/bin/influxd  -config  /usr/bin/influxdb.conf\n\ninfluxd进程重新运行一周之后，再次观察系统状态:\n(1) 内存消耗约占55%左右:\n\n(2) 磁盘iops约为200左右，util占用6.2%。\n1avg-cpu:  %user   %nice %system %iowait  %steal   %idle          7.21    0.00    1.00   0.16    0.00   91.64 Device:         rrqm/s   wrqm/s     r/s     w/s       rkB/s    wkB/s      avgrq-sz   avgqu-sz   await   r_await   w_await  svctm   %utilsda                  0.00    39.00       5.00  161.00    20.00   28036.00   338.02     0.25           1.51     10.40      1.24         0.37    6.10dm-4              0.00     0.00         5.00  189.00    20.00   28036.00   289.24     0.26           1.32     10.60      1.07         0.32    6.20\n\n发现进程运行符合预期,问题得到初步解决。\n\n服务器体系(SMP, NUMA, MPP)与共享存储器架构(UMA和NUMA): https://cloud.tencent.com/developer/article/1372348\n理解virt res shr之间的关系: https://www.orchome.com/298\nSWAP的罪与罚: https://blog.huoding.com/2012/11/08/198\ngo调度器: https://draveness.me/golang/docs/part3-runtime/ch06-concurrency/golang-goroutine/\nNUMA-aware scheduler for Go: https://docs.google.com/document/u/0/d/1d3iI2QWURgDIsSR6G2275vMeQ_X7w-qxM2Vp7iGwwuM/pub\n\n","slug":"MIDDLEWARE/influxdb内存消耗分析及性能优化【追踪篇】","date":"2023-04-19T16:40:55.000Z","categories_index":"runtime,MIDDLEWARE","tags_index":"https,使用,influx","author_index":"dandeliono"},{"id":"9736c920c0447b484184935c1c343c5b","title":"influxdb内存消耗分析及性能优化","content":"influxdb内存消耗分析及性能优化\ninfluxdb目前支持内存型索引inmem及文件型索引tsi1。之前追踪篇将influxd索引修改为tsi1之后，经过一段时间的运行，从监控观察到，由于调用方采用异步队列+批处理的方案将数据写入influxdb，会在某些时刻调用方内部出现数据堆积，指标如图:\n\n横坐标: 时间轴，从12-29 00:00 到 12-30 00:00\n纵坐标: 队列中数据堆积长度，坐标最大值250k，即最大25w个数据堆积\n\n\n从上图可以看到，当天监控出现数次堆积，上午7:00-10:00尤为严重。在堆积时，登录influxdb服务器，查看机器状态如下:\n12345678top - 09:40:58 up 120 days, 19:18,  1 user,  load average: 32.29, 32.32, 29.82Tasks: 364 total,   1 running, 363 sleeping,   0 stopped,   0 zombie%Cpu(s):  0.4 us,  0.1 sy,  0.0 ni, 57.7 id, 41.8 wa,  0.0 hi,  0.0 si,  0.0 stKiB Mem : 65433892 total,   376024 free, 30179144 used, 34878724 buff/cacheKiB Swap: 32833532 total, 32689404 free,   26624 used. 34607748 avail Mem   PID USER      PR  NI    VIRT    RES    SHR    S   %CPU %MEM     TIME+ COMMAND 9571 root      20   0     0.269t 0.053t 0.025t  D  14.6 86.2     1081:18 influxd\n\n在出现堆积时，wa很高，说明问题再次出现在磁盘io上，而且influxd的SHR空间占用了25g，又是为什么？ 面对新出现的问题，当前的主要手段仍然是从influxdb配置文件入手，但是该如何优化?\n工欲善其事，必先利其器。 \n查阅了相关资料之后，整理了influxdb使用tsi索引时原理图:\n\n说明:\n\n写入influxdb时，会同时写wal文件及cache内存, wal用于宕机恢复cache\ncache在达到配置中的阈值时，会进行snapshot快照,进行落盘\ninfluxdb的series及index索引会在内存中全量保存，用于快速检索\nwal文件大小在达到配置中阈值时，会进行压缩转换到index索引\ninfluxdb会对磁盘数据文件NaN按照分片shard维度进行四次压缩(level1,2,3及full)，以节约磁盘空间\n\n在队列堆积的时间点，经过多次对比influxdb的日志:\n12345678910#调用方队列堆积时，influxdb的关键日志如下:#influx开始执行第四次全量压缩策略，tsm1_strategy=fullts=2021-01-05T10:30:02.049644Z lvl=info msg=&quot;TSM compaction (start)&quot; log\\_id=0RVRbtjl000 engine=tsm1 tsm1\\_strategy=full tsm1\\_optimize=false trace\\_id=0RWXOKGG000 op\\_name=tsm1\\_compact\\_group op\\_event=start...#省略...#第四次全量压缩结束ts=2021-01-05T10:44:13.931365Z lvl=info msg=&quot;TSM compaction (end)&quot; log\\_id=0RVRbtjl000 engine=tsm1 tsm1\\_strategy=full tsm1\\_optimize=false trace\\_id=0RWXOKGG000 op\\_name=tsm1\\_compact\\_group op\\_event=end op_elapsed=851881.724ms......\n\n此时influxdb进行分片数据的第四次的全量压缩，会进行大量的磁盘io及cpu压缩计算，导致服务的压力增大，所以调用方队列出现数据堆积，相关参数见配置文件:\n12345678910111213141516171819202122232425262728#influxdb部分配置文件#描述：内存快照的冷冻写入周期，默认10m#场景: 当创建新的shard分片开始接受数据之后，上个shard分片进入冷冻期，#冷冻期的shard分片不再接收写入的请求，10分钟之后，会将内存里冷冻shard的cache进行落盘操作#应用: 本文设置为30分钟。个人认为时间越短，会越快释放上个shard的内存cachecache-snapshot-write-cold-duration = &quot;30m&quot;#描述: 使用全量策略压缩冷冻期分片的周期，默认4h#场景: 当shard进入冷冻期后，会经过4h,开始进行全量压缩策略，进一步减少shard落盘数据占用的空间#与cache-snapshot-write-cold-duration配合使用#可以从日志中看到，新分片开始写入数据之后，在4h+10m之后，会对上个分片进行全量压缩策略#应用: 本文设置为80小时。目的是不进行full压缩策略，来避免io过多消耗，后面会介绍由于设置的retention policy为72小时，所以此处大于72即可。compact-full-write-cold-duration = &quot;80h&quot;#描述: 最大并行压缩数,默认会使用golang的逻辑处理器的一半#场景: 当进行level1，level2,level3及full策略压缩文件时使用的处理器数量，#当前服务器为16core32物理线程，则会在压缩时默认使用16个处理器#应用: 本文设置为8。用于减轻压缩策略时，cpu与磁盘io的压力，但相应的会导致压缩周期变长。max-concurrent-compactions = 8#描述: 压缩文件时,每秒写入磁盘的数据量，默认48MB#应用：本文设置为16MB，用于减轻磁盘写入时的压力，同样会导致压缩周期变长。compact-throughput = &quot;16m&quot;#描述: 压缩文件时,每秒最大写入磁盘的峰值数据量，默认48MB#应用：本文设置为16MB，用于减轻磁盘写入时的压力，同样会导致压缩周期变长。compact-throughput-burst = &quot;16m&quot;\n\n在本文第1小节中，使用linux的top命令可以看到，influxd进程占用RES内存为53g, SHR内存为25g。所以引申问题:\n\n何为SHR?\n为什么会有那么多的SHR?\nSHR对系统有什么影响?\n\n使用man查看top命令的解释:SHR – Shared Memory Size (KiB) The amount of shared memory available to a task, not all of which is typically resident. It simply reflects memory that could be potentially shared with other processes. 共享内存的大小 程序共享内存的数据量，并不是全部驻留在内存空间，通常反映了与其他程序潜在共用的内存。\n那为什么influxd会有那么高的共享内存？通过linux进程smaps分析当前实际占用内存的大小。\n注: smaps中有些文件的引用仅占用虚拟内存，而不占用物理内存\n12345678910111213141516171819202122232425262728293031#influxdb当前程序运行pid为: 9571#1.计算influxdb数据文件通过shr占用物理内存大小,筛选出内存占用大于10m的文件:\\&gt; cat /proc/9571/smaps | sed -n &#x27;/rp\\_iot\\_cloud/,+2p&#x27; | grep -v &#x27;Size:&#x27; | sed &#x27;s/\\ kB//&#x27; | awk &#x27;&#123;print $NF&#125;&#x27; | awk &#x27;BEGIN&#123;i=-1&#125;&#123;i++;a\\[i\\]=$0&#125;END&#123;for(k=0;k&lt;length(a);k=k+2) &#123;if(a\\[k+1\\]/1000&gt;10)&#123;m+=a\\[k+1\\]/1000;print a\\[k\\],a\\[k+1\\]/1000,m&#125;&#125;&#125;&#x27;\\-\\-\\-\\-\\-\\-\\-\\-\\- 文件名称  \\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\- 当前文件占用内存(MB) ----- 每行累计占用内存(MB) ----/data/influxdb/data/iot\\_cloud/rp\\_iot_cloud/233/000000640-000000010.tsm 550.148 550.148/data/influxdb/data/iot\\_cloud/rp\\_iot_cloud/233/000000640-000000009.tsm 322.392 872.54/data/influxdb/data/iot\\_cloud/rp\\_iot_cloud/233/000000640-000000008.tsm 228.524 1101.06/data/influxdb/data/iot\\_cloud/rp\\_iot_cloud/233/000000512-000000010.tsm 636.132 1737.2/data/influxdb/data/iot\\_cloud/rp\\_iot_cloud/233/000000512-000000009.tsm 336.532 2073.73/data/influxdb/data/iot\\_cloud/rp\\_iot_cloud/233/000000512-000000008.tsm 231.336 2305.06...#省略.../data/influxdb/data/iot\\_cloud/rp\\_iot_cloud/233/000000384-000000005.tsm 560.716 17183.6/data/influxdb/data/iot\\_cloud/rp\\_iot_cloud/233/000001209-000000002.tsm 315.204 17498.8/data/influxdb/data/iot\\_cloud/rp\\_iot_cloud/233/000001216-000000001.tsm 55.644 17554.4/data/influxdb/data/iot\\_cloud/rp\\_iot_cloud/233/000000256-000000005.tsm 514 18068.4#2.计算influxdb索引series文件通过shr占用物理内存大小,筛选出内存占用大于10m的文件:\\&gt; cat /proc/9571/smaps | sed -n &#x27;/series/,+2p&#x27; | grep -v &#x27;Size:&#x27; | sed &#x27;s/\\ kB//&#x27; | awk &#x27;&#123;print $NF&#125;&#x27; | awk &#x27;BEGIN&#123;i=-1&#125;&#123;i++;a\\[i\\]=$0&#125;END&#123;for(k=0;k&lt;length(a);k=k+2) &#123;if(a\\[k+1\\]/1000&gt;10)&#123;m+=a\\[k+1\\]/1000;print a\\[k\\],a\\[k+1\\]/1000,m&#125;&#125;&#125;&#x27;\\-\\-\\-\\-\\-\\-\\-\\-\\- 文件名称  \\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\- 当前文件占用内存(MB) ----- 每行累计占用内存(MB) ----/data/influxdb/data/iot\\_cloud/\\_series/07/index 517.4 517.4/data/influxdb/data/iot\\_cloud/\\_series/07/0007 99.032 616.432/data/influxdb/data/iot\\_cloud/\\_series/07/0006 98.896 715.328/data/influxdb/data/iot\\_cloud/\\_series/07/0005 62.732 778.06...#省略.../data/influxdb/data/iot\\_cloud/\\_series/01/0002 16.352 7014.15/data/influxdb/data/iot\\_cloud/\\_series/00/0002 16.36 7030.51\n\n可以看出程序运行时，会加载iot_cloud库文件:\n\ntsm数据文件(主要为当前shard的文件，共约18G)\nseries文件(所有series文件，共约7G)\n\n加载的内存大小与SHR内存基本吻合。基于当前的分片策略retention policy周期为7天，每天一分片，所以可以从分片角度减少shard占用的内存。笔者尝试调整分片策略:\n1234#当前influxdb中数据库名use iot_cloud#修改保留策略为周期为3天，每2小时一分片alter retention policy rp\\_iot\\_cloud on iot_cloud duration 3d  REPLICATION 1  SHARD DURATION 2h default\n\n调整为2小时一分片之后，SHR内存峰值会减少10-15g左右的占用。但是缩短分片间隔之后，influxdb会更频繁的进行内部自检及数据压缩，会造成cpu及磁盘io的消耗。所以继续考虑SHR占用较大对系统会有什么影响？应用程序在启动之后，会共享系统一些内存:\n\n堆内存(共享函数库消耗的堆空间)\n文件缓存(从磁盘读取文件进行缓存)\n\n对于共享堆内存则是必须占用的物理空间，而文件缓存则是系统针对磁盘读取的优化。目前influxdb在内存中引入了大量文件，在内存充足时，会占用较多的空间，用于提高程序读取性能。\ncpu硬件体系架构可以分为:\n\nSMP(Symmetric Multi-Processor)&#x2F;UMA(Uniform Memory Access)模式SMP架构，所有的CPU争用一个总线来访问所有内存，优点是资源共享，而缺点是总线争用激烈。 实验证明，SMP服务器CPU利用率最好的情况是2至4个CPU\nNUMA(Non-Uniform Memory Access)模式NUMA架构引入了node和distance的概念。对于CPU和内存这两种最宝贵的硬件资源， NUMA用近乎严格的方式划分了所属的资源组（node），而每个资源组内的CPU和内存是几乎相等。\n\n在influxdb服务器上，查看当前cpu及numa相关信息如下:\n1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#lscpu用于查看当前cpu相关信息\\&gt; lscpuArchitecture:          x86_64CPU op-mode(s):        32-bit, 64-bitByte Order:            Little EndianCPU(s):                32         #共32个逻辑核数On-line CPU(s) list:   0-31       Thread(s) per core:    2          #每个核心支持2个物理线程Core(s) per socket:    8          #每个卡槽有8个核心Socket(s):             2          #共2个卡槽，总共16个核心NUMA node(s):          2          #共2个numa节点......L1d cache:             32KL1i cache:             32KL2 cache:              256KL3 cache:              20480KNUMA node0 CPU(s):     0-7,16-23       #numa节点分布NUMA node1 CPU(s):     8-15,24-31#numactl --hardware用于查看相关硬件信息\\&gt; numactl --hardwareavailable: 2 nodes (0-1)node 0 cpus: 0 1 2 3 4 5 6 7 16 17 18 19 20 21 22 23node 0 size: 32365 MB    #节点0分配内存为32Gnode 0 free: 185 MB      #节点0剩余内存185Mnode 1 cpus: 8 9 10 11 12 13 14 15 24 25 26 27 28 29 30 31node 1 size: 32768 MB    #节点1分配内存为32Gnode 1 free: 2492 MB     #节点1剩余内存2492Mnode distances:node   0   1   0:  10  21   1:  21  10#numastat查看当前状态#miss值和foreign值越高，就要考虑绑定的问题。\\&gt; numastat                           node0           node1numa_hit             46726204589     16958785317numa\\_miss             1636704898     11155932344 #numa\\_miss较高numa_foreign         11155932344      1636704898interleave_hit             22442           22958local_node           46726202598     16958838294other\\_node            1636706889     11155879367 #other\\_node较高#numactl --show 用于查看当前numa策略\\&gt; numactl --showpolicy: default        #使用默认策略(localalloc)preferred node: currentphyscpubind: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 cpubind: 0 1 nodebind: 0 1 membind: 0 1\n\n通过查询，发现当前的numa策略会出现大量的miss。由于influxdb基于go语言开发，go语言社区中有关于 numa 感知调度的设计文档，但是本身的实现过于复杂，所以 go 语言团队在最新1.15版本还没有着手实现。目前根据相关资料，考虑influxdb运行时会占用大规模内存，建议通过如下方式启动influxdb:\n1numactl --interleave=all /usr/bin/influxd  -config  /usr/bin/influxdb.conf\n\n从系统角度，应该同时关注influxdb写入和读取两个维度。 写入端应从具体业务场景，提前划分好写入的tags及fields，从而避免产生大量的series导致内存膨胀过快。而读取端，应明确查询时间范围，命中更少的分片数据，来防止加载大量的无用查询结果而导致程序OOM。 走查了读取端相关influxdb查询语句，发现几处类似如下消耗内存及性能的语句:\n123456789101112#表描述\\- 表名: table\\- tags: productKey,deviceName\\- fields: identifier#修改之前的sql#由于没有时间范围，会导致查询所有shard数据，并从磁盘加载到内存，最后进行排序select * from table where productKey=? and deviceName=? and identifier=? order by time desc limit 1#优化之后的sql#根据业务场景，此处可以仅查询最近2小时内的数据，避免全分片select * from table where time &gt; now()-2h and productKey=? and deviceName=? and identifier=? order by time desc limit 1\n\n本文整理了业务中使用influxdb遇到的问题，并提出了一些优化方案。目前来看，influxdb对于笔者仍然是一个黑盒程序，更细致的内容就需要从源码追寻。 当前采用了如下方式进行优化(由于需要对配置参数，策略各方面权衡，这是一个持续的过程)：\n(1) retention policy\n12#将保留策略修改为：3天一周期；1天一分片alter retention policy rp\\_iot\\_cloud on iot_cloud duration 3d  REPLICATION 1  SHARD DURATION 1d default\n\n(2) 配置文件\n1234567891011121314151617181920212223242526272829#influxdb配置文件，主要参数如下:\\[data\\]  #wal日志落盘周期，官方建议0-100ms  #尝试了100ms,50ms,20ms之后，目前折中采用50ms  wal-fsync-delay = &quot;50ms&quot;  #使用tsi1索引  index-version = &quot;tsi1&quot;  #分片允许最大内存,当超过最大内存会拒绝写入  #内存越大，多个新老分片会占用更多的堆空间  cache-max-memory-size = &quot;2g&quot;  #当cache超过128m时，会进行快照落盘  cache-snapshot-memory-size = &quot;128m&quot;  #cache冷冻写入时间  cache-snapshot-write-cold-duration = &quot;30m&quot;  #进行全量压缩时间  #由于retention policy为72小时  #超过72小时，可以认为不进行全量压缩  compact-full-write-cold-duration = &quot;80h&quot;  #并行压缩处理器  max-concurrent-compactions = 8  #压缩每秒落盘数据量  compact-throughput = &quot;16m&quot;  #压缩每秒最大落盘数据量  compact-throughput-burst = &quot;16m&quot;  #wal日志超过128m时会被压缩为索引文件，并删除  max-index-log-file-size = &quot;128m&quot;\\[monitor\\]  #关闭监控  store-enabled = false\n\n(3) 程序启动\n1numactl --interleave=all env GODEBUG=madvdontneed=1 /usr/bin/influxd  -config  /usr/bin/influxdb.conf\n","slug":"MIDDLEWARE/influxdb内存消耗分析及性能优化","date":"2023-04-19T16:34:36.000Z","categories_index":"influxdb,MIDDLEWARE","tags_index":"CPU,SHR,influxd","author_index":"dandeliono"},{"id":"b6f5eb01abd12277953691dc9b2fec2e","title":"用手写一个请求合并","content":"用手写一个请求合并推荐阅读： \n一、服务器崩溃的思考老板说，他要做个现场营销活动，线上线下都要参与推广，这个活动参与人数可能很大哦··· 果然，由于不是我写的代码，所以那天服务器就崩了，崩的时候很安静，写代码的那个人一个人走的，走的时候很安详。当请求量到达百万级时候，为啥会崩溃呢？\n微服务中是通过接口去向服务提供者发起http请求或者rpc（tcp）请求去获取数据，事实上大量请求中，服务端能处理的请求数量有限，服务中充斥着大量的线程，以及数据库等的连接也会被占用完，导致请求响应速度也越来越慢。：\n\n响应速度和我们的数据层有关系吗？\n能不能去添加服务端服务器呢？\n如果能减少客户端向服务端的请求就好了？\n限流吗？当前场景能限流吗？\n每个线程去查询数据，每次都只查询某一个结果，是不是太浪费了？\n我们能不能想办法，提升我们系统的调用性能？\n\n二、有人想看请求合并，今天她来了上面的一些思路可以用加缓存，加MQ的方式去解决。但是缓存有限，MQ是个队列，有限流的效果。那么，如何才能提高系统的调用能力，我们学习一下，请求合并，结果分发。  \n\n正常的请求都是一个请求一个线程，到后台触发相关的业务需求，去调用数据获取业务。\n当请求合并后，我们要将多个多个请求合并后统一去批量去调用。\n\n大概的设计思路便是如下图所示：\n1. 常规请求\n\n2. 请求合并  \n\n3. 说下我们的思路\n\n解决请求调用多，比如调用商品数据，经过的服务多，调用链很长，所以查询数据库的次数也就非常多，数据库连接池很快就被用光，导致很多请求被阻塞，也导致应用整体线程数非常高。虽然通过增加数据库连接池大小可以缓解问题，并且可以通过压力测试，但这治标不治本。\n查询商品信息的时候，如果同一商品同一时刻有100个请求，那么其中的99次查询是多余的，可以把100个请求合并成一个真实的后台接口调用，只要控制好线程安全即可。我的想法是使用并发计数器来实现再配合本地缓存，计数器可直接用JDK提供的AtomicInteger，线程安全又提供原子操作。\n以获取商品信息为例，每个商品id对应一个计数器，计数器初始值默认是0，当一个请求过来后通过incrementAndGet()使计数器自增1并返回自增后的值。当该值等于1，表明该线程在这个时间点上是第一个到达的线程，然后就去调用真实的业务逻辑，在查询到结果后放入到本地缓存中。当该值大于1的时候，表明之前已有线程正在调用业务逻辑，则进入等待状态，并循环的查询本地缓存中是否已有数据可用。获取到结果后都调用decrementAndGet()使计数器减1，计数器被减到0的时候就回到了初始状态，并且当减到0（代表最后一个线程）时清除缓存。\n那还有在1000次请求中，请求的数据id不同，但是使用的服务接口相同，都是查询商品库的商品id从1~1000的数据，都是从表里面查询，queryDataById(dataId),那我也可以合并这些请求，改为批量查询，然后将数据分发返回。思路就是设计每个请求携带一个请求唯一的traceId,有点像链路跟踪的感觉，简单点可以使用查询的id进行最为跟踪id，将请求放入一个队中，使用定时任务，比如每隔10ms去扫描队列，将这些业务合并请求统一去请求数据库层。\n此方案有个数据延迟的地方，就是每次循环时的等待状态的时间。因为一次包含多次查库的业务调用，耗时基本都在几十毫秒，甚至是上百毫秒，可以把该等待睡眠设置小一点，比如10毫秒。这样即不会浪费CPU时间，实时性也比较高，但然也可以通过主动唤醒等待线程的方式，这个操作起来就比较复杂些。在这其中还可以添加一些异常处理、超时控制、最大重试次数，最大并发数（超时最大并发数就快速失败）等。\n\n三、开始演练\n模拟一个远程调用接口\n\n12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import org.springframework.stereotype.Service;import java.util.*;/** * 模拟远程调用ShopData接口 * @author Lijing */@Servicepublic class QueryServiceRemoteCall &#123;    /**     * 调用远程的商品信息查询接口     *     * @param code 商品编码     * @return 返回商品信息，map格式     */    public HashMap&lt;String, Object&gt; queryShopDataInfoByCode(String code) &#123;        try &#123;            Thread.sleep(50L);        &#125; catch (InterruptedException e) &#123;            e.printStackTrace();        &#125;        HashMap&lt;String, Object&gt; hashMap = new HashMap&lt;&gt;();        hashMap.put(&quot;shopDataId&quot;, new Random().nextInt(999999999));        hashMap.put(&quot;code&quot;, code);        hashMap.put(&quot;name&quot;, &quot;小玩具&quot;);        hashMap.put(&quot;isOk&quot;, &quot;true&quot;);        hashMap.put(&quot;price&quot;,&quot;3000&quot;);        return hashMap;    &#125;    /**     * 批量查询 - 调用远程的商品信息查询接口     *     * @param codes 多个商品编码     * @return 返回多个商品信息     */    public List&lt;Map&lt;String, Object&gt;&gt; queryShopDataInfoByCodeBatch(List&lt;String&gt; codes) &#123;        List&lt;Map&lt;String, Object&gt;&gt; result = new ArrayList&lt;&gt;();        for (String code : codes) &#123;            HashMap&lt;String, Object&gt; hashMap = new HashMap&lt;&gt;();            hashMap.put(&quot;shopDataId&quot;, new Random().nextInt(999999999));            hashMap.put(&quot;code&quot;, code);            hashMap.put(&quot;name&quot;, &quot;棉花糖&quot;);            hashMap.put(&quot;isOk&quot;, &quot;true&quot;);            hashMap.put(&quot;price&quot;,&quot;6000&quot;);            result.add(hashMap);        &#125;        return result;    &#125;&#125;\n\n\n使用CountDownLatch模拟并发请求的公共测试类\n\n12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485@RunWith(SpringRunner.class)@SpringBootTest(classes = MyBotApplication.class)public class MergerApplicationTests &#123;    long timed = 0L;    @Before    public void start() &#123;        System.out.println(&quot;开始测试&quot;);        timed = System.currentTimeMillis();    &#125;    @After    public void end() &#123;        System.out.println(&quot;结束测试,执行时长：&quot; + (System.currentTimeMillis() - timed));    &#125;    // 模拟的请求数量    private static final int THREAD_NUM = 1000;    // 倒计数器 juc包中常用工具类    private CountDownLatch countDownLatch = new CountDownLatch(THREAD_NUM);    @Autowired    private ShopDataService shopDataService;    @Test    public void simulateCall() throws IOException &#123;        // 创建 并不是马上发起请求        for (int i = 0; i &lt; THREAD_NUM; i++) &#123;            final String code = &quot;code-&quot; + (i + 1);            // 多线程模拟用户查询请求            Thread thread = new Thread(() -&gt; &#123;                try &#123;                    // 代码在这里等待，等待countDownLatch为0，代表所有线程都start，再运行后续的代码                    countDownLatch.await();                    // 模拟 http请求，实际上就是多线程调用这个方法                    Map&lt;String, Object&gt; result = shopDataService.queryData(code);                    System.out.println(Thread.currentThread().getName() + &quot; 查询结束，结果是：&quot; + result);                &#125; catch (Exception e) &#123;                    System.out.println(Thread.currentThread().getName() + &quot; 线程执行出现异常:&quot; + e.getMessage());                &#125;            &#125;);            thread.setName(&quot;price-thread-&quot; + code);            thread.start();            // 启动后，倒计时器倒计数 减一，代表又有一个线程就绪了            countDownLatch.countDown();        &#125;        System.in.read();    &#125;&#125;先来个普通调用演示/** * 商品数据服务类 * @author lijing */@Servicepublic class ShopDataService &#123;    @Autowired    QueryServiceRemoteCall queryServiceRemoteCall;    // 1000 用户请求，1000个线程    public Map&lt;String, Object&gt; queryData(String shopDataId) throws ExecutionException, InterruptedException &#123;         return queryServiceRemoteCall.queryShopDataInfoByCode(shopDataId);    &#125;&#125;查询结果展示开始测试price-thread-code-3 查询结束，结果是：&#123;code=code-3, shopDataId=165800794, price=3000, isOk=true, name=小玩具&#125;price-thread-code-994 查询结束，结果是：&#123;code=code-994, shopDataId=735455508, price=3000, isOk=true, name=小玩具&#125;price-thread-code-36 查询结束，结果是：&#123;code=code-36, shopDataId=781610507, price=3000, isOk=true, name=小玩具&#125;price-thread-code-993 查询结束，结果是：&#123;code=code-993, shopDataId=231087525, price=3000, isOk=true, name=小玩具&#125;....... 省略代码中。。。。price-thread-code-25 查询结束，结果是：&#123;code=code-25, shopDataId=149193873, price=3000, isOk=true, name=小玩具&#125;price-thread-code-2 查询结束，结果是：&#123;code=code-2, shopDataId=324877405, price=3000, isOk=true, name=小玩具&#125;.......共计1000次的查询结果结束测试,执行时长：150\n\n\n那么我们发现我们可以用code作为一个追踪traceId,然后使用\n\nScheduledExecutorService,CompletableFuture,LinkedBlockingQueue等一些多线程技术，就可以实现这个请求合并，请求分发的简单实现demo.\n12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182import javax.annotation.PostConstruct;import java.util.ArrayList;import java.util.HashMap;import java.util.List;import java.util.Map;import java.util.concurrent.*;/** * 商品数据服务类 * * @author lijing */@Servicepublic class ShopDataService &#123;    class Request &#123;        String shopDataId;        CompletableFuture&lt;Map&lt;String, Object&gt;&gt; completableFuture;    &#125;    // 集合，积攒请求，每N毫秒处理    LinkedBlockingQueue&lt;Request&gt; queue = new LinkedBlockingQueue&lt;&gt;();    @PostConstruct    public void init() &#123;        ScheduledExecutorService scheduledExecutorPool = Executors.newScheduledThreadPool(5);        scheduledExecutorPool.scheduleAtFixedRate(() -&gt; &#123;            // TODO 取出所有queue的请求，生成一次批量查询            int size = queue.size();            if (size == 0) &#123;                return;            &#125;            System.out.println(&quot;此次合并了多少请求：&quot; + size);            // 1、 取出            ArrayList&lt;Request&gt; requests = new ArrayList&lt;&gt;();            ArrayList&lt;String&gt; shopDataIds = new ArrayList&lt;&gt;();            for (int i = 0; i &lt; size; i++) &#123;                Request request = queue.poll();                requests.add(request);                shopDataIds.add(request.shopDataId);            &#125;            // 2、 组装一个批量查询 （不会比单次查询慢很多）            List&lt;Map&lt;String, Object&gt;&gt; mapList = queryServiceRemoteCall.queryShopDataInfoByCodeBatch(shopDataIds);            // 3、 分发响应结果，给每一个request用户请求 （多线程 之间的通信）            HashMap&lt;String, Map&lt;String, Object&gt;&gt; resultMap = new HashMap&lt;&gt;(); //  1000---- 007            for (Map&lt;String, Object&gt; map : mapList) &#123;                String code = map.get(&quot;code&quot;).toString();                resultMap.put(code, map);            &#125;            // 1000个请求            for (Request req : requests) &#123;                 Map&lt;String, Object&gt; result = resultMap.get(req.shopDataId);                // 怎么通知对应的1000多个线程，取结果呢？                req.completableFuture.complete(result);            &#125;        &#125;, 0, 10, TimeUnit.MILLISECONDS);    &#125;    @Autowired    QueryServiceRemoteCall queryServiceRemoteCall;    /**     * 1000 用户请求，1000个线程     *     * @param shopDataId     * @return     * @throws ExecutionException     * @throws InterruptedException     */    public Map&lt;String, Object&gt; queryData(String shopDataId) throws ExecutionException, InterruptedException &#123;        Request request = new Request();        request.shopDataId = shopDataId;        CompletableFuture&lt;Map&lt;String, Object&gt;&gt; future = new CompletableFuture&lt;&gt;();        request.completableFuture = future;        queue.add(request);        // 等待其他线程通知拿结果        return future.get();    &#125;&#125;\n\n\n测试结果\n\n12345678910111213141516171819开始测试结束测试,执行时长：164此次合并了多少请求：63此次合并了多少请求：227此次合并了多少请求：32此次合并了多少请求：298此次合并了多少请求：68此次合并了多少请求：261此次合并了多少请求：51price-thread-code-747 查询结束，结果是：&#123;code=code-747, shopDataId=113980125, price=6000, isOk=true, name=棉花糖&#125;price-thread-code-821 查询结束，结果是：&#123;code=code-821, shopDataId=568038265, price=6000, isOk=true, name=棉花糖&#125;price-thread-code-745 查询结束，结果是：&#123;code=code-745, shopDataId=998247608, price=6000, isOk=true, name=棉花糖&#125;....... 省略代码中。。。。price-thread-code-809 查询结束，结果是：&#123;code=code-809, shopDataId=479029433, price=6000, isOk=true, name=棉花糖&#125;price-thread-code-806 查询结束，结果是：&#123;code=code-806, shopDataId=929748878, price=6000, isOk=true, name=棉花糖&#125;\n\n四、总结弊端： \n\n启用请求的成本是执行实际逻辑之前增加的延迟。\n如果平均仅需要5毫秒的执行时间，放在一个10毫秒的做一次批处理的合并场景下，则在最坏的情况下，执行时间可能会变为15毫秒。(一定不适合低延迟的RPC场景、一定不适合低并发场景)\n\n场景:\n\n如果很少有超过1或2个请求会并发在一起，则没有必要用。\n一个特定的查询同时被大量使用，并且可以将几+个甚至数百个批处理在一起，那么如果能接受处理时间变长一点点，用来减少网络连接欲，这是值得的。(典型如:数据库、Http接口）\n\n扩展:\n\n我们不重复造轮子，在SpringCloud的组件spring-cloud-starter-netflix-hystrix中已经有封装好的轮子Hystrix的HystrixCollapser来实现请求的合并，以减少通信消耗和线程数的占用。\n当然他的组件比较复杂，也更全面，支持异步，同步，超时，异常等的处理机制。\n但是，从底层思路来说，无非是线程之间的通信，线程的切换，队列等一些并发编程相关的技术，只要我们高度封装和抽象，那也可以手撸一个合并请求的框架处理。\n\n","slug":"JAVA/用手写一个请求合并","date":"2023-04-19T15:45:48.000Z","categories_index":"https,JAVA","tags_index":"com,pic,zhimg","author_index":"dandeliono"},{"id":"a48fe76127f611e471ecf4672c3ea452","title":"Spring RestTemplate使用指南","content":"Spring RestTemplate使用指南〇、介绍 spring-web框架下的一个http客户端，提供了一种通用的接口。\nHTTP请求方式可以选择：\n\n原生的 java.net.HttpURLConnection\nApache HttpClient\n…\n\nHTTP请求方式通过 ClientHttpRequestFactory 接口适配，默认使用 SimpleClientHttpRequestFactoryAbstractClientHttpRequestFactoryWrapper, BufferingClientHttpRequestFactory, HttpComponentsAsyncClientHttpRequestFactory, HttpComponentsClientHttpRequestFactory, InterceptingClientHttpRequestFactory, MockMvcClientHttpRequestFactory, Netty4ClientHttpRequestFactory, OkHttp3ClientHttpRequestFactory, SimpleClientHttpRequestFactory提供开箱即用的请求和响应序列化和反序列化工具，且支持高度定制化。接口为 HttpMessageConverter&lt;&gt;\n一、构造方式 1RestTemplate restTemplate = new RestTemplate();\n\n\nHTTP请求方式使用 SimpleClientHttpRequestFactory 及 java.net.HttpURLConnection\n默认添加了众多消息处理器，参见：RestTemplate#RestTemplate()\n\n当然spring还提供了其他的构造方式，例如：\n1234SimpleClientHttpRequestFactory requestFactory = new SimpleClientHttpRequestFactory();requestFactory.setConnectTimeout(1000);requestFactory.setReadTimeout(1000);RestTemplate restTemplate1 = new RestTemplate(requestFactory);\n\n二、相关配置 1、requestFactory HTTP请求方式配置\n1restTemplate.setRequestFactory(requestFactory);\n\n支持介绍所描述的客户端，当然也可以自己实现 ClientHttpRequestFactory 接口\n2、messageConverters 请求体与相应体的序列化、反序列化器\n1restTemplate.setMessageConverters(messageConverters);\n\n\nmessageConverters 类型为 List&lt;HttpMessageConverter&lt;?&gt;&gt;\n比如Json请求体的序列化和反序列化\n\n支持如下方法：\nAbstractGenericHttpMessageConverter, AbstractHttpMessageConverter, AbstractJackson2HttpMessageConverter, AbstractJaxb2HttpMessageConverter, AbstractJsonHttpMessageConverter, AbstractWireFeedHttpMessageConverter, AbstractXmlHttpMessageConverter, AllEncompassingFormHttpMessageConverter, AtomFeedHttpMessageConverter, BufferedImageHttpMessageConverter, ByteArrayHttpMessageConverter, FormHttpMessageConverter, GsonHttpMessageConverter, Jaxb2CollectionHttpMessageConverter, Jaxb2RootElementHttpMessageConverter, JsonbHttpMessageConverter, MappingJackson2CborHttpMessageConverter, MappingJackson2HttpMessageConverter, MappingJackson2SmileHttpMessageConverter, MappingJackson2XmlHttpMessageConverter, MarshallingHttpMessageConverter, ObjectToStringHttpMessageConverter, ProtobufHttpMessageConverter, ProtobufJsonFormatHttpMessageConverter, ResourceHttpMessageConverter, ResourceRegionHttpMessageConverter, RssChannelHttpMessageConverter, SourceHttpMessageConverter, StringHttpMessageConverter\n3、interceptors 拦截器：配置请求之前的过滤请求\n1restTemplate.setInterceptors(interceptors);\n\n\ninterceptors 类型为 List&lt;ClientHttpRequestInterceptor&gt;\n用于实现请求之前的校验或者增强\n\n4、errorHandler 异常处理器：用于处理请求响应码的异常。\n1restTemplate.setErrorHandler(errorHandler);\n\n\n只能处理http响应码的错误\n一定不要读取ClientHttpResponse.getBody，因为inputsteam只能读一次\n默认为 DefaultResponseErrorHandler\n面对国内http状态码为200的错误，只能在 messageConverter 层处理\n\n三、相关API 常用API如下：\n1234 getForObjectgetForEntitypostForXxx \n\n\nForObject表示 返回 T\nForEntity表示 返回 ResponseEntity&lt;T&gt;\n\n通用API如下：\n\n一般可以使用exchange进行封装，具有很高的灵活性\n\n四、实例 1、处理url参数（get参数） 12String url = &quot;http://localhost:8080/test/sendSms?phone=&#123;phone&#125;&amp;msg=&#123;phone&#125;&quot;;String result = restOperations.getForObject(url, String.class, &quot;151xxxxxxxx&quot;, &quot;测试短信内容&quot;);\n\n以下方式默认情况下不支持 （spring 5.x）\n1234567String url = &quot;http://localhost:8080/test/sendSms?phone=&#123;phone&#125;&amp;msg=&#123;phone&#125;&quot;;Map&lt;String, Object&gt; uriVariables = new HashMap&lt;String, Object&gt;();uriVariables.put(&quot;phone&quot;, &quot;151xxxxxxxx&quot;);uriVariables.put(&quot;msg&quot;, &quot;测试短信内容&quot;);String result = restOperations.getForObject(url, String.class, uriVariables);\n\n2、配置请求头 例如配置cookie\n12345678910HttpHeaders jsonHeaders = new HttpHeaders();jsonHeaders.add(HttpHeaders.COOKIE, &quot;sessionid=&quot; + sessionid);jsonHeaders.add(HttpHeaders.USER_AGENT, &quot;test&quot;);jsonHeaders.add(HttpHeaders.ACCEPT, &quot;application/json&quot;);jsonHeaders.add(HttpHeaders.CONTENT_TYPE, &quot;application/json&quot;);HttpEntity&lt;?&gt; requestEntity = new HttpEntity&lt;&gt;(request, jsonHeaders);ResponseEntity&lt;T&gt; response = restTemplate.exchange(buildUrl(path), HttpMethod.POST, requestEntity, responseType,    uriVariables);return response.getBody();\n\n3、form方式post请求 12345678910111213HttpHeaders formHeaders = new HttpHeaders();formHeaders.add(HttpHeaders.COOKIE, &quot;sessionid=&quot; + sessionid);formHeaders.add(HttpHeaders.USER_AGENT, &quot;test&quot;);formHeaders.add(HttpHeaders.ACCEPT, &quot;application/json&quot;);formHeaders.add(HttpHeaders.CONTENT_TYPE, &quot;application/x-www-form-urlencoded&quot;);MultiValueMap&lt;String, String&gt; body = new LinkedMultiValueMap&lt;&gt;();for (Map.Entry&lt;String, ?&gt; entry : request.entrySet()) &#123;    body.add(entry.getKey(), entry.getValue().toString());&#125;HttpEntity&lt;?&gt; requestEntity = new HttpEntity&lt;&gt;(body, formHeaders);ResponseEntity&lt;T&gt; response = restTemplate.exchange(buildUrl(path), HttpMethod.POST, requestEntity, responseType, uriVariables);return response.getBody();\n\n3、定制json解析器 注意使用了Jackson：\n\n定制日期序列化方式\n\n使用java8的日期时间标准\n123456789101112131415161718192021restTemplate = new RestTemplate();ObjectMapper objectMapper = new ObjectMapper();JavaTimeModule javaTimeModule = new JavaTimeModule();javaTimeModule.addDeserializer(LocalDateTime.class,new LocalDateTimeDeserializer(DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;)));objectMapper.registerModule(new ParameterNamesModule()).registerModule(new Jdk8Module()).registerModule(javaTimeModule);objectMapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false);List&lt;HttpMessageConverter&lt;?&gt;&gt; messageConverters = restTemplate.getMessageConverters().stream().filter(c -&gt; !(c instanceof MappingJackson2HttpMessageConverter)).collect(Collectors.toList());MappingJackson2HttpMessageConverter jsonMessageConverter = new MappingJackson2HttpMessageConverter();jsonMessageConverter.setObjectMapper(objectMapper);messageConverters.add(new JsonHttpMessageConverterProxy(jsonMessageConverter));restTemplate.setMessageConverters(messageConverters);\n\n依赖如下\n12345678910111213141516&lt;dependency&gt;           &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;           &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt;       &lt;/dependency&gt;       &lt;dependency&gt;           &lt;groupId&gt;com.fasterxml.jackson.module&lt;/groupId&gt;           &lt;artifactId&gt;jackson-module-parameter-names&lt;/artifactId&gt;       &lt;/dependency&gt;       &lt;dependency&gt;           &lt;groupId&gt;com.fasterxml.jackson.datatype&lt;/groupId&gt;           &lt;artifactId&gt;jackson-datatype-jdk8&lt;/artifactId&gt;       &lt;/dependency&gt;       &lt;dependency&gt;           &lt;groupId&gt;com.fasterxml.jackson.datatype&lt;/groupId&gt;           &lt;artifactId&gt;jackson-datatype-jsr310&lt;/artifactId&gt;       &lt;/dependency&gt;\n\n4、定制200错误处理 例如如下情况：\n正确返回\n1234567http-status-code 200&#123;  &quot;code&quot;: 0,  &quot;data&quot;: &#123;&#125;,  &quot;message&quot;: &quot;success&quot;&#125;\n\n错误返回\n1234567http-status-code 200&#123;  &quot;code&quot;: 1, // 非零  &quot;data&quot;: 11111, // 一个数字  &quot;message&quot;: &quot;错误信息&quot; // 错误信息&#125;\n\n通过代理Jackson解析器实现\n12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455 class JsonHttpMessageConverterProxy implements HttpMessageConverter&lt;Object&gt; &#123;    private HttpMessageConverter&lt;Object&gt; converter;    public JsonHttpMessageConverterProxy(HttpMessageConverter&lt;Object&gt; converter) &#123;        this.converter = converter;    &#125;    @Override    public boolean canRead(Class&lt;?&gt; clazz, MediaType mediaType) &#123;        return converter.canRead(clazz, mediaType);    &#125;    @Override    public boolean canWrite(Class&lt;?&gt; clazz, MediaType mediaType) &#123;        return converter.canWrite(clazz, mediaType);    &#125;    @Override    public List&lt;MediaType&gt; getSupportedMediaTypes() &#123;        return converter.getSupportedMediaTypes();    &#125;    @Override    public Object read(Class&lt;? extends Object&gt; clazz, HttpInputMessage inputMessage)        throws IOException, HttpMessageNotReadableException &#123;        String body = new BufferedReader(new InputStreamReader(inputMessage.getBody()))            .lines().collect(Collectors.joining(System.lineSeparator())).toString();        Map&lt;String, Object&gt; bodyMap = JSON.parseObject(body);        Integer code = (Integer) bodyMap.get(&quot;code&quot;);        if (code != null &amp;&amp; code != 0) &#123;            bodyMap.put(&quot;data&quot;, null);            body = JSON.toJSONString(bodyMap);        &#125;        InputStream fixBodyInputStream = new ByteArrayInputStream(body.getBytes());        return converter.read(clazz, new HttpInputMessage() &#123;            @Override            public HttpHeaders getHeaders() &#123;                return inputMessage.getHeaders();            &#125;            @Override            public InputStream getBody() throws IOException &#123;                return fixBodyInputStream;            &#125;        &#125;);    &#125;    @Override    public void write(Object t, MediaType contentType, HttpOutputMessage outputMessage)        throws IOException, HttpMessageNotWritableException &#123;        converter.write(t, contentType, outputMessage);    &#125;&#125;\n\n使用发方式\n1234567891011121314restTemplate = new RestTemplate();ObjectMapper objectMapper = new ObjectMapper();MappingJackson2HttpMessageConverter jsonMessageConverter = new MappingJackson2HttpMessageConverter();jsonMessageConverter.setObjectMapper(objectMapper);List&lt;HttpMessageConverter&lt;?&gt;&gt; messageConverters = restTemplate                      .getMessageConverters()                      .stream()                      .filter(c -&gt; !(c instanceof MappingJackson2HttpMessageConverter))                      .collect(Collectors.toList());messageConverters.add(new JsonHttpMessageConverterProxy(jsonMessageConverter));restTemplate.setMessageConverters(messageConverters);\n","slug":"JAVA/Spring RestTemplate使用指南","date":"2023-04-07T14:56:26.000Z","categories_index":"HttpMessageConverter,JAVA","tags_index":"HTTP,ClientHttpRequestFactory,MappingJackson","author_index":"dandeliono"},{"id":"d877b3555659291721a2f384975f0ecc","title":"SQL解析器组件","content":"SQL解析器组件\n\n\n\n\n\n\n\n\nhzero-starter-sqlparser是基于druid的sql解析器移植而来，未来会逐渐完善功能、增强api。\nsql语句首先需要由parser转化为AST，并通过visitor获取sql语句的类型、from条件、where条件、groupby等信息。\nAST(abstract syntax tree)中文含义为抽象语法树，以下是一个简单sql语句的ast图。\nSELECT a FROM table_a WHERE table_a.a &#x3D; ‘a’;\n\n简单理解，就是把字符串形式的sql语句结构化，变成具有层次的树形结构，便于sql语句的分析、改造。如果需要对现有的sql语句改造，则可以直接更新ast上的节点信息。\nast节点主要包括SQLObject、SQLExpr、SQLStatement三种抽象类型。\n1234interface SQLObject &#123;&#125;interface SQLExpr extends SQLObject &#123;&#125;interface SQLStatement extends SQLObject &#123;&#125;\n\nSQLExpr1234567891011121314151617181920212223242526272829303132333435363738394041424344public interface SQLName extends SQLExpr &#123;&#125;class SQLIdentifierExpr implements SQLExpr, SQLName &#123;    String name;&#125; class SQLPropertyExpr implements SQLExpr, SQLName &#123;    SQLExpr owner;    String name;&#125; class SQLBinaryOpExpr implements SQLExpr &#123;    SQLExpr left;    SQLExpr right;    SQLBinaryOperator operator;&#125;class SQLVariantRefExpr extends SQLExprImpl &#123;     String name;&#125;public class SQLIntegerExpr extends SQLNumericLiteralExpr implements SQLValuableExpr &#123;     Number number;        @Override    public Object getValue() &#123;        return this.number;    &#125;&#125;public class SQLCharExpr extends SQLTextLiteralExpr implements SQLValuableExpr&#123;    String text;&#125;\n\nSQLStatement123456789101112131415161718class SQLSelectStatement implements SQLStatement &#123;    SQLSelect select;&#125;class SQLUpdateStatement implements SQLStatement &#123;    SQLExprTableSource tableSource;     List&lt;SQLUpdateSetItem&gt; items;     SQLExpr where;&#125;class SQLDeleteStatement implements SQLStatement &#123;    SQLTableSource tableSource;     SQLExpr where;&#125;class SQLInsertStatement implements SQLStatement &#123;    SQLExprTableSource tableSource;    List&lt;SQLExpr&gt; columns;    SQLSelect query;&#125;\n\nSQLTableSource1234567891011121314151617181920212223242526272829303132class SQLTableSourceImpl extends SQLObjectImpl implements SQLTableSource &#123;     String alias;&#125;class SQLExprTableSource extends SQLTableSourceImpl &#123;    SQLExpr expr;&#125;class SQLJoinTableSource extends SQLTableSourceImpl &#123;    SQLTableSource left;    SQLTableSource right;    JoinType joinType;     SQLExpr condition;&#125;SQLSubqueryTableSource extends SQLTableSourceImpl &#123;    SQLSelect select;&#125;class SQLWithSubqueryClause &#123;    static class Entry extends SQLTableSourceImpl &#123;          SQLSelect subQuery;    &#125;&#125;\n\nSQLSelectStatementSQLSelectStatement包含一个SQLSelect，SQLSelect包含一个SQLSelectQuery，都是组成的关系。SQLSelectQuery有主要的两个派生类，分别是SQLSelectQueryBlock和SQLUnionQuery。\n1234567891011121314151617181920212223class SQLSelect extends SQLObjectImpl &#123;     SQLWithSubqueryClause withSubQuery;    SQLSelectQuery query;&#125;interface SQLSelectQuery extends SQLObject &#123;&#125;class SQLSelectQueryBlock implements SQLSelectQuery &#123;    List&lt;SQLSelectItem&gt; selectList;    SQLTableSource from;    SQLExprTableSource into;    SQLExpr where;    SQLSelectGroupByClause groupBy;    SQLOrderBy orderBy;    SQLLimit limit;&#125;class SQLUnionQuery implements SQLSelectQuery &#123;    SQLSelectQuery left;    SQLSelectQuery right;    SQLUnionOperator operator; &#125;\n\nSQLCreateTableStatement123456789101112131415public class SQLCreateTableStatement extends SQLStatementImpl implements SQLDDLStatement, SQLCreateStatement &#123;    SQLExprTableSource tableSource;    List&lt;SQLTableElement&gt; tableElementList;    Select select;        public SQLColumnDefinition findColumn(String columName) &#123;&#125;        public SQLTableElement findIndex(String columnName) &#123;&#125;        public boolean isReferenced(String tableName) &#123;&#125;&#125;\n\n使用案例ast全语法示例代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100public class Test &#123;    public static void main(String[] args) &#123;        String sql = &quot;select a,b from (select * from table_a) temp where temp.a = &#x27;a&#x27;;&quot;;                List&lt;SQLStatement&gt; statements = SQLUtils.parseStatements(sql, JdbcConstants.MYSQL);                SQLStatement statement = statements.get(0);                SQLSelectStatement sqlSelectStatement = (SQLSelectStatement) statement;        SQLSelectQuery sqlSelectQuery = sqlSelectStatement.getSelect().getQuery();                if (sqlSelectQuery instanceof SQLSelectQueryBlock) &#123;            SQLSelectQueryBlock sqlSelectQueryBlock = (SQLSelectQueryBlock) sqlSelectQuery;                        List&lt;SQLSelectItem&gt; selectItems         = sqlSelectQueryBlock.getSelectList();            selectItems.forEach(x -&gt; &#123;                            &#125;);                        SQLTableSource table = sqlSelectQueryBlock.getFrom();                        if (table instanceof SQLExprTableSource) &#123;                                            &#125; else if (table instanceof SQLJoinTableSource) &#123;                                            &#125; else if (table instanceof SQLSubqueryTableSource) &#123;                            &#125;                        SQLExpr where = sqlSelectQueryBlock.getWhere();                        if (where instanceof SQLBinaryOpExpr) &#123;                SQLBinaryOpExpr   sqlBinaryOpExpr = (SQLBinaryOpExpr) where;                SQLExpr           left            = sqlBinaryOpExpr.getLeft();                SQLBinaryOperator operator        = sqlBinaryOpExpr.getOperator();                SQLExpr           right           = sqlBinaryOpExpr.getRight();                                            &#125; else if (where instanceof SQLInSubQueryExpr) &#123;                SQLInSubQueryExpr sqlInSubQueryExpr = (SQLInSubQueryExpr) where;                            &#125;                        SQLSelectGroupByClause groupBy = sqlSelectQueryBlock.getGroupBy();                                    SQLOrderBy orderBy = sqlSelectQueryBlock.getOrderBy();                                    SQLLimit limit = sqlSelectQueryBlock.getLimit();                                &#125; else if (sqlSelectQuery instanceof SQLUnionQuery) &#123;                    &#125;                SQLInsertStatement sqlInsertStatement = (SQLInsertStatement) statement;                SQLWithSubqueryClause with = sqlInsertStatement.getWith();                SQLTableSource insertTable = sqlInsertStatement.getTableSource();                List&lt;SQLExpr&gt; insertColumns = sqlInsertStatement.getColumns();                SQLSelect insertSelect = sqlInsertStatement.getQuery();                List&lt;SQLInsertStatement.ValuesClause&gt; values = sqlInsertStatement.getValuesList();                SQLUpdateStatement sqlUpdateStatement = (SQLUpdateStatement) statement;                List&lt;SQLUpdateSetItem&gt; updateSetItems = sqlUpdateStatement.getItems();                SQLTableSource updateTable = sqlUpdateStatement.getFrom();                SQLExpr updateWhere = sqlUpdateStatement.getWhere();                SQLDeleteStatement sqlDeleteStatement = (SQLDeleteStatement) statement;                SQLTableSource deleteTable = sqlDeleteStatement.getFrom();                SQLExpr deleteWhere = sqlDeleteStatement.getWhere();                SQLCreateTableStatement sqlCreateTableStatement = (SQLCreateTableStatement) statement;                sqlCreateTableStatement.findIndex(&quot;a&quot;);                sqlCreateTableStatement.findColumn(&quot;a&quot;);            &#125;&#125;\n\n使用visitor获取sql信息1234567891011121314151617181920212223242526272829public class Test &#123;    public static void main(String[] args) &#123;        String sql = &quot;select a,b from (select * from table_a) temp where temp.a = &#x27;a&#x27;;&quot;;                List&lt;SQLStatement&gt; statements = SQLUtils.parseStatements(sql, JdbcConstants.MYSQL);                SQLStatement statement = statements.get(0);                MySqlSchemaStatVisitor visitor = new MySqlSchemaStatVisitor();                statement.accept(visitor);                System.out.println(&quot;数据库类型\\t\\t&quot; + visitor.getDbType());                System.out.println(&quot;查询的字段\\t\\t&quot; + visitor.getColumns());                System.out.println(&quot;表名\\t\\t\\t&quot; + visitor.getTables().keySet());                System.out.println(&quot;条件\\t\\t\\t&quot; + visitor.getConditions());                System.out.println(&quot;group by\\t\\t&quot; + visitor.getGroupByColumns());                System.out.println(&quot;order by\\t\\t&quot; + visitor.getOrderByColumns());    &#125;&#125;\n","slug":"JAVA/SQL解析器组件","date":"2023-03-22T14:25:04.000Z","categories_index":"sql,JAVA","tags_index":"ast,AST,starter","author_index":"dandeliono"},{"id":"1d828a50cab789d76b35014d3e9c8893","title":"Linux 下 journal 日志清理","content":"Linux 下 journal 日志清理Linux 在运行的过程中会产生很多日志文件，一般存放在 /var/log 目录下，而其中 journal 目录中存放的是 journald daemon 程序生成的日志，其中包括了所有 kernel, initrd, services 等等产生的日志。这些日志在系统发生状况排查问题的时候非常有用。\njounrnald daemon 程序会收集系统运行的日志并存储到二进制文件中。为了查看这些二进制文件通常会使用到 journalctl 命令。但是默认情况下这些日志文件会占用磁盘空间的 10%，而大部分情况下这些日志文件是不需要查看的。所以可以配置减小一些 journal 日志的占用。\n默认的日志文件保存在 /var/log/journal 下，可以使用 du 查看。不过我个人推荐使用可视化的 gdu 来 查看 。\n\ndu -sh /var/log/journal 查看占用磁盘空间\n\n可以使用 journalctl 命令查看日志占用：\n1sudo journalctl --disk-usage \n\n如果要去清理 journal 日志，可以先执行 rotate 命令：\n12journalctl --rotate &amp;&amp; \\systemctl restart systemd-journald \n\n删除两天前的日志：\n1journalctl --vacuum-time=2days \n\n删除两个礼拜前的日志：\n1journalctl --vacuum-time=2weeks \n\n或者删除超出文件大小的日志：\n1journalctl --vacuum-size=20M \n\n123456journalctl --disk-usage# ORdu -sh /run/log/journaljournalctl --verifyls -l /run/log/journal/*systemctl status systemd-journald \n\n修改配置文件：\nsudo vi /etc/systemd/journald.conf\n修改其中的两项：\n12SystemMaxUse=100MRuntimeMaxUse=100M \n\nSystemMaxUse 设置 /var/log/journal RuntimeMaxUse 设置 /run/log/journal\n然后使设置生效：\n1sudo systemctl daemon-reload \n\n问题\n\n\n\n\n\n\n\n\n\nWarning: Journal has been rotated since unit was started. Log output is incomplete or unavailable.\n原因是：\nthe disk usage of the log files for journald journal 日志空间达到了上限\n","slug":"LINUX/Linux 下 journal 日志清理","date":"2023-02-22T11:56:19.000Z","categories_index":"journal,LINUX","tags_index":"log,var,journald","author_index":"dandeliono"},{"id":"443c51534ca02ffae6ff782b6bfd2503","title":"Keepalived 原理介绍和配置实践","content":"Keepalived 原理介绍和配置实践参考文章Keepalived - http://www.keepalived.org/doc/The Keepalived Solution - http://www.linuxvirtualserver.org/docs/ha/keepalived.htmlLVS 和 Keepalived 官方中文手册 PDF - https://pan.baidu.com/s/1s0P6nUt8WF6o_N3wdE3uKg\n相关术语以下术语涉及 LVS 三种工作模式的原理\n\nLB (Load Balancer 负载均衡)\nHA (High Available 高可用)\nFailover (失败切换)\nCluster (集群)\nLVS (Linux Virtual Server Linux 虚拟服务器)\nDS (Director Server)，指的是前端负载均衡器节点\nRS (Real Server)，后端真实的工作服务器\nVIP (Virtual IP)，虚拟的 IP 地址，向外部直接面向用户请求，作为用户请求的目标的 IP 地址\nDIP (Director IP)，主要用于和内部主机通讯的 IP 地址\nRIP (Real Server IP)，后端服务器的 IP 地址\nCIP (Client IP)，访问客户端的 IP 地址\n\n负载均衡(LB)\n\n\n\n\n\n\n\n\n负载均衡实现方法有两种：硬件实现和软件实现\n硬件比较常见的有：\n\nF5 Big-IP\nCitrix Netscaler\n\n软件比较常见的有：\n\nLVS（Linux Virtual Server）\nHAProxy\nNginx\n\nLVS 特点是：\n\n首先它是基于 4 层的网络协议的，抗负载能力强，对于服务器的硬件要求除了网卡外，其他没有太多要求；\n配置性比较低，这是一个缺点也是一个优点，因为没有可太多配置的东西，大大减少了人为出错的几率；\n应用范围比较广，不仅仅对 web 服务做负载均衡，还可以对其他应用（mysql）做负载均衡；\nLVS 架构中存在一个虚拟 IP 的概念，需要向 IDC 多申请一个 IP 来做虚拟 IP。\n\nNginx 负载均衡器的特点是：\n\n工作在网络的 7 层之上，可以针对 http 应用做一些分流的策略，比如针对域名、目录结构；\nNginx 安装和配置比较简单，测试起来比较方便；\n也可以承担高的负载压力且稳定，一般能支撑超过上万次的并发；\nNginx 可以通过端口检测到服务器内部的故障，比如根据服务器处理网页返回的状态码、超时等等，并且会把返回错误的请求重新提交到另一个节点，不过其中缺点就是不支持 url 来检测；\nNginx 对请求的异步处理可以帮助节点服务器减轻负载；\nNginx 能支持 http 和 Email，这样就在适用范围上面小很多；\n默认有三种调度算法: 轮询、weight 以及 ip_hash（可以解决会话保持的问题），还可以支持第三方的 fair 和 url_hash 等调度算法；\n\nHAProxy 的特点是：\n\nHAProxy 是工作在网络 7 层之上；\n支持 Session 的保持，Cookie 的引导等；\n支持 url 检测后端的服务器出问题的检测会有很好的帮助；\n支持的负载均衡算法：动态加权轮循(Dynamic Round Robin)，加权源地址哈希(Weighted Source Hash)，加权 URL 哈希和加权参数哈希(Weighted Parameter Hash)；\n单纯从效率上来讲 HAProxy 更会比 Nginx 有更出色的负载均衡速度；\nHAProxy 可以对 Mysql 进行负载均衡，对后端的 DB 节点进行检测和负载均衡。\n\nkeepalived 简介Keepalived 是运行在 lvs 之上，是一个用于做双机热备（HA）的软件，它的主要功能是实现真实机的故障隔离及负载均衡器间的失败切换，提高系统的可用性。\n运行原理keepalived 通过选举（看服务器设置的权重）挑选出一台热备服务器做 MASTER 机器，MASTER 机器会被分配到一个指定的虚拟 ip，外部程序可通过该 ip 访问这台服务器，如果这台服务器出现故障（断网，重启，或者本机器上的 keepalived crash 等），keepalived 会从其他的备份机器上重选（还是看服务器设置的权重）一台机器做 MASTER 并分配同样的虚拟 IP，充当前一台 MASTER 的角色。\n选举策略选举策略是根据 VRRP 协议，完全按照权重大小，权重最大（0～255）的是 MASTER 机器，下面几种情况会触发选举\n\nkeepalived 启动的时候\nmaster 服务器出现故障（断网，重启，或者本机器上的 keepalived crash 等，而本机器上其他应用程序 crash 不算）\n有新的备份服务器加入且权重最大\n\nkeepalived 的配置文件说明Keepalived 是运行在 lvs 之上, 它的主要功能是实现 RealServer(真实服务器)的故障隔离及 Director(负载均衡器)间的 FailOver(失败切换).\n\nkeepalived 是 lvs 的扩展项目, 因此它们之间具备良好的兼容性\n对 RealServer 的健康检查, 实现对失效机器 &#x2F; 服务的故障隔离\n负载均衡器之间的失败切换 failover\n\n全局定义全局配置又包括两个子配置\n\n全局定义(global definition)\n静态路由配置(static ipaddress&#x2F;routes)\n\n12345678910111213141516171819202122232425262728293031global_defs &#123;                         notification_email &#123;         acassen@firewall.loc        failover@firewall.loc   sysadmin@firewall.loc   &#125;   notification_email_from Alexandre.Cassen@firewall.loc      smtp_server 192.168.200.1                            smtp_connect_timeout 30                                     router_id LVS_DEVEL     &#125;notification_email: 表示 keepalived 在发生诸如切换操作时需要发送 email 通知以及 email 发送给哪些邮件地址邮件地址可以多个每行一个notification_email_from admin@example.com: 表示发送通知邮件时邮件源地址是谁smtp_server 127.0.0.1: 表示发送 email 时使用的 smtp 服务器地址这里可以用本地的 sendmail 来实现smtp_connect_timeout 30: 连接 smtp 连接超时时间router_id node1: 机器标识，通常配置主机名static_ipaddress &#123;    192.168.1.1/24 brd + dev eth0 scope global    192.168.1.2/24 brd + dev eth1 scope global&#125;static_routes &#123;    src $SRC_IP to $DST_IP dev $SRC_DEVICE    src $SRC_IP to $DST_IP via $GW dev $SRC_DEVICE&#125; 这里实际上和系统里面命令配置 IP 地址和路由一样例如 192.168.1.1/24 brd + dev eth0 scope global 相当于: ip addr add 192.168.1.1/24 brd + dev eth0 scope global 就是给 eth0 配置 IP 地址路由同理, 一般这个区域不需要配置  这里实际上就是给服务器配置真实的 IP 地址和路由的在复杂的环境下可能需要配置一般不会用这个来配置我们可以直接用 vi /etc/sysconfig/network-script/ifcfg-eth1 来配置切记这里可不是 VIP 不要搞混淆了切记切记\n\nVRRPD 配置包括三个类:\n\nVRRP 同步组(synchroization group)\nVRRP 实例(VRRP Instance)\nVRRP 脚本\n\n1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253vrrp_sync_group VG_1 &#123;   // 注意 vrrp_sync_group  后面可自定义名称如 lvs_httpd ,httpdgroup &#123;httpmysql&#125;notify_master /path/to/to_master.shnotify_backup /path_to/to_backup.shnotify_fault &quot;/path/fault.sh VG_1&quot;notify /path/to/notify.shsmtp_alert &#125; 其中 http 和 mysql 是实例名和下面的实例名一致 notify_master /path/to/to_master.sh // 表示当切换到 master 状态时要执行的脚本notify_backup /path_to/to_backup.sh // 表示当切换到 backup 状态时要执行的脚本notify_fault &quot;/path/fault.sh VG_1&quot;  // keepalived 出现故障时执行的脚本notify /path/to/notify.sh  smtp_alert           // 表示切换时给 global defs 中定义的邮件地址发送邮件通知vrrp_instance http &#123;  // 注意 vrrp_instance 后面可自定义名称如 lvs_httpd ,httpdstate MASTERinterface eth0dont_track_primarytrack_interface &#123;eth0eth1&#125;mcast_src_ip &lt;IPADDR&gt;garp_master_delay 10virtual_router_id 51priority 100advert_int 1authentication &#123;auth_type PASSautp_pass 1234&#125;virtual_ipaddress &#123;192.168.200.17/24 dev eth1192.168.200.18/24 dev eth2 label eth2:1&#125;virtual_routes &#123;src 192.168.100.1 to 192.168.109.0/24 via 192.168.200.254 dev eth1192.168.110.0/24 via 192.168.200.254 dev eth1192.168.111.0/24 dev eth2192.168.112.0/24 via 192.168.100.254&#125;nopreemptpreemtp_delay 300debug&#125;\n\nstate: state 指定 instance(Initial)的初始状态就是说在配置好后这台 服务器的初始状态就是这里指定的但这里指定的不算还是得要通过竞选通过优先级来确定里如果这里设置为 master 但如若他的优先级不及另外一台 那么这台在发送通告时会发送自己的优先级另外一台发现优先级不如自己的高那么他会就回抢占为 master\ninterface: 实例绑定的网卡因为在配置虚拟 VIP 的时候必须是在已有的网卡上添加的\ndont track primary: 忽略 VRRP 的 interface 错误\ntrack interface: 跟踪接口设置额外的监控里面任意一块网卡出现问题都会进入故障 (FAULT) 状态例如用 nginx 做均衡器的时候内网必须正常工作如果内网出问题了这个均衡器也就无法运作了所以必须对内外网同时做健康检查\nmcast src ip: 发送多播数据包时的源 IP 地址这里注意了这里实际上就是在那个地址上发送 VRRP 通告这个非常重要一定要选择稳定的网卡端口来发送这里相当于 heartbeat 的心跳端口如果没有设置那么就用默认的绑定的网卡的 IP 也就是 interface 指定的 IP 地址\ngarp master delay: 在切换到 master 状态后延迟进行免费的 ARP(gratuitous ARP)请求，默认 5s\nvirtual router id: 这里设置 VRID 这里非常重要相同的 VRID 为一个组他将决定多播的 MAC 地址\npriority 100: 设置本节点的优先级优先级高的为 master\nadvert int: 设置 MASTER 与 BACKUP 负载均衡之间同步即主备间通告时间检查的时间间隔, 单位为秒，默认 1s\nvirtual ipaddress: 这里设置的就是 VIP 也就是虚拟 IP 地址他随着 state 的变化而增加删除当 state 为 master 的时候就添加当 state 为 backup 的时候删除这里主要是有优先级来决定的和 state 设置的值没有多大关系这里可以设置多个 IP 地址\nvirtual routes: 原理和 virtual ipaddress 一样只不过这里是增加和删除路由\nlvs sync daemon interface: lvs syncd 绑定的网卡，类似 HA 中的心跳检测绑定的网卡\nauthentication: 这里设置认证\nauth type: 认证方式可以是 PASS 或 AH 两种认证方式\nauth pass: 认证密码\nnopreempt: 设置不抢占 master，这里只能设置在 state 为 backup 的节点上而且这个节点的优先级必须别另外的高，比如 master 因为异常将调度圈交给了备份 serve，master serve 检修后没问题，如果不设置 nopreempt 就会将调度权重新夺回来，这样就容易造成业务中断问题\npreempt delay: 抢占延迟多少秒，即延迟多少秒后竞选 master\ndebug：debug 级别\nnotify master：和 sync group 这里设置的含义一样可以单独设置例如不同的实例通知不同的管理人员 http 实例发给网站管理员 mysql 的就发邮件给 DBA\n123456789101112131415161718192021222324252627282930313233343536vrrp_script check_running &#123;   script &quot;/usr/local/bin/check_running&quot;   interval 10   weight 10&#125;vrrp_instance http &#123;   state BACKUP   smtp_alert   interface eth0   virtual_router_id 101   priority 90   advert_int 3   authentication &#123;   auth_type PASS   auth_pass whatever   &#125;   virtual_ipaddress &#123;   1.1.1.1   &#125;   track_script &#123;   check_running    &#125;&#125;vrrp_script check_running &#123;            script &quot;/usr/local/bin/check_running&quot;            interval 10                 weight 10                   &#125;track_script &#123;      check_running &#125;\n\n注意:VRRP 脚本 (vrrp_script) 和 VRRP 实例 (vrrp_instance) 属于同一个级别keepalived 会定时执行脚本并对脚本执行的结果进行分析，动态调整 vrrp_instance 的优先级。一般脚本检测返回的值为 0，说明脚本检测成功，如果为非 0 数值，则说明检测失败如果脚本执行结果为 0，并且 weight 配置的值大于 0，则优先级相应的增加, 如果 weight 为非 0，则优先级不变如果脚本执行结果非 0，并且 weight 配置的值小于 0，则优先级相应的减少, 如果 weight 为 0，则优先级不变其他情况，维持原本配置的优先级，即配置文件中 priority 对应的值。这里需要注意的是：1） 优先级不会不断的提高或者降低2） 可以编写多个检测脚本并为每个检测脚本设置不同的 weight3） 不管提高优先级还是降低优先级，最终优先级的范围是在[1,254]，不会出现优先级小于等于 0 或者优先级大于等于 255 的情况这样可以做到利用脚本检测业务进程的状态，并动态调整优先级从而实现主备切换。\nvirtual_server 虚拟主机配置关于 keeplived 的虚拟主机配置有三种如下所示virtual server IP portvirtual server fwmark intvirtual server group string\n以常用的第一种为例virtual_server 192.168.1.2 80含义: 设置一个 virtual server: VIP:Vport\ndelay_loop 3含义: 设置 service polling 的 delay 时间即服务轮询的时间间隔\nlb_algo rr|wrr|lc|wlc|lblc|sh|dh含义: 设置 LVS 调度算法\nlb_kind NAT|DR|TUN含义: 设置 LVS 集群模式\npersistence_timeout 120含义: 设置会话保持时间秒为单位即以用户在 120 秒内被分配到同一个后端 realserver, 超过此时间就重新分配\npersistence_granularity含义: 设置 LVS 会话保持粒度 ipvsadm 中的 - M 参数默认是 0xffffffff 即每个客户端都做会话保持\nprotocol TCP含义: 设置健康检查用的是 TCP 还是 UDP\nha_suspend含义: suspendhealthchecker’s activity\nvirtualhost含义: HTTP_GET 做健康检查时检查的 web 服务器的虚拟主机即 host 头\nsorry_server含义: 设置 backupserver 就是当所有后端 realserver 节点都不可用时就用这里设置的也就是临时把所有的请求都发送到这里\nreal_server含义: 设置后端真实节点主机的权重等设置主要后端有几台这里就要设置几个\nweight 1含义: 设置给每台的权重 0 表示失效 (不知给他转发请求知道他恢复正常) 默认是 1\ninhibit_on_failure含义: 表示在节点失败后把他权重设置成 0 而不是冲 IPVS 中删除\nnotify_up |含义: 设置检查服务器正常 (UP) 后要执行的脚本notify_down |含义: 设置检查服务器失败 (down) 后要执行的脚本\n注: keepalived 检查机制说明keepalived 健康检查方式有: HTTP_GET|SSL_GET|TCP_CHECK|SMTP_CHECK|MISC_CHECK 几种如下所示\n1234567891011121314151617181920212223242526272829303132333435363738394041424344454647HTTP_GET|SSL_GET &#123;      url &#123;                   path /                  digest &lt;STRING&gt;         status_code 200         &#125;connect_port 80         bindto  &lt;IPADD&gt;         connect_timeout   3     nb_get_retry  3         delay_before_retry  2   &#125; TCP_CHECK     &#123;connect_port 80         bindto  &lt;IPADD&gt;         connect_timeout   3     nb_get_retry  3         delay_before_retry  2   &#125;SMTP_CHECK &#123;host &#123;connect_ip &lt;IP ADDRESS&gt;connect_port &lt;PORT&gt;     14 KEEPALIVEDbindto &lt;IP ADDRESS&gt;&#125;connect_timeout &lt;INTEGER&gt;retry &lt;INTEGER&gt;delay_before_retry &lt;INTEGER&gt;helo_name &lt;STRING&gt;|&lt;QUOTED-STRING&gt;&#125; MISC_CHECK &#123;misc_path &lt;STRING&gt;|&lt;QUOTED-STRING&gt;  misc_timeout &lt;INT&gt;                  misc_dynamic                                              &#125;\n\n以上就是 keepalived 的配置项说明虽然配置项很多但很多时候很多配置项保持默认即可，以下是默认配置文件，方便大家做个对比参考\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158[root@sg-gop-10-65-32-140 wangao]! Configuration File for keepalivedglobal_defs &#123;   notification_email &#123;     acassen@firewall.loc     failover@firewall.loc     sysadmin@firewall.loc   &#125;   notification_email_from Alexandre.Cassen@firewall.loc   smtp_server 192.168.200.1   smtp_connect_timeout 30   router_id LVS_DEVEL   vrrp_skip_check_adv_addr   vrrp_strict   vrrp_garp_interval 0   vrrp_gna_interval 0&#125;vrrp_instance VI_1 &#123;    state MASTER    interface eth0    virtual_router_id 51    priority 100    advert_int 1    authentication &#123;        auth_type PASS        auth_pass 1111    &#125;    virtual_ipaddress &#123;        192.168.200.16        192.168.200.17        192.168.200.18    &#125;&#125;virtual_server 192.168.200.100 443 &#123;    delay_loop 6    lb_algo rr    lb_kind NAT    persistence_timeout 50    protocol TCP    real_server 192.168.201.100 443 &#123;        weight 1        SSL_GET &#123;            url &#123;              path /              digest ff20ad2481f97b1754ef3e12ecd3a9cc            &#125;            url &#123;              path /mrtg/              digest 9b3a0c85a887a256d6939da88aabd8cd            &#125;            connect_timeout 3            nb_get_retry 3            delay_before_retry 3        &#125;    &#125;&#125;virtual_server 10.10.10.2 1358 &#123;    delay_loop 6    lb_algo rr    lb_kind NAT    persistence_timeout 50    protocol TCP    sorry_server 192.168.200.200 1358    real_server 192.168.200.2 1358 &#123;        weight 1        HTTP_GET &#123;            url &#123;              path /testurl/test.jsp              digest 640205b7b0fc66c1ea91c463fac6334d            &#125;            url &#123;              path /testurl2/test.jsp              digest 640205b7b0fc66c1ea91c463fac6334d            &#125;            url &#123;              path /testurl3/test.jsp              digest 640205b7b0fc66c1ea91c463fac6334d            &#125;            connect_timeout 3            nb_get_retry 3            delay_before_retry 3        &#125;    &#125;    real_server 192.168.200.3 1358 &#123;        weight 1        HTTP_GET &#123;            url &#123;              path /testurl/test.jsp              digest 640205b7b0fc66c1ea91c463fac6334c            &#125;            url &#123;              path /testurl2/test.jsp              digest 640205b7b0fc66c1ea91c463fac6334c            &#125;            connect_timeout 3            nb_get_retry 3            delay_before_retry 3        &#125;    &#125;&#125;virtual_server 10.10.10.3 1358 &#123;    delay_loop 3    lb_algo rr    lb_kind NAT    persistence_timeout 50    protocol TCP    real_server 192.168.200.4 1358 &#123;        weight 1        HTTP_GET &#123;            url &#123;              path /testurl/test.jsp              digest 640205b7b0fc66c1ea91c463fac6334d            &#125;            url &#123;              path /testurl2/test.jsp              digest 640205b7b0fc66c1ea91c463fac6334d            &#125;            url &#123;              path /testurl3/test.jsp              digest 640205b7b0fc66c1ea91c463fac6334d            &#125;            connect_timeout 3            nb_get_retry 3            delay_before_retry 3        &#125;    &#125;    real_server 192.168.200.5 1358 &#123;        weight 1        HTTP_GET &#123;            url &#123;              path /testurl/test.jsp              digest 640205b7b0fc66c1ea91c463fac6334d            &#125;            url &#123;              path /testurl2/test.jsp              digest 640205b7b0fc66c1ea91c463fac6334d            &#125;            url &#123;              path /testurl3/test.jsp              digest 640205b7b0fc66c1ea91c463fac6334d            &#125;            connect_timeout 3            nb_get_retry 3            delay_before_retry 3        &#125;    &#125;&#125;\n\n最简单的 Keepalived HA 配置实例1234567891011121314151617181920212223242526272829303132333435yum install keepalived -y-A INPUT -p vrrp -j ACCEPT-A INPUT -p igmp -j ACCEPT-A INPUT -d 224.0.0.18 -j ACCEPTvi /etc/keepalived/keepalived.confvrrp_sync_group VI_GOP_NC1_HA &#123;    group &#123;        VI_GOP_NC1_HA_PRI    &#125;&#125;vrrp_instance VI_GOP_NC1_HA_PRI &#123;    state BACKUP    interface bond0    virtual_router_id 139    priority 100    advert_int 1    nopreempt    authentication &#123;        auth_type PASS        auth_pass 1111    &#125;    virtual_ipaddress &#123;        10.65.33.139/23 dev bond0    &#125;&#125;service keepalived start\n\nKeepalived 双活实践\n\n\n\n\n\n\n\n\n最简单的 keepalived 双活，只需要修改 state 和 priority\n\n优点：配置文件简单\n缺点：\n当 master 恢复后会自动回切，影响业务流量\n两个节点配置不完全一致，对自动化运维管理不友好\n\n\n\n1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162# node1vrrp_instance VI_1 &#123;    state MASTER    interface bond0    virtual_router_id 32    priority 100    advert_int 1    authentication &#123;        auth_type PASS        auth_pass 1111    &#125;    virtual_ipaddress &#123;        10.71.17.32/23 dev bond0    &#125;&#125;vrrp_instance VI_2 &#123;    state BACKUP    interface bond0    virtual_router_id 33    priority 98    advert_int 1    authentication &#123;        auth_type PASS        auth_pass 1111    &#125;    virtual_ipaddress &#123;        10.71.17.33/23 dev bond0    &#125;&#125;# node2vrrp_instance VI_1 &#123;    state BACKUP    interface bond0    virtual_router_id 32    priority 98    advert_int 1    authentication &#123;        auth_type PASS        auth_pass 1111    &#125;    virtual_ipaddress &#123;        10.71.17.32/23 dev bond0    &#125;&#125;vrrp_instance VI_2 &#123;    state MASTER    interface bond0    virtual_router_id 33    priority 100    advert_int 1    authentication &#123;        auth_type PASS        auth_pass 1111    &#125;    virtual_ipaddress &#123;        10.71.17.33/23 dev bond0    &#125;&#125;\n\n\n\n\n\n\n\n\n\n\n合理的 keepalived 双活\n\n优点：\n添加 nopreempt 可以防止自动回切\n添加 track_script 可以人为控制切换\n节点之间配置完全一致，便于自动化运维管理\n\n\n缺点：配置文件较为复杂\n\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105# node1vrrp_script maint-10.71.17.32 &#123;    script &quot;/bin/bash -c &#x27;[[ -e /etc/keepalived/10.71.17.32 ]]&#x27; &amp;&amp; exit 1 || exit 0&quot;    interval 2    fall 2    rise 2&#125;vrrp_script maint-10.71.17.33 &#123;    script &quot;/bin/bash -c &#x27;[[ -e /etc/keepalived/10.71.17.33 ]]&#x27; &amp;&amp; exit 1 || exit 0&quot;    interval 2    fall 2    rise 2&#125;vrrp_instance VI_1 &#123;    state BACKUP    interface bond0    virtual_router_id 32    priority 100    advert_int 1    nopreempt    authentication &#123;        auth_type PASS        auth_pass 1111    &#125;    virtual_ipaddress &#123;        10.71.17.32/23 dev bond0    &#125;    track_script &#123;        maint-10.71.17.32    &#125;&#125;vrrp_instance VI_2 &#123;    state BACKUP    interface bond0    virtual_router_id 33    priority 100    advert_int 1    nopreempt    authentication &#123;        auth_type PASS        auth_pass 1111    &#125;    virtual_ipaddress &#123;        10.71.17.33/23 dev bond0    &#125;    track_script &#123;        maint-10.71.17.33    &#125;&#125;# node2vrrp_script maint-10.71.17.32 &#123;    script &quot;/bin/bash -c &#x27;[[ -e /etc/keepalived/10.71.17.32 ]]&#x27; &amp;&amp; exit 1 || exit 0&quot;    interval 2    fall 2    rise 2&#125;vrrp_script maint-10.71.17.33 &#123;    script &quot;/bin/bash -c &#x27;[[ -e /etc/keepalived/10.71.17.33 ]]&#x27; &amp;&amp; exit 1 || exit 0&quot;    interval 2    fall 2    rise 2&#125;vrrp_instance VI_1 &#123;    state BACKUP    interface bond0    virtual_router_id 32    priority 100    advert_int 1    nopreempt    authentication &#123;        auth_type PASS        auth_pass 1111    &#125;    virtual_ipaddress &#123;        10.71.17.32/23 dev bond0    &#125;    track_script &#123;        maint-10.71.17.32    &#125;&#125;vrrp_instance VI_2 &#123;    state BACKUP    interface bond0    virtual_router_id 33    priority 100    advert_int 1    nopreempt    authentication &#123;        auth_type PASS        auth_pass 1111    &#125;    virtual_ipaddress &#123;        10.71.17.33/23 dev bond0    &#125;    track_script &#123;        maint-10.71.17.33    &#125;&#125;\n\n如果需要配合自定义脚本监控使用，可以参考Redis 主从同步配置实践\n简单的 Keepalived 邮件告警实例\n编写 sendmail.py 邮件发送脚本\n在 keepalived.conf 中配置 notify_backup\n\n12345678910111213141516171819202122232425262728293031import sysimport socketimport smtplibEMAIL_CONFIG = &#123;    &#x27;EMAIL_HOST&#x27;: &#x27;xxx&#x27;,    &#x27;EMAIL_HOST_USER&#x27;: &#x27;xxx&#x27;,    &#x27;EMAIL_RECEIVER&#x27;: &#x27;xxx&#x27;&#125;def _get_private_ip():    sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)    try:        sock.connect((&#x27;10.255.255.255&#x27;, 1))        return sock.getsockname()[0]    except:        return &#x27;127.0.0.1&#x27;    finally:        sock.close()def send_email():    ip = _get_private_ip()    hostname = socket.gethostname()    message = &#x27;Subject: Keepalived Failover Alert %s \\n\\nHOSTNAME %s on LANIP %s HA status has changed to %s&#x27; % (        sys.argv[1], hostname, ip, sys.argv[1])    server = smtplib.SMTP(EMAIL_CONFIG[&quot;EMAIL_HOST&quot;])    server.sendmail(EMAIL_CONFIG[&#x27;EMAIL_HOST_USER&#x27;],                    EMAIL_CONFIG[&#x27;EMAIL_RECEIVER&#x27;], message)    server.quit()send_email()\n\n\n\n\n\n\n\n\n\n\n如果有设置 vrrp_sync_group 可以添加在这里通过群组控制，如果没有就跟在 vrrp_instance 独立设置\n12345678910111213141516171819202122232425           # notify scripts and alerts are optional           #           # filenames of scripts to run on transitions can be unquoted (if           # just filename) or quoted (if it has parameters)           # The username and groupname specify the user and group           # under which the scripts should be run. If username is           # specified, the group defaults to the group of the user.           # If username is not specified, they default to the           # global script_user and script_group to MASTER transition           notify_master /path/to_master.sh [username [groupname]]           # to BACKUP transition           notify_backup /path/to_backup.sh [username [groupname]]           # FAULT transition           notify_fault &quot;/path/fault.sh VG_1&quot; [username [groupname]]vrrp_sync_group NC-CLOUD-LOADTEST &#123;    group &#123;        NC-CLOUD-LOADTEST-PUB        NC-CLOUD-LOADTEST-PRI    &#125;    notify_master &quot;/bin/python /etc/keepalived/sendmail.py master&quot;    notify_backup &quot;/bin/python /etc/keepalived/sendmail.py backup&quot;&#125;\n\nKeepalived Notification and Tracking ScriptsKeepalived 官方的文档并没有给出实践案例，我对上面的代码改进之后的效果如下\n\n实现双活，支持不中断 LVS 人工干预任意节点运行位置\n实现 status 状态无变化时无告警邮件\n\nKeepalived is a Linux implementation of the VRRP (Virtual Router Redundancy Protocol) protocol to make IPs highly available. Keepalived check and notify scripts can be used to check anything you want to ensure the Master is on the right node and take action if a state change.\nnotify scripts and alerts are optional\nAbout Keepalived Notification and Tracking Scripts\nCheck script has two reutrn value:\n\n0 for everything is fine\n1 or other than 0 means something went wrong.\n\nFor example:\n123456vrrp_script maint-xxx &#123;    script &quot;/bin/bash -c &#x27;[[ -e /etc/keepalived/xxx ]]&#x27; &amp;&amp; exit 1 || exit 0&quot;    interval 2    fall 2    rise 2&#125;\n\nThis script defines file check to check whether the file xxx is exist. The check interval is 2 seconds, check fail and succeed twice for KO and OK.\nThe check script is used in a vrrp_instance as follows\nThe track_script returns other code than 0 two times, the VRRP instance will change the state to FAULT, or the instance will change the state to runningif return code 0 two times.\n\n\n\n\n\n\n\n\n\nkeepalived.conf\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119vrrp_sync_group VI_GROUP_xxx &#123;    group &#123;        VI_PRI_xxx        VI_PUB_xxx    &#125;    notify /etc/keepalived/notify.sh&#125;vrrp_sync_group VI_GROUP_xxx &#123;    group &#123;        VI_PRI_xxx        VI_PUB_xxx    &#125;    notify /etc/keepalived/notify.sh&#125;vrrp_script maint-xxx &#123;    script &quot;/bin/bash -c &#x27;[[ -e /etc/keepalived/xxx ]]&#x27; &amp;&amp; exit 1 || exit 0&quot;    interval 10    fall 6    rise 2&#125;vrrp_script maint-xxx &#123;    script &quot;/bin/bash -c &#x27;[[ -e /etc/keepalived/xxx ]]&#x27; &amp;&amp; exit 1 || exit 0&quot;    interval 10    fall 6    rise 2&#125;vrrp_script maint-xxx &#123;    script &quot;/bin/bash -c &#x27;[[ -e /etc/keepalived/xxx ]]&#x27; &amp;&amp; exit 1 || exit 0&quot;    interval 10    fall 6    rise 2&#125;vrrp_script maint-xxx &#123;    script &quot;/bin/bash -c &#x27;[[ -e /etc/keepalived/xxx ]]&#x27; &amp;&amp; exit 1 || exit 0&quot;    interval 10    fall 6    rise 2&#125;vrrp_instance VI_PRI_xxx &#123;    state BACKUP    interface bond0    virtual_router_id 138    priority 100    advert_int 1    nopreempt    authentication &#123;        auth_type PASS        auth_pass 1111    &#125;    virtual_ipaddress &#123;        xxx/23 dev bond0    &#125;    track_script &#123;        maint-xxx    &#125;&#125;vrrp_instance VI_PRI_xxx &#123;    state BACKUP    interface bond0    virtual_router_id 139    priority 100    advert_int 1    nopreempt    authentication &#123;        auth_type PASS        auth_pass 1111    &#125;    virtual_ipaddress &#123;        xxx/23 dev bond0    &#125;    track_script &#123;        maint-xxx    &#125;&#125;vrrp_instance VI_PUB_xxx &#123;    state BACKUP    interface bond1    virtual_router_id 101    priority 100    advert_int 1    nopreempt    authentication &#123;        auth_type PASS        auth_pass 1111    &#125;    virtual_ipaddress &#123;        xxx/26 dev bond1    &#125;    track_script &#123;        maint-xxx    &#125;&#125;vrrp_instance VI_PUB_xxx &#123;    state BACKUP    interface bond1    virtual_router_id 102    priority 100    advert_int 1    nopreempt    authentication &#123;        auth_type PASS        auth_pass 1111    &#125;    virtual_ipaddress &#123;        xxx/26 dev bond1    &#125;    track_script &#123;        maint-xxx    &#125;&#125;\n\n\n\n\n\n\n\n\n\n\nnotify.sh\nKeepalived tasks some action depending on the VRRP state.\n1234567vrrp_sync_group VI_GROUP_xxx &#123;    group &#123;        VI_PRI_xxx        VI_PUB_xxx    &#125;    notify /etc/keepalived/notify.sh&#125;\n\nThe script is called after any state change with the following parameters:\n$1 &#x3D; “GROUP” or “INSTANCE”$2 &#x3D; name of group or instance$3 &#x3D; target state of transition (“MASTER”, “BACKUP”, “FAULT”)\n12345678910111213141516171819202122232425262728293031323334#!/bin/bashTYPE=$1NAME=$2STATE=$3FILE=&quot;/etc/keepalived/$&#123;NAME&#125;&quot;if [ ! -f &quot;$&#123;FILE&#125;&quot; ]; then  touch &quot;$&#123;FILE&#125;&quot;fiORI_STATE=`cat $&#123;FILE&#125;`if [ $&#123;STATE&#125; == $&#123;ORI_STATE&#125; ];then   exit 0else    case $STATE in            &quot;MASTER&quot;) /bin/python /etc/keepalived/sendmail.py $&#123;STATE&#125; $&#123;TYPE&#125; $&#123;NAME&#125;                      echo &quot;$&#123;STATE&#125;&quot; &gt; &quot;$&#123;FILE&#125;&quot;                      exit 0                      ;;            &quot;BACKUP&quot;) /bin/python /etc/keepalived/sendmail.py $&#123;STATE&#125; $&#123;TYPE&#125; $&#123;NAME&#125;                      echo &quot;$&#123;STATE&#125;&quot; &gt; &quot;$&#123;FILE&#125;&quot;                      exit 0                      ;;            &quot;FAULT&quot;)  /bin/python /etc/keepalived/sendmail.py $&#123;STATE&#125; $&#123;TYPE&#125; $&#123;NAME&#125;                      echo &quot;$&#123;STATE&#125;&quot; &gt; &quot;$&#123;FILE&#125;&quot;                      exit 0                      ;;            *)        echo &quot;unknown state&quot;                      exit 1                      ;;    esacfi\n\n\n\n\n\n\n\n\n\n\nsendmail.py\n1234567891011121314151617181920212223242526272829303132333435import sysimport socketimport smtplibEMAIL_CONFIG = &#123;    &#x27;EMAIL_HOST&#x27;: &#x27;xxx&#x27;,    &#x27;EMAIL_HOST_USER&#x27;: &#x27;xxx&#x27;,    &#x27;EMAIL_RECEIVER&#x27;: &#x27;xxx&#x27;&#125;def _get_private_ip():    sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)    try:        sock.connect((&#x27;10.255.255.255&#x27;, 1))        return sock.getsockname()[0]    except:        return &#x27;127.0.0.1&#x27;    finally:        sock.close()def send_email():    ip = _get_private_ip()    hostname = socket.gethostname()    message = &#x27;Subject: Keepalived Failover Alert %s \\n\\nHOSTNAME %s on LANIP %s %s %s status has changed to %s&#x27; % (        sys.argv[1], hostname, ip, sys.argv[2], sys.argv[3], sys.argv[1])    server = smtplib.SMTP(EMAIL_CONFIG[&quot;EMAIL_HOST&quot;])    server.sendmail(EMAIL_CONFIG[&#x27;EMAIL_HOST_USER&#x27;],                    EMAIL_CONFIG[&#x27;EMAIL_RECEIVER&#x27;], message)    server.quit()send_email()\n\nKeepalived 常见问题\n\n\n\n\n\n\n\n\nvirual_router_id 冲突\n1234567891011121314151617181920212223# 检查 keepalived 错误日志会发现tailf /var/log/messagesMay  7 23:25:18 xxx Keepalived_vrrp[90851]: bogus VRRP packet received on bond1 !!!May  7 23:25:18 xxx Keepalived_vrrp[90851]: VRRP_Instance(VI_PUB_xxx) ignoring received advertisment...May  7 23:25:19 xxx Keepalived_vrrp[90851]: (VI_PUB_xxx): ip address associated with VRID 101 not present in MASTER advert : xxx# 通过检查配置文件的方法效率太低grep &#x27;virtual_router_id&#x27; /etc/keepalived/keepalived.conf    virtual_router_id 148    virtual_router_id 149    virtual_router_id 101    virtual_router_id 104# 如果是 vrrp 广播可以通过 tcpdump 抓包分析tcpdump -i bond1 -nn &#x27; vrrp&#x27;tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on bond1, link-type EN10MB (Ethernet), capture size 262144 bytes00:16:44.919824 IP xxx &gt; 224.0.0.18: VRRPv2, Advertisement, vrid 105, prio 100, authtype simple, intvl 1s, length 2000:16:44.995030 IP xxx &gt; 224.0.0.18: VRRPv2, Advertisement, vrid 101, prio 100, authtype simple, intvl 1s, length 2000:16:44.995046 IP xxx &gt; 224.0.0.18: VRRPv2, Advertisement, vrid 104, prio 100, authtype simple, intvl 1s, length 2000:16:44.996107 IP xxx &gt; 224.0.0.18: VRRPv2, Advertisement, vrid 123, prio 100, authtype simple, intvl 1s, length 20\n\n\n\n\n\n\n\n\n\n\nkeepalived 单播模式\nkeepalived 在组播模式下所有的信息都会向 224.0.0.18 的组播地址发送，产生众多的无用信息，并且会产生干扰和冲突，所以需要将其组播的模式改为单播。这是一种安全的方法，避免局域网内有大量的 keepalived 造成虚拟路由 id 的冲突。\n单薄模式需要关闭 vrrp_strict，严格遵守 vrrp 协议这个选项\n单薄需要在 VIP 实例配置段加入单播的源地址和目标地址\n123456789global_defs &#123;   #vrrp_strict                         #将严格遵守 vrrp 协议这一项关闭，否则会因为不是组播而无法启动 keepalived&#125;  # 主备节点地址注意互换    unicast_src_ip 172.20.27.10         #配置单播的源地址    unicast_peer &#123;     172.20.27.11                        #配置单播的目标地址    &#125;\n\n123456789101112131415161718192021222324# See description of global vrrp_strict# If vrrp_strict is not specified, it takes the value of vrrp_strict# If strict_mode without a parameter is specified, it defaults to onstrict_mode [on|off|true|false|yes|no]# default IP for binding vrrpd is the primary IP# on interface. If you want to hide the location of vrrpd,# use this IP as src_addr for multicast or unicast vrrp# packets. (since it&#x27;s multicast, vrrpd will get the reply# packet no matter what src_addr is used).# optionalmcast_src_ip &lt;IPADDR&gt;unicast_src_ip &lt;IPADDR&gt;# Do not send VRRP adverts over a VRRP multicast group.# Instead it sends adverts to the following list of# ip addresses using unicast. It can be cool to use# the VRRP FSM and features in a networking# environment where multicast is not supported!# IP addresses specified can be IPv4 as well as IPv6.unicast_peer &#123;    &lt;IPADDR&gt;    ...&#125;\n\nkeepalived 单播模式\nLVS 和 Keepalived 系列LVS 和 Keepalived 的原理介绍和配置实践LVS 原理介绍和配置实践Keepalived 原理介绍和配置实践LVS-NAT 原理介绍和配置实践LVS-DR 原理介绍和配置实践LVS-TUN 原理介绍和配置实践\n参考文档Keepalived Configuration Manual Page\nKeepalived User Guide\nkeepalived 实战\n实现高可用集群的神器 详解 Keepalived\nLVS 小宇宙爆发！ 当 Keepalived 遇上 LVS，实现集群高可用\n","slug":"MIDDLEWARE/Keepalived 原理介绍和配置实践","date":"2023-02-22T10:59:21.000Z","categories_index":"Keepalived,MIDDLEWARE","tags_index":"https,keepalived,LVS","author_index":"dandeliono"},{"id":"b7cb565dcfadfaf74b2fbb30bbd40cf9","title":"Redis 中常见的集群部署方案","content":"Redis 中常见的集群部署方案Redis 的高可用集群前言这里来了解一下，Redis 中常见的集群方案\n几种常用的集群方案\n主从集群模式\n\n哨兵机制\n\n切片集群(分片集群)\n\n\n主从集群模式主从集群，主从库之间采用的是读写分离\n\n主库：所有的写操作都在主库发生，然后主库同步数据到从库，同时也可以进行读操作；\n\n从库：只负责读操作；\n\n\n\n主库需要复制数据到从库，主从双方的数据库需要保存相同的数据，将这种情况称为”数据库状态一致”\n来看下如何同步之前先来了解下几个概念\n\n1、服务器的运行ID(run ID)：每个 Redis 服务器在运行期间都有自己的run ID，run ID在服务器启动的时候自动生成。\n\n从服务器会记录主服务器的run ID，这样如果发生断网重连，就能判断新连接上的主服务器是不是上次的那一个，这样来决定是否进行数据部分重传还是完整重新同步。\n\n2、复制偏移量 offset：主服务器和从服务器都会维护一个复制偏移量\n\n主服务器每次向从服务器中传递 N 个字节的时候，会将自己的复制偏移量加上 N。\n从服务器中收到主服务器的 N 个字节的数据，就会将自己额复制偏移量加上 N。\n通过主从服务器的偏移量对比可以很清楚的知道主从服务器的数据是否处于一致。\n如果不一致就需要进行增量同步了，具体参加下文的增量同步\n全量同步从服务器首次加入主服务器中发生的是全量同步\n如何进行第一次同步？\n\n1、从服务器连接到主服务器，然后发送 psync 到主服务器，因为第一次复制，不知道主库run ID,所以run ID为？；\n2、主服务器接收到同步的响应，回复从服务器自己的run ID和复制进行进度 offset；\n3、主服务器开始同步所有数据到从库中，同步依赖 RDB 文件，主库会通过 bgsave 命令，生成 RDB 文件，然后将 RDB 文件传送到从库中；\n4、从库收到 RDB 文件,清除自己的数据，然后载入 RDB 文件；\n5、主库在同步的过程中不会被阻塞，仍然能接收到命令，但是新的命令是不能同步到从库的，所以主库会在内存中用专门的 replication buffer，记录 RDB 文件生成后收到的所有写操作，然后在 RDB 文件，同步完成之后，再将replication buffer中的命令发送到从库中，这样就保证了从库的数据同步。\n增量同步如果主从服务器之间发生了网络闪断，从从服务将会丢失一部分同步的命令。\n在旧版本，Redis 2.8之前，如果发生了网络闪断，就会进行一次全量复制。\n在 2.8 版本之后，引入了增量同步的技术，这里主要是用到了 repl_backlog_buffer\nRedis 主库接收到写操作的命令，首先会写入replication buffer(主要用于主从数据传输的数据缓冲)，同时也会把这些操作命令也写入repl_backlog_buffer这个缓冲区。\n\n这里可能有点疑惑，已经有了replication buffer为什么还多余引入一个repl_backlog_buffer呢？\n\nrepl_backlog_buffer一个主库对应一个repl_backlog_buffer，也就是所有从库对应一个repl_backlog_buffer，从库自己记录自己的slave_repl_offset。\n\nreplication buffer用于主节点与各个从节点间，数据的批量交互。主节点为各个从节点分别创建一个缓冲区，由于各个从节点的处理能力差异，各个缓冲区数据可能不同。\n\n\n如何主从断开了，当然对应的replication buffer也就没有了。这时候就依赖repl_backlog_buffer进行数据的增量同步了。\nrepl_backlog_buffer是一个环形缓冲区，主库会记录自己写到的位置，从库则会记录自己已经读到的位置。\n这里借用Redis核心技术与实战的一张图片\n\n刚开始主服务器的 master_repl_offset 和从服务器 slave_repl_offset 的位置是一样的，在从库因为网络原因断连之后，随着主库写操作的进行，主从偏移量会出现偏移距离。\n当从服务器连上主服务器之后，从服务把自己当前的 slave_repl_offset 告诉主服务器，然后主服务器根据自己的 master_repl_offset 计算出和从服务器之间的差距，然后把两者之间相差的命令操作同步给从服务器。\n举个栗子\n比如这里从服务器1，刚刚由于网络原因断连了一会，然后又恢复了连接，这时候，可能缺失了一段时间的命令同步，repl_backlog_buffer的增量同步机制就登场了。\nrepl_backlog_buffer会根据主服务器的master_repl_offset和从服务器slave_repl_offset，计算出两者命令之间的差距，之后把差距同步给replication buffer，然后发送到从服务器中。\n\nrepl_backlog_buffer中的缓冲空间要设置的大一点，如果从库读的过慢，因为是环形缓冲区，可能出现命令覆盖的情况，如果出现命令被覆盖了，从库的增量同步就无法进行了，这时候会进行一次全量的复制。\n缓冲空间的计算公式是：缓冲空间大小 &#x3D; 主库写入命令速度 * 操作大小 - 主从库间网络传输命令速度 * 操作大小。在实际应用中，考虑到可能存在一些突发的请求压力，我们通常需要把这个缓冲空间扩大一倍，即 repl_backlog_size &#x3D; 缓冲空间大小 * 2，这也就是 repl_backlog_size 的最终值。\n哨兵机制对于主从集群模式，如果从库发生了故障，还有主库和其它的从库可以接收请求，但是如果主库挂了，就不能进行正常的数据写入，同时数据同步也不能正常的进行了，当然这种情况，我们需要想办法避免，于是就引入了下面的哨兵机制。\n什么是哨兵机制sentinel(哨兵机制)：是 Redis 中集群的高可用方式，哨兵节点是特殊的 Redis 服务，不提供读写，主要来监控 Redis 中的实例节点，如果监控服务的主服务器下线了，会从所属的从服务器中重新选出一个主服务器，代替原来的主服务器提供服务。\n\n核心功能就是:监控，选主，通知。\n监控：哨兵机制，会周期性的给所有主服务器发出 PING 命令，检测它们是否仍然在线运行，如果在规定的时间内响应了 PING 通知则认为，仍在线运行；如果没有及时回复，则认为服务已经下线了，就会进行切换主库的动作。\n选主：当主库挂掉的时候，会从从库中按照既定的规则选出一个新的的主库，\n通知：当一个主库被新选出来，会通知其他从库，进行连接，然后进行数据的复制。当客户端试图连接失效的主库时，集群也会向客户端返回新主库的地址，使得集群可以使用新的主库。\n如何保证选主的准确性哨兵会通过 PING 命令检测它和从库，主库之间的连接情况，如果发现响应超时就会认为给服务已经下线了。\n当然这会存在误判的情况，如果集群的网络压力比较大，网路堵塞，这时候会存在误判的情况。\n如果误判的节点是从节点，影响不会很大，拿掉一个从节点，对整体的服务，影响不大，还是会不间断的对外提供服务。\n如果误判的节点是主节点，影响就很大了，主节点被标注下线了，就会触发后续的选主，数据同步，等一连串的动作，这一连串的动作很很消耗性能的。所以对于误判，应该去规避。\n如何减少误判呢？\n引入哨兵集群，一个哨兵节点可能会进行误判，引入多个少哨兵节点一起做决策，就能减少误判了。\n当有多个哨兵节点的时候，大多数哨兵节点认为主库下线了，主库才会真正的被标记为下线了，一般来讲当有 N 个哨兵实例时，最好要有N/2 + 1个实例判断主库下线了，才能最终判定主库的下线状态。当然这个数值在 Redis 中是可以配置的。\n如何选主选举主节点的规则1、过滤掉已经下线的服务器；\n2、过滤掉最近5秒钟没有回复过主节点的 INFO(用于观察服务器的角色) 命令的服务器，保证选中的服务器都是最近成功通过信的；\n3、过滤掉和下线主服务器连接超过down-after-milliseconds*10毫秒的从服务器，down-after-milliseconds是主服务器下线的时间，这一操作避免从服务器与主服务器过早的断开，影响到从库中数据同步，因为断开时间越久，从库里面的数据就越老旧过时。\n然后对这些服务器根据slave-priority优先级(这个优先级是手动设置的，比如希望那个从服务器优先变成主服务器，优先级就设置的高一点) 进行排序。\n如果几台从服务器优先级相同，然后根据复制偏移量从大到小进行排序，如果还有相同偏移量的从服务器，然后按照 runID 从小到大进行排序，直到选出一台从服务器。\n哨兵进行主节点切换当根据选举规则，选出了可以成为主节点的从节点，如何进行切换呢？\n在哨兵中也是有一个 Leader 节点的，当一个从库被选举出来，从库的切换是由 Leader 节点完成的。\nLeader 节点的选举用的是 Raft 算法，关于什么是 Raft 算法可参考Raft一致性算法原理\n在raft算法中，在任何时刻，每一个服务器节点都处于这三个状态之一：\n\nFollower:追随者，跟随者都是被动的：他们不会发送任何请求，只是简单的响应来自领导者或者候选人的请求；\n\nCandidate:候选人，如果跟随者接收不到消息，那么他就会变成候选人并发起一次选举，获得集群中大多数选票的候选人将成为领导者。\n\nLeader:领导者，系统中只有一个领导人并且其他的节点全部都是跟随者，领导人处理所有的客户端请求（如果一个客户端和跟随者联系，那么跟随者会把请求重定向给领导人）\n\n\n哨兵节点的选举总结起来就是：\n1、每个做主观下线的sentinel节点向其他sentinel节点发送命令，要求将自己设置为领导者；\n2、接收到的sentinel可以同意或者拒绝；\n3、如果该sentinel节点发现自己的票数已经超过半数并且超过了 quorum，quorum 用来配置判断主节点宕机的哨兵节点数。简单点讲就是：如果 Sentinel 集群有 quorum 个哨兵认为 master 宕机了，就「客观」的认为 master 宕机了；\n4、如果此过程选举出了多个领导者，那么将等待一段时重新进行选举；\n故障转移\n\nsentinel的领导者从从机中选举出合适的丛机进行故障转移；\n\n对选取的从节点进行slave of no one命令，（这个命令用来让从机关闭复制功能，并从从机变为主机）；\n\n更新应用程序端的链接到新的主节点；\n\n对其他从节点变更 master 为新的节点；\n\n修复原来的 master 并将其设置为新的 master 的从机。\n\n\n消息通知\n哨兵和哨兵之前，哨兵和从库之间，哨兵和客户端是如何相互发现，进行消息传递？\n哨兵和哨兵之间的相互发现，通过 Redis 提供的pub/sub机制实现，因为每个哨兵节点都会和主库进行连接，通过在主库中发布信息，订阅信息，就能找到其他实例的连接信息。\n哨兵节点和从库，通过哨兵向主库发送 INFO 命令来完成，哨兵给主库发送 INFO 命令，主库接受到这个命令后，就会把从库列表返回给哨兵。接着，哨兵就可以根据从库列表中的连接信息，和每个从库建立连接，并在这个连接上持续地对从库进行监控。\n哨兵和客户端之间：每个哨兵实例也提供pub/sub机制，客户端可以从哨兵订阅消息，来获知主从库切换过程中的不同关键事件。\n哨兵提升一个从库为新主库后，哨兵会把新主库的地址写入自己实例的 pubsub（switch-master） 中。客户端需要订阅这 个pubsub，当这个 pubsub 有数据时，客户端就能感知到主库发生变更，同时可以拿到最新的主库地址，然后把写请求写到这个新主库即可，这种机制属于哨兵主动通知客户端。\n如果客户端因为某些原因错过了哨兵的通知，或者哨兵通知后客户端处理失败了，安全起见，客户端也需要支持主动去获取最新主从的地址进行访问。\n切片集群对于数据库我们知道，如果数据量大会进行分库分表，一般有两种方案纵向拆分和横向拆分。这在 Redis 中，同样适用。\nRedis 中的扩展\n\n纵向扩展：更改节点类型以调整集群大小，升级单个Redis实例的资源配置，包括增加内存容量、增加磁盘容量、使用更高配置的CPU。\n\n横向扩展：通过添加或删除节点组（分片）来更改复制组中的节点组（分片）数量。\n\n\n\n简单点讲就是：垂直扩容就是增加自身的容量，横向扩容就是加机器。\n缺点对比\n纵向扩容：\n1、如果一味的增加自身的容量，意味着自身存储的数据将会越来越大，过大的数据，持久化时间将会变得很长，影响自身的响应速度；\n2、同样堆硬件总归是有上线，达到一定量之后，还是要考虑进行横向扩容；\n横向扩容：\n横向扩容要面临的问题，如果发生了分片的扩容，就需要考虑数据的迁移，同时数据切片后，在多个实例之间如何分布？，客户端如何知道访问的数据在哪个实例中。。。\n虽然有这些问题的存在，好在已经有一些成熟的方案来处理横向扩容所遇到的问题了\n官方的集群解决方案就是Redis Cluster；社区的解决方案有 Codis 和 Twemproxy，Codis 是由我国的豌豆荚团队开源的，Twemproxy 是 Twitter 团队的开源的。\n这里主要看下Redis Cluster是如何进行处理的\nRedis Cluster方案1、Redis Cluster方案采用哈希槽来处理 KEY 在不同实例中的分布，一个切片集群共有16384个哈希槽，这些哈希槽类似于数据分区，每个键值对都会根据它的key，被映射到一个哈希槽中。\n2、一个 KEY ，首先会根据CRC16算法计算一个16 bit的值；然后，再用这个 16bit 值对 16384 取模，得到0~16383范围内的模数，每个模数代表一个相应编号的哈希槽。\n3、然后把哈希槽分配到所有的实例中，例如，如果集群中有N个实例，那么，每个实例上的槽个数为16384&#x2F;N个。\n当然这是平均分配的，如果平均分配额哈希槽中，某一个实例中 KEY，存储的数据比较大，造成某一个实例的内存过大，这时候可以通过cluster addslots手动调节哈希槽的分配。\n当手动分配哈希槽时，需要把16384个槽都分配完，否则Redis集群无法正常工作。\n客户端中的 KEY 如何找到对应的实例\n在集群刚刚创建的时候，每个实例只知道自己被分配了哪些哈希槽，是不知道其他实例拥有的哈希槽信息的。但是，Redis 实例会把自己的哈希槽信息发给和它相连接的其它实例，来完成哈希槽分配信息的扩散。\n所以当客户端和集群实例连接后，就可以知道所有的哈希槽的映射，客户端会把哈希槽的映射保存在本地，这样如果客户端响应一个 KEY ，计算出哈希槽，然后就可以向对应的实例发送请求了。\n哈希槽重新分配数据在可能发生迁移，这时候哈希槽就会重新分配了\n栗如：\n1、集群中的实例，有增加或减少；\n2、引入了负载均衡，需要重新分配哈希槽；\n因为重新分配了哈希槽，所以原来的映射关系可能发生了改变，实例之间可以通过相互通知，快速的感知到映射关系的变化。但是，客户端无法主动感知这些变化，客户端对 KEY 的响应，可能依旧映射到了之前的实例节点，面对这种情况，如何处理呢？\n1、如果数据已经迁移完了Redis Cluster中提供重定向机制，如果一个实例接收到客户端的请求，但是对应的 KEY 已经转移到别的实例节点中了，这时候会计算出 KEY 当前所处实例的地址，然后返回给客户端，客户端拿到最新的实例地址，重新发送请求就可以了。\n12$ GET hello(error) MOVED 12320 172.168.56.111:6379 \n\n2、数据迁移了一半如果在迁移的过程中，只迁移了一半的数据，这时候服务器端就会返回 ASK 告知客户端\n12GET hello(error) ASK 1332 012320 172.168.56.111:6379 \n\nASK 就表示当前正在迁移中，客户端需要访问数据，就还需要向返回的地址信息，发送一条 ASKING 命令，让这个实例允许客户端的访问请求，然后客户端再发送具体的业务操作命令。\nRedis Cluster 的规模是越大越好吗Redis Cluster 能保存的数据量以及支撑的吞吐量，跟集群的实例规模密切相关，原则上如果实例越多，能够承担的吞吐量和就越大，不过真的是这样吗？\nRedis 官方给出了 Redis Cluster 规模上限，就是一个集群运行1000个实例。那么为什么要来限制集群的规模呢，这里分析下：\nRedis Cluster 在运行时，每个实例上都会保存 Slot 和实例的对应关系（也就是Slot映射表），以及自身的状态信息。\n为了让集群中的每个实例都知道其它所有实例的状态信息，实例之间会按照一定的规则进行通信。这个规则就是 Gossip 协议。\nGossip 协议的工作原理可以概括成两点。\n1、每个实例之间会按照一定的频率，从集群中随机挑选一些实例，把 PING 消息发送给挑选出来的实例，用来检测这些实例是否在线，并交换彼此的状态信息。PING消息中封装了发送消息的实例自身的状态信息、部分其它实例的状态信息，以及 Slot 映射表。\n2、一个实例在接收到 PING 消息后，会给发送PING消息的实例，发送一个 PONG 消息。PONG 消息包含的内容和 PING 消息一样。\n通信的开销受通信消息大小和通信频率的影响\nGossip消息大小\nRedis实例发送的PING消息的消息体是由clusterMsgDataGossip结构体组成的，这个结构体的定义如下所示：\n12345678910typedef struct &#123;    char nodename[CLUSTER_NAMELEN];  //40字节    uint32_t ping_sent; //4字节    uint32_t pong_received; //4字节    char ip[NET_IP_STR_LEN]; //46字节    uint16_t port;  //2字节    uint16_t cport;  //2字节    uint16_t flags;  //2字节    uint32_t notused1; //4字节&#125; clusterMsgDataGossip; \n\n这样看下来一个 Gossip 消息大小在 104 字节。\n每个实例在发送一个Gossip消息时，除了会传递自身的状态信息，默认还会传递集群十分之一实例的状态信息。\n所以，对于一个包含了1000个实例的集群来说，每个实例发送一个PING消息时，会包含100个实例的状态信息，总的数据量是 10400字节，再加上发送实例自身的信息，一个Gossip消息大约是10KB。\n此外，为了让Slot映射表能够在不同实例间传播，PING消息中还带有一个长度为 16,384 bit 的 Bitmap，这个Bitmap的每一位对应了一个Slot，如果某一位为1，就表示这个Slot属于当前实例。这个Bitmap大小换算成字节后，是2KB。我们把实例状态信息和Slot分配信息相加，就可以得到一个PING消息的大小了，大约是12KB。\nPONG消息和PING消息的内容一样，所以，它的大小大约是12KB。每个实例发送了PING消息后，还会收到返回的PONG消息，两个消息加起来有24KB。\n这样看下来如果集群数量的增加，消息的大小就会增加。越大的消息传输，对性能影响就越大。\n消息传递的频率\nRedis Cluster的实例启动后，默认会每秒从本地的实例列表中随机选出5个实例，再从这5个实例中找出一个最久没有通信的实例，把 PING 消息发送给该实例。这是实例周期性发送 PING 消息的基本做法。\n但是，这里有一个问题：实例选出来的这个最久没有通信的实例，毕竟是从随机选出的5个实例中挑选的，这并不能保证这个实例就一定是整个集群中最久没有通信的实例。\n所以 Redis Cluster 的实例会按照每 100ms 一次的频率，扫描本地的实例列表，如果发现有实例最近一次接收 PONG消息的时间，已经大于配置项 cluster-node-timeout 的一半了（cluster-node-timeout/2），就会立刻给该实例发送 PING 消息，更新这个实例上的集群状态信息。\n所以每秒单实例发送的 PING 数量就是\n1PING消息发送数量 = 1 + 10 * 实例数（最近一次接收PONG消息的时间超出cluster-node-timeout/2） \n\n\n\n\n\n\n\n\n\n\n1 是指单实例常规按照每 1 秒发送一个PING消息，10 是指每 1 秒内实例会执行 10 次检查。\n假设单个实例检测发现，每100毫秒有10个实例的PONG消息接收超时，那么，这个实例每秒就会发送101个PING消息，约占1.2MB&#x2F;s带宽。如果集群中有30个实例按照这种频率发送消息，就会占用36MB&#x2F;s带宽，这就会挤占集群中用于服务正常请求的带宽。\n所以整体看下来如果 Redis Cluster 集群的规模越大，网络拥塞的概率就越高，相应的，PONG消息超时的发生概率就越高，这就会导致集群中有大量的心跳消息，影响集群服务正常请求。\n虽然我们可以通过调整cluster-node-timeout配置项减少心跳消息的占用带宽情况，但是，在实际应用中，如果不是特别需要大容量集群，我建议你把 Redis Cluster 的规模控制在400~500个实例。\n避免 Hot KeyHot Key就是采用切片集群部署的 Redis ,出现的集群访问倾斜。\n切片集群中的 Key 最终会存储到集群中的一个固定的 Redis 实例中。某一个 Key 在一段时间内访问远高于其它的 Key,也就是该 Key 对应的 Redis 实例,会收到过大的流量请求，该实例容易出现过载和卡顿现象，甚至还会被打挂掉。\n常见引发热点 Key 的情况：\n1、新闻中的热点事件；\n2、秒杀活动中的，性价比高的商品；\n如何发现 Hot Key\n1、提现预判；\n\n根据业务经验进行提前预判；\n\n2、在客户端进行收集；\n\n通过在客户端增加命令的采集，来统计发现热点 Key;\n\n3、使用 Redis 自带的命令排查；\n\n使用monitor命令统计热点key（不推荐，高并发条件下会有造成redis 内存爆掉的隐患）；\nhotkeys参数，redis 4.0.3提供了redis-cli的热点key发现功能，执行redis-cli时加上–hotkeys选项即可。但是该参数在执行的时候，如果key比较多，执行起来比较慢。\n\n4、在Proxy层做收集\n\n如果集群架构引入了 proxy，可以在 proxy 中做统计\n\n5、自己抓包评估\n\nRedis客户端使用TCP协议与服务端进行交互，通信协议采用的是RESP。自己写程序监听端口，按照RESP协议规则解析数据，进行分析。缺点就是开发成本高，维护困难，有丢包可能性。\nHot Key 如何解决知道了Hot Key如何来应对呢\n1、对 Key 进行分散处理\n举个栗子\n有一个热 Key 名字为Hot-key-test,可以将其分散为Hot-key-test1，Hot-key-test2…然后将这些 Key 分散到多个实例节点中，当客户端进行访问的时候，随机一个下标的 Key 进行访问，这样就能将流量分散到不同的实例中了，避免了一个缓存节点的过载。\n一般来讲，可以通过添加后缀或者前缀，把一个 hotkey 的数量变成 redis 实例个数 N 的倍数 M，从而由访问一个redis key变成访问N * M个redis key。 N*M个redis key经过分片分布到不同的实例上，将访问量均摊到所有实例。\n12345678910const M = N * 2//生成随机数random = GenRandom(0, M)//构造备份新keybakHotKey = hotKey + “_” + randomdata = redis.GET(bakHotKey)if data == NULL &#123;    data = GetFromDB()    redis.SET(bakHotKey, expireTime + GenRandom(0,5))&#125; \n\n2、使用本地缓存\n业务端还可以使用本地缓存，将这些热 key 记录在本地缓存，来减少对远程缓存的冲击。\n避免 Big Key什么是 Big Key：我们将含有较大数据或含有大量成员、列表数的Key称之为大Key。\n\n一个 STRING 类型的 Key，它的值为 5MB（数据过大）\n\n一个 LIST 类型的 Key，它的列表数量为 20000 个（列表数量过多）\n\n一个 ZSET 类型的 Key，它的成员数量为 10000 个（成员数量过多）\n\n一个 HASH 格式的 Key，它的成员数量虽然只有 1000 个但这些成员的 value 总大小为 100MB（成员体积过大）\n\n\nBig Key 存在问题\n内存空间不均匀：如果采用切片集群的部署方案，容易造成某些实例节点的内存分配不均匀；\n\n造成网络拥塞：读取 bigkey 意味着需要消耗更多的网络流量，可能会对 Redis 服务器造成影响；\n\n过期删除：big key 不单读写慢，删除也慢，删除过期 big key 也比较耗时；\n\n迁移困难：由于数据庞大，备份和还原也容易造成阻塞，操作失败；\n\n\n如何发现 Big Key\n使用 redis-cli 客户端的命令 –bigkeys;\n\n生成 rdb 文件，离线分析 rdb 文件。比如：redis-rdb-cli，rdbtools;\n\n通过 scan 命令，对扫描出来的key进行类型判断，例如：string长度大于10K，list长度大于10240认为是big bigkeys;\n\n\nBig Key 如何避免对于Big Key可以从以下两个方面进行处理\n合理优化数据结构：\n\n1、对较大的数据进行压缩处理；\n\n2、拆分集合：将大的集合拆分成小集合（如以时间进行分片）或者单个的数据。\n\n\n选择其他的技术来存储 big key：\n\n使用其他的存储形式，考虑使用 cdn 或者文档性数据库 MongoDB。\n\nBig Key 如何删除直接使用 DEL 命令会发生什么？危险：同步删除 bigkey 会阻塞 Redis 其他命令，造成 Redis 阻塞。\n推荐使用 UNLINK 命令，异步删除 bigkey，不影响主线程执行其他命令。\n在业务的低峰期使用 scan 命令查找 big key，对于类型为集合的key，可以使用脚本逐一删除里面的元素。\n参考【Redis核心技术与实战】https://time.geekbang.org/column/intro/100056701【Redis设计与实现】https://book.douban.com/subject/25900156/【估算两台服务器同时故障的概率】https://disksing.com/failure-probability-analysis/【Redis中哨兵选举算法】https://blog.csdn.net/weixin_44324174/article/details/108939199【如何处理redis集群中hot key和big key】https://juejin.cn/post/6844903743083773959【谈谈redis的热key问题如何解决】https://www.cnblogs.com/rjzheng/p/10874537.html\n\n","slug":"MIDDLEWARE/Redis 中常见的集群部署方案","date":"2023-02-15T19:10:53.000Z","categories_index":"Redis,MIDDLEWARE","tags_index":"redis,key,Key","author_index":"dandeliono"},{"id":"9a9dc15a004def10ae461436e34e9bad","title":"k8s部署springboot的demo项目","content":"k8s部署springboot的demo项目1 准备jar包1.1 把你的springboot项目打包成可执行jar包\n1.2 把jar包上传到Linux服务器\n2 构建docker镜像2.1 编写Dockerfile12vim Dockerfile\n\n内容\n123456FROM java:8MAINTAINER luozhanfengADD demo.jar /demo.jarEXPOSE 80ENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;,&quot;/demo.jar&quot;]\n\n2.2 开始构建12docker build -t luozhanfeng/demo:v1 .\n\n\n2.2 查看镜像12docker images\n\n\n3 部署 k8s3.1 编写deployment和service的yaml123456789101112131415161718192021222324252627282930313233343536373839apiVersion: v1kind: Servicemetadata:  name: demo  namespace: default  labels:    app: demospec:  type: NodePort #对外访问的类型  ports:    - port: 8030      nodePort: 30090 #service对外开放端口  selector:    app: demo---apiVersion: apps/v1kind: Deployment #对象类型metadata:  name: demo #名称  labels:    app: demo #标注 spec:  replicas: 3 #运行容器的副本数，修改这里可以快速修改分布式节点数量  selector:    matchLabels:      app: demo  template:    metadata:      labels:        app: demo    spec:      containers: #docker容器的配置        - name: demo          image: luozhanfeng/demo:v1 #本地镜像          #image: harbor.saas.xxx-ift.com/luozhanfeng/demo:v1 #远程仓库          imagePullPolicy: IfNotPresent # Always：总是拉取； IfNotPresent：默认值,本地有则使用本地镜像, 不拉取； Never：只使用本地镜像，从不拉取          ports:            - containerPort: 8030 #容器对外开放端口\n\n3.2 构建yaml123kubectl create -f demo.yamlkubectl get pods\n\n\n3.3 测试设置的端口是否能访问12curl 127.0.0.1:30333/test/test\n\n\n\n\n3.4 删除构建12kubectl delete -f demo.yaml\n\n\n4 推送镜像到harbor4.1 新建自己的项目我们的私有harbor地址：https://harbor.saas.xxx-ift.com/账号密码是BIP\n\n4.2 在项目中标记镜像12docker tag luozhanfeng/demo:v1 harbor.saas.xxx-ift.com/luozhanfeng/demo:v1\n\n\n4.3 安装证书4.3.1 下载证书\n4.3.2 放在 &#x2F;etc&#x2F;pki&#x2F;tls&#x2F;certs&#x2F; 目录下123cd /etc/pki/tls/certs/rz \n\n\n4.4 推送镜像到当前项目4.4.1 登录harbor12docker login https://harbor.saas.xxx-ift.com\n\n4.4.2 推送12docker push harbor.saas.xxx-ift.com/luozhanfeng/demo:v1\n\n\n5 排雷5.1 报错1\n1、需要修改docker.service\n12vim /usr/lib/systemd/system/docker.service\n\nEnvironmentFile&#x3D;-&#x2F;etc&#x2F;sysconfig&#x2F;dockerExecStart&#x3D;&#x2F;usr&#x2F;bin&#x2F;dockerd $OPTIONS\n\n2、修改&#x2F;etc&#x2F;docker&#x2F;daemon.json\n{“insecure-registries”: [“http://192.168.19.4”\\]}\n\n然后重启docker即可\n123systemctl daemon-reloadsystemctl restart docker.service\n\n5.1 报错2[root@test3 ~]# docker push 192.168.0.7&#x2F;gitlab&#x2F;gitlab_back:v1The push refers to a repository [192.168.0.7&#x2F;gitlab&#x2F;gitlab_back]Get https://192.168.0.7/v1/_ping: read tcp 192.168.0.6:40306-&gt;192.168.0.7:443: read: connection reset by peer\n解决办法：编辑vim &#x2F;etc&#x2F;sysconfig&#x2F;docker文件。如果没有则创建。\n\n6 资料网址docker推送镜像到harborhttps://blog.51cto.com/lovejxs/2446037?source=dra\n自签署ssl根证书 certificate signed by unknown authorityhttps://segmentfault.com/q/1010000000938076\ndocker push到私有仓库https://www.cnblogs.com/jifeng/p/9410003.html\nK8S部署SpringBoot应用https://blog.csdn.net/m0_37063785/article/details/101303898\nk8s使用本地镜像https://blog.csdn.net/u010039418/article/details/86578420\n","slug":"OCI/k8s部署springboot的demo项目","date":"2023-01-28T02:17:09.000Z","categories_index":"img,OCI","tags_index":"https,blog,csdnimg","author_index":"dandeliono"},{"id":"a53c723ef12f17b502c1ffd648c4aabe","title":"一行js代码识别Selenium+Webdriver与反识别","content":"一行js代码识别Selenium+Webdriver与反识别一行js代码识别Selenium+Webdriver有不少朋友在开发爬虫的过程中喜欢使用Selenium + Chromedriver，以为这样就能做到不被网站的反爬虫机制发现。\n先不说淘宝这种基于用户行为的反爬虫策略，仅仅是一个普通的小网站，使用一行Javascript代码，就能轻轻松松识别你是否使用了Selenium + Chromedriver模拟浏览器。\n我们来看一个例子，使用下面这一段代码启动Chrome窗口：\n123from selenium.webdriver import Chromedriver = Chrome()\n\n现在，在这个窗口中打开开发者工具，并定位到Console选项卡，如下图所示。\n\n现在，在这个窗口输入如下的js代码并按下回车键：\n12window.navigator.webdriver\n\n可以看到，开发者工具返回了true。如下图所示。\n\n但是，如果你打开一个普通的Chrome窗口，执行相同的命令，可以发现这行代码的返回值为undefined，如下图所示。\n\n所以，如果网站通过js代码获取这个参数，返回值为undefined说明是正常的浏览器，返回true说明用的是Selenium模拟浏览器。一抓一个准。这里给出一个检测Selenium的js代码例子：\n1234567webdriver = window.navigator.webdriver;if(webdriver)&#123;    console.log(&#x27;你这个傻逼你以为使用Selenium模拟浏览器就可以了？&#x27;)&#125; else &#123;    console.log(&#x27;正常浏览器&#x27;)&#125;\n\n网站只要在页面加载的时候运行这个js代码，就可以识别访问者是不是用的Selenium模拟浏览器。如果是，就禁止访问或者触发其他反爬虫的机制。\n那么对于这种情况，在爬虫开发的过程中如何防止这个参数告诉网站你在模拟浏览器呢？\n可能有一些会js的朋友觉得可以通过覆盖这个参数从而隐藏自己，但实际上这个值是不能被覆盖的：\n\n对js更精通的朋友，可能会使用下面这一段代码来实现：\n12Object.defineProperties(navigator, &#123;webdriver:&#123;get:()=&gt;undefined&#125;&#125;);\n\n123 js = r&quot;Object.defineProperty(navigator, &#x27;webdriver&#x27;, &#123;get: () =&gt; undefined,&#125;);&quot;                           self.driver.execute_script(js)   \n\n运行效果如下图所示：\n\n确实修改成功了。这种写法就万无一失了吗？并不是这样的，如果此时你在模拟浏览器中通过点击链接、输入网址进入另一个页面，或者开启新的窗口，你会发现，window.navigator.webdriver又变成了true。如下图所示。\n\n那么是不是可以在每一个页面都打开以后，再次通过webdriver执行上面的js代码，从而实现在每个页面都把window.navigator.webdriver设置为undefined呢？也不行。\n因为当你执行：driver.get(网址)的时候，浏览器会打开网站，加载页面并运行网站自带的js代码。所以在你重设window.navigator.webdriver之前，实际上网站早就已经知道你是模拟浏览器了。\n接下来，又有朋友提出，可以通过编写Chrome插件来解决这个问题，让插件里面的js代码在网站自带的所有js代码之前执行。\n这样做当然可以，不过有更简单的办法，只需要设置Chromedriver的启动参数即可解决问题。\n在启动Chromedriver之前，为Chrome开启实验性功能参数excludeSwitches，它的值为[&#39;enable-automation&#39;]，完整代码如下：\n12345678from selenium.webdriver import Chromefrom selenium.webdriver import ChromeOptions option = ChromeOptions()#开发者模式option.add_experimental_option(&#x27;excludeSwitches&#x27;, [&#x27;enable-automation&#x27;])driver = Chrome(options=option)\n\n此时启动的Chrome窗口，在右上角会弹出一个提示，不用管它，不要点击停用按钮。\n再次在开发者工具的Console选项卡中查询window.navigator.webdriver，可以发现这个值已经自动变成undefined了。并且无论你打开新的网页，开启新的窗口还是点击链接进入其他页面，都不会让它变成true。运行效果如下图所示。\n\n截至2019年02月12日20:46分，本文所讲的方法可以用来登录知乎。如果使用 Selenium 直接登录知乎，会弹出验证码；先使用本文的方法再登录知乎，能够成功伪装成真实的浏览器，不会弹出验证码。\n实际上，Selenium + Webdriver能被识别的特征不止这一个。\n然而时过境迁，随着 Chrome 版本升级，这一方法也宣告失效。\n如何正确移除Selenium中的 window.navigator.webdriver（最新版）我们今天的方法非常简单。就是使用 Google 的Chrome Devtools-Protocol（Chrome 开发工具协议）简称CDP。\n我们打开 CPD 的官方文档[1]，可以看到如下的命令：\n\n\n\n\n\n\n\n\n\n\n在每个Frame 刚刚打开，还没有运行 Frame 的脚本前，运行给定的脚本。\n通过这个命令，我们可以给定一段 JavaScript 代码，让 Chrome 刚刚打开每一个页面，还没有运行网站自带的 JavaScript 代码时，就先执行我们给定的这段代码。\n那么如何在 Selenium 中调用 CDP 的命令呢？实际上非常简单，我们使用driver.execute_cdp_cmd。根据 Selenium 的官方文档[2]，传入需要调用的 CDP 命令和参数即可：\n\n于是我们可以写出如下代码：\n123456789101112from selenium.webdriver import Chromedriver = Chrome(&#x27;./chromedriver&#x27;)driver.execute_cdp_cmd(&quot;Page.addScriptToEvaluateOnNewDocument&quot;, &#123;  &quot;source&quot;: &quot;&quot;&quot;    Object.defineProperty(navigator, &#x27;webdriver&#x27;, &#123;      get: () =&gt; undefined    &#125;)  &quot;&quot;&quot;&#125;)driver.get(&#x27;http://exercise.kingname.info&#x27;)\n\n运行效果如下图所示：\n完美隐藏window.navigator.webdriver。并且，关键语句：\n12345678driver.execute_cdp_cmd(&quot;Page.addScriptToEvaluateOnNewDocument&quot;, &#123;  &quot;source&quot;: &quot;&quot;&quot;    Object.defineProperty(navigator, &#x27;webdriver&#x27;, &#123;      get: () =&gt; undefined    &#125;)  &quot;&quot;&quot;&#125;)\n\n只需要执行一次，之后只要你不关闭这个driver开启的窗口，无论你打开多少个网址，他都会自动提前在网站自带的所有 js 之前执行这个语句，隐藏window.navigator.webdriver。\n如果有人运行上面的代码，出现如下报错：\n\n那么请升级你的 ChromeDriver。老版本的 Chrome + ChromeDriver 只能用以前的方法，不能用今天的方法。新版本的 Chrome + ChromeDriver 可以使用今天的方法，但不能用老方法。正应了那句话：\n\n\n\n\n\n\n\n\n\n上帝给你关上一扇门的时候，悄悄为你开了一扇窗。\n虽然使用以上代码就可以达到目的了，不过为了实现更好的隐藏效果，大家也可以继续加入两个实验选项：\n1234567891011121314from selenium import webdriveroptions = webdriver.ChromeOptions()options.add_experimental_option(&quot;excludeSwitches&quot;, [&quot;enable-automation&quot;])options.add_experimental_option(&#x27;useAutomationExtension&#x27;, False)driver = webdriver.Chrome(options=options, executable_path=&#x27;./chromedriver&#x27;)driver.execute_cdp_cmd(&quot;Page.addScriptToEvaluateOnNewDocument&quot;, &#123;  &quot;source&quot;: &quot;&quot;&quot;    Object.defineProperty(navigator, &#x27;webdriver&#x27;, &#123;      get: () =&gt; undefined    &#125;)  &quot;&quot;&quot;&#125;)driver.get(&#x27;http://exercise.kingname.info&#x27;)\n\n附一些网站检测selenium的示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107`runBotDetection = function () &#123;    var documentDetectionKeys = [        &quot;__webdriver_evaluate&quot;,        &quot;__selenium_evaluate&quot;,        &quot;__webdriver_script_function&quot;,        &quot;__webdriver_script_func&quot;,        &quot;__webdriver_script_fn&quot;,        &quot;__fxdriver_evaluate&quot;,        &quot;__driver_unwrapped&quot;,        &quot;__webdriver_unwrapped&quot;,        &quot;__driver_evaluate&quot;,        &quot;__selenium_unwrapped&quot;,        &quot;__fxdriver_unwrapped&quot;,    ];    var windowDetectionKeys = [        &quot;_phantom&quot;,        &quot;__nightmare&quot;,        &quot;_selenium&quot;,        &quot;callPhantom&quot;,        &quot;callSelenium&quot;,        &quot;_Selenium_IDE_Recorder&quot;,    ];    for (const windowDetectionKey in windowDetectionKeys) &#123;        const windowDetectionKeyValue = windowDetectionKeys[windowDetectionKey];        if (window[windowDetectionKeyValue]) &#123;            return true;        &#125;    &#125;;    for (const documentDetectionKey in documentDetectionKeys) &#123;        const documentDetectionKeyValue = documentDetectionKeys[documentDetectionKey];        if (window[&#x27;document&#x27;][documentDetectionKeyValue]) &#123;            return true;        &#125;    &#125;;    for (const documentKey in window[&#x27;document&#x27;]) &#123;        if (documentKey.match(/\\$[a-z]dc_/) &amp;&amp; window[&#x27;document&#x27;][documentKey][&#x27;cache_&#x27;]) &#123;            return true;        &#125;    &#125;    if (window[&#x27;external&#x27;] &amp;&amp; window[&#x27;external&#x27;].toString() &amp;&amp; (window[&#x27;external&#x27;].toString()[&#x27;indexOf&#x27;](&#x27;Sequentum&#x27;) != -1)) return true;    if (window[&#x27;document&#x27;][&#x27;documentElement&#x27;][&#x27;getAttribute&#x27;](&#x27;selenium&#x27;)) return true;    if (window[&#x27;document&#x27;][&#x27;documentElement&#x27;][&#x27;getAttribute&#x27;](&#x27;webdriver&#x27;)) return true;    if (window[&#x27;document&#x27;][&#x27;documentElement&#x27;][&#x27;getAttribute&#x27;](&#x27;driver&#x27;)) return true;    return false;&#125;;` ![](https://github.com/dandeliono/note/blob/main/source/_posts/resources/img/2023-1-10%2016-10-50/c1217365-ed69-4933-a9cd-e508e6fe6c45.png?raw=true)*   1*   2*   3*   4*   5*   6*   7*   8*   9*   10*   11*   12*   13*   14*   15*   16*   17*   18*   19*   20*   21*   22*   23*   24*   25*   26*   27*   28*   29*   30*   31*   32*   33*   34*   35*   36*   37*   38*   39*   40*   41*   42*   43*   44*   45*   46*   47*   48*   49*   50*   51\n\n123456789try &#123; if (window.document.documentElement.getAttribute(&quot;webdriver&quot;)) return !+[]&#125; catch (IDLMrxxel) &#123;&#125;try &#123; if (&quot;_Selenium_IDE_Recorder&quot; in window) return !+&quot;&quot;&#125; catch (KknKsUayS) &#123;&#125;try &#123; if (&quot;__webdriver_script_fn&quot; in document) return !+&quot;&quot;\n\n改写特征参数的js\n12345678910111213141516171819202122232425262728293031323334353637``// 改写 `languages` Object.defineProperty(navigator, &quot;languages&quot;, &#123;  get: function() &#123;    return [&quot;en&quot;, &quot;es&quot;];  &#125;&#125;);//改写 `plugins`Object.defineProperty(navigator, &quot;plugins&quot;, &#123;  get: () =&gt; new Array(Math.floor(Math.random() * 6) + 1),&#125;);// 改写`webdriver`Object.defineProperty(navigator, &quot;webdriver&quot;, &#123;  get: () =&gt; false,&#125;);`` ![](https://github.com/dandeliono/note/blob/main/source/_posts/resources/img/2023-1-10%2016-10-50/f925f9cf-8163-47d9-a06a-de7237a04a84.png?raw=true)*   1*   2*   3*   4*   5*   6*   7*   8*   9*   10*   11*   12*   13*   14*   15*   16\n\ndriver.execute_script\n执行单行javascript代码。例：已知‘id’，向表格里注入用户名\n\n12driver.execute_script(&quot;document.getElementById(&#x27;id&#x27;).value=&#x27;用户名&#x27;;&quot;\n\n\n执行多行javascript代码。例：用javascript注入js文件\n\n1234567891011javacriptCode = &#x27;&#x27;&#x27;var scriptElt = document.createElement(&#x27;script&#x27;);scriptElt.type = &#x27;text/javascript&#x27;;scriptElt.src = jsFile;document.getElementsByTagName(&#x27;head&#x27;)[0].appendChild(scriptElt);&#x27;&#x27;&#x27;try    driver.execute_script(javacriptCode)except:    pass\n\n为什么使用Try &#x2F; Except 语句？执行javascript代码时很多情况python会报错，但往往代码却可以被正确的执行。加上Try &#x2F; Except来保护程序的执行。\n\n根据坐标，命令selenium点击特定位置。有的时候很难定位一个位置，我们可以用坐标这样的绝对位置来定位\n\n123#例如点击屏幕上 （271，100）这个位置driver.execute_script(&quot;document.elementFromPoint(271, 100).click();&quot;)\n\n参考资料[1]CPD 的官方文档: https://chromedevtools.github.io/devtools-protocol/tot/Page#method-addScriptToEvaluateOnNewDocument[2]官方文档: https://www.selenium.dev/selenium/docs/api/py/webdriver\\_chrome/selenium.webdriver.chrome.webdriver.html#selenium.webdriver.chrome.webdriver.WebDriver.execute\\_cdp_cmd\n参考：https://blog.csdn.net/weixin_33816821/article/details/88590077https://blog.csdn.net/jiduochou963/article/details/88200217https://blog.csdn.net/sinat_38682860/article/details/86221844https://mp.weixin.qq.com/s/U2aAC6K6RuQDRqfb8m35_w\n","slug":"WORK/一行js代码识别Selenium+Webdriver与反识别","date":"2023-01-10T16:14:41.000Z","categories_index":"https,WORK","tags_index":"true,webdriver,Selenium","author_index":"dandeliono"},{"id":"ae3e49e2fd32f60fd6f14ef67049c415","title":"MAC地址正则匹配","content":"MAC地址正则匹配关于MAC地址MAC地址，也就是网卡MAC码。网卡MAC码是由IEEE的注册管理机构固定分配的,因此每一个主机会有一个MAC地址，具有全球唯一性。\n需求分析因为业务需要验证MAC地址是否合法，因而想到用正则进行匹配。\n因为考虑到MAC地址一般有两种格式，使用-连接或是:连接，于是我稍作改动，改动结果如下\n\n/((([a-f0-9]&#123;2&#125;:)&#123;5&#125;)|(([a-f0-9]&#123;2&#125;-)&#123;5&#125;))[a-f0-9]&#123;2&#125;/gi\n\n以上正则表达式就是最终版的MAC地址验证表达式，如果不清楚具体原理可以接着往下看。\n拆解分析1. [a-f0-9]匹配a到f或0到9中的任意一位字符。\n\n\n\n\n\n\n\n\n\n匹配的结果例如2或d。\n2. [a-f0-9]&#123;2&#125;匹配连续两位的括号中任意字符。\n\n\n\n\n\n\n\n\n\n匹配的结果例如d2或ac。\n3. [a-f0-9]&#123;2&#125;:连续两位的括号中任意字符再拼接一个:(冒号)。\n\n\n\n\n\n\n\n\n\n匹配的结果例如b2:或23:\n4. ([a-f0-9]&#123;2&#125;:)|([a-f0-9]&#123;2&#125;-)在第三步的基础上可以将冒号替换为横杠。\n\n\n\n\n\n\n\n\n\n匹配的结果例如f3:或79-\n5. (([a-f0-9]&#123;2&#125;:)|([a-f0-9]&#123;2&#125;-))&#123;5&#125;将第四步的结果重复5次。\n\n\n\n\n\n\n\n\n\n匹配的结果例如00-01-6C-06-A6-或00:01:6C:06:A6:\n6. (([a-f0-9]&#123;2&#125;:)|([a-f0-9]&#123;2&#125;-))&#123;5&#125;[a-f0-9]&#123;2&#125;在第五步的结果上再拼接两个[A-F0-9]范围内的两个字符\n\n\n\n\n\n\n\n\n\n匹配的结果例如00-01-6C-06-A6-29或00:01:6C:06:A6:29\n7. 设置大小写不敏感与全局匹配在正则最后加上/ig。\n完整的正则表达式也就是：\n/(([a-f0-9]&#123;2&#125;:)|([a-f0-9]&#123;2&#125;-))&#123;5&#125;[a-f0-9]&#123;2&#125;/gi8.解决AA:BB-CC:DD:EE-FF这类符号混用也能通过验证的问题第7步的正则无法剔除AA:BB-CC:DD:EE-FF这类·-·与‘:’混用的脏数据，因此稍作改动，将重复匹配5次这一操作分别作用于([a-f0-9]&#123;2&#125;:)与([a-f0-9]&#123;2&#125;-)。最终正则如下：\n12/((([a-f0-9]&#123;2&#125;:)&#123;5&#125;)|(([a-f0-9]&#123;2&#125;-)&#123;5&#125;))[a-f0-9]&#123;2&#125;/gi\n\nEND","slug":"ALG/MAC地址正则匹配","date":"2023-01-05T11:59:40.000Z","categories_index":"MAC,ALG","tags_index":"匹配的结果例如,地址,地址正则匹配","author_index":"dandeliono"},{"id":"536679e8a77999953c3193ac12ff85df","title":"徒手撸框架--高并发环境下的请求合并","content":"徒手撸框架–高并发环境下的请求合并在高并发系统中，我们经常遇到这样的需求：系统产生大量的请求，但是这些请求实时性要求不高。我们就可以将这些请求合并，达到一定数量我们统一提交。最大化的利用系统性IO,提升系统的吞吐性能。\n所以请求合并框架需要考虑以下两个需求：\n\n当请求收集到一定数量时提交数据\n一段时间后如果请求没有达到指定的数量也进行提交\n\n我们就聊聊一如何实现这样一个需求。\n\nScheduledThreadPoolExecutor\n阻塞队列\n线程安全的参数\nLockSupport的使用\n\n我们就聊一聊实现这个东西的具体思路是什么。希望大家能够学习到分析问题，设计模块的一些套路。\n1. 底层使用什么数据结构来持有需要合并的请求？\n既然我们的系统是在高并发的环境下使用，那我们肯定不能使用，普通的ArrayList来持有。我们可以使用阻塞队列来持有需要合并的请求。\n我们的数据结构需要提供一个 add() 的方法给外部，用于提交数据。当外部add数据以后，需要检查队列里面的数据的个数是否达到我们限额？达到数量提交数据，不达到继续等待。\n数据结构还需要提供一个timeOut()的方法，外部有一个计时器定时调用这个timeOut方法，如果方法被调用，则直接向远程提交数据。\n条件满足的时候线程执行提交动作，条件不满足的时候线程应当暂停，等待队列达到提交数据的条件。所以我们可以考虑使用 LockSupport.park()和LockSupport.unpark 来暂停和激活操作线程。\n\n经过上面的分析，我们就有了这样一个数据结构：\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293private static class FlushThread&lt;Item&gt; implements Runnable&#123;        private final String name;                private final int bufferSize;                private int flushInterval;                private volatile long lastFlushTime;        private volatile Thread writer;                private final BlockingQueue&lt;Item&gt; queue;                private final Processor&lt;Item&gt; processor;                public FlushThread(String name, int bufferSize, int flushInterval,int queueSize,Processor&lt;Item&gt; processor) &#123;            this.name = name;            this.bufferSize = bufferSize;            this.flushInterval = flushInterval;            this.lastFlushTime = System.currentTimeMillis();            this.processor = processor;            this.queue = new ArrayBlockingQueue&lt;&gt;(queueSize);        &#125;                public boolean add(Item item)&#123;            boolean result = queue.offer(item);            flushOnDemand();            return result;        &#125;                public void timeOut()&#123;                        if(System.currentTimeMillis() - lastFlushTime &gt;= flushInterval)&#123;                start();            &#125;        &#125;                private void start()&#123;            LockSupport.unpark(writer);        &#125;                private void flushOnDemand()&#123;            if(queue.size() &gt;= bufferSize)&#123;                start();            &#125;        &#125;                public void flush()&#123;            lastFlushTime = System.currentTimeMillis();            List&lt;Item&gt; temp = new ArrayList&lt;&gt;(bufferSize);            int size = queue.drainTo(temp,bufferSize);            if(size &gt; 0)&#123;                try &#123;                    processor.process(temp);                &#125;catch (Throwable e)&#123;                    log.error(&quot;process error&quot;,e);                &#125;            &#125;        &#125;                private boolean canFlush()&#123;            return queue.size() &gt; bufferSize || System.currentTimeMillis() - lastFlushTime &gt; flushInterval;        &#125;        @Override        public void run() &#123;            writer = Thread.currentThread();            writer.setName(name);            while (!writer.isInterrupted())&#123;                while (!canFlush())&#123;                                        LockSupport.park(this);                &#125;                flush();            &#125;        &#125;    &#125;\n\n2. 如何实现定时提交呢？通常我们遇到定时相关的需求，首先想到的应该是使用 ScheduledThreadPoolExecutor定时来调用FlushThread 的 timeOut 方法,如果你想到的是 Thread.sleep()…那需要再努力学习，多看源码了。\n3. 怎样进一步的提升系统的吞吐量？我们使用的FlushThread 实现了 Runnable 所以我们可以考虑使用线程池来持有多个FlushThread。\n所以我们就有这样的代码：\n12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class Flusher&lt;Item&gt; &#123;    private final FlushThread&lt;Item&gt;[] flushThreads;    private AtomicInteger index;        private static final Random r = new Random();    private static final int delta = 50;    private static ScheduledExecutorService TIMER = new ScheduledThreadPoolExecutor(1);    private static ExecutorService POOL = Executors.newCachedThreadPool();    public Flusher(String name,int bufferSiz,int flushInterval,int queueSize,int threads,Processor&lt;Item&gt; processor) &#123;        this.flushThreads = new FlushThread[threads];        if(threads &gt; 1)&#123;            index = new AtomicInteger();        &#125;        for (int i = 0; i &lt; threads; i++) &#123;            final FlushThread&lt;Item&gt; flushThread = new FlushThread&lt;Item&gt;(name+ &quot;-&quot; + i,bufferSiz,flushInterval,queueSize,processor);            flushThreads[i] = flushThread;            POOL.submit(flushThread);                        TIMER.scheduleAtFixedRate(flushThread::timeOut, r.nextInt(delta), flushInterval, TimeUnit.MILLISECONDS);        &#125;    &#125;        public boolean add(Item item)&#123;        int len = flushThreads.length;        if(len == 1)&#123;            return flushThreads[0].add(item);        &#125;        int mod = index.incrementAndGet() % len;        return flushThreads[mod].add(item);    &#125;        private static class FlushThread&lt;Item&gt; implements Runnable&#123;        ...省略    &#125;&#125;\n\n4. 面向接口编程，提升系统扩展性：123public interface Processor&lt;T&gt; &#123;    void process(List&lt;T&gt; list);&#125;\n\n我们写个测试方法测试一下：\n12345678910111213public class PrintOutProcessor implements Processor&lt;String&gt;&#123;    @Override    public void process(List&lt;String&gt; list) &#123;        System.out.println(&quot;start flush&quot;);        list.forEach(System.out::println);        System.out.println(&quot;end flush&quot;);    &#125;&#125;\n\n123456789101112131415public class Test &#123;    public static void main(String[] args) throws InterruptedException &#123;        Flusher&lt;String&gt; stringFlusher = new Flusher&lt;&gt;(&quot;test&quot;,5,1000,30,1,new PrintOutProcessor());        int index = 1;        while (true)&#123;            stringFlusher.add(String.valueOf(index++));            Thread.sleep(1000);        &#125;    &#125;&#125;\n\n执行的结果：\n12345678910111213start flush123end flushstart flush4567end flush\n\n我们发现并没有达到10个数字就触发了flush。因为出发了超时提交，虽然还没有达到规定的5个数据，但还是执行了 flush。\n如果我们去除 Thread.sleep(1000); 再看看结果：\n1234567891011121314start flush12345end flushstart flush678910end flush\n\n每5个数一次提交。完美。。。。\n","slug":"JAVA/徒手撸框架--高并发环境下的请求合并","date":"2022-12-28T19:57:07.000Z","categories_index":"LockSupport,JAVA","tags_index":"底层使用什么数据结构来持有需要合并的请求,timeOut,如何实现定时提交呢","author_index":"dandeliono"},{"id":"bf7b35be44144689d93f501402e9fb0d","title":"Too many open files的四种解决办法","content":"Too many open files的四种解决办法【摘要】 Too many open files有四种可能:一 单个进程打开文件句柄数过多,二 操作系统打开的文件句柄数过多,三 systemd对该进程进行了限制,四 inotify达到上限.\n领导见了孔乙己，也每每这样问他，引人发笑。孔乙己自己知道不能和他们谈天，便只好向我们新员工说话。有一回对我说道，“你定位过问题么？”我略略点一点头。他说，“定位过，……我便考你一考。Too many open files，怎样解决？”我想，考评垫底的人，也配考我么？便回过脸去，不再理会。孔乙己等了许久，很恳切的说道，“不能解决罢？……我教给你，记着！这些方法应该记着。将来做接口人的时候，定位问题要用。”我暗想我和接口人的等级还很远呢，而且我们领导也从不将问题定位记功；又好笑，又不耐烦，懒懒的答他道，“谁要你教，不是ulimit太小么？”孔乙己显出极高兴的样子，将两个指头的长指甲敲着白板，点头说，“对呀对呀！……Too many open files有四种可能，你知道么？”我愈不耐烦了，努着嘴走远。孔乙己却像是没有看到，自顾自的在白板上画了起来。\n一  单个进程打开文件句柄数过多ulimit中的nofile表示单进程可以打开的最大文件句柄数，可以通过ulimit -a查看，子进程默认继承父进程的限制（注意，是继承，不是共享，子进程和父进程打开的文件句柄数是单独算的）。\n网上还有一种解读是nofile表示单用户可以打开的文件句柄数，因为他们在limit.conf中看到类似于“openstack soft nofile 65536”，便认为是openstack用户最多可以打开的文件句柄数。该解读是错误的，“openstack soft nofile 65536”表示的含义是当你执行”su - openstack”切换到openstack用户后，你创建的所有进程最大可以打开的文件句柄数是65536。\n要查看一个进程可以打开的文件句柄数，可以通过“cat &#x2F;proc&#x2F;&#x2F;limits”查看。\n要修改ulimit中的nofile，可以通过修改&#x2F;etc&#x2F;security&#x2F;limits.conf文件，在其中加入类似“openstack soft nofile 65536”的语句来进行修改。修改完成后，可以通过“su - openstack”切换用户，或者重新登录，来使该配置生效。\n要动态修改一个进程的限制，可以使用prlimit命令，具体用法为：“prlimit –pid ${pid} –nofile&#x3D;102400:102400”。\n二 操作系统打开的文件句柄数过多整个操作系统可以打开的文件句柄数是有限的，受内核参数“fs.file-max”影响。\n可以通过执行“echo 100000000 &gt; &#x2F;proc&#x2F;sys&#x2F;fs&#x2F;file-max”命令来动态修改该值，也可以通过修改”&#x2F;etc&#x2F;sysctl.conf”文件来永久修改该值。\n三 systemd对该进程进行了限制该场景仅针对被systemd管理的进程（也就是可以通过systemctl来控制的进程）生效，可以通过修改该进程的service文件（通常在&#x2F;etc&#x2F;systemd&#x2F;system&#x2F;目录下），在“[Service]”下面添加“LimitNOFILE&#x3D;20480000”来实现，修改完成之后需要执行”systemctl daemon-reload”来使该配置生效。\n四 inotify达到上限inotify是linux提供的一种监控机制，可以监控文件系统的变化。该机制受到2个内核参数的影响：“fs.inotify.max_user_instances”和“fs.inotify.max_user_watches”，其中“fs.inotify.max_user_instances”表示每个用户最多可以创建的inotify instances数量上限，“fs.inotify.max_user_watches”表示么个用户同时可以添加的watch数目，当出现too many open files问题而上面三种方法都无法解决时，可以尝试通过修改这2个内核参数来生效。修改方法是修改”&#x2F;etc&#x2F;sysctl.conf”文件，并执行”sysctl -p”。\n","slug":"LINUX/Too many open files的四种解决办法","date":"2022-12-07T00:21:50.000Z","categories_index":"inotify,LINUX","tags_index":"nofile,openstack,max","author_index":"dandeliono"},{"id":"a9faca4b916382c99b92d1b36024956a","title":"strace 常用操作","content":"strace 常用操作2019-03-30\n前言¶strace 可以用来查看&#x2F;记录程序运行过程中调用的 系统调用 以及接收到的进程信号（signal）， 对于我们日常 debug 疑难杂症非常的有帮助，是一个非常好的 debug 工具。本文简单记录一下 strace 的常用功能和操作。\n输出的含义¶我们通过一个简单的命令来查看 strace 程序的典型输出：\n$ strace ls &#x2F;tmp&#x2F;traceexecve(“&#x2F;bin&#x2F;ls”, [“ls”, “&#x2F;tmp&#x2F;trace”], [&#x2F;* 41 vars *&#x2F;]) = 0brk(0)                                  = 0x11af000mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f0738c16000access(“&#x2F;etc&#x2F;ld.so.preload”, R_OK)      = -1 ENOENT (No such file or directory)open(“&#x2F;etc&#x2F;ld.so.cache”, O_RDONLY)      = 3fstat(3, {st_mode=S_IFREG|0644, st_size=35759, …}) = 0mmap(NULL, 35759, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7f0738c0a000close(3)                                = 0open(“&#x2F;lib64&#x2F;libselinux.so.1”, O_RDONLY) = 3read(3, “\\177ELF\\2\\1\\1\\0\\0\\0\\0\\0\\0\\0\\0\\0\\3\\0&gt;\\0\\1\\0\\0\\0PX\\0\\0\\0\\0\\0\\0”…, 832) = 832fstat(3, {st_mode=S_IFREG|0755, st_size=122040, …}) = 0….fstat(1, {st_mode=S_IFCHR|0620, st_rdev=makedev(136, 0), …}) = 0mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7fb97a060000write(1, “test_dir  test.txt\\n”, 19test_dir  test.txt)    = 19close(1)                                = 0munmap(0x7fb97a060000, 4096)            = 0close(2)                                = 0exit_group(0)                           = ?+++ exited with 0 +++\n比如下面的这个输出:\n12execve(&quot;/bin/ls&quot;, \\[&quot;ls&quot;, &quot;/tmp/trace&quot;\\], \\[/\\* 41 vars \\*/\\]) = 0\n\n的含义如下：\n\nexecve: 系统调用的名称。\n(“&#x2F;bin&#x2F;ls”, [“ls”, “&#x2F;tmp&#x2F;trace”], [&#x2F;* 41 vars *&#x2F;]): 这个括号里是系统调用的参数。\n= 0 : 这个 0 是系统调用的返回值，不一定是数字看具体的系统调用返回啥结果就是啥值。\n\n上面有些系统调用的返回值比较特殊，比如:\n12access(&quot;/etc/ld.so.preload&quot;, R\\_OK)      = -1 ENOENT (No such file or directory)\n\n后面这个 ENOENT (No such file or directory) 中的 ENOENT 是错误码， (No such file or directory) 是错误码的解释。\n上面是系统调用相关的输出格式和含义，下面再看一个进程信号(signal)的输出格式：\n$ strace -p 15718Process 15718 attachedselect(1, [0], NULL, NULL, NULL)        = ? ERESTARTNOHAND (To be restarted if no handler)— SIGTERM {si_signo=SIGTERM, si_code=SI_USER, si_pid=15757, si_uid=500} —+++ killed by SIGTERM +++\n其中:\n12\\--- SIGTERM &#123;si\\_signo=SIGTERM, si\\_code=SI\\_USER, si\\_pid=15757, si\\_uid=500&#125; ---\n\n这一句就是进程接收到的具体 signal 的信息。\n常用命令行参数¶常用的参数组合:\n12345strace -f -s 1024 -tt -T -yy -p &lt;pid&gt;strace -f -s 1024 -tt -T -yy -p &lt;pid&gt; -o &lt;filename&gt;strace -c -p &lt;pid&gt;strace -f -s 1024 -tt -T -yy -C -p &lt;pid&gt;\n\n\n-p : 附加到进程中，记录某个进程的系统调用和进程信号信息。多个 -p  可以实现同时追踪多个进程。\n\n-f: 同时追踪子进程的系统调用情况（如果是多线程程序的话，会同时追踪所有线程）。如果不加 -f 参数的话默认只追踪指定的单个进程。\n\n-o : 把输出结果保存到文件中（默认是输出到标准错误）。\n\n-ff: 与 -o  一起使用，会把每个 pid (进程 pid 或线程 id) 的 strace 数据保存到 . 的文件中。\n\n-e : 过滤事件，只输出符合规则的事件，可以用来指定只看某些系统调用的情况，比如： -e open,read 。 或者不看某些系统调用： -e ‘!open,read’ ，更强大的过滤功能可以查看 strace(1) 。\n\n-s : 指定字符串数据的大小，默认 32。可以通过 -s 来显示更详细的信息或精简信息（比如很多系统调用的参数的值会被截断，可以通过 -s 配置更大的 size 来查看更详细的参数值）。\n\n-tt: 显示系统调用是在哪个时刻调用的，包含微秒（ -t 参数一样的效果就是时间不包含微秒）:\n1234567891011$ strace -e open,read -s 2 -tt ls14:32:18.877470 read(3, &quot;\\\\177E&quot;..., 832) = 83214:32:18.878249 read(3, &quot;\\\\177E&quot;..., 832) = 83214:32:18.878867 read(3, &quot;\\\\177E&quot;..., 832) = 83214:32:18.879386 read(3, &quot;\\\\177E&quot;..., 832) = 83214:32:18.879871 read(3, &quot;\\\\177E&quot;..., 832) = 83214:32:18.880923 read(3, &quot;no&quot;..., 1024)  = 42014:32:18.881255 read(3, &quot;&quot;, 1024)       = 0上面的第一列时间信息即为 -tt 的效果。\n\n-T: 显示执行系统调用所花费的时间，单位是秒\n12345$ strace -e read -s 2 -T pwdread(3, &quot;\\\\177E&quot;..., 832)                = 832 &lt;0.000012&gt;上面行末的 &lt;0.000012&gt; 即为 -T 的效果。\n\n-y: 输出文件描述符所对应的文件路径\n1234567891011121314$ strace -e read -s 10  cat test.txtread(3, &quot;\\\\177ELF\\\\2\\\\1\\\\1\\\\3\\\\0\\\\0&quot;..., 832)  = 832read(3, &quot;hello\\\\n&quot;, 131072)              = 6helloread(3, &quot;&quot;, 131072)                     = 0$ strace -e read -s 10 -y  cat test.txtread(3&lt;/lib/x86\\_64-linux-gnu/libc-2.27.so&gt;, &quot;\\\\177ELF\\\\2\\\\1\\\\1\\\\3\\\\0\\\\0&quot;..., 832) = 832read(3&lt;/path/to/test.txt&gt;, &quot;hello\\\\n&quot;, 131072) = 6helloread(3&lt;/path/to/test.txt&gt;, &quot;&quot;, 131072) = 0可以看到加了 -y 后文件描述符后面会跟它所对应的文件的路径\n\n-yy: 输出文件描述更详细的信息，比如 socket 文件描述符输出所对应的协议信息。\n123456789101112131415$ strace -e connect -s 3  nc [baidu.com](http://baidu.com/) 80...connect(3, &#123;sa\\_family=AF\\_INET, sin\\_port=htons(53), sin\\_addr=inet\\_addr(&quot;127.0.0.53&quot;)&#125;, 16) = 0...$ strace -e connect -s 3 -y  nc [baidu.com](http://baidu.com/) 80...connect(3&lt;socket:\\[22180\\]&gt;, &#123;sa\\_family=AF\\_INET, sin\\_port=htons(53), sin\\_addr=inet\\_addr(&quot;127.0.0.53&quot;)&#125;, 16) = 0$ strace -e connect -s 3 -yy  nc [baidu.com](http://baidu.com/) 80...connect(3&lt;UDP:\\[22202\\]&gt;, &#123;sa\\_family=AF\\_INET, sin\\_port=htons(53), sin\\_addr=inet\\_addr(&quot;127.0.0.53&quot;)&#125;, 16) = 0可以看到 -yy 会显示上面的 UDP 这个协议信息。\n\n-c: 统计系统调用的次数、种类以及错误信息，会隐藏详细的追踪信息（前面的 -e 过滤参数也可以用于这个）:\n1234567891011121314151617181920$ strace -c cat test.txthello% time     seconds  usecs/call     calls    errors syscall------ ----------- ----------- --------- --------- ---------------- 18.45    0.000019           3         7           mmap 18.45    0.000019           5         4           mprotect 17.48    0.000018           9         2           munmap 13.59    0.000014           4         4           openat  9.71    0.000010          10         1           write  6.80    0.000007           2         3           read  5.83    0.000006           2         3         3 access  3.88    0.000004           1         5           fstat  2.91    0.000003           1         6           close  1.94    0.000002           1         3           brk  0.97    0.000001           1         1           execve  0.00    0.000000           0         1           arch\\_prctl  0.00    0.000000           0         1           fadvise64------ ----------- ----------- --------- --------- ----------------100.00    0.000103                    41         3 total\n\n-C: 跟 -c 类似，只是增加了会输出详细的追踪信息\n123456789101112$ strace -e read -C -s 3 cat test.txtread(3, &quot;\\\\177EL&quot;..., 832)               = 832read(3, &quot;hel&quot;..., 131072)               = 6helloread(3, &quot;&quot;, 131072)                     = 0+++ exited with 0 +++% time     seconds  usecs/call     calls    errors syscall------ ----------- ----------- --------- --------- ----------------  0.00    0.000000           0         3           read------ ----------- ----------- --------- --------- ----------------100.00    0.000000                     3           total\n\n-v: 显示系统调用时传入的参数变量的具体内容，而不是默认显示为 &#x2F;* 8 vars *&#x2F;\n\n\n常用过滤表达式¶-e  这个参数可以用来指定需要 trace 哪些事件，格式如下:\n1234567\\[qualifier=\\]\\[!\\]\\[?\\]value1\\[,\\[?\\]value2\\]...其中 qualifier 的值是 trace, abbrev, verbose, raw,signal, read, write, fault, inject, or kvm 。默认是 tracevalue 就是各种过滤条件了。\n\n\n-e trace&#x3D; :  是系统调用名称（默认是 trace&#x3D;all ），比如 trace&#x3D;open,close,read,write\n-e trace&#x3D;&#x2F;: 可以通过这种方式来用正则表达式指定系统调用名称，支持的正则语法可以参考 regex(7) 。\n-e trace&#x3D;%file: 文件相关系统调用。\n-e trace&#x3D;%desc: 文件描述符相关。\n-e trace&#x3D;%process: 进程管理相关系统调用。\n-e trace&#x3D;%network: 网络相关。\n-e trace&#x3D;%signal: 信号相关。\n-e trace&#x3D;%ipc: IPC 相关。\n-e trace&#x3D;%memory: 内存 mapping 相关。\n-e signal&#x3D;:  是进程信号的名称（默认是 signal&#x3D;all ），比如 -e signal&#x3D;SIGTERM\n-e read&#x3D;: 追踪指定 fd 上的数据并打印 hex 和 ascii 格式的数据，  是 fd 比如:read&#x3D;3,5\n\n总结¶更多关于 strace 的信息可以从参考资料的 strace(1) 中获取，如果对输出中系统调用不了解的话可以参考参考资料中的 syscalls(2) 中的信息，同时也别忘了搜索引擎是你的好朋友，有啥不明白的记得 Google 一下。\n\n","slug":"LINUX/strace 常用操作","date":"2022-12-07T00:21:02.000Z","categories_index":"trace,LINUX","tags_index":"strace,man,signal","author_index":"dandeliono"},{"id":"fb884516282f9be59505de88d795cde5","title":"文件描述符(fd)泄漏排查一篇就够了_写代码的小提的博客-CSDN博客","content":"文件描述符(fd)泄漏排查一篇就够了_写代码的小提的博客-CSDN博客生产多次遇到文件描述符（fd）泄露相关的问题， 文件描述符泄漏一般引起的现象是文件句柄数（封面图）&#x2F;tcp alloc（上图）增长。文章分为两部分介绍文件描述符相关内容，第一部分介绍文件描述基础知识，第二部分通过实际案例进行剖析。\n一. 文件描述符相关基础知识\n\n什么是文件描述符？内核利用文件描述符来访问文件, 打开现存文件或新建文件（建立）时，内核会返回一个文件描述符，读写文件也需要使用文件描述符来指定待读写的文件。所有执行I&#x2F;O操作(包括网络socket操作)的系统调用都通过文件描述符​\n最大文件描述符介绍\n系统最大文件描述符限制sysctl -a | grep fs.file-max （查看系统最大描述符）echo “fs.file-max&#x3D;1610270” &gt;&gt; &#x2F;etc&#x2F;sysctl.conf（修改最大描述符）sysctl -p（立即生效）\n用户级最大文件描述限制ulimit -n （查看用户最大描述符）echo “* hard nofile 65535” &gt;&gt; &#x2F;etc&#x2F;security&#x2F;limits.confecho “* soft nofile 65535” &gt;&gt; &#x2F;etc&#x2F;security&#x2F;limits.conf_代表所有用户，支持具体用户（优先级高，不受_影响）。文件修改即生效，退出或打开新终端执行ulimit -n即看到修改效果\n具体某个进程（PID）最大描述符通过cat &#x2F;proc&#x2F;PID&#x2F;limits | grep “Max open files”Limit Soft Limit Hard Limit UnitsMax open files 65536 65536 files进程最大描述符受限与系统／用户级，以及进程本身相关代码程序限制，比如下面Golang代码将进程打开的最大描述符限制为10var rLimit syscall.RlimitrLimit.Cur &#x3D; 10if err :&#x3D; syscall.Setrlimit(syscall.RLIMIT_NOFILE, &amp;rLimit); err !&#x3D; nil {panic(err)}\n\n\n\n二. 文件描述符泄漏的实际案例\n某个周末，ops同学在运维群反馈某核心业务应用文件描述符以及tcp alloc非常高，导致服务不可用（这块监控不到位）。业务架构同学为了排查相关问题具体原因保留了一台问题服务，当然咯，闲着无事，参与线上故障排查。从监控图看到文件描述符不断&#x2F;tcp alloc 不断增长\n\n首先考虑是否由于Socket连接建立以后未close导致，这类也是最容易排查，netstat显示的tcp连接数正常\n\n12345678netstat -tan|awk &#x27;$1~/tcp/&#123;print $NF&#125;&#x27;|sort|uniq -c|sort -nr156  TIME_WAIT141  FIN_WAIT280  ESTABLISHED10  LISTEN3 CLOSE_WAIT2 LAST_ACK\n\n\nss -s 查看大量处于closed 状态\n\n通过lsof 查看tomcat 进程（进程4730）打开的文件描述符相关详细信息，lsof -p 4730。大量Prtocol：TCP异常描述符如下图所示：\n\n通过lsof 相关信息我们找不出具体由于某原因导致的，我们通过strace查看系统调用， 查看fd泄漏的具体原因 （抓取5分钟）strace -f -p 4730 -T -tt -o &#x2F;home&#x2F;futi&#x2F;strace_4730.log-tt 在每行输出的前面，显示毫秒级别的时间-T 显示每次系统调用所花费的时间-v 对于某些相关调用，把完整的环境变量，文件stat结构等打出来。-f 跟踪目标进程，以及目标进程创建的所有子进程-e 控制要跟踪的事件和跟踪行为,比如指定要跟踪的系统调用名称-o 把strace的输出单独写到指定的文件-s 当系统调用的某个参数是字符串时，最多输出指定长度的内容，默认是32个字节-p 指定要跟踪的进程pid, 要同时跟踪多个pid, 重复多次-p选项即可。tomcate多线程应用，我们需要追踪子进程运行情况，所以-f，其它参数大家看解析应该可以理解\n\n&#x2F;home&#x2F;futi&#x2F;strace_4730.log，找到strace抓取这段时间内最近泄漏的fd进行分析，通过lsof -d 49959 ，可以看到出现Prtocol：TCP异常情况。下面截一小部分内容前面有大量对fd为49959打开，关闭等操作。但从4783线程操作这个fd以后strace抓取的内容未有再使用49959这个fd，且fd 不断增大，有使用大于49959的fd，所以我们可以断定是这个fd 在这块出了问题。\n\n从上面似乎我们找不到根本原因，《Linux环境编程：从应用到内核》有这么一段：在多线程下，可能会在fcntl调用前，就已经fork出子进程。从这点出发我们查看tomcat线程ID为4783在执行fcntl前做了哪些操作，可以看出4783线程写入了一条ERROR日志7. lsof -d 369 可以找到fd为369对应打开的文件：&#x2F;data&#x2F;applogs&#x2F;cat&#x2F;cat_20190722.log查看具体log 如下，由于连接Cat失败导致fd泄漏（由于cat上线很久了，忽略了查看cat 日志）\n[07-21 23:13:21.204] [ERROR] [ChannelManager] Error when try connecting to /host:2280\nstrace + lsof 能解决大部分fd泄漏的问题, 感兴趣可以公众号，定期更新后端技术\n\n\n","slug":"LINUX/文件描述符(fd)泄漏排查一篇就够了_写代码的小提的博客-CSDN博客","date":"2022-12-01T11:42:30.000Z","categories_index":"tcp,LINUX","tags_index":"strace,lsof,alloc","author_index":"dandeliono"},{"id":"bffbc184e1f43c5ae0809f3a56168a65","title":"Spring-SpringBoot-MyBatis","content":"Spring-SpringBoot-MyBatis本文将全局介绍 MyBatis 的整体架构。\n参考：《MyBatis技术内幕》\nMyBatis整体架构\n\n基础支持层反射模块该模块对Java原生的反射进行了良好的封装，提供了更加简洁易用的API，方便上层使调用，并且对反射操作进行了一系列优化，例如缓存了类的元数据，提高了反射操作的性能。\n类型转换模块MyBatis为简化配置文件提供了别名机制，该机制是类型转换模块的主要功能之一。类型转换模块的另一个功能是实现JDBC类型与Java类型之间的转换，该功能在为SQL语句绑定实参以及映射查询结果集时都会涉及。在为SQL语句绑定实参时，会将数据由Java类型转换成JDBC类型；而在映射结果集时，会将数据由JDBC类型转换成Java类型。\n\n\nTypeHandlersetParameter(): 将数据由JdbcType类型转换成Java类型\ngetResult(): 将数据由Java类型转成JdbcType\nTypeHandler的实现子类：\n\nIntegerTypeHandler实现：\n\nTypeHandlerRegistry\n\n除了MyBatis本身提供的TypeHandler实现，我们也可以添加自定义的TypeHandler接口实现，添加方式是在mybatis-config.xml配置文件中的＜typeHandlers＞节点下，添加相应的＜typeHandler＞节点配置，并指定自定义的TypeHandler接口实现类。在MyBatis初始化时会解析该节点，并将该TypeHandler类型的对象注册到TypeHandlerRegistry中，供MyBatis后续使用。\nTypeAliasRegistryMyBatis可以为一个类添加一个别名，之后就可以通过别名引用该类。 MyBatis通过TypeAliasRegistry类完成别名注册和管理的功能，TypeAliasRegistry的结构比较简单，它通过TYPE_ALIASES字段（Map＜String, Class＜？＞＞类型）管理别名与Java类型之间的对应关系，通过TypeAliasRegistry.registerAlias（）方法完成注册别名。\n日志模块MyBatis作为一个设计优良的框架，除了提供详细的日志输出信息，还要能够集成多种日志框架，其日志模块的一个主要功能就是集成第三方日志框架。\n日志适配器\n\nLog接口定义了日志模块功能，LogFactory工厂类负责创建对应的日志组件适配器。在LogFactory类加载时会执行其静态代码块，其逻辑是按序加载并实例化对应日志组件的适配器，然后使用LogFactory.logConstructor这个静态字段，记录当前使用的第三方日志组件的适配器。\n\n\n\n\n\n\nJdbc调试\n\n使用了代理模式，\n\n\n资源加载模块资源加载模块主要是对类加载器进行封装，确定类加载器的使用顺序，并提供了加载类文件以及其他资源文件的功能。单例模式。\n解析器模块解析器模块的主要提供了两个功能：一个功能是对XPath进行封装，为MyBatis初始化时解析mybatis-config.xml配置文件以及映射配置文件提供支持；另一个功能是为处理动态SQL语句中的占位符提供支持。\nXML处理方式\nDOM\n\nDOM是基于树形结构的XML解析方式，它会将整个XML文档读入内存并构建一个DOM树，基于这棵树形结构对各个节点（Node）进行操作。XML 文档中的每个成分都是一个节点：整个文档是一个文档节点，每个XML标签对应一个元素节点，包含在XML标签中的文本是文本节点，每一个 XML 属性是一个属性节点，注释属于注释节点。\n\n\n经过DOM解析后得到的树形结构如下：\n\n\n\nSAX\n\nSAX是基于事件模型的XML解析方式，它并不需要将整个XML文档加载到内存中，而只需将XML文档的一部分加载到内存中，即可开始解析，在处理过程中并不会在内存中记录XML中的数据，所以占用的资源比较小。当程序处理过程中满足条件时，也可以立即停止解析过程，这样就不必解析剩余的XML内容。\n\n\n\nStAX\n\n\n\nXPathXPath之于XML就好比SQL语言之于数据库。\nXPath中常用的表达式：\n\n查找所有书籍的XPath表达式是：”&#x2F;&#x2F;book”。查找作者为Neal Stephenson的所有图书需要指定＜author＞节点的值，得到表达式：”&#x2F;&#x2F;book[author&#x3D;’Neal Stephenson’]”。为了找出这些图书的标题，需要选取＜title＞节点，得到表达式：”&#x2F;&#x2F;book[author&#x3D;’Neal Stephenson’]&#x2F;title”。最后，真正需要的信息是＜title＞节点中的文本节点，得到的完整XPath表达式是：”&#x2F;&#x2F;book[author&#x3D;”Neal Stephenson”]&#x2F;title&#x2F;text（）”。\nXPathParser\n\n数据源模块现在开源的数据源都提供了比较丰富的功能，例如，连接池功能、检测连接状态等，选择性能优秀的数据源组件对于提升ORM框架乃至整个应用的性能都是非常重要的。MyBatis自身提供了相应的数据源实现，当然MyBatis也提供了与第三方数据源集成的接口，这些功能都位于数据源模块之中。\n\n\n\n\n事务模块MyBatis对数据库中的事务进行了抽象，其自身提供了相应的事务接口和简单实现。在很多场景中，MyBatis会与Spring框架集成，并由Spring框架管理事务，\nJdbcTransaction依赖于JDBC Connection控制事务的提交和回滚。\n\n\n缓存模块在优化系统性能时，优化数据库性能是非常重要的一个环节，而添加缓存则是优化数据库时最有效的手段之一。正确、合理地使用缓存可以将一部分数据库请求拦截在缓存这一层，这就能够减少相当一部分数据库的压力。MyBatis中提供了一级缓存和二级缓存，而这两级缓存都是依赖于基础支持层中的缓存模块实现的。这里需要读者注意的是，MyBatis中自带的这两级缓存与MyBatis以及整个应用是运行在同一个JVM中的，共享同一块堆内存。如果这两级缓存中的数据量较大，则可能影响系统中其他功能的运行，所以当需要缓存大量数据时，优先考虑使用Redis、Memcache等缓存产品。\n\n\n装饰器模式\nBinding模块开发人员无须编写自定义Mapper接口的实现，MyBatis会自动为其创建动态代理对象。在有些场景中，自定义Mapper接口可以完全代替映射配置文件，但有的映射规则和SQL语句的定义还是写在映射配置文件中比较方便，例如动态SQL语句的定义。\nMapper接口\n核心处理层配置解析在MyBatis初始化过程中，会加载mybatis-config.xml配置文件、映射配置文件以及Mapper接口中的注解信息，解析后的配置信息会形成相应的对象并保存到Configuration对象中。例如，示例中定义的＜resultMap＞节点（即ResultSet的映射规则）会被解析成ResultMap对象；示例中定义的＜result＞节点（即属性映射）会被解析成ResultMapping对象。之后，利用该Configuration对象创建SqlSessionFactory对象。待MyBatis初始化之后，开发人员可以通过初始化得到SqlSessionFactory创建SqlSession对象并完成数据库操作。\n建造者模式\nBaseBuilder\n12345678// All-In-One配置对象protected final Configuration configuration;// 在mybatis-config.xml配置文件中可以使用&lt;typeAliases&gt;标签定义别名，在Configuration对象初始化时创建的protected final TypeAliasRegistry typeAliasRegistry;// 在mybatis-config.xml配置文件中可以使用&lt;typeHandlers&gt;标签添加自定义TypeHandler器，在Configuration对象初始化时创建的protected final TypeHandlerRegistry typeHandlerRegistry;\n\nXMLConfigBuilder负责解析 mybatis-config.xml 文件\n\n解析 节点生成 java.util.Properties 对象，设置到 XPathParser 和 Configuration 的 variables 字段中。\n\n解析 节点MyBatis全局性配置，如\n\n解析 、 节点\n\n解析 节点插件是MyBatis提供的扩展机制之一，用户可以通过添加自定义插件在SQL语句执行过程中的某一点进行拦截。MyBatis中的自定义插件只需要实现Interceptor接口。Configuration 的 interceptorChain 字段。\n\n解析 节点\n\n解析 节点在实际生产中，同一项目可能分为开发、测试和生产多个不同的环境，每个环境的配置可能不尽相同，每个 节点对应一种环境的配置。根据 XMLConfigBuilder.environment 字段值确定要使用的 配置，之后创建对应的 TransactionFactory 和 DataSource 对象，并封装进 Environment 对象中。\n\n解析 节点告诉 MyBatis 去哪些位置查找映射配置文件以及使用了配置注解标识的接口。会创建 XMLMapperBuilder 加载映射文件，加载相应的Mapper接口，解析其中的注解并完成向 MapperRegistry 的注册。\n\n\nXMLMapperBuilder负责解析 mapper.xml 文件\n\n解析 节点\n\n解析 节点\n\n解析 节点定义了结果集与结果对象之间的映射规则。\n\n解析 节点定义了可重用的SQL语句片段。\n\n\nXMLStatementBuilder负责解析定义的SQL语句。\n12345678// 节点中的id属性（包括命名空间前缀）private String resource;// 对应一条SQL语句，getBoundSqlprivate SqlSource sqlSource;// SQL的类型，INSERT、UPDATE、DELETE、SELECTprivate SqlCommandType sqlCommandType;\n\n\n解析 节点在解析SQL节点之前，首先通过XMLIncludeTransformer 解析SQL语句中节点，该过程会将节点替换成节点中定义的SQL片段，并将其中的${}占位符替换成真实的参数，可能涉及多层递归。\n12345678910&lt;include id=&quot;someinclude&quot;&gt;\tfrom $&#123;tablename&#125;&lt;/include&gt;&lt;select id=&quot;&quot; resultType=&quot;&quot;&gt;   select B.id as blog_id,B.title as blog_title,B.author_id as blog_author_id   &lt;include refid=&quot;someinclude&quot;&gt;   \t&lt;property name=&quot;tablename&quot; value=&quot;Blog&quot;/&gt;   &lt;/include&gt;&lt;/select&gt;\n\n\n解析 节点\n\n解析 SQL 节点\n\n\n绑定Mapper接口每个映射配置文件中的命名空间可以绑定一个Mapper接口，并注册到 MapperRegistry 中。在 XMLMapperBuilder.bindMapperForNamespace()中完成。\nSQL解析与scripting模块拼凑SQL语句是一件烦琐且易出错的过程，为了将开发人员从这项枯燥无趣的工作中解脱出来，MyBatis实现动态SQL语句的功能，提供了多种动态SQL语句对应的节点，例如，＜where＞节点、＜if＞节点、＜foreach＞节点等。通过这些节点的组合使用，开发人员可以写出几乎满足所有需求的动态SQL语句。 MyBatis中的scripting模块会根据用户传入的实参，解析映射文件中定义的动态SQL节点，并形成数据库可执行的SQL语句。之后会处理SQL语句中的占位符，绑定用户传入的实参。\n组合模式。\nSQL执行SQL语句的执行涉及多个组件，其中比较重要的是Executor、StatementHandler、ParameterHandler和ResultSetHandler。Executor主要负责维护一级缓存和二级缓存，并提供事务管理的相关操作，它会将数据库相关操作委托给StatementHandler完成。StatementHandler首先通过ParameterHandler完成SQL语句的实参绑定，然后通过java.sql.Statement对象执行SQL语句并得到结果集，最后通过ResultSetHandler完成结果集的映射，得到结果对象并返回。\n模板模式。\n\n\n插件接口层接口层相对简单，其核心就是SqlSession接口，该接口中定义了MyBatis暴露给应用程序调用的API，也就是上层应用与MyBatis交互的桥梁。接口层在接收到调用请求时，会调用核心处理层的相应模块来完成具体的数据库操作。\n策略模式。\nSpring问题IoC 控制反转\n\n类A和类B的依赖关系通过配置文件告诉IoC容器，由IoC容器创建对象A和对象B并维护两者之间的关系，客户在使用对象A时，可以直接从IoC容器中获取。\nBeanFactory 与 ApplicationContext 的区别都可以当作 Spring 容器，其中 ApplicationContext 是 BeanFactory 的字接口。\n\nBeanFactory：是 Spring 里面最底层的接口，包含了各种Bean的定义，读取bean配置文件，管理bean的加载、实例化，控制bean的生命周期，维护bean之间的依赖关系。ApplicationContext：作为 BeanFactory 的派生，除了提供 BeanFactory 所具有的功能外，还提供了更完整的框架功能：\n\n继承 MessageSource，因此支持国际化\n统一的资源文件访问方式\n提供在监听器中注册bean的事件\n同时加载多个配置文件\n载入多个（有继承关系）上下文，使得每一个上下文都专注于一个特定的层次，比如应用的web层\n\n\nBeanFactory：采用的是延迟加载形式来注入 Bean 的，即只有在使用到某个Bean时（调用getBean()），才对该 Bean 进行加载实例化。这样，我们就不能发现一些存在的Spring配置问题。如果Bean的某一个属性没有注入，BeanFactory加载后，直到第一次使用调用getBean方法才会抛出异常。ApplicationContext：在容器启动时，一次性创建了所有bean。这样，在容器启动时，就可以发现Spring中存在的配置错误，这样有利于检查所依赖属性是否注入。ApplicationContext 启动后预载入所有的单实例 Bean。ApplicationContext 唯一的不足是占用内存空间，当应用程序配置bean较多时，程序启动较慢。\n\nBeanFactory 通常以编程的方式被创建，ApplicationContext 还能以声明的方式创建，如使用ContextLoader。\n\nBeanFactory 和 ApplicationContext 都支持 BeanPostProcessor、BeanFactoryPostProcessor的使用，但两者之间的区别是：BeanFactory 需要手动注册，而 ApplicationContext 是自动注册的。\n\n\nrefresh 函数中包含了几乎 ApplicationContext 中提供的全部功能，而且此函数中逻辑非常清晰明了：\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778@Override  public void refresh() throws BeansException, IllegalStateException &#123;    synchronized (this.startupShutdownMonitor) &#123;      // Prepare this context for refreshing.      // 准备刷新的上下文      // 对系统属性及环境变量的初始化及验证      prepareRefresh();      // Tell the subclass to refresh the internal bean factory.      // 初始化BeanFactory，并进行XML文件的读取      ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory();      // Prepare the bean factory for use in this context.      // 对BeanFactory进行各种功能填充      // @Qualifier 和 @Autowired 应该是大家非常熟悉的注解，那么这两个注解正是在这一步骤中增加的支持。      prepareBeanFactory(beanFactory);      try &#123;        // Allows post-processing of the bean factory in context subclasses.        // 子类覆盖方法做额外的处理        postProcessBeanFactory(beanFactory);        // Invoke factory processors registered as beans in the context.        // 激活各种BeanFactory处理器        invokeBeanFactoryPostProcessors(beanFactory);        // Register bean processors that intercept bean creation.        // 注册拦截Bean创建的Bean处理器，这里只是注册，真正的调用是在getBean的时候        registerBeanPostProcessors(beanFactory);        // Initialize message source for this context.        // 为上下文初始化Message源，即不同语言的消息体，国际化处理        initMessageSource();        // Initialize event multicaster for this context.        // 初始化应用消息广播器，并放入 “applicationEventMulticaster” bean中        initApplicationEventMulticaster();        // Initialize other special beans in specific context subclasses.        // 留给子类来初始化其他的bean        onRefresh();        // Check for listener beans and register them.        // 在所有注册的bean中查找 Listener bean，注册到消息广播器中        registerListeners();        // Instantiate all remaining (non-lazy-init) singletons.        // 初始化剩下的单实例（非惰性的）        finishBeanFactoryInitialization(beanFactory);        // Last step: publish corresponding event.        // 完成刷新过程，通知生命周期处理器lifeCycleProcessor 刷新过程，同时发出 ContextRefreshEvent 通知别人        finishRefresh();      &#125;      catch (BeansException ex) &#123;        if (logger.isWarnEnabled()) &#123;          logger.warn(&quot;Exception encountered during context initialization - &quot; +              &quot;cancelling refresh attempt: &quot; + ex);        &#125;        // Destroy already created singletons to avoid dangling resources.        destroyBeans();        // Reset &#x27;active&#x27; flag.        cancelRefresh(ex);        // Propagate exception to caller.        throw ex;      &#125;      finally &#123;        // Reset common introspection caches in Spring&#x27;s core, since we        // might not ever need metadata for singleton beans anymore...        resetCommonCaches();      &#125;    &#125;  &#125;\n\nBean 的生命周期首先说说一下Servlet的生命周期：实例化，初始init，接收请求service，销毁destory。Spring 上下文中的Bean生命周期也类似：\n\n实例化Bean\n\n对于 BeanFactory 容器，当客户向容器请求一个尚未初始化的bean时，或初始化bean的时候需要注入另一个尚未初始化的依赖时，容器就会调用createBean进行实例化。对于 ApplicationContext 容器，当容器启动结束后，通过获取 BeanDefinition 对象中的信息，实例化所有的bean。\n\n设置对象属性（依赖注入）\n\n实例化后的对象被封装在 BeanWrapper 对象中，紧接着，Spring 根据 BeanDefinition 中的信息以及通过 BeanWrapper 提供的设置属性的接口完成依赖注入。\n\n处理Aware接口\n\nBeanFactoryAware、ApplicationContextAware、ResourceLoaderAware、ServletContextAware等\n①如果这个Bean已经实现了BeanNameAware接口，会调用它实现的setBeanName(String beanId)方法，此处传递的就是Spring配置文件中Bean的id值；\n②如果这个Bean已经实现了BeanFactoryAware接口，会调用它实现的setBeanFactory()方法，传递的是Spring工厂自身。\n③如果这个Bean已经实现了ApplicationContextAware接口，会调用setApplicationContext(ApplicationContext)方法，传入Spring上下文\n\nBeanPostProcessor的before\n\n对Bean初始化前的一些自定义前置处理\n\ninit-method\n\n激活自定义的init方法，如果Bean在Spring配置文件中配置了init-method属性，则会自动调用其配置的初始化方法。\n\nBeanPostProcessor的after\n\n对Bean初始化后的一些自定义后置处理。此时，Bean已经被正确创建了，可以使用了。\n\nDisposableBean\n\nSprin中不但提供了对于初始化方法的扩展入口，同样也提供了销毁方法的扩展入口，除了熟知的配置属性 destory-method 外，用户还可以注册后处理器 DestructionAwareBeanPostProcessor 来统一处理 bean 的销毁方法。当Bean不再需要时，会经过清理阶段，如果Bean实现了DisposableBean这个接口，会调用其实现的destroy()方法；\n\ndestory-method\n\n最后，如果这个Bean的Spring配置中配置了destroy-method属性，会自动调用其配置的销毁方法。\nautowiring 的实现原理自动装配，减少用户配置Bean的工作量。对属性 autowire 的处理是 populateBean 处理过程的一部分，在处理一般的Bean之前，先对 autowiring 属性进行处理。\nautowire_by_name，autowire_by_type\n在Spring框架xml配置中有5种自动装配：\n\nno：默认的方式是不进行自动装配的，通过手工设置ref属性来进行装配bean\nbyName：通过bean的名称进行自动装配，如果一个bean的property与另一个bean的name相同，就进行自动装配\nbyType：通过参数的数据类型进行自动装配\nconstructor：利用构造函数进行装配，并且构造函数的参数通过byType进行装配。\nautodetect：自动探测，如果有构造方法，通过construct的方式自动装配，否则使用byType的方式自动装配\n\n基于注解的方式：使用@Autowired注解来自动装配指定的bean，需要在Spring配置文件中进行配置，context:annotation-config&#x2F;。在启动Spring IOC时，容器自动装载了一个 AutowiredAnnotationBeanPostProcessor 处理器，当容器扫描到@Autowired、@Resource、@Inject 时，就会在Ioc容器自动查找需要的bean,并装配该对象的属性。在使用@Autowired的时候，首先在容器中查询对应类型的bean：如果查询的结果刚好为一个，就将该bean装配给@Autowired指定的数据；如果查询的结果不止一个，那么@Autowired会根据名称来查找；如果上述查找的结果为空，那么会抛出异常，解决办法是使用required&#x3D;false\n@Autowired可用于：构造函数、成员变量、Setter方法@Autowired 和 @Resource 之间的区别：\n\n@Autowired 默认是按照类型装配注入的，默认情况下它要求依赖对象必须存在（可以设置它的属性为false）\n@Resource 默认是按名称装配注入的，只有当找不到与名称匹配的bean才会按照类型来装配注入\n\nSpring支持的几种bean的作用域\nsingleton：默认，每个容器中只有一个bean的实例，单例的模式由BeanFactory自身来维护\nprototype：为每一个bean请求提供一个实例\nrequest：为每一个网络请求创建一个实例，在请求完成以后，bean会失效并被垃圾回收器回收\nsession：与request范围类似，确保每个 session 中有一个bean的实例，在 session 过期后，bean 会随之失效\nglobal-session：全局作用域，global-session 和 Portlet 应用相关。当应用部署在 Portlet 容器中工作时，它包含很多 portlet。如果你想要声明让所有的portlet共用全局的存储变量的话，那么这全局变量需要存储在global-session中。全局作用域与Servlet中的session作用域效果相同。\n\nAOP\n\n\nAdvice通知\n\n定义在连接点做什么，为切面增强提供织入接口。\nMethodBeforeAdvice.before()\nAfterReturningAdvice.afterReturning()\nThrowsAdvice.afterThrowing()\n\nPointcut切点\n\n需要增强的地方可以是被某个正则表达式进行标识，或根据某个方法名进行匹配等。\n在面向对象编程 OOP 中，开发人员可以通过封装、继承、多态等概念建立对象的层次结构。在系统中，除了核心的业务逻辑，还会有权限检测、日志输出、事务管理等相关的代码，它们会散落在多个对象中，横跨整个对象层次结构，但是这些功能与核心业务逻辑并无直接关系。AOP利用“横切”技术将那些影响了多个类的公共代码抽取出来，封装到一个可重用的模块中，并将其称为Aspect（切面）。这样就可以减少重复的代码，降低模块之间的耦合度，提高了系统的可维护性。\nAOP实现的关键在于代理模式，AOP代理主要分为静态代理和动态代理。静态代理的代表为 AspectJ ，动态代理则以 Spring AOP 为代表：\n\nAspectJ 是静态代理的增强，所谓静态代理，就是 AOP 框架会在编译阶段生成 AOP 代理类，因此也称为编译时增强，它会在编译阶段将 AspectJ(切面) 织入到Java字节码中，运行的时候就是增强之后的AOP对象。\n\nSpring AOP 使用的是动态代理，所谓的动态代理就是说 AOP 框架不会去修改字节码，而是每次运行时在内存中临时为方法生成一个 AOP 对象，这个 AOP 对象包含了目标对象的全部方法，并且在特定的切点做了增强处理，并回调原对象的方法。\n\n\nSpring AOP 中的动态代理主要有两种方式，JDK动态代理和CGLIB动态代理：\n\nJDK 动态代理只提供接口的代理，不支持类的代理。核心 InvocationHandler 接口和 Proxy 类，InvocationHandler 通过 invoke() 方法反射来调用目标类中的代码，动态地将横切逻辑和业务编织在一起；接着，Proxy 利用 InvocationHandler 动态创建一个符合某一接口的实例，生成目标类的代理对象。\n\nInvocationHandler 的 invoke(Object proxy, Method method, Object[] args)：proxy是最终生成的代理实例；method是被代理目标实现的某个具体方法；args是被代理目标实现的某个方法的具体入参，在方法反射调用时使用。\n\n如果代理类没有实现 InvocationHandler 接口，那么 Spring AOP 会选择使用 CGLIB 来动态代理目标类。CGLIB（Code Generation Library），是一个代码生成的类库，可以在运行时动态的生成指定类的一个子类对象，并覆盖其中特定方法并增加增强代码，从而实现AOP。CGLIB 是通过继承的方式做的动态代理，因此如果某个类被标记为final，那么它是无法使用CGLIB做动态代理的。\n\n\n静态代理与动态代理区别在于生成 AOP 代理对象的时机不同，相对来说 AspectJ 的静态代理方式具有更好的性能，但是 AspectJ 需要特定的编译器进行处理，而 Spring AOP 则无需特定的编译器处理。\n\nSpringMVC\n\n\n用户发送请求至前端控制器 DispatcherServlet\nDispatcherServlet 收到请求调用处理器映射器 HandlerMapping\n处理器映射器根据请求 url 找到具体的处理器，生成处理器执行链 HandlerExecutionChain (包括处理器对象和处理器拦截器) 一并返回给 DispatcherServlet\nDispatcherServlet 根据处理器 Handler 获取处理器适配器 HandlerAdapter 执行 HandlerAdapter 处理一系列操作，如：参数封装、数据格式转换、数据验证等操作\n执行处理器Handler（Controller，也叫页面控制器）\nHandler 执行完返回 ModelAndView\nHanlerAdapter 将 Handler 执行结果 ModeAndView 返回到 DispatcherServlet\nDispatcherServlet 将 ModelAndView 传给 ViewResolver 视图解析器\nViewResolver 解析后返回具体 View\nDispatcherServlet 对 View 进行渲染视图（即将模型数据model填充至视图中）\nDispatcherServlet 响应用户\n\nSpring事务实现Spring事务的本质其实就是数据库对事务的支持，没有数据库的事务支持，spring是无法提供事务功能的。真正的数据库层的事务提交和回滚是通过binlog或者redo log实现的。\nSpring事务的种类\n编程式事务管理使用 TransactionTemplate\n@Transactional 声明式事务管理建立在AOP之上的，对方法前后进行拦截，将事务处理的功能编织到拦截的方法中，在目标方法开始之前加入一个事务，在执行完目标方法之后根据执行情况提交或者回滚事务。\n\nSpring的事务传播行为Spring 事务的传播行为说的是，当多个事务同时存在的时候，Spring 如何处理这些事务的行为。\n\nPROPAGATION_REQUIRED：如果当前没有事务，就创建一个新事务，如果当时存在事务，就加入该事务，该设置是最常用的设置。\n\nPROPAGATION_SUPPORTS：支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就以非事务执行。\n\nPROPAGATION_MANDATORY：支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就抛出异常。\n\nPROPAGATION_REQUIRES_NEW：创建新事务，无论当前存不存在事务，都创建新事务。\n\nPROPAGATION_NOT_SUPPORTED：以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。\n\nPROPAGATION_NEVER：以非事务方式执行，如果当前存在事务，则抛出异常。\n\nPROPAGATION_NESTED：如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则按 required 属性执行。\n\n\nSpring中的隔离级别\nISOLATION_DEFAULT：这是个 PlatfromTransactionManager 默认的隔离级别，使用数据库默认的事务隔离级别。\n\nISOLATION_READ_UNCOMMITTED：读未提交，允许另外一个事务可以看到这个事务未提交的数据。\n\nISOLATION_READ_COMMITTED：读已提交，保证一个事务修改的数据提交后才能被另一事务读取，而且能看到该事务对已有记录的更新。\n\nISOLATION_REPEATABLE_READ：可重复读，保证一个事务修改的数据提交后才能被另一事务读取，但是不能看到该事务对已有记录的更新。\n\nISOLATION_SERIALIZABLE：一个事务在执行的过程中完全看不到其他事务对数据库所做的更新。\n\n\nSpring 框架中的不同类型的事件Spring 提供了5种标准的事件：\n\n上下文更新事件（ContextRefreshedEvent）：在调用 ConfigurableApplicationContext 接口中的refresh()方法时被触发。\n\n上下文开始事件（ContextStartedEvent）：当容器调用 ConfigurableApplicationContext 的 start() 方法开始&#x2F;重新开始容器时触发该事件。\n\n上下文停止事件 (ContextStoppedEvent)：当容器调用 ConfigurableApplicationContext 的 stop() 方法开始&#x2F;重新开始容器时触发该事件。\n\n上下文关闭事件（ContextClosedEvent）：当ApplicationContext被关闭时触发该事件。容器被关闭时，其管理的所有单例Bean都被销毁。\n\n请求处理事件（RequestHandledEvent）：在Web应用中，当一个http请求（request）结束触发该事件。\n\n\n如果一个bean实现了ApplicationListener接口，当一个ApplicationEvent 被发布以后，bean会自动被通知。\nDI 依赖注入对象之间的依赖关系是由容器在运行期决定的，也就是说，由容器动态地确定并维持两个对象之间的某个依赖关系。通过依赖注入机制，开发人员只需要通过简单的配置（XML或注解），就可以确定依赖关系，实现组件的重用。\nSpringBoot问题什么是 SpringBoot ？SpringBoot 是 Spring 开源组织下的子项目，是 Spring 组件一站式解决方案，主要简化了使用 Spring 的难度，简省了繁重的配置，提供了各种启动器，开发者能快速上手。\n什么是Spring\n为什么要用 SpringBoot ？\n独立运行\n简化配置\n自动配置\n无代码生成和XML配置\n应用监控\n上手容易\n\nSpringBoot组件\n\n\n名称\n描述\n\n\n\nspring-boot-starter\n核心 Spring Boot starter，包括自动配置支持，日志和 YAML\n\n\nspring-boot-starter-actuator\n生产准备的特性，用于帮我们监控和管理应用\n\n\nspring-boot-starter-amqp\n对”高级消息队列协议”的支持，通过 spring-rabbit 实现\n\n\nspring-boot-starter-aop\n对面向切面编程的支持，包括 spring-aop 和 AspectJ\n\n\nspring-boot-starter-batch\n对 Spring Batch 的支持，包括 HSQLDB 数据库\n\n\nspring-boot-starter-cloudconnectors\n对 Spring Cloud Connectors 的支持，简化在云平台下（例如，Cloud Foundry 和 Heroku）服务的连接\n\n\nspring-boot-starter-dataelasticsearch\n对 Elasticsearch 搜索和分析引擎的支持，包括 spring-data-elasticsearch\n\n\nspring-boot-starter-datagemfire\n对 GemFire 分布式数据存储的支持，包括 spring-data-gemfire\n\n\nspring-boot-starter-data-jpa\n对”Java 持久化 API”的支持，包括 spring-data-jpa，spring-orm 和 Hibernate\n\n\nspring-boot-starter-datamongodb\n对 MongoDB NOSQL 数据库的支持，包括 spring-data-mongodb\n\n\nspring-boot-starter-data-rest\n对通过 REST 暴露 Spring Data 仓库的支持，通过 spring-data-rest-webmvc 实现\n\n\nspring-boot-starter-data-solr\n对 Apache Solr 搜索平台的支持，包括 spring-data-solr\n\n\nspring-boot-starter-freemarker\n对 FreeMarker 模板引擎的支持\n\n\nspring-boot-starter-groovytemplates\n对 Groovy 模板引擎的支持\n\n\nspring-boot-starter-hateoas\n对基于 HATEOAS 的 RESTful 服务的支持，通过 spring-hateoas 实现\n\n\nspring-boot-starter-hornetq\n对”Java 消息服务 API”的支持，通过 HornetQ 实现\n\n\nspring-boot-starter-integration\n对普通 spring-integration 模块的支持\n\n\nspring-boot-starter-jdbc\n对 JDBC 数据库的支持\n\n\nspring-boot-starter-jersey\n对 Jersey RESTful Web 服务框架的支持\n\n\nspring-boot-starter-jta-atomikos\n对 JTA 分布式事务的支持，通过 Atomikos 实现\n\n\nspring-boot-starter-jta-bitronix\n对 JTA 分布式事务的支持，通过 Bitronix 实现\n\n\nspring-boot-starter-mail\n对 javax.mail 的支持\n\n\nspring-boot-starter-mobile\n对 spring-mobile 的支持\n\n\nspring-boot-starter-redis\n对 REDIS 键值数据存储的支持，包括 spring-redis\n\n\nspring-boot-starter-security\n对 spring-security 的支持\n\n\nspring-boot-starter-socialfacebook\n对 spring-social-facebook 的支持\n\n\nspring-boot-starter-sociallinkedin\n对 spring-social-linkedin 的支持\n\n\nspring-boot-starter-socialtwitter\n对 spring-social-twitter 的支持\n\n\nspring-boot-starter-test\n对常用测试依赖的支持，包括 JUnit, Hamcrest 和 Mockito，还有 spring-test 模块\n\n\nspring-boot-starter-thymeleaf\n对 Thymeleaf 模板引擎的支持，包括和 Spring 的集成\n\n\nspring-boot-starter-velocity\n对 Velocity 模板引擎的支持\n\n\nspring-boot-starter-web\n对全栈 web 开发的支持， 包括 Tomcat 和 spring-webmvc\n\n\nspring-boot-starter-websocket\n对 WebSocket 开发的支持\n\n\nspring-boot-starter-ws\n对 Spring Web 服务的支持\n\n\nspring-boot-starter-remote-shell\n添加远程 ssh shell支持\n\n\nspring-boot-starter-jetty\n导入 Jetty HTTP 引擎（作为 Tomcat 的替代）\n\n\nspring-boot-starter-log4j\n对 Log4J 日志系统的支持\n\n\nspring-boot-starter-logging\n导入 Spring Boot 的默认日志系统\n\n\nspring-boot-starter-tomcat\n导入 Spring Boot 的默认 HTTP 引擎\n\n\nspring-boot-starter-undertow\n导入 Undertow HTTP 引擎（作为 Tomcat 的替代）\n\n\nSpring的启动流程\n\n\n组装 SpringApplication\n\n\nresourceLoader：设置resourceload\n设置 primarySources：可以把启动类加载进入Spring容器\nwebApplicationType：判断当前 application 应该运行在什么环境下\nmainApplicationClass：找出main方法启动的class\n\n\n执行 SpringApplication 的 run\n\n获取 SpringApplicationRunListeners\n\n从 META-INF&#x2F;spring.factories 获取 SpringApplicationRunListeners 的集合，并依次调用 SpringApplicationRunListener 的 starting 方法，即最终调用 ApplicationListener 的 onApplicationEvent 方法，发布 springboot 启动事件\n\nprepareEnvironment（目前profile功能已经被maven取代了）\n\n\nConfigurableEnvironment：代表两种意义：一种是profiles，用来描述哪些beandefinitions是可用的；一种是properties，用来描述系统的配置，其来源可能是配置文件、JVM属性文件、操作系统环境变量等等\ngetOrCreateEnvironment()：根据webApplicationType创建不同的Environment\nconfigureEnvironment(XX)：通过configurePropertySources(environment, args)设置properties，通过configureProfiles(environment, args)设置profiles\nlisteners.environmentPrepared(environment);发布environmentPrepared事件，即调用ApplicationListener的onApplicationEvent事件\nbindToSpringApplication：即把当前的environment和当前的springApplication绑定\nConfigurationPropertySources.attach(environment)：将ConfigurationPropertySource放入environment的propertysource中的第一个\n\n\ncreateApplicationContext：创建Spring的容器\n\n\n根据不同的 webApplicationType 设置不同的 contextClass (容器的class类型)，然后生成不同的容器实例对象\n生成容器实例的时候，对于Kotlin类使用’primary’构造函数实例化一个类，如果不是就使用默认构造函数，根据得到构造函数生成实例对象，如果构造函数不是公共的，我们尝试去改变并访问\n\n\nprepareContext：准备容器，在准备刷新容器前准备好容器\n\n\ncontext.setEnvironment(environment)：设置spring容器的environment\npostProcessApplicationContext(context)：设置beanNameGenerator和resourceLoader\napplyInitializers(context)：调用ApplicationContextInitializer的initialize来初始化context，其中还检测各个ApplicationContextInitializer是否接受该类型的容器\nlisteners.contextPrepared(context);即调用SpringApplicationRunListener的contextPrepared方法，但目前是个空实现。\n分别注册springApplicationArguments和springBootBanner这两个bean\ngetAllSources就是获取我们的primarySources和sources\nload(context, sources.toArray(new Object[0]))：首先创建BeanDefinitionLoader，设置该loader的sources，annotatedReader，xmlReader，scanner，以及添加scanner的ExcludeFilter（即过滤springboot的启动类），若用户启动的时候设置了beanNameGenerator，resourceLoader，environment的话就替代我们自身设置的属性。同时根据source的类型选择不同的load方法，这边我们是load（class），最终判断是否是component注解，是的话就通过annotatedReader将启动类注册成bean\nlisteners.contextLoaded(context):首先判断ApplicationListener是否属于ApplicationContextAware，如果是的话就将spring容器赋值给该listener，然后将该ApplicationListener赋值给spring容器，然后调用ApplicationListener的onApplicationEvent方法\n\n\ncontext.registerShutdownHook()：注册一个线程，该线程主要指向doclose方法，doClose方法的逻辑就是：从applicationContexts集合中删除当前容器，MBeanServer卸载MBean，发布容器关闭事件，调用了实现了lifecycleProcessor接口的bean，destroyBeans，closeBeanFactory，onClose：关闭内置tomcat，active设置为false\n\nrefreshContext(context)：真正的刷新spring容器\n\n\n\nprepareRefresh():设置些初始的操作比如，开启激活，启动日期，初始化propertySource。\n获取beanFactory\nprepareBeanFactory(beanFactory)：设置beanFactory的classloader，BeanExpressionResolver，PropertyEditorRegistrar，ApplicationContextAwareProcessor和忽略xxxxAware，注 册依赖，还有ApplicationListenerDetectorApplicationContextAwareProcessor：只是将applicationContext传递给ApplicationContextAwareProcessor，方便后面的xxxAware调用忽略xxxxAware：忽略这些Aware接口实现类中与接口set方法中入参类型相同的属性的的自动注入这样就保证了关键的类是由spring容器自己产生的而不是我们注入的，自动注入不是指的@AutoWire 而是指的是beans的default-autowire&#x3D;”byType” 或在bean的autowire&#x3D;”byType” ，这样spring 回去ioc容器寻找类型相似的类型给其注入，如果实现了spring 的xxaware接口，就不会自动注入记载filterPropertyDescriptorsForDependencyCheck删除与入参类型相同的属性注册依赖：即指定一些类自动注入的实例是spring指定的实例对象ApplicationListenerDetector：检测实现了ApplicationListener的实现类，因为有些实现类，无法通过getBeanNamesForType获取到。\npostProcessBeanFactory(beanFactory)：继续设置ignoreDependencyInterface（ServletContextAware）还有annotatedClasses，basePackages如果存在就设置。\ninvokeBeanFactoryPostProcessors(beanFactory)：从beanFactoryPostProcessors获取BeanFactoryPostProcessor，然后先执行BeanDefinitionRegistryPostProcessor类型的postProcessBeanDefinitionRegistry，继续从beanFactory获取BeanDefinitionRegistryPostProcessor类型的bean然后执行postProcessBeanDefinitionRegistry，执行的过程按照PriorityOrdered，Ordered，普通的类型进行执行，然后优先执行registryProcessors的postProcessBeanFactory在执行regularPostProcessors的postProcessBeanFactory，再从BeanFactory获取PriorityOrdered，Ordered，普通的类型三种类型的BeanFactoryPostProcessor，并按照顺序执行。总结：从之前加入的beanFactoryPostProcessor先执行postProcessBeanDefinitionRegistry（假如是BeanDefinitionRegistryPostProcessor）然后在执行postProcessBeanFactory方法，然后从beanFactory获取BeanFactoryPostProcessor 然后执行postProcessBeanFactory，执行过程中都要按照PriorityOrdered，Ordered，普通的类型三种类型的顺序执行。\nregisterBeanPostProcessors：从beanFactory获取BeanPostProcessor分别按照PriorityOrdered，Ordered，普通的类型注册BeanPostProcessor\nBeanPostProcessor和BeanFactoryPostProcessor:前者是对bean初始化前后进行设置，后者可以对beanFactory进行修改 或者，可以对beanDefinition进行修改或者增加或者初始化渴望提前初始化的bean\ninitMessageSource()：一般是我们用来初始化我们国际化文件的\ninitApplicationEventMulticaster():设置applicationEventMulticaster，spring发布各种事件就依靠他，这个和springboot发布事件使用相同的类\nonRefresh()：初始化其他的子容器类中的bean，同时创建spring的内置tomcat，这在后期Springboot内嵌式tomcat中详细阐述\nregisterListeners()：添加用户设置applicationListeners，然后从beanFactory获取ApplicationListener，然后发布需要earlyApplicationEvents事件\nfinishBeanFactoryInitialization(beanFactory)：实例化非懒加载的剩余bean\nfinishRefresh：清理资源缓存，初始化lifecycle，调用lifecycle的onrefresh，发布ContextRefreshedEvent的事件,激活JMX,启动tomcat\n\n\nafterRefresh(context, applicationArguments)：目前是空实现\n\nlisteners.started(context)：发布started事件\n\ncallRunners(context, applicationArguments)\n\n\n从 spring 容器中获取 ApplicationRunner 和 CommandLineRunner 对象，然后按照顺序排序，循环调用他们的run 方法\n\nhandleRunFailure(context, ex, exeptionReporters, listeners)\n\n处理不同的异常状态，然后调用 listeners.failed(context, exception)，并关闭spring容器\n\nlisteners.running(context)：发布running事件\n\n\n\nSpringCloud 问题熔断、限流\nMyBatis问题什么是 Mybatis ?（1）Mybatis是一个半ORM（对象关系映射）框架，它内部封装了JDBC，开发时只需要关注SQL语句本身，不需要花费精力去处理加载驱动、创建连接、创建statement等繁杂的过程。程序员直接编写原生态sql，可以严格控制sql执行性能，灵活度高。\n（2）Mybatis 可以使用 XML 或注解来配置和映射原生信息，将 POJO映射成数据库中的记录，避免了几乎所有的 JDBC 代码和手动设置参数以及获取结果集。\n（3）通过xml 文件或注解的方式将要执行的各种 statement 配置起来，并通过java对象和 statement中sql的动态参数进行映射生成最终执行的sql语句，最后由mybatis框架执行sql并将结果映射为java对象并返回。（从执行sql到返回result的过程）。\nMybatis的优点（1）基于SQL语句编程，相当灵活，不会对应用程序或者数据库的现有设计造成任何影响，SQL写在XML里，解除sql与程序代码的耦合，便于统一管理；提供XML标签，支持编写动态SQL语句，并可重用。（2）与JDBC相比，减少了50%以上的代码量，消除了JDBC大量冗余的代码，不需要手动开关连接；（3）很好的与各种数据库兼容（因为MyBatis使用JDBC来连接数据库，所以只要JDBC支持的数据库MyBatis都支持）。（4）能够与Spring很好的集成；（5）提供映射标签，支持对象与数据库的ORM字段关系映射；提供对象关系映射标签，支持对象关系组件维护。\nMybatis的$和#的区别？1）#{}是预编译处理，${}是字符串替换。2）Mybatis在处理#{}时，会将sql中的#{}替换为?号，调用PreparedStatement的set方法来赋值；3）Mybatis在处理${}时，就是把${}替换成变量的值。4）使用#{}可以有效的防止SQL注入，提高系统安全性。\n讲下Mybatis的缓存Mybatis缓存分为一级缓存和二级缓存，一级缓存放在session里面，二级缓存放在它的命名空间里，默认是不打开的，使用二级缓存属性类需要实现Serializable序列化接口(可用来保存对象的状态)，可在它的映射文件中配置。\nMybatis是如何分页的？分页插件的原理是什么？1）Mybatis使用RowBounds对象进行分页，可以直接编写sql实现分页，也可以使用Mybatis的分页插件。2）分页插件的原理：实现Mybatis提供的接口，实现自定义插件，在插件的拦截方法内拦截待执行的sql，然后重写sql。举例：select * from student，拦截sql后重写为select t.* from (select * from student) t limit 0,10\n简述Mybatis的运行原理，以及如何编写一个插件？1）Mybatis仅可以编写针对ParameterHandler、ResultSetHandler、StatementHandler、Executor这4种接口的插件，Mybatis通过动态代理，为需要拦截的接口生成代理对象以实现接口方法拦截功能，每当执行这4种接口对象的方法时，就会进入拦截方法，具体就是InvocationHandler的invoke()方法，当然，只会拦截那些你指定需要拦截的方法。2）实现Mybatis的Interceptor接口并复写intercept()方法，然后在给插件编写注解，指定要拦截哪一个接口的哪些方法即可，记住，别忘了在配置文件中配置你编写的插件。\nMybatis动态sql是做什么的？都有哪些动态sql？能简述一下动态sql的执行原理不？1）Mybatis动态sql可以让我们在Xml映射文件内，以标签的形式编写动态sql，完成逻辑判断和动态拼接sql的功能。2）Mybatis提供了9种动态sql标签：trim|where|set|foreach|if|choose|when|otherwise|bind。3）其执行原理为，使用OGNL从sql参数对象中计算表达式的值，根据表达式的值动态拼接sql，以此来完成动态sql的功能。\n简述Mybatis的Xml映射文件和Mybatis内部数据结构之间的映射关系？Mybatis将所有Xml配置信息都封装到All-In-One重量级对象Configuration内部。在Xml映射文件中，标签会被解析为ParameterMap对象，其每个子元素会被解析为ParameterMapping对象。标签会被解析为ResultMap对象，其每个子元素会被解析为ResultMapping对象。每一个\nMyBatis中遇到的设计模式？Builder模式，例如SqlSessionFactoryBuilder、XMLConfigBuilder、XMLMapperBuilder、XMLStatementBuilder、CacheBuilder；工厂模式，例如SqlSessionFactory、ObjectFactory、MapperProxyFactory；单例模式，例如ErrorContext和LogFactory；代理模式，Mybatis实现的核心，比如MapperProxy、ConnectionLogger，用的jdk的动态代理；还有executor.loader包使用了cglib或者javassist达到延迟加载的效果；组合模式，例如SqlNode和各个子类ChooseSqlNode等；模板方法模式，例如BaseExecutor和SimpleExecutor，还有BaseTypeHandler和所有的子类例如IntegerTypeHandler；适配器模式，例如Log的Mybatis接口和它对jdbc、log4j等各种日志框架的适配实现；装饰者模式，例如Cache包中的cache.decorators子包中等各个装饰者的实现；迭代器模式，例如迭代器模式PropertyTokenizer；\n","slug":"JAVA/Spring-SpringBoot-MyBatis","date":"2022-11-23T10:41:09.000Z","categories_index":"Spring,JAVA","tags_index":"MyBatis,SpringBoot,spring","author_index":"dandeliono"},{"id":"774f79caf493f8fbe7cfcf9850f2624f","title":"MySQL修改表和字段的字符集和排序规则","content":"MySQL修改表和字段的字符集和排序规则  关于MySQL查询时不区分字母大小写、插入时不支持特殊字符的问题，只有修改表的字符集和排序规则才能根治，而且事半功倍。utf8mb4支持的最低mysql版本为5.5.3+，若不是，请升级到较新版本。下面以把字符集和排序规则分别修改为utf8mb4 和 utf8mb4_bin为例，进行展开描述，当然，童鞋们也可以换成其它字符集和排序规则，请量体裁衣，自行决定。\n查看表的字符集和排序规则  查看schema中某张表的排序规则：\n12show table status from schema_name like &#x27;%table_name%&#x27;;\n\n  查看schema中所有表的排序规则：\n123SELECT table_name, table_type, engine, version, table_collation FROM information_schema.tables WHERE table_schema = &#x27;schema_name&#x27; ORDER BY table_name DESC;\n\n  上述SQL脚本支持表名模糊匹配。查看某张表中字段的字符集和排序规则：\n12show full COLUMNS FROM table_name;\n\n修改表的字符集和排序规则  修改数据库的字符集编码命令如下：\n12ALTER DATABASE database_name CHARACTER SET utf8mb4 COLLATE utf8mb4_bin;\n\n  修改表的字符集编码：\n12alter table table_name character set utf8mb4;\n\n  它只修改表新增列的默认定义，已有列的字符集不受影响，请格外注意。若要同时修改表字符集和已有列字符集，并将已有数据进行字符集编码转换，请使用如下CONVERT TO脚本：\n12ALTER TABLE table_name CONVERT TO CHARACTER SET utf8mb4 COLLATE utf8mb4_bin;\n\n修改字段的字符集和排序规则  修改某个字段的字符集编码：\n12ALTER TABLE table_name CHANGE column_name column_name VARCHAR(64) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL;\n\n  注：执行命令过程中字段名无需加引号。修改完数据库字符集，立即生效。\n","slug":"MIDDLEWARE/MySQL修改表和字段的字符集和排序规则","date":"2022-11-11T11:27:58.000Z","categories_index":"utf,MIDDLEWARE","tags_index":"MySQL,查看,schema","author_index":"dandeliono"},{"id":"2e30e1dd404d8ce00027cb5e2a0055b3","title":"Mybatis自定义TypeHandler实现数据库Json数组转List T 对象","content":"Mybatis自定义TypeHandler实现数据库Json数组转List对象实现功能在开发过程中经常会遇到将整个JSON数据当作一个字段存到数据库中的情况，但是我们在对应实体表中又不想用一个String去接收，如下图，数据库t_user表中有一个address字段，存的是一个JSON数组，TUserEntity实体中使用List\n对象去接收，这时就需要自定义TypeHanlder帮我们定义入库和出库时的JSON序列化和反序列化。\n\n12345678@Data@TableName(&quot;t_user&quot;)public class TUserEntity  implements Serializable &#123;    private static final long serialVersionUID = 1L;    private Long id;    @TableField(typeHandler = AddressListTypeHandler.class)    private List&lt;Address&gt; address;&#125;\n@Datapublic class Address implements Serializable {    private static final long serialVersionUID &#x3D; 1L;    private Long id;    private String name;}\n1\n\n定义抽象List处理TypeHandler，List中可能还存入其它类型数据12345678910111213141516171819202122232425262728293031323334import com.fasterxml.jackson.core.type.TypeReference;import org.apache.ibatis.type.BaseTypeHandler;import org.apache.ibatis.type.JdbcType;import java.sql.CallableStatement;import java.sql.PreparedStatement;import java.sql.ResultSet;import java.sql.SQLException;import java.util.List;public abstract class ListTypeHandler&lt;T&gt; extends BaseTypeHandler&lt;List&lt;T&gt;&gt; &#123;    @Override    public void setNonNullParameter(PreparedStatement ps, int i, List&lt;T&gt; parameter, JdbcType jdbcType) throws SQLException &#123;        ps.setString(i, JsonUtils.toJson(parameter));    &#125;    @Override    public List&lt;T&gt; getNullableResult(ResultSet rs, String columnName) throws SQLException &#123;        return rs.wasNull() ? null : JsonUtils.readList(rs.getString(columnName), specificType());    &#125;    @Override    public List&lt;T&gt; getNullableResult(ResultSet rs, int columnIndex) throws SQLException &#123;        return rs.wasNull() ? null : JsonUtils.readList(rs.getString(columnIndex), specificType());    &#125;    @Override    public List&lt;T&gt; getNullableResult(CallableStatement cs, int columnIndex) throws SQLException &#123;        return cs.wasNull() ? null : JsonUtils.readList(cs.getString(columnIndex), specificType());    &#125;    protected abstract TypeReference&lt;List&lt;T&gt;&gt; specificType();\n\n定义AddressListTypeHandler类 ,如果有其它类继承ListTypeHandler就可以了123456789@MappedTypes(&#123;List.class&#125;)public class AddressListTypeHandler extends  ListTypeHandler&lt;Address&gt; &#123;    @Override    protected TypeReference&lt;List&lt;Address&gt;&gt; specificType() &#123;        return new TypeReference&lt;List&lt;Address&gt;&gt;() &#123;        &#125;;    &#125;&#125;\n\njaskson中JsonUtils.readList()方法，也可使用fastjson JSON.parseObject(content, this.specificType())123456789101112public static &lt;T&gt; List&lt;T&gt; readList(String json, TypeReference&lt;List&lt;T&gt;&gt; tTypeReference) &#123;    if (json == null || &quot;&quot;.equals(json)) &#123;        return null;    &#125;    try &#123;        return mapper.readValue(json,tTypeReference);    &#125; catch (IOException e) &#123;        throw new RuntimeException(e);    &#125;&#125;\n\n12#  \n\n\n","slug":"JAVA/Mybatis自定义TypeHandler实现数据库Json数组转List T 对象","date":"2022-10-20T18:28:11.000Z","categories_index":"List,JAVA","tags_index":"https,JSON,TypeHandler","author_index":"dandeliono"},{"id":"1545a0366a689b58c66fbe8604cabd3f","title":"BAT 批处理 基础语法 教程","content":"BAT 批处理 基础语法 教程特点\n批处理脚本存储在文本文件中，其中包含的命令按顺序依次执行，其功能是为了自动执行重复的命令序列\n批处理文件具有特殊的扩展名BAT或CMD，可以通过双击、或在命令提示符(cmd.exe)或在开始 - 运行行中运行\n批处理文件通过称为命令解释器的系统文件提供的接口(shell)来识别和执行，在Windows系统上是cmd.exe\n批处理文件可以读取用户的输入，有if、for等控制结构，支持函数、数组等高级功能，支持正则表达式，可以包含其他编程代码\n\n编码问题在windows上，bat文件默认以GBK编码格式保存(很多开发语言、环境的默认编码为UTF-8格式)，如果文件中有中文或特殊符号，强烈建议以GBK编码格式保存，否则会因各种各样的问题导致乱码进而导致执行出错！\n如果不能修改文件编码格式，例如通过 for 等命令读写其他编码格式的文件，那需要使用CHCP修改编码格式。具体可用的编码参考 Code Page Identifiers\nbash\n12@echo off &amp; CHCP 65001:: 会在命令行中输出【Active code page: 65001】的提示\n\n注释注释有三种：\n\nRem 注释内容：Rem相当于命令，如果Rem的行数太多，可能会使代码变慢\n:: 注释内容：建议采用这种方式的注释\n%注释内容%：行内注释\n\nbash\n123@echo offRem Rem注释在未关闭命令回显时会在屏幕显示出来，而 :: 则什么情况下都不会显示echo 注释方式三 %行内注释%\n\n变量命令行参数可以在调用批处理文件时传递参数，%n(n为自然数)表示在调用批处理文件时传递的第n个参数：\n\n%0 批处理文件本身，包括完整的路径和扩展名\n%1 第一个参数\n%9 第九个参数\n%* 从第一个参数开始的所有参数\n\n参数%0具有特殊的功能，可以调用批处理自身，以达到批处理本身循环的目的，也可以复制文件自身等等。\nbash\n123@echo offecho 第一个参数为：%1copy %0 d:\\\\new\\_file.bat %最简单的复制文件自身的方法%\n\nset命令bash\n123set \\[/A\\] varName=value:: 仅当该值是数值类型时，才可以使用【/A】，才能正确的进行数值计算，否则会被当做字符串进行计算:: 使用变量的时候，变量需要包含在％符号中，如【%varName%】\n\nbash\n123456set message=Hello Worldecho %message%SET /A a=5SET /A b=10SET /A c=%a% + %b%echo %c%\n\n局部与全局变量bash\n123456789set globalvar=默认情况下都是全局变量SETLOCALset var=SETLOCAL后定义的变量只在ENDLOCAL之前有效set globalvar=SETLOCAL后重新赋的值也只在ENDLOCAL之前有效echo %var%echo %globalvar%ENDLOCALecho %var% %会打印：【ECHO 处于关闭状态。】%echo %globalvar%\n\n环境变量bash\n1echo %JAVA\\_HOME%\n\n判断变量是否定义bash\n123456SET /A a=5SET str=包青天if defined a echo 【变量a存在】if not defined b echo 【变量b不存在】if not \\[%1\\]==\\[\\] echo 【参数存在：%1】if exist %1 echo 【文件 %1 存在】\n\n字符串基本操作bash\n123456789101112set message=字符串echo 【%message%】set var=3set /A var=%var% + %var%echo 定义为字符串变量后，还可以使用【/A】开关转换为整数：%var%:: 上面打印内容为【6】，如果不加【/A】，那么打印结果为【3 + 3】:: 判断字符串是否相等if \\[%message%\\]==\\[字符串\\] echo 方式一if %message%==字符串 echo 方式二:: 创建一个空字符串Set a=if \\[%a%\\]==\\[\\] echo 检查是否为空字符串\n\n字符串截取其实和其他语言的规则是一致的，截取的标准格式为：\nbash\n123%var:~num\\_chars\\_to\\_skip%%var:~num\\_chars\\_to\\_skip,num\\_chars\\_to\\_keep%即【%var:~fromIndex,length%】\n\nbash\n1234567891011SET a=abcdecho 【%a:~0%】【%a:~1%】【%a:~2%】【%a:~3%】【%a:~4%】:: 【abcd】【bcd】【cd】【d】【】echo 【%a:~-4%】【%a:~-3%】【%a:~-2%】【%a:~-1%】:: 【abcd】【bcd】【cd】【d】-n和4-n的效果是一样的echo 【%a:~0,0%】【%a:~0,1%】【%a:~0,2%】【%a:~0,3%】【%a:~0,4%】:: 【】【a】【ab】【abc】【abcd】echo 【%a:~0,-4%】【%a:~0,-3%】【%a:~0,-2%】【%a:~0,-1%】:: 【】【a】【ab】【abc】【abcd】echo 【%a:~1,1%】【%a:~1,2%】【%a:~2,1%】【%a:~3,1%】【%a:~3,2%】:: 【b】【bc】【c】【d】【d】\n\n替换子字符串bash\n1%str:new=old%\n\nbash\n123set str=包青天 白  乾涛 包青天echo 【%str:包青=老%】【%str: =\\_%】:: 【老天 白  乾涛 老天】【包青天\\_白\\_\\_乾涛\\_包青天】\n\n删除子字符串bash\n12%str:subStr=%:: 可以认为是一种特殊的替换：将指定字符替换为空\n\nbash\n123set str=包青天 白  乾涛 包青天echo 【%str:包青=%】【%str: =%】:: 【天 白  乾涛 天】【包青天白乾涛包青天】\n\nif&#x2F;else 语句基本语法\n只有 if 语句时，if 语句的小括号是可选的\n有 else 语句时，if 语句必须带小括号，而 else 语句的小括号是可选的\nif 条件不能加小括号，否则判断条件为 false\n\nbash\n12345678910SET /A c=5SET str=包青天if %c%==5 echo 只有 if 语句时，if 语句的小括号是可选的if %str%==包青天 (    echo 如果换行的话必须加小括号，且左小括号不能换行--因为换行后就不是一条命令了，右括号不限制    REM 注意：小括号内不能有 :: 格式的注释，可以有REM格式的注释(因为REM是命令)):: 【==】既可以用来判断数值型，也可以用来判断字符串if \\[%1\\]==\\[\\] (echo 呵呵) else echo 有 else 语句时，if 语句必须带小括号，而 else 语句小括号可选if (\\[%1\\]==\\[\\]) (echo 呵呵) else (echo if 条件不能加小括号，否则判断条件为 false)\n\n嵌套 if 语句bash\n12if(condition1) if (condition2) do\\_something:: 只有当条件1和条件2都满足时，才会执行 do\\_something 块中的代码\n\nif errorlevel环境变量 errorlevel 的初始值为0，当一些命令执行不成功，就会返回一个数值，如：1 ，2 等。\n注意：IF ERRORLEVEL 是用来测试它的上一个DOS命令的返回值的，注意只是上一个命令的返回值，而且返回值必须依照从大到小次序顺序判断。\nbash\n1if ERRORLEVEL nubmer commend\n\nbash\n1234copy %0 new\\_file.batif errorlevel 0 echo 命令成功完成copy %0 new/file.batif errorlevel 1 echo 命令失败\n\ngoto 语句bash\n1234567891011@echo off &amp; setlocal enabledelayedexpansionSET /A a=0:addset /A a+=1 &amp; echo echo a当前的值为：%a%，a+1后的值为：!a!if %a% LSS 3 (    goto :add) else (    goto :end):endecho 执行完毕，a的值为：!a!\n\n运算符赋值运算符bash\n1234SET /A a=3SET /A a+=5echo %a%:: 其他类似的运算还有【a-=5】【a\\*=5】【a/=5】【a%=5】\n\n算术运算符bash\n1234567891011SET /A a=5SET /A b=3SET /A c=%a%+%b%SET /A d=%a%-%b%SET /A e=%b%\\*%a%SET /A f=%b%/%a%SET /A g=%a%/%b%SET /A h=%b%%%%a%SET /A i=%a%%%%b%echo 【%c%】【%d%】【%e%】【%f%】【%g%】【%h%】【%i%】:: 【8】【2】【15】【0】【1】【3】【2】\n\n关系运算符bash\n12345678SET /A a=5SET /A b=10if %a% EQU %b% echo 相等if %a% NEQ %b% echo 不相等性if %a% LSS %b% echo 左小于右if %a% LEQ %b% echo 左小于等于右if %a% GTR %b% echo 左大于右if %a% GEQ %b% echo 左大于等于右\n\n逻辑运算符\n批处理语言配备了一整套布尔逻辑运算符，如AND，OR，XOR，但只适用于二进制数字\n对于TRUE或FALSE也没有任何值\n可用于条件的唯一逻辑运算符是NOT运算符\n为非二进制数字实现AND/OR运算符的方法是使用嵌套的IF条件或goto语句\n\n逻辑非bash\n12SET /A a=5IF NOT %a%==6 echo 逻辑非运算符NOT的使用\n\n逻辑与借助 if 语句实现\nbash\n1234567SET /A a=5if %a% LSS 6 (    if %a% GTR 4 (        echo 嵌套if实现and的功能    ))if %a% LSS 6 if %a% GTR 4 echo 嵌套if实现and的功能\n\n借助 goto 语句实现\nbash\n123456789SET /A a=5if not %a% LSS 6 goto endif not %a% GTR 4 goto end:: 为了保持和上面if的结构一致，上面是用了两个not，实际肯定是可以不用not的goto and\\_function:and\\_functionecho 嵌套if实现and的功能:endecho end\n\n逻辑或逻辑或实现起来比逻辑与还要复杂一些，一般都需要借助 goto 语句来实现！\n不借助 goto 语句实现的情况\n这种方式只适合满足条件时要执行的命令比较少的场景，否则会有大量的冗余代码\nbash\n1234567SET /A a=3IF %a% LSS 4 (    echo 如果满足条件一，则执行n条命令) else IF %a% GTR 6 (    echo 否则，如果满足条件二，也会执行相同的n条命令)IF %a% LSS 4 (echo 满足条件) else IF %a% GTR 6 (echo 满足条件)\n\n借助 goto 语句实现\nbash\n12345678SET /A a=5IF %a% LSS 4 goto or\\_functionIF %a% GTR 6 goto or\\_functiongoto end:or\\_functionecho 嵌套if实现or的功能:endecho end\n\n数组数组类型并没有明确定义为批处理脚本中的类型，但可以模拟出来，但是有些功能使用起来会有诸多限制。\n\n\n\n\n\n\n\n\n\n白哥说：其实批处理脚本中的数组就是多个名称类似的变量，其本身没有任何特性或语法，只不过因为这些变量的格式比较像C、Java中的数组，而数组又是那么深入人心，所以我们将其当做数组来看待了而已。\n数组元素其实就是一系列相互之间没任何关系的变量！\nbash\n12345678set a\\[0\\]=10set a\\[1\\]=1set a\\[1\\]=11set a\\[3\\]=14echo 【%a\\[0\\]%】【%a\\[1\\]%】【%a\\[2\\]%】【%a\\[3\\]%】【%a\\[4\\]%】:: 【10】【11】【值不确定】【14】【值不确定】:: 数组中的每个元素都需要使用set命令专门定义:: 注意，未定义的元素的值是无法确定的，并不一定是没有值(虽然大部分情况都是没有defined、也没有值的)！\n\n数组元素下标为变量数组元素下标为变量时，可以认为是：一个变量的名称取决于另一个变量！\nbash\n1234567set a\\[0\\]=10set /a k=0:: 以下3种方式均可正确访问 a\\[k\\]echo %k%-%a\\[0\\]%call echo %k%-%%a\\[%k%\\]%%setlocal enabledelayedexpansion &amp; echo %k%-!a\\[%k%\\]!:: 因为a\\[%k%\\]之前并没有定义(仅仅是定义了a\\[0\\])，所以必须\\`设置本地为延迟扩展\\`才能正常访问，且访问时必须使用!代替%\n\n数组的长度没有直接的函数来确定数组中元素的数量，只能通过遍历数组中的值列表手动计算。\nbash\n1234567891011set d\\[0\\]=10set d\\[1\\]=11set x=0:getArrayLengthif defined d\\[%x%\\] (   call echo %x%-%%d\\[%x%\\]%%   set /a x+=1   goto :getArrayLength)echo 数组长度为 %x%:: 注意，如果数组元素定义时不连续，这种方法计算出来的值可能就是错误的，核心在于上面的【defined】表达式\n\n在数组中创建结构bash\n12345set obj\\[0\\].Name=包青天set obj\\[0\\].ID=66set cur.Name=%obj\\[0\\].Name%echo 【%obj\\[0\\]%】【%obj\\[0\\].Name%】【%cur.Name%】【%cur.ID%】:: 【】【包青天】【包青天】【11】 cur.ID的值没有定义，所以打印结果是不确定的\n\n函数函数定义定义函数的格式\nbash\n1234:function\\_name Do\\_something EXIT /B 0:: EXIT语句用于确保函数正常退出\n\n调用函数的格式\nbash\n12Call :function\\_name parameter1, parameter2… parametern:: 需要确保在主程序中放入【EXIT /B %ERRORLEVEL%】语句，以便将主程序的代码与函数分开\n\n函数的基本用法可以通过使用%~n(n代表参数的位置)，来在函数内部访问参数\n\n\n\n\n\n\n\n\n\n亲测，完全可以省略掉其中的~符号\nbash\n12345678910set p=包青天CALL :function\\_1CALL :function\\_2 a p %p%EXIT /B %ERRORLEVEL%:function\\_1echo 调用了【%0】函数EXIT /B 0:function\\_2echo 调用了【%0】函数，参数为【%~1】【%2】【%3】，所有参数为【%\\*】EXIT /B 0\n\n函数的返回值函数可以通过简单地传递变量名称来处理返回值，这些变量名称将在调用该函数时保存返回值\nbash\n12345678910CALL :function\\_3 c &amp; echo 一行代码时必须使用延迟变量【!c!】【%c%】CALL :function\\_3 decho 不是一行代码时两种方式都可以【!d!】【%d%】set p=包青天CALL :function\\_3 %p% &amp; echo 在函数中修改变量的值时，参数不是传入变量的引用【!p!】【%p%】CALL :function\\_3 p &amp; echo 而是传入变量的名称【!p!】【%p%】EXIT /B %ERRORLEVEL%:function\\_3if NOT \\[%1\\]==\\[\\] set %1=baiqiantaoEXIT /B 0\n\n参数作为输出参数时，此文件不要设置 setlocal enabledelayedexpansion，且函数内不要使用SETLOCAL、ENDLOCAL\n函数中的局部变量\n函数中的局部变量可以用来避免名称冲突，并保持函数本地的变量变化\n调用SETLOCAL命令后可确保命令处理器对所有环境变量进行备份，并可在调用ENDLOCAL命令后恢复\n当到达批处理文件结束时，即通过调用GOTO：EOF，ENDLOCAL被自动调用\n使用SETLOCAL对变量进行本地化允许在函数中自由使用变量名称，而不必担心与函数外使用的变量名称冲突\n可以递归地调用一个函数，使用SETLOCAL可以确保每个级别的递归都使用自己的一组变量，即使变量名被重用\n\nbash\n1234567891011121314151617set x=Outerset y=OuterCALL :function\\_1 x y &amp; echo 【!x!-!y!】--【%x%-%y%】CALL :function\\_2 x y &amp; echo 【!x!-!y!】--【%x%-%y%】EXIT /B %ERRORLEVEL%:function\\_1set x=Inner1set %2=Inner1:: 当没有SETLOCAL时，以上两种set方式都能改变函数外的变量的值EXIT /B 0:function\\_2SETLOCALset x=Inner2set %2=Inner2:: 当有SETLOCAL时，函数内部对变量的修改不影响函数外部的变量的值ENDLOCALEXIT /B 0\n","slug":"LINUX/BAT 批处理 基础语法 教程","date":"2022-10-13T13:55:12.000Z","categories_index":"bash,LINUX","tags_index":"goto,语句,SETLOCAL","author_index":"dandeliono"},{"id":"d8866dac3e3d178ff93b0801099e3728","title":"Spring Spring boot正确集成Quartz及解决Autowired失效问题","content":"Spring&#x2F;Spring boot正确集成Quartz及解决@Autowired失效问题 - 从此寂静无声 - 博客园quartz(1) 项目依赖:\n12345678910111213&lt;parent&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;    &lt;version&gt;2.1.0.RELEASE&lt;/version&gt;    &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;&lt;/parent&gt;&lt;dependencies&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.quartz-scheduler&lt;/groupId&gt;        &lt;artifactId&gt;quartz&lt;/artifactId&gt;    &lt;/dependency&gt;&lt;/dependencies&gt;\n\n(2) 问题代码:\n1234567891011121314151617181920@Componentpublic class UnprocessedTaskJob extends QuartzJobBean &#123;    private TaskMapper taskMapper;    @Autowired    public UnprocessedTaskJob(TaskMapper taskMapper)&#123;        this.taskMapper = taskMapper;    &#125;&#125;private JobDetail generateUnprocessedJobDetail(Task task) &#123;    JobDataMap jobDataMap = new JobDataMap();    jobDataMap.put(UnprocessedTaskJob.TASK_ID, task.getId());    return JobBuilder.newJob(UnprocessedTaskJob.class)            .withIdentity(UnprocessedTaskJob.UNPROCESSED_TASK_KEY_PREFIX + task.getId(), UnprocessedTaskJob.UNPROCESSED_TASK_JOB_GROUP)            .usingJobData(jobDataMap)            .storeDurably()            .build();    &#125;\n\n(3) 提炼问题:\n以上代码存在错误的原因是,UnprocessedTaskJob添加@Component注解, 表示其是Spring IOC容器中的单例类.然而Quartz在创建Job是通过相应的Quartz Job Bean的class反射创建相应的Job. 也就是说, 每次创建新的Job时, 都会生成相应的Job实例. 从而, 这与UnprocessedTaskJob是单例相冲突.查看代码提交记录, 原因是当时认为不添加@Component注解, 则无法通过@Autowired引入由Spring IOC托管的taskMapper实例, 即无法实现依赖注入.\n然而令人感到奇怪的是, 当我在开发环境去除了UnprocessedTaskJob的@Component注解之后, 运行程序后发现TaskMapper实例依然可以注入到Job中, 程序正常运行…\nSpring 托管 Quartz代码分析网上搜索Spring托管Quartz的文章, 大多数都是Spring MVC项目, 集中于如何解决在Job实现类中通过@Autowired实现Spring的依赖注入.网上大多实现均依赖SpringBeanJobFactory去实现Spring与Quartz的集成.\n12345678public class SpringBeanJobFactory extends AdaptableJobFactory        implements ApplicationContextAware, SchedulerContextAware &#123;&#125;public class AdaptableJobFactory implements JobFactory &#123;&#125;\n\n通过上述代码以及注释可以发现:(1) AdaptableJobFactory实现了JobFactory接口, 可以藉此创建标准的Quartz实例 (仅限于Quartz 2.1.4 及以上版本);(2) SpringBeanJobFactory继承于AdaptableJobFactory, 从而实现对Quartz封装实例的属性依赖注入.(3) SpringBeanJobFactory实现了ApplicationContextAware以及SchedulerContextAware接口 (Quartz任务调度上下文), 因此可以在创建Job Bean的时候注入ApplicationContex以及SchedulerContext.\nTips:以上代码基于Spring 5.1.8 版本.在Spring 4.1.0版本, SpringBeanJobFactory的实现如以下代码所示:\n12345public class SpringBeanJobFactory extends AdaptableJobFactory    implements SchedulerContextAware&#123;    &#125;\n\n因此, 在早期的Spring项目中, 需要封装SpringBeanJobFactory并实现ApplicationContextAware接口 (惊不惊喜?).\nSpring 老版本解决方案基于老版本Spring给出解决Spring集成Quartz解决方案.解决方案由第三十九章：基于 SpringBoot &amp; Quartz 完成定时任务分布式单节点持久化提供 (大神的系列文章质量很棒).\n1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556@Configurationpublic class QuartzConfiguration&#123;        public static class AutowiringSpringBeanJobFactory extends SpringBeanJobFactory implements            ApplicationContextAware &#123;        private transient AutowireCapableBeanFactory beanFactory;        @Override        public void setApplicationContext(final ApplicationContext context) &#123;            beanFactory = context.getAutowireCapableBeanFactory();        &#125;                @Override        protected Object createJobInstance(final TriggerFiredBundle bundle) throws Exception &#123;            final Object job = super.createJobInstance(bundle);                        beanFactory.autowireBean(job);            return job;        &#125;    &#125;        @Bean    public JobFactory jobFactory(ApplicationContext applicationContext)    &#123;                AutowiringSpringBeanJobFactory jobFactory = new AutowiringSpringBeanJobFactory();        jobFactory.setApplicationContext(applicationContext);        return jobFactory;    &#125;        @Bean(destroyMethod = &quot;destroy&quot;,autowire = Autowire.NO)    public SchedulerFactoryBean schedulerFactoryBean(JobFactory jobFactory, DataSource dataSource) throws Exception    &#123;        SchedulerFactoryBean schedulerFactoryBean = new SchedulerFactoryBean();                schedulerFactoryBean.setJobFactory(jobFactory);                schedulerFactoryBean.setOverwriteExistingJobs(true);                schedulerFactoryBean.setStartupDelay(2);                schedulerFactoryBean.setAutoStartup(true);                schedulerFactoryBean.setDataSource(dataSource);                schedulerFactoryBean.setApplicationContextSchedulerContextKey(&quot;applicationContext&quot;);                schedulerFactoryBean.setConfigLocation(new ClassPathResource(&quot;/quartz.properties&quot;));        return schedulerFactoryBean;    &#125;&#125;\n\n通过以上代码, 就实现了由SpringBeanJobFactory的createJobInstance创建Job实例, 并将生成的Job实例交付由AutowireCapableBeanFactory来托管.schedulerFactoryBean则设置诸如JobFactory(实际上是AutowiringSpringBeanJobFactory, 内部封装了applicationContext) 以及DataSource(数据源, 如果不设置, 则Quartz默认使用RamJobStore).\n\n\n\n\n\n\n\n\n\nRamJobStore优点是运行速度快, 缺点则是调度任务无法持久化保存.\n因此, 我们可以在定时任务内部使用Spring IOC的@Autowired等注解进行依赖注入.\nSpring 新版本解决方案(1) 解释\n如果你使用Spring boot, 并且版本好大于2.0, 则推荐使用spring-boot-starter-quartz.\n1234&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-quartz&lt;/artifactId&gt;&lt;/dependency&gt;\n\n\n\n\n\n\n\n\n\n\nAuto-configuration support is now include for the Quartz Scheduler. We’ve also added a new spring-boot-starter-quartz starter POM.You can use in-memory JobStores, or a full JDBC-based store. All JobDetail, Calendar and Trigger beans from your Spring application context will be automatically registered with the Scheduler.For more details read the new “Quartz Scheduler” section of the reference documentation.\n以上是spring-boot-starter-quartz的介绍, 基于介绍可知, 如果你没有关闭Quartz的自动配置, 则SpringBoot会帮助你完成Scheduler的自动化配置, 诸如JobDetail&#x2F;Calendar&#x2F;Trigger等Bean会被自动注册至Shceduler中. 你可以在QuartzJobBean中自由的使用@Autowired等依赖注入注解.\n其实, 不引入spring-boot-starter-quartz, 而仅仅导入org.quartz-scheduler,Quartz的自动化配置依然会起效 (这就是第一节问题分析中, 去除@Bean注解, 程序依然正常运行原因, 悲剧中万幸).\n(2) 代码分析\n12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152@Configuration@ConditionalOnClass(&#123; Scheduler.class, SchedulerFactoryBean.class, PlatformTransactionManager.class &#125;)@EnableConfigurationProperties(QuartzProperties.class)@AutoConfigureAfter(&#123; DataSourceAutoConfiguration.class, HibernateJpaAutoConfiguration.class &#125;)public class QuartzAutoConfiguration&#123;        @Bean    @ConditionalOnMissingBean    public SchedulerFactoryBean quartzScheduler() &#123;                        SchedulerFactoryBean schedulerFactoryBean = new SchedulerFactoryBean();        SpringBeanJobFactory jobFactory = new SpringBeanJobFactory();                jobFactory.setApplicationContext(this.applicationContext);                        schedulerFactoryBean.setJobFactory(jobFactory);                if (this.properties.getSchedulerName() != null) &#123;            schedulerFactoryBean.setSchedulerName(this.properties.getSchedulerName());        &#125;        schedulerFactoryBean.setAutoStartup(this.properties.isAutoStartup());schedulerFactoryBean.setStartupDelay((int) this.properties.getStartupDelay().getSeconds());        schedulerFactoryBean.setWaitForJobsToCompleteOnShutdown(this.properties.isWaitForJobsToCompleteOnShutdown());        schedulerFactoryBean.setOverwriteExistingJobs(this.properties.isOverwriteExistingJobs());        if (!this.properties.getProperties().isEmpty()) &#123;            schedulerFactoryBean.setQuartzProperties(asProperties(this.properties.getProperties()));        &#125;        if (this.jobDetails != null &amp;&amp; this.jobDetails.length &gt; 0) &#123;            schedulerFactoryBean.setJobDetails(this.jobDetails);        &#125;        if (this.calendars != null &amp;&amp; !this.calendars.isEmpty()) &#123;            schedulerFactoryBean.setCalendars(this.calendars);        &#125;        if (this.triggers != null &amp;&amp; this.triggers.length &gt; 0) &#123;            schedulerFactoryBean.setTriggers(this.triggers);        &#125;        customize(schedulerFactoryBean);        return schedulerFactoryBean;    &#125;    @Configuration    @ConditionalOnSingleCandidate(DataSource.class)    protected static class JdbcStoreTypeConfiguration &#123;            &#125;&#125;\n\n下面对SpringBeanJobFactory进行分析, 它是生成Job实例, 以及进行依赖注入操作的关键类.\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475public class SpringBeanJobFactory extends AdaptableJobFactory        implements ApplicationContextAware, SchedulerContextAware &#123;    @Nullable    private String[] ignoredUnknownProperties;    @Nullable    private ApplicationContext applicationContext;    @Nullable    private SchedulerContext schedulerContext;        public void setIgnoredUnknownProperties(String... ignoredUnknownProperties) &#123;        this.ignoredUnknownProperties = ignoredUnknownProperties;    &#125;    @Override    public void setApplicationContext(ApplicationContext applicationContext) &#123;        this.applicationContext = applicationContext;    &#125;    @Override    public void setSchedulerContext(SchedulerContext schedulerContext) &#123;        this.schedulerContext = schedulerContext;    &#125;        @Override    protected Object createJobInstance(TriggerFiredBundle bundle) throws Exception &#123;                                Object job = (this.applicationContext != null ?                        this.applicationContext.getAutowireCapableBeanFactory().createBean(                            bundle.getJobDetail().getJobClass(), AutowireCapableBeanFactory.AUTOWIRE_CONSTRUCTOR, false) :                        super.createJobInstance(bundle));        if (isEligibleForPropertyPopulation(job)) &#123;            BeanWrapper bw = PropertyAccessorFactory.forBeanPropertyAccess(job);            MutablePropertyValues pvs = new MutablePropertyValues();            if (this.schedulerContext != null) &#123;                pvs.addPropertyValues(this.schedulerContext);            &#125;            pvs.addPropertyValues(bundle.getJobDetail().getJobDataMap());            pvs.addPropertyValues(bundle.getTrigger().getJobDataMap());            if (this.ignoredUnknownProperties != null) &#123;                for (String propName : this.ignoredUnknownProperties) &#123;                    if (pvs.contains(propName) &amp;&amp; !bw.isWritableProperty(propName)) &#123;                        pvs.removePropertyValue(propName);                    &#125;                &#125;                bw.setPropertyValues(pvs);            &#125;            else &#123;                bw.setPropertyValues(pvs, true);            &#125;        &#125;        return job;    &#125;    &#125;public class AdaptableJobFactory implements JobFactory &#123;        protected Object createJobInstance(TriggerFiredBundle bundle) throws Exception &#123;                Class&lt;?&gt; jobClass = bundle.getJobDetail().getJobClass();        return ReflectionUtils.accessibleConstructor(jobClass).newInstance();    &#125;&#125;\n\n此处需要解释下AutowireCapableBeanFactory的作用.项目中, 有部分实现并未与Spring深度集成, 因此其实例并未被Spring容器管理.然而, 出于需要, 这些并未被Spring管理的Bean需要引入Spring容器中的Bean.此时, 就需要通过实现AutowireCapableBeanFactory, 从而让Spring实现依赖注入等功能.\n https://www.cnblogs.com/jason1990/p/11110196.html\n","slug":"JAVA/Spring Spring boot正确集成Quartz及解决@Autowired失效问题","date":"2022-10-07T14:04:24.000Z","categories_index":"Spring,JAVA","tags_index":"Quartz,Job,SpringBeanJobFactory","author_index":"dandeliono"},{"id":"b3c9ac518ccca39bee20ee352e4467cb","title":"Zookeeper客户端Curator使用详解","content":"Zookeeper客户端Curator使用详解 - 简书Curator 是 Netflix 公司开源的一套 zookeeper 客户端框架，解决了很多 Zookeeper 客户端非常底层的细节开发工作，包括连接重连、反复注册 Watcher 和 NodeExistsException 异常等等。Patrixck Hunt（Zookeeper）以一句 “Guava is to Java that Curator to Zookeeper” 给 Curator 予高度评价。引子和趣闻：Zookeeper 名字的由来是比较有趣的，下面的片段摘抄自《从 PAXOS 到 ZOOKEEPER 分布式一致性原理与实践》一书：Zookeeper 最早起源于雅虎的研究院的一个研究小组。在当时，研究人员发现，在雅虎内部很多大型的系统需要依赖一个类似的系统进行分布式协调，但是这些系统往往存在分布式单点问题。所以雅虎的开发人员就试图开发一个通用的无单点问题的分布式协调框架。在立项初期，考虑到很多项目都是用动物的名字来命名的 (例如著名的 Pig 项目)，雅虎的工程师希望给这个项目也取一个动物的名字。时任研究院的首席科学家 Raghu Ramakrishnan 开玩笑说：再这样下去，我们这儿就变成动物园了。此话一出，大家纷纷表示就叫动物园管理员吧——因为各个以动物命名的分布式组件放在一起，雅虎的整个分布式系统看上去就像一个大型的动物园了，而 Zookeeper 正好用来进行分布式环境的协调——于是，Zookeeper 的名字由此诞生了。\nCurator 无疑是 Zookeeper 客户端中的瑞士军刀，它译作 “馆长” 或者’’管理者’’，不知道是不是开发小组有意而为之，笔者猜测有可能这样命名的原因是说明 Curator 就是 Zookeeper 的馆长 (脑洞有点大：Curator 就是动物园的园长)。Curator 包含了几个包：curator-framework： 对 zookeeper 的底层 api 的一些封装curator-client： 提供一些客户端的操作，例如重试策略等curator-recipes： 封装了一些高级特性，如：Cache 事件监听、选举、分布式锁、分布式计数器、分布式 Barrier 等Maven 依赖 (使用 curator 的版本：2.12.0，对应 Zookeeper 的版本为：3.4.x，如果跨版本会有兼容性问题，很有可能导致节点操作失败)：\n &lt;dependency&gt;\n            &lt;groupId&gt;org.apache.curator&lt;/groupId&gt;\n            &lt;artifactId&gt;curator-framework&lt;/artifactId&gt;\n            &lt;version&gt;2.12.0&lt;/version&gt;\n        &lt;/dependency&gt;\n        &lt;dependency&gt;\n            &lt;groupId&gt;org.apache.curator&lt;/groupId&gt;\n            &lt;artifactId&gt;curator-recipes&lt;/artifactId&gt;\n            &lt;version&gt;2.12.0&lt;/version&gt;\n        &lt;/dependency&gt; \n\n创建会话1. 使用静态工程方法创建客户端一个例子如下：\nRetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 3);\nCuratorFramework client =\nCuratorFrameworkFactory.newClient(\n                        connectionInfo,\n                        5000,\n                        3000,\n                        retryPolicy); \n\nnewClient 静态工厂方法包含四个主要参数：\n\n\n\n参数名\n说明\n\n\n\nconnectionString\n服务器列表，格式 host1:port1,host2:port2,…\n\n\nretryPolicy\n重试策略, 内建有四种重试策略, 也可以自行实现 RetryPolicy 接口\n\n\nsessionTimeoutMs\n会话超时时间，单位毫秒，默认 60000ms\n\n\nconnectionTimeoutMs\n连接创建超时时间，单位毫秒，默认 60000ms\n\n\n2. 使用 Fluent 风格的 Api 创建会话核心参数变为流式设置，一个列子如下：\n RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 3);\n        CuratorFramework client =\n        CuratorFrameworkFactory.builder()\n                .connectString(connectionInfo)\n                .sessionTimeoutMs(5000)\n                .connectionTimeoutMs(5000)\n                .retryPolicy(retryPolicy)\n                .build(); \n\n3. 创建包含隔离命名空间的会话为了实现不同的 Zookeeper 业务之间的隔离，需要为每个业务分配一个独立的命名空间（NameSpace），即指定一个 Zookeeper 的根路径（官方术语：**为 Zookeeper 添加 “Chroot” 特性**）。例如（下面的例子）当客户端指定了独立命名空间为 “&#x2F;base”，那么该客户端对 Zookeeper 上的数据节点的操作都是基于该目录进行的。通过设置 Chroot 可以将客户端应用与 Zookeeper 服务端的一课子树相对应，在多个应用共用一个 Zookeeper 集群的场景下，这对于实现不同应用之间的相互隔离十分有意义。\nRetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 3);\n        CuratorFramework client =\n        CuratorFrameworkFactory.builder()\n                .connectString(connectionInfo)\n                .sessionTimeoutMs(5000)\n                .connectionTimeoutMs(5000)\n                .retryPolicy(retryPolicy)\n                .namespace(&quot;base&quot;)\n                .build(); \n\n启动客户端当创建会话成功，得到 client 的实例然后可以直接调用其 start( ) 方法：\n数据节点操作创建数据节点Zookeeper 的节点创建模式： \n\nPERSISTENT：持久化\nPERSISTENT_SEQUENTIAL：持久化并且带序列号\nEPHEMERAL：临时\nEPHEMERAL_SEQUENTIAL：临时并且带序列号\n\n**创建一个节点，初始内容为空 **\nclient.create().forPath(&quot;path&quot;); \n\n注意：如果没有设置节点属性，节点创建模式默认为持久化节点，内容默认为空\n创建一个节点，附带初始化内容\nclient.create().forPath(&quot;path&quot;,&quot;init&quot;.getBytes()); \n\n创建一个节点，指定创建模式（临时节点），内容为空\nclient.create().withMode(CreateMode.EPHEMERAL).forPath(&quot;path&quot;); \n\n创建一个节点，指定创建模式（临时节点），附带初始化内容\nclient.create().withMode(CreateMode.EPHEMERAL).forPath(&quot;path&quot;,&quot;init&quot;.getBytes()); \n\n创建一个节点，指定创建模式（临时节点），附带初始化内容，并且自动递归创建父节点\nclient.create()\n      .creatingParentContainersIfNeeded()\n      .withMode(CreateMode.EPHEMERAL)\n      .forPath(&quot;path&quot;,&quot;init&quot;.getBytes()); \n\n这个 creatingParentContainersIfNeeded() 接口非常有用，因为一般情况开发人员在创建一个子节点必须判断它的父节点是否存在，如果不存在直接创建会抛出 NoNodeException，使用 creatingParentContainersIfNeeded() 之后 Curator 能够自动递归创建所有所需的父节点。\n删除数据节点删除一个节点\nclient.delete().forPath(&quot;path&quot;); \n\n注意，此方法只能删除叶子节点，否则会抛出异常。\n删除一个节点，并且递归删除其所有的子节点\nclient.delete().deletingChildrenIfNeeded().forPath(&quot;path&quot;); \n\n删除一个节点，强制指定版本进行删除\nclient.delete().withVersion(10086).forPath(&quot;path&quot;); \n\n删除一个节点，强制保证删除\nclient.delete().guaranteed().forPath(&quot;path&quot;); \n\nguaranteed() 接口是一个保障措施，只要客户端会话有效，那么 Curator 会在后台持续进行删除操作，直到删除节点成功。\n注意： 上面的多个流式接口是可以自由组合的，例如：\nclient.delete().guaranteed().deletingChildrenIfNeeded().withVersion(10086).forPath(&quot;path&quot;); \n\n读取数据节点数据读取一个节点的数据内容\nclient.getData().forPath(&quot;path&quot;); \n\n注意，此方法返的返回值是 byte[ ];\n读取一个节点的数据内容，同时获取到该节点的 stat\nStat stat = new Stat();\nclient.getData().storingStatIn(stat).forPath(&quot;path&quot;); \n\n更新数据节点数据更新一个节点的数据内容\nclient.setData().forPath(&quot;path&quot;,&quot;data&quot;.getBytes()); \n\n注意：该接口会返回一个 Stat 实例\n更新一个节点的数据内容，强制指定版本进行更新\nclient.setData().withVersion(10086).forPath(&quot;path&quot;,&quot;data&quot;.getBytes()); \n\n检查节点是否存在client.checkExists().forPath(&quot;path&quot;); \n\n注意：该方法返回一个 Stat 实例，用于检查 ZNode 是否存在的操作. 可以调用额外的方法 (监控或者后台处理) 并在最后调用 forPath( )指定要操作的 ZNode\n获取某个节点的所有子节点路径client.getChildren().forPath(&quot;path&quot;); \n\n注意：该方法的返回值为 List, 获得 ZNode 的子节点 Path 列表。 可以调用额外的方法 (监控、后台处理或者获取状态 watch, background or get stat) 并在最后调用 forPath() 指定要操作的父 ZNode\n事务CuratorFramework 的实例包含 inTransaction() 接口方法，调用此方法开启一个 ZooKeeper 事务. 可以复合 create, setData, check, and&#x2F;or delete 等操作然后调用 commit() 作为一个原子操作提交。一个例子如下：\nclient.inTransaction().check().forPath(&quot;path&quot;)\n      .and()\n      .create().withMode(CreateMode.EPHEMERAL).forPath(&quot;path&quot;,&quot;data&quot;.getBytes())\n      .and()\n      .setData().withVersion(10086).forPath(&quot;path&quot;,&quot;data2&quot;.getBytes())\n      .and()\n      .commit();\n\n异步接口上面提到的创建、删除、更新、读取等方法都是同步的，Curator 提供异步接口，引入了BackgroundCallback接口用于处理异步接口调用之后服务端返回的结果信息。BackgroundCallback接口中一个重要的回调值为 CuratorEvent，里面包含事件类型、响应吗和节点的详细信息。\nCuratorEventType\n\n\n\n事件类型\n对应 CuratorFramework 实例的方法\n\n\n\nCREATE\n#create()\n\n\nDELETE\n#delete()\n\n\nEXISTS\n#checkExists()\n\n\nGET_DATA\n#getData()\n\n\nSET_DATA\n#setData()\n\n\nCHILDREN\n#getChildren()\n\n\nSYNC\n#sync(String,Object)\n\n\nGET_ACL\n#getACL()\n\n\nSET_ACL\n#setACL()\n\n\nWATCHED\n#Watcher(Watcher)\n\n\nCLOSING\n#close()\n\n\n响应码 (#getResultCode())\n\n\n\n响应码\n意义\n\n\n\n0\nOK，即调用成功\n\n\n-4\nConnectionLoss，即客户端与服务端断开连接\n\n\n-110\nNodeExists，即节点已经存在\n\n\n-112\nSessionExpired，即会话过期\n\n\n一个异步创建节点的例子如下：\nExecutor executor = Executors.newFixedThreadPool(2);\nclient.create()\n      .creatingParentsIfNeeded()\n      .withMode(CreateMode.EPHEMERAL)\n      .inBackground((curatorFramework, curatorEvent) -&gt; &#123;      System.out.println(String.format(&quot;eventType:%s,resultCode:%s&quot;,curatorEvent.getType(),curatorEvent.getResultCode()));\n      &#125;,executor)\n      .forPath(&quot;path&quot;); \n\n注意：如果 #inBackground() 方法不指定 executor，那么会默认使用 Curator 的 EventThread 去进行异步处理。\nCurator 食谱 (高级特性)提醒：首先你必须添加 curator-recipes 依赖，下文仅仅对 recipes 一些特性的使用进行解释和举例，不打算进行源码级别的探讨\n &lt;dependency&gt;\n            &lt;groupId&gt;org.apache.curator&lt;/groupId&gt;\n            &lt;artifactId&gt;curator-recipes&lt;/artifactId&gt;\n            &lt;version&gt;2.12.0&lt;/version&gt;\n        &lt;/dependency&gt; \n\n重要提醒：强烈推荐使用 ConnectionStateListener 监控连接的状态，当连接状态为 LOST，curator-recipes 下的所有 Api 将会失效或者过期，尽管后面所有的例子都没有使用到 ConnectionStateListener。 \n缓存Zookeeper 原生支持通过注册 Watcher 来进行事件监听，但是开发者需要反复注册 (Watcher 只能单次注册单次使用)。Cache 是 Curator 中对事件监听的包装，可以看作是对事件监听的本地缓存视图，能够自动为开发者处理反复注册监听。Curator 提供了三种 Watcher(Cache) 来监听结点的变化。\nPath CachePath Cache 用来监控一个 ZNode 的子节点. 当一个子节点增加， 更新，删除时， Path Cache 会改变它的状态， 会包含最新的子节点， 子节点的数据和状态，而状态的更变将通过 PathChildrenCacheListener 通知。\n实际使用时会涉及到四个类：\n\nPathChildrenCache\nPathChildrenCacheEvent\nPathChildrenCacheListener\nChildData\n\n通过下面的构造函数创建 Path Cache:\npublic PathChildrenCache(CuratorFramework client, String path, boolean cacheData) \n\n想使用 cache，必须调用它的start方法，使用完后调用close方法。 可以设置 StartMode 来实现启动的模式 https://www.jianshu.com/p/70151fc0ef5d\n","slug":"JAVA/Zookeeper客户端Curator使用详解","date":"2022-10-07T14:04:24.000Z","categories_index":"Zookeeper,JAVA","tags_index":"Curator,注意,Cache","author_index":"dandeliono"},{"id":"0432547e86941bc1570c73863bb3d6ac","title":"使用spring validation完成数据后端校验","content":"使用spring validation完成数据后端校验_徐靖峰的专栏-CSDN博客前言数据的校验是交互式网站一个不可或缺的功能，前端的 js 校验可以涵盖大部分的校验职责，如用户名唯一性，生日格式，邮箱格式校验等等常用的校验。但是为了避免用户绕过浏览器，使用 http 工具直接向后端请求一些违法数据，服务端的数据校验也是必要的，可以防止脏数据落到数据库中，如果数据库中出现一个非法的邮箱格式，也会让运维人员头疼不已。我在之前保险产品研发过程中，系统对数据校验要求比较严格且追求可变性及效率，曾使用 drools 作为规则引擎，兼任了校验的功能。而在一般的应用，可以使用本文将要介绍的 validation 来对数据进行校验。\n简述 JSR303&#x2F;JSR-349，hibernate validation，spring validation 之间的关系。JSR303 是一项标准, JSR-349 是其的升级版本，添加了一些新特性，他们规定一些校验规范即校验注解，如 @Null，@NotNull，@Pattern，他们位于 javax.validation.constraints 包下，只提供规范不提供实现。而 hibernate validation 是对这个规范的实践（不要将 hibernate 和数据库 orm 框架联系在一起），他提供了相应的实现，并增加了一些其他校验注解，如 @Email，@Length，@Range 等等，他们位于 org.hibernate.validator.constraints 包下。而万能的 spring 为了给开发者提供便捷，对 hibernate validation 进行了二次封装，显示校验 validated bean 时，你可以使用 spring validation 或者 hibernate validation，而 spring validation 另一个特性，便是其在 springmvc 模块中添加了自动校验，并将校验信息封装进了特定的类中。这无疑便捷了我们的 web 开发。本文主要介绍在 springmvc 中自动校验的机制。\n引入依赖我们使用 maven 构建 springboot 应用来进行 demo 演示。\n1234567&lt;dependencies&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;        &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;    &lt;/dependency&gt;&lt;/dependencies&gt;\n\n我们只需要引入 spring-boot-starter-web 依赖即可，如果查看其子依赖，可以发现如下的依赖：\n123456789&lt;dependency&gt;    &lt;groupId&gt;org.hibernate&lt;/groupId&gt;    &lt;artifactId&gt;hibernate-validator&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;    &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt;&lt;/dependency&gt;\n\n验证了我之前的描述，web 模块使用了 hibernate-validation，并且 databind 模块也提供了相应的数据绑定功能。\n构建启动类无需添加其他注解，一个典型的启动类\n12345678@SpringBootApplicationpublic class ValidateApp &#123;    public static void main(String[] args) &#123;        SpringApplication.run(ValidateApp.class, args);    &#125;&#125;\n\n创建需要被校验的实体类12345678910111213141516171819public class Foo &#123;    @NotBlank    private String name;    @Min(18)    private Integer age;    @Pattern(regexp = &quot;^1(3|4|5|7|8)\\\\d&#123;9&#125;$&quot;,message = &quot;手机号码格式错误&quot;)    @NotBlank(message = &quot;手机号码不能为空&quot;)    private String phone;    @Email(message = &quot;邮箱格式错误&quot;)    private String email;    //... getter setter&#125;\n\n使用一些比较常用的校验注解，还是比较浅显易懂的，字段上的注解名称即可推断出校验内容，每一个注解都包含了 message 字段，用于校验失败时作为提示信息，特殊的校验注解，如 Pattern（正则校验），还可以自己添加正则表达式。\n在 @Controller 中校验数据springmvc 为我们提供了自动封装表单参数的功能，一个添加了参数校验的典型 controller 如下所示。\n12345678910111213141516@Controllerpublic class FooController &#123;    @RequestMapping(&quot;/foo&quot;)    public String foo(@Validated Foo foo &lt;1&gt;, BindingResult bindingResult &lt;2&gt;) &#123;        if(bindingResult.hasErrors())&#123;            for (FieldError fieldError : bindingResult.getFieldErrors()) &#123;                //...            &#125;            return &quot;fail&quot;;        &#125;        return &quot;success&quot;;    &#125;&#125;\n\n值得注意的地方：\n&lt;1&gt; 参数 Foo 前需要加上 @Validated 注解，表明需要 spring 对其进行校验，而校验的信息会存放到其后的 BindingResult 中。注意，必须相邻，如果有多个参数需要校验，形式可以如下。foo(@Validated Foo foo, BindingResult fooBindingResult ，@Validated Bar bar, BindingResult barBindingResult); 即一个校验类对应一个校验结果。\n&lt;2&gt; 校验结果会被自动填充，在 controller 中可以根据业务逻辑来决定具体的操作，如跳转到错误页面。\n一个最基本的校验就完成了，总结下框架已经提供了哪些校验：\n1234567891011121314151617181920212223JSR提供的校验注解：         @Null   被注释的元素必须为 null    @NotNull    被注释的元素必须不为 null    @AssertTrue     被注释的元素必须为 true    @AssertFalse    被注释的元素必须为 false    @Min(value)     被注释的元素必须是一个数字，其值必须大于等于指定的最小值    @Max(value)     被注释的元素必须是一个数字，其值必须小于等于指定的最大值    @DecimalMin(value)  被注释的元素必须是一个数字，其值必须大于等于指定的最小值    @DecimalMax(value)  被注释的元素必须是一个数字，其值必须小于等于指定的最大值    @Size(max=, min=)   被注释的元素的大小必须在指定的范围内    @Digits (integer, fraction)     被注释的元素必须是一个数字，其值必须在可接受的范围内    @Past   被注释的元素必须是一个过去的日期    @Future     被注释的元素必须是一个将来的日期    @Pattern(regex=,flag=)  被注释的元素必须符合指定的正则表达式    Hibernate Validator提供的校验注解：  @NotBlank(message =)   验证字符串非null，且长度必须大于0    @Email  被注释的元素必须是电子邮箱地址    @Length(min=,max=)  被注释的字符串的大小必须在指定的范围内    @NotEmpty   被注释的字符串的必须非空    @Range(min=,max=,message=)  被注释的元素必须在合适的范围内\n\n校验实验我们对上面实现的校验入口进行一次测试请求：访问 http://localhost:8080/foo?name=xujingfeng&email=000&age=19 可以得到如下的 debug 信息：\n\n实验告诉我们，校验结果起了作用。并且，可以发现当发生多个错误，spring validation 不会在第一个错误发生后立即停止，而是继续试错，告诉我们所有的错误。debug 可以查看到更多丰富的错误信息，这些都是 spring validation 为我们提供的便捷特性，基本适用于大多数场景。\n你可能不满足于简单的校验特性，下面进行一些补充。\n分组校验如果同一个类，在不同的使用场景下有不同的校验规则，那么可以使用分组校验。未成年人是不能喝酒的，而在其他场景下我们不做特殊的限制，这个需求如何体现同一个实体，不同的校验规则呢？\n改写注解，添加分组：\n12345678910Class Foo&#123;    @Min(value = 18,groups = &#123;Adult.class&#125;)    private Integer age;    public interface Adult&#123;&#125;    public interface Minor&#123;&#125;&#125;\n\n这样表明，只有在 Adult 分组下，18 岁的限制才会起作用。\nController 层改写：\n12345678910111213141516171819202122@RequestMapping(&quot;/drink&quot;)public String drink(@Validated(&#123;Foo.Adult.class&#125;) Foo foo, BindingResult bindingResult) &#123;    if(bindingResult.hasErrors())&#123;        for (FieldError fieldError : bindingResult.getFieldErrors()) &#123;            //...        &#125;        return &quot;fail&quot;;    &#125;    return &quot;success&quot;;&#125;@RequestMapping(&quot;/live&quot;)public String live(@Validated Foo foo, BindingResult bindingResult) &#123;    if(bindingResult.hasErrors())&#123;        for (FieldError fieldError : bindingResult.getFieldErrors()) &#123;            //...        &#125;        return &quot;fail&quot;;    &#125;    return &quot;success&quot;;&#125;\n\ndrink 方法限定需要进行 Adult 校验，而 live 方法则不做限制。\n自定义校验业务需求总是比框架提供的这些简单校验要复杂的多，我们可以自定义校验来满足我们的需求。自定义 spring validation 非常简单，主要分为两步。\n1 自定义校验注解我们尝试添加一个 “字符串不能包含空格” 的限制。\n12345678910111213141516171819202122232425@Target(&#123;METHOD, FIELD, ANNOTATION_TYPE, CONSTRUCTOR, PARAMETER&#125;)@Retention(RUNTIME)@Documented@Constraint(validatedBy = &#123;CannotHaveBlankValidator.class&#125;)&lt;1&gt;public @interface CannotHaveBlank &#123;    //默认错误消息    String message() default &quot;不能包含空格&quot;;    //分组    Class&lt;?&gt;[] groups() default &#123;&#125;;    //负载    Class&lt;? extends Payload&gt;[] payload() default &#123;&#125;;    //指定多个时使用    @Target(&#123;FIELD, METHOD, PARAMETER, ANNOTATION_TYPE&#125;)    @Retention(RUNTIME)    @Documented    @interface List &#123;        CannotHaveBlank[] value();    &#125;&#125;\n\n我们不需要关注太多东西，使用 spring validation 的原则便是便捷我们的开发，例如 payload，List ，groups，都可以忽略。\n&lt;1&gt; 自定义注解中指定了这个注解真正的验证者类。\n2 编写真正的校验者类\n12345678910111213141516171819202122232425public class CannotHaveBlankValidator implements &lt;1&gt; ConstraintValidator&lt;CannotHaveBlank, String&gt; &#123;    @Override    public void initialize(CannotHaveBlank constraintAnnotation) &#123;    &#125;    @Override    public boolean isValid(String value, ConstraintValidatorContext context &lt;2&gt;) &#123;        //null时不进行校验        if (value != null &amp;&amp; value.contains(&quot; &quot;)) &#123;            &lt;3&gt;            //获取默认提示信息            String defaultConstraintMessageTemplate = context.getDefaultConstraintMessageTemplate();            System.out.println(&quot;default message :&quot; + defaultConstraintMessageTemplate);            //禁用默认提示信息            context.disableDefaultConstraintViolation();            //设置提示语            context.buildConstraintViolationWithTemplate(&quot;can not contains blank&quot;).addConstraintViolation();            return false;        &#125;        return true;    &#125;&#125;\n\n&lt;1&gt; 所有的验证者都需要实现 ConstraintValidator 接口，它的接口也很形象，包含一个初始化事件方法，和一个判断是否合法的方法。\n1234567public interface ConstraintValidator&lt;A extends Annotation, T&gt; &#123;    void initialize(A constraintAnnotation);    boolean isValid(T value, ConstraintValidatorContext context);&#125;\n\n&lt;2&gt; ConstraintValidatorContext 这个上下文包含了认证中所有的信息，我们可以利用这个上下文实现获取默认错误提示信息，禁用错误提示信息，改写错误提示信息等操作。\n&lt;3&gt; 一些典型校验操作，或许可以对你产生启示作用。\n值得注意的一点是，自定义注解可以用在METHOD, FIELD, ANNOTATION_TYPE, CONSTRUCTOR, PARAMETER之上，ConstraintValidator 的第二个泛型参数 T，是需要被校验的类型。\n手动校验可能在某些场景下需要我们手动校验，即使用校验器对需要被校验的实体发起 validate，同步获得校验结果。理论上我们既可以使用 Hibernate Validation 提供 Validator，也可以使用 Spring 对其的封装。在 spring 构建的项目中，提倡使用经过 spring 封装过后的方法，这里两种方法都介绍下：\nHibernate Validation：\n12345678910Foo foo = new Foo();foo.setAge(22);foo.setEmail(&quot;000&quot;);ValidatorFactory vf = Validation.buildDefaultValidatorFactory();Validator validator = vf.getValidator();Set&lt;ConstraintViolation&lt;Foo&gt;&gt; set = validator.validate(foo);for (ConstraintViolation&lt;Foo&gt; constraintViolation : set) &#123;    System.out.println(constraintViolation.getMessage());&#125;\n\n由于依赖了 Hibernate Validation 框架，我们需要调用 Hibernate 相关的工厂方法来获取 validator 实例，从而校验。\n在 spring framework 文档的 Validation 相关章节，可以看到如下的描述：\n\n\n\n\n\n\n\n\n\nSpring provides full support for the Bean Validation API. This includes convenient support for bootstrapping a JSR-303&#x2F;JSR-349 Bean Validation provider as a Spring bean. This allows for a javax.validation.ValidatorFactory or javax.validation.Validator to be injected wherever validation is needed in your application. Use the LocalValidatorFactoryBean to configure a default Validator as a Spring bean:\nbean id&#x3D;”validator” class&#x3D;”org.springframework.validation.beanvalidation.LocalValidatorFactoryBean”\nThe basic configuration above will trigger Bean Validation to initialize using its default bootstrap mechanism. A JSR-303&#x2F;JSR-349 provider, such as Hibernate Validator, is expected to be present in the classpath and will be detected automatically.\n上面这段话主要描述了 spring 对 validation 全面支持 JSR-303、JSR-349 的标准，并且封装了 LocalValidatorFactoryBean 作为 validator 的实现。值得一提的是，这个类的责任其实是非常重大的，他兼容了 spring 的 validation 体系和 hibernate 的 validation 体系，也可以被开发者直接调用，代替上述的从工厂方法中获取的 hibernate validator。由于我们使用了 springboot，会触发 web 模块的自动配置，LocalValidatorFactoryBean 已经成为了 Validator 的默认实现，使用时只需要自动注入即可。\n1234567891011121314151617@AutowiredValidator globalValidator; &lt;1&gt;@RequestMapping(&quot;/validate&quot;)public String validate() &#123;    Foo foo = new Foo();    foo.setAge(22);    foo.setEmail(&quot;000&quot;);    Set&lt;ConstraintViolation&lt;Foo&gt;&gt; set = globalValidator.validate(foo);&lt;2&gt;    for (ConstraintViolation&lt;Foo&gt; constraintViolation : set) &#123;        System.out.println(constraintViolation.getMessage());    &#125;    return &quot;success&quot;;&#125;\n\n&lt;1&gt; 真正使用过 Validator 接口的读者会发现有两个接口，一个是位于 javax.validation 包下，另一个位于 org.springframework.validation 包下，注意我们这里使用的是前者 javax.validation，后者是 spring 自己内置的校验接口，LocalValidatorFactoryBean 同时实现了这两个接口。\n&lt;2&gt; 此处校验接口最终的实现类便是 LocalValidatorFactoryBean。\n基于方法校验1234567891011121314151617181920212223@RestController@Validated &lt;1&gt;public class BarController &#123;    @RequestMapping(&quot;/bar&quot;)    public @NotBlank &lt;2&gt; String bar(@Min(18) Integer age &lt;3&gt;) &#123;        System.out.println(&quot;age : &quot; + age);        return &quot;&quot;;    &#125;    @ExceptionHandler(ConstraintViolationException.class)    public Map handleConstraintViolationException(ConstraintViolationException cve)&#123;        Set&lt;ConstraintViolation&lt;?&gt;&gt; cves = cve.getConstraintViolations();&lt;4&gt;        for (ConstraintViolation&lt;?&gt; constraintViolation : cves) &#123;            System.out.println(constraintViolation.getMessage());        &#125;        Map map = new HashMap();        map.put(&quot;errorCode&quot;,500);        return map;    &#125;&#125;\n\n&lt;1&gt; 为类添加 @Validated 注解\n&lt;2&gt; &lt;3&gt; 校验方法的返回值和入参\n&lt;4&gt; 添加一个异常处理器，可以获得没有通过校验的属性相关信息\n基于方法的校验，个人不推荐使用，感觉和项目结合的不是很好。\n使用校验框架的一些想法理论上 spring validation 可以实现很多复杂的校验，你甚至可以使你的 Validator 获取 ApplicationContext，获取 spring 容器中所有的资源，进行诸如数据库校验，注入其他校验工具，完成组合校验（如前后密码一致）等等操作，但是寻求一个易用性和封装复杂性之间的平衡点是我们作为工具使用者应该考虑的，我推崇的方式，是仅仅使用自带的注解和自定义注解，完成一些简单的，可复用的校验。而对于复杂的校验，则包含在业务代码之中，毕竟如用户名是否存在这样的校验，仅仅依靠数据库查询还不够，为了避免并发问题，还是得加上唯一索引之类的额外工作，不是吗？ https://blog.csdn.net/u013815546/article/details/77248003\n","slug":"JAVA/使用spring validation完成数据后端校验","date":"2022-10-07T14:04:24.000Z","categories_index":"validation,JAVA","tags_index":"spring,JSR,hibernate","author_index":"dandeliono"},{"id":"39567589e8d82514e8e405343a06adf8","title":"掌握MyBatis插件原理轻松写出自己的PageHelper分页插件","content":"掌握MyBatis插件原理轻松写出自己的PageHelper分页插件-阿里云开发者社区在 MyBatis 中插件式通过拦截器来实现的，那么既然是通过拦截器来实现的，就会有一个问题，哪些对象才允许被拦截呢？\n真正执行 Sql 的是四大对象：Executor，StatementHandler，ParameterHandler，ResultSetHandler。\n而 MyBatis 的插件正是基于拦截这四大对象来实现的。需要注意的是，虽然我们可以拦截这四大对象，但是并不是这四大对象中的所有方法都能被拦截，下面就是官网提供的可拦截的对象和方法汇总：\n首先我们先来通过一个例子来看看如何使用插件。\n1、 首先建立一个 MyPlugin 实现接口 Interceptor，然后重写其中的三个方法 (注意，这里必须要实现 Interceptor 接口，否则无法被拦截)。1234567891011121314151617181920212223242526272829package com.lonelyWolf.mybatis.plugin;import org.apache.ibatis.executor.Executor;import org.apache.ibatis.mapping.MappedStatement;import org.apache.ibatis.plugin.*;import org.apache.ibatis.session.ResultHandler;import org.apache.ibatis.session.RowBounds;import java.util.Properties;@Intercepts(&#123;@Signature(type = Executor.class,method = &quot;query&quot;,args = &#123;MappedStatement.class,Object.class, RowBounds.class, ResultHandler.class&#125;)&#125;)public class MyPlugin implements Interceptor &#123;        @Override    public Object intercept(Invocation invocation) throws Throwable &#123;        System.out.println(&quot;成功拦截了Executor的query方法，在这里我可以做点什么&quot;);        return invocation.proceed();    &#125;    @Override    public Object plugin(Object target) &#123;        return Plugin.wrap(target,this);    &#125;    @Override    public void setProperties(Properties properties) &#123;        System.out.println(&quot;自定义属性:userName-&gt;&quot; + properties.getProperty(&quot;userName&quot;));    &#125;&#125;\n\n@Intercepts 是声明当前类是一个拦截器，后面的 @Signature 是标识需要拦截的方法签名，通过以下三个参数来确定\n(1)type：被拦截的类名\n(2)method：被拦截的方法名\n(3)args：标注方法的参数类型\n2、 我们还需要在 mybatis-config 中配置好插件。12345&lt;plugins&gt;    &lt;plugin interceptor=&quot;com.lonelyWolf.mybatis.plugin.MyPlugin&quot;&gt;        &lt;property name=&quot;userName&quot; value=&quot;张三&quot;/&gt;    &lt;/plugin&gt;&lt;/plugins&gt;\n\n这里如果配置了 property 属性，那么我们可以在 setProperties 获取到。\n完成以上两步，我们就完成了一个插件的配置了，接下来我们运行一下：\n可以看到，setProperties 方法在加载配置文件阶段就会被执行了。\n接下来让我们分析一下从插件的加载到初始化到运行整个过程的实现原理。\n插件的加载既然插件需要在配置文件中进行配置，那么肯定就需要进行解析，我们看看插件式如何被解析的。我们进入 XMLConfigBuilder 类看看\n解析出来之后会将插件存入 InterceptorChain 对象的 list 属性\n看到 InterceptorChain 我们是不是可以联想到，MyBatis 的插件就是通过责任链模式实现的。\n既然插件类已经被加载到配置文件了，那么接下来就有一个问题了，插件类何时会被拦截我们需要拦截的对象呢？\n其实插件的拦截是和对象有关的，不同的对象进行拦截的时间也会不一致，接下来我们就逐一分析一下。\n拦截 Executor 对象我们知道，SqlSession 对象是通过 openSession() 方法返回的，而 Executor 又是属于 SqlSession 内部对象，所以让我们跟随 openSession 方法去看一下 Executor 对象的初始化过程。\n可以看到，当初始化完成 Executor 之后，会调用 interceptorChain 的 pluginAll 方法，pluginAll 方法本身非常简单，就是把我们存到 list 中的插件进行循环，并调用 Interceptor 对象的 plugin 方法：\n再次点击进去：\n到这里我们是不是发现很熟悉，没错，这就是我们上面示例中重写的方法，而 plugin 方法是接口中的一个默认方法。\n这个方法是关键，我们进去看看：\n可以看到这个方法的逻辑也很简单，但是需要注意的是 MyBatis 插件是通过 JDK 动态代理来实现的。\n而 JDK 动态代理的条件就是被代理对象必须要有接口，这一点和 Spring 中不太一样，Spring 中是如果有接口就采用 JDK 动态代理，没有接口就是用 CGLIB 动态代理。\n正因为 MyBatis 的插件只使用了 JDK 动态代理，所以我们上面才强调了一定要实现 Interceptor 接口。\n而代理之后汇之星 Plugin 的 invoke 方法，我们最后再来看看 invoke 方法：\n而最终执行的 intercept 方法，就是我们上面示例中重写的方法。\n其他对象插件解析接下来我们再看看 StatementHandler，StatementHandler 是在 Executor 中的 doQuery 方法创建的，其实这个原理就是一样的了，找到初始化 StatementHandler 对象的方法：\n进去之后里面执行的也是 pluginAll 方法：\n其他两个对象就不在举例了，其实搜一下全局就很明显了：\n四个对象初始化的时候都会调用 pluginAll 来进行判定是否有被代理。\n下面就是实现了插件之后的执行时序图：\n一个对象是否可以被多个代理对象进行代理？也就是说同一个对象的同一个方法是否可以被多个拦截器进行拦截？\n答案是肯定的，因为被代理对象是被加入到 list，所以我们配置在最前面的拦截器最先被代理，但是执行的时候却是最外层的先执行。\n具体点：\n假如依次定义了三个插件：插件 A，插件 B 和 插件 C。\n那么 List 中就会按顺序存储：插件 A，插件 B 和 插件 C。\n而解析的时候是遍历 list，所以解析的时候也是按照：插件 A，插件 B 和 插件 C 的顺序。\n但是执行的时候就要反过来了，执行的时候是按照：插件 C，插件 B 和插件 A 的顺序进行执行。\n上面我们了解了在 MyBatis 中的插件是如何定义以及 MyBatis 中是如何处理插件的，接下来我们就以经典分页插件 PageHelper 为例来进一步加深理解。\n首先我们看看 PageHelper 的用法：\n12345678910111213141516171819202122232425262728293031323334353637package com.lonelyWolf.mybatis;import com.alibaba.fastjson.JSONObject;import com.github.pagehelper.Page;import com.github.pagehelper.PageHelper;import com.github.pagehelper.PageInfo;import com.lonelyWolf.mybatis.mapper.UserMapper;import com.lonelyWolf.mybatis.model.LwUser;import org.apache.ibatis.executor.result.DefaultResultHandler;import org.apache.ibatis.io.Resources;import org.apache.ibatis.session.ResultHandler;import org.apache.ibatis.session.SqlSession;import org.apache.ibatis.session.SqlSessionFactory;import org.apache.ibatis.session.SqlSessionFactoryBuilder;import java.io.IOException;import java.io.InputStream;import java.util.List;public class MyBatisByPageHelp &#123;    public static void main(String[] args) throws IOException &#123;        String resource = &quot;mybatis-config.xml&quot;;                InputStream inputStream = Resources.getResourceAsStream(resource);                SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream);                SqlSession session = sqlSessionFactory.openSession();        PageHelper.startPage(0,10);        UserMapper userMapper = session.getMapper(UserMapper.class);        List&lt;LwUser&gt; userList = userMapper.listAllUser();        PageInfo&lt;LwUser&gt; pageList = new PageInfo&lt;&gt;(userList);        System.out.println(null == pageList ? &quot;&quot;: JSONObject.toJSONString(pageList));    &#125;&#125;\n\n输出如下结果：\n可以看到对象已经被分页，那么这是如何做到的呢？\n我们上面提到，要实现插件必须要实现 MyBatis 提供的 Interceptor 接口，所以我们去找一下，发现 PageHeler 实现了 Interceptor：\n经过上面的介绍这个类应该一眼就能看懂，我们关键要看看 SqlUtil 的 intercept 方法做了什么：\n这个方法的逻辑比较多，因为要考虑到不同的数据库方言的问题，所以会有很多判断，我们主要是关注 PageHelper 在哪里改写了 sql 语句，上图中的红框就是改写了 sql 语句的地方：\n这里面会获取到一个 Page 对象，然后在爱写 sql 的时候也会将一些分页参数设置到 Page 对象，我们看看 Page 对象是从哪里获取的：\n我们看到对象是从 LOCAL_PAGE 对象中获取的，这个又是什么呢？\n这是一个本地线程池变量，那么这里面的 Page 又是什么时候存进去的呢？这就要回到我们的示例上了，分页的开始必须要调用：\n1PageHelper.startPage(0,10);\n\n\n这里就会构建一个 Page 对象，并设置到 ThreadLocal 内。\n为什么 PageHelper 只对 startPage 后的第一条 select 语句有效这个其实也很简单哈，但是可能会有人有这个以为，我们还是要回到上面的 intercept 方法：\n在 finally 内把 ThreadLocal 中的分页数据给清除掉了，所以只要执行一次查询语句就会清除分页信息，故而后面的 select 语句自然就无效了。 https://developer.aliyun.com/article/780497\n","slug":"JAVA/掌握MyBatis插件原理轻松写出自己的PageHelper分页插件","date":"2022-10-07T14:04:24.000Z","categories_index":"https,JAVA","tags_index":"com,raw,githubusercontent","author_index":"dandeliono"},{"id":"b118a4f61bac8ee2550c64d69640d355","title":"记一次spring boot 功能模块化 freemarker只能识别一个resources目录下前端展示模板问题","content":"记一次spring boot 功能模块化 freemarker只能识别一个resources目录下前端展示模板问题 - 一枚码农的个人空间 - OSCHINA - 中文开源技术交流社区今天开发个人开源框架，完善了系统模块后，集成工作流功能模块，在新的 resources 下面新建 freemarker 前端页面 (ftl 页面)，然后正常开发到展示流程表的时候，后台报错情景重新：Could not resolve view with name ‘&#x2F;actList’ in servlet with name ‘dispatcherServlet’\n\n当时我就想是不是映射地址出错了，检查多次后发现也没错，怎么 spring mvc 就映射不到呢，而且是相同的的地址路径 (模块不一样)，和其他模块 resources 一致的路径，然后我系统模块是完全没问题的，我感觉应该是 freemarker 配置的问题，然后各种尝试，各种检查，各种查阅资料，发现，的确是 freemarker 的问题，我少配置了一项 freemarker 配置：FreeMarkerConfigurationFactory类下的属性：preferFileSystemAccess，&lt;— 对就是它，我们来看源码发现默认是true\npublic class FreeMarkerConfigurationFactory &#123;\n\n    protected final Log logger = LogFactory.getLog(getClass());\n\n    private Resource configLocation;\n\n    private Properties freemarkerSettings;\n\n    private Map&lt;String, Object&gt; freemarkerVariables;\n\n    private String defaultEncoding;\n\n    private final List&lt;TemplateLoader&gt; templateLoaders = new ArrayList&lt;TemplateLoader&gt;();\n\n    private List&lt;TemplateLoader&gt; preTemplateLoaders;\n\n    private List&lt;TemplateLoader&gt; postTemplateLoaders;\n\n    private String[] templateLoaderPaths;\n\n    private ResourceLoader resourceLoader = new DefaultResourceLoader();\n\n    private boolean preferFileSystemAccess = true;\n\n那么他是干嘛用的呢，我们看官方介绍是介绍：\nSet whether to prefer file system access for template loading. File system access enables hot detection of template changes.If this is enabled, FreeMarkerConfigurationFactory will try to resolve the specified “templateLoaderPath” as file system resource (which will work for expanded class path resources and ServletContext resources too).\nDefault is “true”. Turn this off to always load via SpringTemplateLoader (i.e. as stream, without hot detection of template changes), which might be necessary if some of your templates reside in an expanded classes directory while others reside in jar files.\n我们随便翻一下：第一句的意思是是否以更喜欢方式来访问模板\n如果启用就以 templateLoaderPath 设置的路径来加载，默认为 true，设置为 false 的话就以 SpringTemplateLoader 方式访问，那么就是流的方式访问，这可能是必要的, 如果你的一些模板驻留在一个扩展的类而其他人驻留在 jar 文件的目录。\n意思就是 如果你的文件在 jar 包文件目录下，就有必要设置为 false，我们是多模块的，最终只运行一个模块，其他模块都是以 jar 包来加载的，我们要加载 jar 包里面的文件目录，必须要把这个属性设置为 false 才行，debug 源码看看，先上配置文件：\n@Bean\n  public FreeMarkerConfigurer freemarkerConfig() throws IOException, TemplateException &#123;\n    FreeMarkerConfigurationFactory factory = new FreeMarkerConfigurationFactory();\n    factory.setTemplateLoaderPath(&quot;classpath:/ftl/&quot;);\n    factory.setDefaultEncoding(&quot;UTF-8&quot;);\n    factory.setPreferFileSystemAccess(true);\n    FreeMarkerConfigurer result = new FreeMarkerConfigurer();\n    freemarker.template.Configuration configuration = factory.createConfiguration();\n    configuration.setClassicCompatible(true);\n    result.setConfiguration(configuration);\n    Properties settings = new Properties();\n    settings.put(&quot;template_update_delay&quot;, &quot;0&quot;);\n    settings.put(&quot;default_encoding&quot;, &quot;UTF-8&quot;);\n    settings.put(&quot;number_format&quot;, &quot;0.######&quot;);\n    settings.put(&quot;classic_compatible&quot;, true);\n    settings.put(&quot;template_exception_handler&quot;, &quot;ignore&quot;);\n    result.setFreemarkerSettings(settings);\n    return result;\n  &#125;\n\nFreeMarkerConfigurationFactory\n\n返回\n最终返回：\n\n看到没，是单个模块的绝对路径 ftl 文件夹。\n看图我们可以明确知道为 true 的话，加载的是设置的 setTemplateLoaderPath 的路径 也就是我其中一个模块下的 ftl 路径，所以其他模块下 ftl 路径是加载不出来的，也就是 spring mvc 映射不到的原因。 \n我们试试 将isPreferFileSystemAccess设置为false： \n\n最终我们跟到这里：\n\n发现路径是 &#x2F; ftl &#x2F; 而不是之前的绝对路径，然后 set 进 spring 对 freemarker 的配置对象中。\n好的，我们通过分析源码我们可以知道，多模块下，我们设置为 false，spring 才能以相对路径来访问我们的模板文件夹，问题解决。\n12#spring boot 配置项spring.freemarker.prefer-file-system-access=false\n","slug":"JAVA/记一次spring boot 功能模块化 freemarker只能识别一个resources目录下前端展示模板问题","date":"2022-10-07T14:04:24.000Z","categories_index":"resource,JAVA","tags_index":"https,raw,freemarker","author_index":"dandeliono"},{"id":"84d510f335cb1c9f3ff98caa4f69cfc2","title":"zookeeper curator处理会话过期session expired","content":"zookeeper curator处理会话过期session expired - 扎心了老铁 - 博客园本文介绍在使用 curator 框架的时候如何 handle session expire。\n1、什么是 zookeeper 的会话过期？\n 一般来说，我们使用 zookeeper 是集群形式，如下图，client 和 zookeeper 集群 (3 个实例) 建立一个会话 session。\n\n 在这个会话 session 当中，client 其实是随机与其中一个 zk provider 建立的链接，并且互发心跳 heartbeat。zk 集群负责管理这个 session，并且在所有的 provider 上维护这个 session 的信息，包括这个 session 中定义的临时数据和监视点 watcher。\n如果再网络不佳或者 zk 集群中某一台 provider 挂掉的情况下，有可能出现 connection loss 的情况，例如 client 和 zk provider1 连接断开，这时候 client 不需要任何的操作 (zookeeper api 已经给我们做好了)，只需要等待 client 与其他 provider 重新连接即可。这个过程可能导致两个结果：\n1）在 session timeout 之内连接成功\n这个时候 client 成功切换到连接另一个 provider 例如是 provider2，由于 zk 在所有的 provider 上同步了 session 相关的数据，此时可以认为无缝迁移了。\n2）在 session timeout 之内没有重新连接\n这就是 session expire 的情况，这时候 zookeeper 集群会任务会话已经结束，并清除和这个 session 有关的所有数据，包括临时节点和注册的监视点 Watcher。\n在 session 超时之后，如果 client 重新连接上了 zookeeper 集群，很不幸，zookeeper 会发出 session expired 异常，且不会重建 session，也就是不会重建临时数据和 watcher。\n2、如何使用 curator 实现 session expired 异常的捕获和处理？\n1）首先我们先创建一个链接\n这里设置了重试策略 retryPolicy 和会话超时时间 sessionTimeoutMs，并打开链接。\n123456public void init() &#123;        RetryPolicy retryPolicy \\= new ExponentialBackoffRetry(baseSleepTimeMs, maxRetries);        client \\= CuratorFrameworkFactory.builder().connectString(zookeeperServer).retryPolicy(retryPolicy)                .sessionTimeoutMs(sessionTimeoutMs).connectionTimeoutMs(connectionTimeoutMs).build();        client.start();    &#125;\n\n2）客户端注册\n12345678910111213public void register() &#123; try &#123;            String rootPath \\= &quot;/services&quot;;            String hostAddress \\= InetAddress.getLocalHost().getHostAddress();            String serviceInstance \\= &quot;/prometheus&quot; + &quot;-&quot; +  hostAddress + &quot;-&quot;;            String path \\= rootPath + serviceInstance;            SessionConnectionListener sessionConnectionListener \\= new SessionConnectionListener(path, &quot;&quot;);            client.getConnectionStateListenable().addListener(sessionConnectionListener);            client.create().creatingParentsIfNeeded().withMode(CreateMode.EPHEMERAL\\_SEQUENTIAL).forPath(path);        &#125; catch (Exception e) &#123;            logger.error(&quot;注册出错&quot;, e);        &#125;    &#125;\n\n 这里我们创建了一个临时有序节点 node，这个节点将会在 session expired 触发的时候被自动删除。当 session 又重新恢复的时候，client 只会收到 session expired 异常和不会自动将临时节点添加到 zookeeper 中。\n为了解决这个问题，我们增加了一个监听器，\nclient.getConnectionStateListenable().addListener(sessionConnectionListener)\n这个监听器监听 session expired 事件，并且在事件发生的时候进行处理，监听器处理的流程如下。\n注意：这个监听器注册是可以复用的，即如果多次 session expired，不用重复注册监听器。\n3、监听器 sessionConnectionListener\n1234567891011121314151617package com.xiaoju.dqa.prometheus.client.zookeeper; import org.apache.curator.framework.CuratorFramework; import org.apache.curator.framework.state.ConnectionState; import org.apache.curator.framework.state.ConnectionStateListener; import org.apache.zookeeper.CreateMode; import org.slf4j.Logger; import org.slf4j.LoggerFactory; public class SessionConnectionListener implements ConnectionStateListener &#123; private final Logger logger = LoggerFactory.getLogger(this.getClass()); private String path; private String data; public SessionConnectionListener(String path, String data) &#123; this.path = path; this.data = data;    &#125;    @Override public void stateChanged(CuratorFramework curatorFramework, ConnectionState connectionState)&#123; if(connectionState == ConnectionState.LOST)&#123;            logger.error(&quot;\\[负载均衡失败\\]zk session超时&quot;); while(true)&#123; try &#123; if(curatorFramework.getZookeeperClient().blockUntilConnectedOrTimedOut())&#123;                        curatorFramework.create().creatingParentsIfNeeded().withMode(CreateMode.EPHEMERAL\\_SEQUENTIAL).forPath(path, data.getBytes(&quot;UTF-8&quot;));                        logger.info(&quot;\\[负载均衡修复\\]重连zk成功&quot;); break;                    &#125;                &#125; catch (InterruptedException e) &#123; break;                &#125; catch (Exception e)&#123;                &#125;            &#125;        &#125;    &#125;&#125;\n\n 这里的 ConnectionState.LOST 等同于 session expired 事件，对这个事件的处理是，在一个死循环中重试链接 zk，知道链接成功才退出循环。\n需要注意的是：一旦重新创建了会话，那么之前会话的所有观察点都会失效，需要重新初始化观察点。 https://www.cnblogs.com/kangoroo/p/7538314.html\n","slug":"MIDDLEWARE/zookeeper curator处理会话过期session expired","date":"2022-10-07T14:04:24.000Z","categories_index":"session,MIDDLEWARE","tags_index":"zookeeper,client,expired","author_index":"dandeliono"},{"id":"5216b5eeb1a6215d7622123c3d5153cb","title":"30张图解： TCP 重传、滑动窗口、流量控制、拥塞控制","content":"30张图解： TCP 重传、滑动窗口、流量控制、拥塞控制相信大家都知道 TCP 是一个可靠传输的协议，那它是如何保证可靠的呢？\n为了实现可靠性传输，需要考虑很多事情，例如数据的破坏、丢包、重复以及分片顺序混乱等问题。如不能解决这些问题，也就无从谈起可靠传输。\n那么，TCP 是通过序列号、确认应答、重发控制、连接管理以及窗口控制等机制实现可靠性传输的。\n今天，将重点介绍 TCP 的重传机制、滑动窗口、流量控制、拥塞控制。 \n\n重传机制TCP 实现可靠传输的方式之一，是通过序列号与确认应答。\n在 TCP 中，当发送端的数据到达接收主机时，接收端主机会返回一个确认应答消息，表示已收到消息。\n正常的数据传输\n但在错综复杂的网络，并不一定能如上图那么顺利能正常的数据传输，万一数据在传输过程中丢失了呢？\n所以 TCP 针对数据包丢失的情况，会用重传机制解决。\n接下来说说常见的重传机制：\n\n超时重传\n快速重传\nSACK\nD-SACK\n\n超时重传重传机制的其中一个方式，就是在发送数据时，设定一个定时器，当超过指定的时间后，没有收到对方的 ACK 确认应答报文，就会重发该数据，也就是我们常说的超时重传。\nTCP 会在以下两种情况发生超时重传：\n\n数据包丢失\n确认应答丢失\n\n超时重传的两种情况\n\n\n\n\n\n\n\n\n\n超时时间应该设置为多少呢？\n我们先来了解一下什么是 RTT（Round-Trip Time 往返时延），从下图我们就可以知道：\nRTT\nRTT 就是数据从网络一端传送到另一端所需的时间，也就是包的往返时间。\n超时重传时间是以 RTO （Retransmission Timeout 超时重传时间）表示。\n假设在重传的情况下，超时时间 RTO 「较长或较短」时，会发生什么事情呢？\n超时时间较长与较短\n上图中有两种超时时间不同的情况：\n\n当超时时间 RTO 较大时，重发就慢，丢了老半天才重发，没有效率，性能差；\n当超时时间 RTO 较小时，会导致可能并没有丢就重发，于是重发的就快，会增加网络拥塞，导致更多的超时，更多的超时导致更多的重发。\n\n精确的测量超时时间 RTO 的值是非常重要的，这可让我们的重传机制更高效。\n根据上述的两种情况，我们可以得知，超时重传时间 RTO 的值应该略大于报文往返 RTT 的值。\nRTO 应略大于 RTT\n至此，可能大家觉得超时重传时间 RTO 的值计算，也不是很复杂嘛。\n好像就是在发送端发包时记下 t0 ，然后接收端再把这个 ack 回来时再记一个 t1，于是 RTT = t1 – t0。没那么简单，这只是一个采样，不能代表普遍情况。\n实际上「报文往返 RTT 的值」是经常变化的，因为我们的网络也是时常变化的。也就因为「报文往返 RTT 的值」 是经常波动变化的，所以「超时重传时间 RTO 的值」应该是一个动态变化的值。\n我们来看看 Linux 是如何计算 RTO 的呢？\n估计往返时间，通常需要采样以下两个：\n\n需要 TCP 通过采样 RTT 的时间，然后进行加权平均，算出一个平滑 RTT 的值，而且这个值还是要不断变化的，因为网络状况不断地变化。\n除了采样 RTT，还要采样 RTT 的波动范围，这样就避免如果 RTT 有一个大的波动的话，很难被发现的情况。\n\nRFC6289 建议使用以下的公式计算 RTO：\nRFC6289 建议的 RTO 计算\n其中 SRTT 是计算平滑的RTT ，DevRTR 是计算平滑的RTT 与 最新 RTT 的差距。\n在 Linux 下，α &#x3D; 0.125，β &#x3D; 0.25， μ &#x3D; 1，∂ &#x3D; 4。别问怎么来的，问就是大量实验中调出来的。\n如果超时重发的数据，再次超时的时候，又需要重传的时候，TCP 的策略是超时间隔加倍。 \n也就是每当遇到一次超时重传的时候，都会将下一次超时时间间隔设为先前值的两倍。两次超时，就说明网络环境差，不宜频繁反复发送。 \n超时触发重传存在的问题是，超时周期可能相对较长。那是不是可以有更快的方式呢？\n于是就可以用「快速重传」机制来解决超时重发的时间等待。\n快速重传TCP 还有另外一种快速重传（Fast Retransmit）机制，它不以时间为驱动，而是以数据驱动重传。\n快速重传机制，是如何工作的呢？其实很简单，一图胜千言。\n快速重传机制\n在上图，发送方发出了 1，2，3，4，5 份数据：\n\n第一份 Seq1 先送到了，于是就 Ack 回 2；\n结果 Seq2 因为某些原因没收到，Seq3 到达了，于是还是 Ack 回 2；\n后面的 Seq4 和 Seq5 都到了，但还是 Ack 回 2，因为 Seq2 还是没有收到；\n发送端收到了三个 Ack &#x3D; 2 的确认，知道了 Seq2 还没有收到，就会在定时器过期之前，重传丢失的 Seq2。 \n最后，收到了 Seq2，此时因为 Seq3，Seq4，Seq5 都收到了，于是 Ack 回 6 。\n\n所以，快速重传的工作方式是当收到三个相同的 ACK 报文时，会在定时器过期之前，重传丢失的报文段。\n快速重传机制只解决了一个问题，就是超时时间的问题，但是它依然面临着另外一个问题。就是重传的时候，是重传之前的一个，还是重传所有的问题。 \n比如对于上面的例子，是重传 Seq2 呢？还是重传 Seq2、Seq3、Seq4、Seq5 呢？因为发送端并不清楚这连续的三个 Ack 2 是谁传回来的。\n根据 TCP 不同的实现，以上两种情况都是有可能的。可见，这是一把双刃剑。\n为了解决不知道该重传哪些 TCP 报文，于是就有 SACK 方法。\nSACK 方法还有一种实现重传机制的方式叫：SACK（ Selective Acknowledgment 选择性确认）。\n这种方式需要在 TCP 头部「选项」字段里加一个 SACK 的东西，它可以将缓存的地图发送给发送方，这样发送方就可以知道哪些数据收到了，哪些数据没收到，知道了这些信息，就可以只重传丢失的数据。\n如下图，发送方收到了三次同样的 ACK 确认报文，于是就会触发快速重发机制，通过 SACK 信息发现只有 200~299 这段数据丢失，则重发时，就只选择了这个 TCP 段进行重复。\n选择性确认\n如果要支持 SACK，必须双方都要支持。在 Linux 下，可以通过 net.ipv4.tcp_sack 参数打开这个功能（Linux 2.4 后默认打开）。\nDuplicate SACKDuplicate SACK 又称 D-SACK，其主要使用了 SACK 来告诉「发送方」有哪些数据被重复接收了。 \n下面举例两个栗子，来说明 D-SACK 的作用。\n栗子一号：ACK 丢包\nACK 丢包\n\n「接收方」发给「发送方」的两个 ACK 确认应答都丢失了，所以发送方超时后，重传第一个数据包（3000 ~ 3499）\n于是「接收方」发现数据是重复收到的，于是回了一个 SACK &#x3D; 3000~3500，告诉「发送方」 3000~3500 的数据早已被接收了，因为 ACK 都到了 4000 了，已经意味着 4000 之前的所有数据都已收到，所以这个 SACK 就代表着 D-SACK。\n这样「发送方」就知道了，数据没有丢，是「接收方」的 ACK 确认报文丢了。\n\n栗子二号：网络延时\n网络延时\n\n数据包（1000~1499） 被网络延迟了，导致「发送方」没有收到 Ack 1500 的确认报文。\n而后面报文到达的三个相同的 ACK 确认报文，就触发了快速重传机制，但是在重传后，被延迟的数据包（1000~1499）又到了「接收方」；\n所以「接收方」回了一个 SACK&#x3D;1000~1500，因为 ACK 已经到了 3000，所以这个 SACK 是 D-SACK，表示收到了重复的包。 \n这样发送方就知道快速重传触发的原因不是发出去的包丢了，也不是因为回应的 ACK 包丢了，而是因为网络延迟了。\n\n可见，D-SACK 有这么几个好处：\n\n可以让「发送方」知道，是发出去的包丢了，还是接收方回应的 ACK 包丢了;\n可以知道是不是「发送方」的数据包被网络延迟了;\n可以知道网络中是不是把「发送方」的数据包给复制了;\n\n在 Linux 下可以通过 net.ipv4.tcp_dsack 参数开启&#x2F;关闭这个功能（Linux 2.4 后默认打开）。\n\n滑动窗口\n\n\n\n\n\n\n\n\n引入窗口概念的原因\n我们都知道 TCP 是每发送一个数据，都要进行一次确认应答。当上一个数据包收到了应答了， 再发送下一个。\n这个模式就有点像我和你面对面聊天，你一句我一句。但这种方式的缺点是效率比较低的。\n如果你说完一句话，我在处理其他事情，没有及时回复你，那你不是要干等着我做完其他事情后，我回复你，你才能说下一句话，很显然这不现实。\n按数据包进行确认应答\n所以，这样的传输方式有一个缺点：数据包的往返时间越长，通信的效率就越低。\n为解决这个问题，TCP 引入了窗口这个概念。即使在往返时间较长的情况下，它也不会降低网络通信的效率。\n那么有了窗口，就可以指定窗口大小，窗口大小就是指无需等待确认应答，而可以继续发送数据的最大值。\n窗口的实现实际上是操作系统开辟的一个缓存空间，发送方主机在等到确认应答返回之前，必须在缓冲区中保留已发送的数据。如果按期收到确认应答，此时数据就可以从缓存区清除。\n假设窗口大小为 3 个 TCP 段，那么发送方就可以「连续发送」 3 个 TCP 段，并且中途若有 ACK 丢失，可以通过「下一个确认应答进行确认」。如下图：\n用滑动窗口方式并行处理\n图中的 ACK 600 确认应答报文丢失，也没关系，因为可以通过下一个确认应答进行确认，只要发送方收到了 ACK 700 确认应答，就意味着 700 之前的所有数据「接收方」都收到了。这个模式就叫累计确认或者累计应答。\n\n\n\n\n\n\n\n\n\n窗口大小由哪一方决定？\nTCP 头里有一个字段叫 Window，也就是窗口大小。\n这个字段是接收端告诉发送端自己还有多少缓冲区可以接收数据。于是发送端就可以根据这个接收端的处理能力来发送数据，而不会导致接收端处理不过来。 \n所以，通常窗口的大小是由接收方的窗口大小来决定的。\n发送方发送的数据大小不能超过接收方的窗口大小，否则接收方就无法正常接收到数据。\n\n\n\n\n\n\n\n\n\n发送方的滑动窗口\n我们先来看看发送方的窗口，下图就是发送方缓存的数据，根据处理的情况分成四个部分，其中深蓝色方框是发送窗口，紫色方框是可用窗口：\n\n\n#1 是已发送并收到 ACK确认的数据：1~31 字节\n#2 是已发送但未收到 ACK确认的数据：32~45 字节\n#3 是未发送但总大小在接收方处理范围内（接收方还有空间）：46~51字节\n#4 是未发送但总大小超过接收方处理范围（接收方没有空间）：52字节以后\n\n在下图，当发送方把数据「全部」都一下发送出去后，可用窗口的大小就为 0 了，表明可用窗口耗尽，在没收到 ACK 确认之前是无法继续发送数据了。\n可用窗口耗尽\n在下图，当收到之前发送的数据 32~36 字节的 ACK 确认应答后，如果发送窗口的大小没有变化，则滑动窗口往右边移动 5 个字节，因为有 5 个字节的数据被应答确认，接下来 52~56 字节又变成了可用窗口，那么后续也就可以发送 52~56 这 5 个字节的数据了。\n32 ~ 36 字节已确认\n\n\n\n\n\n\n\n\n\n程序是如何表示发送方的四个部分的呢？\nTCP 滑动窗口方案使用三个指针来跟踪在四个传输类别中的每一个类别中的字节。其中两个指针是绝对指针（指特定的序列号），一个是相对指针（需要做偏移）。\nSND.WND、SND.UN、SND.NXT\n\nSND.WND：表示发送窗口的大小（大小是由接收方指定的）；\n\nSND.UNA：是一个绝对指针，它指向的是已发送但未收到确认的第一个字节的序列号，也就是 #2 的第一个字节。\n\nSND.NXT：也是一个绝对指针，它指向未发送但可发送范围的第一个字节的序列号，也就是 #3 的第一个字节。\n\n指向 #4 的第一个字节是个相对指针，它需要 SND.UNA 指针加上 SND.WND 大小的偏移量，就可以指向 #4 的第一个字节了。\n\n\n那么可用窗口大小的计算就可以是：\n可用窗口大 &#x3D; SND.WND -（SND.NXT - SND.UNA）\n\n\n\n\n\n\n\n\n\n接收方的滑动窗口\n接下来我们看看接收方的窗口，接收窗口相对简单一些，根据处理的情况划分成三个部分：\n\n#1 + #2 是已成功接收并确认的数据（等待应用进程读取）；\n#3 是未收到数据但可以接收的数据；\n#4 未收到数据并不可以接收的数据；\n\n接收窗口\n其中三个接收部分，使用两个指针进行划分:\n\nRCV.WND：表示接收窗口的大小，它会通告给发送方。\nRCV.NXT：是一个指针，它指向期望从发送方发送来的下一个数据字节的序列号，也就是 #3 的第一个字节。\n指向 #4 的第一个字节是个相对指针，它需要 RCV.NXT 指针加上 RCV.WND 大小的偏移量，就可以指向 #4 的第一个字节了。\n\n\n\n\n\n\n\n\n\n\n接收窗口和发送窗口的大小是相等的吗？\n并不是完全相等，接收窗口的大小是约等于发送窗口的大小的。\n因为滑动窗口并不是一成不变的。比如，当接收方的应用进程读取数据的速度非常快的话，这样的话接收窗口可以很快的就空缺出来。那么新的接收窗口大小，是通过 TCP 报文中的 Windows 字段来告诉发送方。那么这个传输过程是存在时延的，所以接收窗口和发送窗口是约等于的关系。\n\n流量控制发送方不能无脑的发数据给接收方，要考虑接收方处理能力。\n如果一直无脑的发数据给对方，但对方处理不过来，那么就会导致触发重发机制，从而导致网络流量的无端的浪费。\n为了解决这种现象发生，TCP 提供一种机制可以让「发送方」根据「接收方」的实际接收能力控制发送的数据量，这就是所谓的流量控制。 \n下面举个栗子，为了简单起见，假设以下场景：\n\n客户端是接收方，服务端是发送方\n假设接收窗口和发送窗口相同，都为 200\n假设两个设备在整个传输过程中都保持相同的窗口大小，不受外界影响\n\n流量控制\n根据上图的流量控制，说明下每个过程：\n\n客户端向服务端发送请求数据报文。这里要说明下，本次例子是把服务端作为发送方，所以没有画出服务端的接收窗口。\n服务端收到请求报文后，发送确认报文和 80 字节的数据，于是可用窗口 Usable 减少为 120 字节，同时 SND.NXT 指针也向右偏移 80 字节后，指向 321，这意味着下次发送数据的时候，序列号是 321。 \n客户端收到 80 字节数据后，于是接收窗口往右移动 80 字节，RCV.NXT 也就指向 321，这意味着客户端期望的下一个报文的序列号是 321，接着发送确认报文给服务端。\n服务端再次发送了 120 字节数据，于是可用窗口耗尽为 0，服务端无法再继续发送数据。\n客户端收到 120 字节的数据后，于是接收窗口往右移动 120 字节，RCV.NXT 也就指向 441，接着发送确认报文给服务端。\n服务端收到对 80 字节数据的确认报文后，SND.UNA 指针往右偏移后指向 321，于是可用窗口 Usable 增大到 80。\n服务端收到对 120 字节数据的确认报文后，SND.UNA 指针往右偏移后指向 441，于是可用窗口 Usable 增大到 200。\n服务端可以继续发送了，于是发送了 160 字节的数据后，SND.NXT 指向 601，于是可用窗口 Usable 减少到 40。\n客户端收到 160 字节后，接收窗口往右移动了 160 字节，RCV.NXT 也就是指向了 601，接着发送确认报文给服务端。\n服务端收到对 160 字节数据的确认报文后，发送窗口往右移动了 160 字节，于是 SND.UNA 指针偏移了 160 后指向 601，可用窗口 Usable 也就增大至了 200。\n\n操作系统缓冲区与滑动窗口的关系前面的流量控制例子，我们假定了发送窗口和接收窗口是不变的，但是实际上，发送窗口和接收窗口中所存放的字节数，都是放在操作系统内存缓冲区中的，而操作系统的缓冲区，会被操作系统调整。\n当应用进程没办法及时读取缓冲区的内容时，也会对我们的缓冲区造成影响。\n\n\n\n\n\n\n\n\n\n那操心系统的缓冲区，是如何影响发送窗口和接收窗口的呢？\n我们先来看看第一个例子。\n当应用程序没有及时读取缓存时，发送窗口和接收窗口的变化。\n考虑以下场景：\n\n客户端作为发送方，服务端作为接收方，发送窗口和接收窗口初始大小为 360；\n服务端非常的繁忙，当收到客户端的数据时，应用层不能及时读取数据。\n\n\n根据上图的流量控制，说明下每个过程：\n\n客户端发送 140 字节数据后，可用窗口变为 220 （360 - 140）。\n服务端收到 140 字节数据，但是服务端非常繁忙，应用进程只读取了 40 个字节，还有 100 字节占用着缓冲区，于是接收窗口收缩到了 260 （360 - 100），最后发送确认信息时，将窗口大小通告给客户端。\n客户端收到确认和窗口通告报文后，发送窗口减少为 260。\n客户端发送 180 字节数据，此时可用窗口减少到 80。\n服务端收到 180 字节数据，但是应用程序没有读取任何数据，这 180 字节直接就留在了缓冲区，于是接收窗口收缩到了 80 （260 - 180），并在发送确认信息时，通过窗口大小给客户端。\n客户端收到确认和窗口通告报文后，发送窗口减少为 80。\n客户端发送 80 字节数据后，可用窗口耗尽。\n服务端收到 80 字节数据，但是应用程序依然没有读取任何数据，这 80 字节留在了缓冲区，于是接收窗口收缩到了 0，并在发送确认信息时，通过窗口大小给客户端。\n客户端收到确认和窗口通告报文后，发送窗口减少为 0。\n\n可见最后窗口都收缩为 0 了，也就是发生了窗口关闭。当发送方可用窗口变为 0 时，发送方实际上会定时发送窗口探测报文，以便知道接收方的窗口是否发生了改变，这个内容后面会说，这里先简单提一下。\n我们先来看看第二个例子。\n当服务端系统资源非常紧张的时候，操心系统可能会直接减少了接收缓冲区大小，这时应用程序又无法及时读取缓存数据，那么这时候就有严重的事情发生了，会出现数据包丢失的现象。\n\n说明下每个过程：\n\n客户端发送 140 字节的数据，于是可用窗口减少到了 220。\n服务端因为现在非常的繁忙，操作系统于是就把接收缓存减少了 120 字节，当收到 140 字节数据后，又因为应用程序没有读取任何数据，所以 140 字节留在了缓冲区中，于是接收窗口大小从 360 收缩成了 100，最后发送确认信息时，通告窗口大小给对方。\n此时客户端因为还没有收到服务端的通告窗口报文，所以不知道此时接收窗口收缩成了 100，客户端只会看自己的可用窗口还有 220，所以客户端就发送了 180 字节数据，于是可用窗口减少到 40。\n服务端收到了 180 字节数据时，发现数据大小超过了接收窗口的大小，于是就把数据包丢失了。 \n客户端收到第 2 步时，服务端发送的确认报文和通告窗口报文，尝试减少发送窗口到 100，把窗口的右端向左收缩了 80，此时可用窗口的大小就会出现诡异的负值。\n\n所以，如果发生了先减少缓存，再收缩窗口，就会出现丢包的现象。\n为了防止这种情况发生，TCP 规定是不允许同时减少缓存又收缩窗口的，而是采用先收缩窗口，过段时间再减少缓存，这样就可以避免了丢包情况。 \n窗口关闭在前面我们都看到了，TCP 通过让接收方指明希望从发送方接收的数据大小（窗口大小）来进行流量控制。\n如果窗口大小为 0 时，就会阻止发送方给接收方传递数据，直到窗口变为非 0 为止，这就是窗口关闭。 \n\n\n\n\n\n\n\n\n\n窗口关闭潜在的危险\n接收方向发送方通告窗口大小时，是通过 ACK 报文来通告的。\n那么，当发生窗口关闭时，接收方处理完数据后，会向发送方通告一个窗口非 0 的 ACK 报文，如果这个通告窗口的 ACK 报文在网络中丢失了，那麻烦就大了。\n窗口关闭潜在的危险\n这会导致发送方一直等待接收方的非 0 窗口通知，接收方也一直等待发送方的数据，如不采取措施，这种相互等待的过程，会造成了死锁的现象。\n\n\n\n\n\n\n\n\n\nTCP 是如何解决窗口关闭时，潜在的死锁现象呢？\n为了解决这个问题，TCP 为每个连接设有一个持续定时器，只要 TCP 连接一方收到对方的零窗口通知，就启动持续计时器。 \n如果持续计时器超时，就会发送窗口探测 ( Window probe ) 报文，而对方在确认这个探测报文时，给出自己现在的接收窗口大小。\n窗口探测\n\n如果接收窗口仍然为 0，那么收到这个报文的一方就会重新启动持续计时器；\n如果接收窗口不是 0，那么死锁的局面就可以被打破了。\n\n窗口探测的次数一般为 3 次，每次大约 30-60 秒（不同的实现可能会不一样）。如果 3 次过后接收窗口还是 0 的话，有的 TCP 实现就会发 RST 报文来中断连接。\n糊涂窗口综合症如果接收方太忙了，来不及取走接收窗口里的数据，那么就会导致发送方的发送窗口越来越小。\n到最后，如果接收方腾出几个字节并告诉发送方现在有几个字节的窗口，而发送方会义无反顾地发送这几个字节，这就是糊涂窗口综合症。\n要知道，我们的 TCP + IP 头有 40 个字节，为了传输那几个字节的数据，要达上这么大的开销，这太不经济了。\n就好像一个可以承载 50 人的大巴车，每次来了一两个人，就直接发车。除非家里有矿的大巴司机，才敢这样玩，不然迟早破产。要解决这个问题也不难，大巴司机等乘客数量超过了 25 个，才认定可以发车。\n现举个糊涂窗口综合症的栗子，考虑以下场景：\n接收方的窗口大小是 360 字节，但接收方由于某些原因陷入困境，假设接收方的应用层读取的能力如下：\n\n接收方每接收 3 个字节，应用程序就只能从缓冲区中读取 1 个字节的数据；\n在下一个发送方的 TCP 段到达之前，应用程序还从缓冲区中读取了 40 个额外的字节；\n\n糊涂窗口综合症\n每个过程的窗口大小的变化，在图中都描述的很清楚了，可以发现窗口不断减少了，并且发送的数据都是比较小的了。\n所以，糊涂窗口综合症的现象是可以发生在发送方和接收方：\n\n接收方可以通告一个小的窗口\n而发送方可以发送小数据\n\n于是，要解决糊涂窗口综合症，就解决上面两个问题就可以了\n\n让接收方不通告小窗口给发送方\n让发送方避免发送小数据\n\n\n\n\n\n\n\n\n\n\n怎么让接收方不通告小窗口呢？\n接收方通常的策略如下:\n当「窗口大小」小于 min( MSS，缓存空间&#x2F;2 ) ，也就是小于 MSS 与 1&#x2F;2 缓存大小中的最小值时，就会向发送方通告窗口为 0，也就阻止了发送方再发数据过来。\n等到接收方处理了一些数据后，窗口大小 &gt;&#x3D; MSS，或者接收方缓存空间有一半可以使用，就可以把窗口打开让发送方发送数据过来。\n\n\n\n\n\n\n\n\n\n怎么让发送方避免发送小数据呢？\n发送方通常的策略:\n使用 Nagle 算法，该算法的思路是延时处理，它满足以下两个条件中的一条才可以发送数据：\n\n要等到窗口大小 &gt;&#x3D; MSS 或是 数据大小 &gt;&#x3D; MSS\n收到之前发送数据的 ack 回包\n\n只要没满足上面条件中的一条，发送方一直在囤积数据，直到满足上面的发送条件。\n另外，Nagle 算法默认是打开的，如果对于一些需要小数据包交互的场景的程序，比如，telnet 或 ssh 这样的交互性比较强的程序，则需要关闭 Nagle 算法。\n可以在 Socket 设置 TCP_NODELAY 选项来关闭这个算法（关闭 Nagle 算法没有全局参数，需要根据每个应用自己的特点来关闭）\n12setsockopt(sock_fd, IPPROTO_TCP, TCP_NODELAY, (char *)&amp;value, sizeof(int));\n\n\n拥塞控制\n\n\n\n\n\n\n\n\n为什么要有拥塞控制呀，不是有流量控制了吗？\n前面的流量控制是避免「发送方」的数据填满「接收方」的缓存，但是并不知道网络的中发生了什么。\n一般来说，计算机网络都处在一个共享的环境。因此也有可能会因为其他主机之间的通信使得网络拥堵。\n在网络出现拥堵时，如果继续发送大量数据包，可能会导致数据包时延、丢失等，这时 TCP 就会重传数据，但是一重传就会导致网络的负担更重，于是会导致更大的延迟以及更多的丢包，这个情况就会进入恶性循环被不断地放大….\n所以，TCP 不能忽略网络上发生的事，它被设计成一个无私的协议，当网络发送拥塞时，TCP 会自我牺牲，降低发送的数据量。\n于是，就有了拥塞控制，控制的目的就是避免「发送方」的数据填满整个网络。 \n为了在「发送方」调节所要发送数据的量，定义了一个叫做「拥塞窗口」的概念。\n\n\n\n\n\n\n\n\n\n什么是拥塞窗口？和发送窗口有什么关系呢？\n拥塞窗口 cwnd是发送方维护的一个的状态变量，它会根据网络的拥塞程度动态变化的。\n我们在前面提到过发送窗口 swnd 和接收窗口 rwnd 是约等于的关系，那么由于加入了拥塞窗口的概念后，此时发送窗口的值是swnd &#x3D; min(cwnd, rwnd)，也就是拥塞窗口和接收窗口中的最小值。\n拥塞窗口 cwnd 变化的规则：\n\n只要网络中没有出现拥塞，cwnd 就会增大；\n但网络中出现了拥塞，cwnd 就减少；\n\n\n\n\n\n\n\n\n\n\n那么怎么知道当前网络是否出现了拥塞呢？\n其实只要「发送方」没有在规定时间内接收到 ACK 应答报文，也就是发生了超时重传，就会认为网络出现了用拥塞。 \n\n\n\n\n\n\n\n\n\n拥塞控制有哪些控制算法？\n拥塞控制主要是四个算法：\n\n慢启动\n拥塞避免\n拥塞发生\n快速恢复\n\n慢启动TCP 在刚建立连接完成后，首先是有个慢启动的过程，这个慢启动的意思就是一点一点的提高发送数据包的数量，如果一上来就发大量的数据，这不是给网络添堵吗？\n慢启动的算法记住一个规则就行：当发送方每收到一个 ACK，拥塞窗口 cwnd 的大小就会加 1。 \n这里假定拥塞窗口 cwnd 和发送窗口 swnd 相等，下面举个栗子：\n\n连接建立完成后，一开始初始化 cwnd = 1，表示可以传一个 MSS 大小的数据。\n当收到一个 ACK 确认应答后，cwnd 增加 1，于是一次能够发送 2 个\n当收到 2 个的 ACK 确认应答后， cwnd 增加 2，于是就可以比之前多发2 个，所以这一次能够发送 4 个\n当这 4 个的 ACK 确认到来的时候，每个确认 cwnd 增加 1， 4 个确认 cwnd 增加 4，于是就可以比之前多发 4 个，所以这一次能够发送 8 个。\n\n慢启动算法\n可以看出慢启动算法，发包的个数是指数性的增长。\n\n\n\n\n\n\n\n\n\n那慢启动涨到什么时候是个头呢？\n有一个叫慢启动门限 ssthresh （slow start threshold）状态变量。\n\n当 cwnd &lt; ssthresh 时，使用慢启动算法。\n当 cwnd &gt;&#x3D; ssthresh 时，就会使用「拥塞避免算法」。\n\n拥塞避免算法前面说道，当拥塞窗口 cwnd 「超过」慢启动门限 ssthresh 就会进入拥塞避免算法。\n一般来说 ssthresh 的大小是 65535 字节。\n那么进入拥塞避免算法后，它的规则是：每当收到一个 ACK 时，cwnd 增加 1&#x2F;cwnd。 \n接上前面的慢启动的栗子，现假定 ssthresh 为 8：\n\n当 8 个 ACK 应答确认到来时，每个确认增加 1&#x2F;8，8 个 ACK 确认 cwnd 一共增加 1，于是这一次能够发送 9 个 MSS 大小的数据，变成了线性增长。\n\n拥塞避免\n所以，我们可以发现，拥塞避免算法就是将原本慢启动算法的指数增长变成了线性增长，还是增长阶段，但是增长速度缓慢了一些。\n就这么一直增长着后，网络就会慢慢进入了拥塞的状况了，于是就会出现丢包现象，这时就需要对丢失的数据包进行重传。\n当触发了重传机制，也就进入了「拥塞发生算法」。\n拥塞发生当网络出现拥塞，也就是会发生数据包重传，重传机制主要有两种：\n\n超时重传\n快速重传\n\n这两种使用的拥塞发送算法是不同的，接下来分别来说说。\n\n\n\n\n\n\n\n\n\n发生超时重传的拥塞发生算法\n当发生了「超时重传」，则就会使用拥塞发生算法。\n这个时候，ssthresh 和 cwnd 的值会发生变化：\n\nssthresh 设为 cwnd/2，\ncwnd 重置为 1\n\n拥塞发送 —— 超时重传\n接着，就重新开始慢启动，慢启动是会突然减少数据流的。这真是一旦「超时重传」，马上回到解放前。但是这种方式太激进了，反应也很强烈，会造成网络卡顿。\n就好像本来在秋名山高速漂移着，突然来个紧急刹车，轮胎受得了吗。。。\n\n\n\n\n\n\n\n\n\n发生快速重传的拥塞发生算法\n还有更好的方式，前面我们讲过「快速重传算法」。当接收方发现丢了一个中间包的时候，发送三次前一个包的 ACK，于是发送端就会快速地重传，不必等待超时再重传。\nTCP 认为这种情况不严重，因为大部分没丢，只丢了一小部分，则 ssthresh 和 cwnd 变化如下：\n\ncwnd = cwnd/2 ，也就是设置为原来的一半;\nssthresh = cwnd;\n进入快速恢复算法\n\n快速恢复快速重传和快速恢复算法一般同时使用，快速恢复算法是认为，你还能收到 3 个重复 ACK 说明网络也不那么糟糕，所以没有必要像 RTO 超时那么强烈。\n正如前面所说，进入快速恢复之前，cwnd 和 ssthresh 已被更新了：\n\ncwnd = cwnd/2 ，也就是设置为原来的一半;\nssthresh = cwnd;\n\n然后，进入快速恢复算法如下：\n\n拥塞窗口 cwnd = ssthresh + 3 （ 3 的意思是确认有 3 个数据包被收到了）；\n重传丢失的数据包；\n如果再收到重复的 ACK，那么 cwnd 增加 1；\n如果收到新数据的 ACK 后，把 cwnd 设置为第一步中的 ssthresh 的值，原因是该 ACK 确认了新的数据，说明从 duplicated ACK 时的数据都已收到，该恢复过程已经结束，可以回到恢复之前的状态了，也即再次进入拥塞避免状态；\n\n快速重传和快速恢复\n也就是没有像「超时重传」一夜回到解放前，而是还在比较高的值，后续呈线性增长。\n拥塞算法示意图好了，以上就是拥塞控制的全部内容了，看完后，你再来看下面这张图片，每个过程我相信你都能明白：\nTCP 拥塞控制\n","slug":"NETWORK/30张图解： TCP 重传、滑动窗口、流量控制、拥塞控制","date":"2022-09-29T15:46:08.000Z","categories_index":"TCP,NETWORK","tags_index":"https,ACK,cwnd","author_index":"dandeliono"},{"id":"f6a976669c665c437623667e8d82197c","title":"GitHub Actions 入门教程","content":"GitHub Actions 入门教程前言Github Actions 是 GitHub 推出的持续集成 (Continuous integration，简称 CI) 服务，它提供了配置非常不错的虚拟服务器环境，基于它可以进行构建、测试、打包、部署项目。简单来讲就是将软件开发中的一些流程交给云服务器自动化处理，比方说开发者把代码 push 到 GitHub 后它会自动测试、编译、发布。有了持续集成服务开发者就可以专心于写代码，其它乱七八糟的事情就不用管了，这样可以大大提高开发效率。本篇文章将介绍 GitHub Actions 的基本使用方法。\n基础概念\nworkflow （工作流程）：持续集成一次运行的过程。\njob （任务）：一个 workflow 由一个或多个 job 构成，含义是一次持续集成的运行，可以完成多个任务。\nstep（步骤）：每个 job 由多个 step 构成，一步步完成。\naction （动作）：每个 step 可以依次执行一个或多个命令（action）。\n\n虚拟环境GitHub Actions 为每个任务 (job) 都提供了一个虚拟机来执行，每台虚拟机都有相同的硬件资源：\n\n2-core CPU\n7 GB RAM 内存\n14 GB SSD 硬盘空间\n\n\n\n\n\n\n\n\n\n\n实测硬盘总容量为90G左右，可用空间为30G左右，评测详见：《GitHub Actions 虚拟服务器环境简单评测》\n使用限制：\n\n每个仓库只能同时支持20个 workflow 并行。\n每小时可以调用1000次 GitHub API 。\n每个 job 最多可以执行6个小时。\n免费版的用户最大支持20个 job 并发执行，macOS 最大只支持5个。\n私有仓库每月累计使用时间为2000分钟，超过后$ 0.008&#x2F;分钟，公共仓库则无限制。\n\n操作系统方面可选择 Windows server、Linux、ma­cOS，并预装了大量软件包和工具。\n\n\n\n\n\n\n\n\n\nTIPS：  虽然名称叫持续集成，但当所有任务终止和完成时，虚拟环境内的数据会随之清空，并不会持续。即每个新任务都是一个全新的虚拟环境。\nworkflow 文件GitHub Ac­tions 的配置文件叫做 work­flow 文件（官方中文翻译为 “工作流程文件”），存放在代码仓库的.github/workflows 目录中。work­flow 文件采用 YAML 格式，文件名可以任意取，但是后缀名统一为.yml，比如 p3terx.yml。一个库可以有多个 work­flow 文件，GitHub 只要发现.github/workflows 目录里面有.yml 文件，就会按照文件中所指定的触发条件在符合条件时自动运行该文件中的工作流程。在 Ac­tions 页面可以看到很多种语言的 work­flow 文件的模版，可以用于简单的构建与测试。\n\n\n下面是一个简单的 work­flow 文件示例：\n123456789101112131415161718192021name: Hello Worldon: pushjobs:  my_first_job:    name: My first job    runs-on: ubuntu-latest    steps:    - name: checkout      uses: actions/checkout@master    - name: Run a single-line script      run: echo &quot;Hello World!&quot;  my_second_job:    name: My second job    runs-on: macos-latest    steps:    - name: Run a multi-line script      env:        MY_VAR: Hello World!        MY_NAME: P3TERX      run: | echo $MY_VAR        echo My name is $MY_NAME\n\n示例文件运行截图：\n\n\nworkflow 语法\n\n\n\n\n\n\n\n\nTIPS：  参照上面的示例阅读。\nnamename 字段是 work­flow 的名称。若忽略此字段，则默认会设置为 work­flow 文件名。\nonon 字段指定 work­flow 的触发条件，通常是某些事件，比如示例中的触发事件是 push，即在代码 push 到仓库后被触发。on 字段也可以是事件的数组，多种事件触发，比如在 push 或 pull_request 时触发：\n1on: [push, pull_request]\n\n完整的事件列表，请查看官方文档。下面是一些比较常见的事件：\n点击查看\npush 指定分支触发\n1234on:  push:    branches:      - master\n\npush tag 时触发\n1234on:  push:    tags:    - &#x27;v*&#x27;\n\n定时触发\n12schedule:  - cron: 0 */6 * * *\n\n发布 re­lease 触发\n123on:  release:    types: [published]\n\n仓库被 star 时触发\n123on:  watch:    types: [started]\n\njobsjobs 表示要执行的一项或多项任务。每一项任务必须关联一个 ID (job_id)，比如示例中的 my_first_job 和 my_second_job。job_id 里面的 name 字段是任务的名称。job_id 不能有空格，只能使用数字、英文字母和 - 或_符号，而 name 可以随意，若忽略 name 字段，则默认会设置为 job_id。\n当有多个任务时，可以指定任务的依赖关系，即运行顺序，否则是同时运行。\n123456jobs:  job1:  job2:    needs: job1  job3:    needs: [job1, job2]\n\n上面代码中，job1 必须先于 job2 完成，而 job3 等待 job1 和 job2 的完成才能运行。因此，这个 work­flow 的运行顺序依次为：job1、job2、job3。\nruns-onruns-on 字段指定任务运行所需要的虚拟服务器环境，是必填字段，目前可用的虚拟机如下：\n\n\n\n\n\n\n\n\n\nTIPS：  每个任务的虚拟环境都是独立的。\n\n\n\n虚拟环境\nYAML workflow 标签\n\n\n\nWindows Server 2019\nwindows-latest\n\n\nUbuntu 18.04\nubuntu-latest or ubuntu-18.04\n\n\nUbuntu 16.04\nubuntu-16.04\n\n\nmacOS X Catalina 10.15\nmacos-latest\n\n\nstepssteps 字段指定每个任务的运行步骤，可以包含一个或多个步骤。步骤开头使用 - 符号。每个步骤可以指定以下字段:\n\nname：步骤名称。\nuses：该步骤引用的action或 Docker 镜像。\nrun：该步骤运行的 bash 命令。\nenv：该步骤所需的环境变量。\n\n其中 uses 和 run 是必填字段，每个步骤只能有其一。同样名称也是可以忽略的。\nactionaction 是 GitHub Ac­tions 中的重要组成部分，这点从名称中就可以看出，actions 是 action 的复数形式。它是已经编写好的步骤脚本，存放在 GitHub 仓库中。\n对于初学者来说可以直接引用其它开发者已经写好的 action，可以在官方 action 仓库或者 GitHub Marketplace 去获取。此外 Awesome Actions 这个项目收集了很多非常不错的 action。\n既然 action 是代码仓库，当然就有版本的概念。引用某个具体版本的 action：\n1234steps:  - uses: actions/setup-node@74bc508   - uses: actions/setup-node@v1.2      - uses: actions/setup-node@master \n\n一般来说 action 的开发者会说明建议使用的版本。\n实例：编译 OpenWrt最近一直在研究 Open­Wrt ，那就写个编译 Open­Wrt 的实例吧。\n既然是编译 Open­Wrt 那么 work­flow 的名称就叫 Build OpenWrt\n1name: Build OpenWrt\n\n触发事件我选择了 push 。\n1on: push\n\n我个人常用的 Open­Wrt 编译环境使用的是 Ubuntu 18.04 ，所以任务所使用的虚拟环境也一样。\n1234jobs:  build:    runs-on: ubuntu-18.04    steps:\n\n我并不确定系统中是否有编译所需要依赖，所以第一个步骤是安装依赖软件包。\n123- name: Installation depends  run: | sudo apt-get update    sudo apt-get -y install build-essential asciidoc binutils bzip2 gawk gettext git libncurses5-dev libz-dev patch unzip zlib1g-dev lib32gcc1 libc6-dev-i386 subversion flex uglifyjs git-core gcc-multilib p7zip p7zip-full msmtp libssl-dev texinfo libglib2.0-dev xmlto qemu-utils upx libelf-dev autoconf automake libtool autopoint\n\n由于我使用的是一个空仓库，所以第二个步骤使用 Git 去拉取 Open­Wrt 官方源码。\n12- name: Clone source code  run: | git clone https://github.com/openwrt/openwrt\n\n\n\n\n\n\n\n\n\n\nTIPS：  如果是有源码的仓库，可以引用 actions&#x2F;checkout 这个官方 ac­tion 把源码签出到工作目录中。工作目录也就是在 Ac­tions 中执行命令的根目录，其绝对路径为/home/runner/work/REPO_NAME/REPO_NAME，环境变量为$GITHUB_WORKSPACE。\n然后还需要拉取 feeds ，它是扩展软件包源码，所以需要单独拉取。既然都是拉取源码，所以就都放在一起吧。\n12345- name: Clone source code  run: | git clone https://github.com/openwrt/openwrt    cd openwrt    ./scripts/feeds update -a    ./scripts/feeds install -a\n\n由于这只是尝试，所以第三个步骤就让它生成一个默认的配置文件。由于每个步骤都会回退到工作目录，所以前面还需要加一条进入 buildroot 的命令。\n123- name: Generate config file  run: | cd openwrt    make defconfig\n\n第四个步骤是下载第三方软件包（俗称 dl 库），最后为了防止下载不完整导致编译失败，加了显示不完整文件和删除不完整文件的命令。\n1234- name: Download package  run: | cd openwrt &amp;&amp; make download -j8    find dl -size -1024c -exec ls -l &#123;&#125; \\;    find dl -size -1024c -exec rm -f &#123;&#125; \\;\n\n第五个步骤进入到最重要的开始编译环节，同样是先进入 buildroot，为了能更快的编译，我自信的选择了多线程编译且不显示详细日志。\n123- name: Build  run: | cd openwrt    make -j$(nproc)\n\n最后编译出的二进制文件如何取出来呢？官方有个 ac­tion 叫 upload-artifact ，它可以将虚拟环境中的指定文件打包上传到 Ac­tions 页面。为了方便我选择了上传整个 bin 目录，文件名为 OpenWrt。\n12345- name : Upload artifact  uses: actions/upload-artifact@master  with:    name: OpenWrt    path: openwrt/bin\n\n最后展示一下完整 work­flow 文件：\n1234567891011121314151617181920212223242526272829name: Build OpenWrton: pushjobs:  build:    runs-on: ubuntu-18.04    steps:    - name: Installation depends      run: | sudo apt-get update        sudo apt-get -y install build-essential asciidoc binutils bzip2 gawk gettext git libncurses5-dev libz-dev patch unzip zlib1g-dev lib32gcc1 libc6-dev-i386 subversion flex uglifyjs git-core gcc-multilib p7zip p7zip-full msmtp libssl-dev texinfo libglib2.0-dev xmlto qemu-utils upx libelf-dev autoconf automake libtool autopoint    - name: Clone source code      run: | git clone https://github.com/openwrt/openwrt        cd openwrt        ./scripts/feeds update -a        ./scripts/feeds install -a    - name: Generate config file      run: | cd openwrt        make defconfig    - name: Download package      run: | cd openwrt &amp;&amp; make download -j8        find dl -size -1024c -exec ls -l &#123;&#125; \\;        find dl -size -1024c -exec rm -f &#123;&#125; \\;    - name: Build      run: | cd openwrt        make -j$(nproc)    - name : Upload artifact      uses: actions/upload-artifact@master      with:        name: OpenWrt        path: openwrt/bin\n\n最后 push 到仓库运行。竟然一次成功，话说编译速度是真的快，而且二进制文件也已经上传了，非常完美。\n\n\n","slug":"OTHER/GitHub Actions 入门教程","date":"2022-09-24T18:34:35.000Z","categories_index":"job,OTHER","tags_index":"https,com,GitHub","author_index":"dandeliono"},{"id":"d28bd52a7c1a8649c5def9f8f82b26d2","title":"时间格式化符号","content":"时间格式化符号用于将日期时间格式化为字符串的字符下表列出了您可以在模式中用来格式化或解析与日期时间相关的字符串的字符。 该表下面是一些注意事项，它们对表中的某些示例做了进一步说明。\n\n\n\n符号\n含义\n表示\n示例\n\n\n\na\nAM 或 PM 标记\nText\n输入 am、AM、pm、PM。 输出 AM 或 PM\n\n\nd\n一月中的某一天（1-31）\nNumber\n1、20\n\n\ndd\n一月中的某一天（01-31）\nNumber\n01、31\n\n\nD\n一年中的某一天（1-366）\nNumber\n3、80、100\n\n\nDD\n一年中的某一天（01-366）\nNumber\n03、80、366\n\n\nDDD\n一年中的某一天（001-366）\nNumber\n003\n\n\ne\n一周中的某一天（1-7）1\nNumber\n2\n\n\nEEE\n一周中的某一天1\nText\nTue\n\n\nEEEE\n一周中的某一天1\nText\nTuesday\n\n\nF\n一月中某一周的某一天（1-5）2\nNumber\n2\n\n\nG\n时代\nText\nBC 或 AD\n\n\nh\n用 AM 或 PM 表示的小时（1-12）\nNumber\n6\n\n\nhh\n用 AM 或 PM 表示的小时（01-12）\nNumber\n06\n\n\nH\n24 小时格式的时间（0-23）3\nNumber\n7\n\n\nHH\n24 小时格式的时间（00-23）3\nNumber\n07\n\n\nI\nISO8601 日期&#x2F;时间（最多 yyyy-MM-dd’T’HH:mm:ss. SSSZZZ)4\nText\n2006-10-07T12:06:56.568+01:00\n\n\nIU\nISO8601 日期&#x2F;时间（类似于 I，但是如果时区为 +00:00，那么 ZZZ 输出为“Z”）4\nText\n2006-10-07T12:06:56.568+01:00, 2003-12 -15T15:42:12.000Z\n\n\nk\n24 小时格式的时间（1-24）3\nNumber\n8\n\n\nkk\n24 小时格式的时间（01-24）3\nNumber\n08\n\n\nK\n用 AM 或 PM 表示的小时（0-11）\nNumber\n9\n\n\nKK\n用 AM 或 PM 表示的小时（00-11）\nNumber\n09\n\n\nm\n分\nNumber\n4\n\n\nmm\n分\nNumber\n04\n\n\nM\n月（数值）\nNumber\n5、12\n\n\nMM\n月（数值）\nNumber\n05、12\n\n\nMMM\n月（字母）\nText\nJan、Feb\n\n\nMMMM\n月（字母）\nText\nJanuary、February\n\n\ns\nseconds10\nNumber\n5\n\n\nss\nseconds10\nNumber\n05\n\n\nS\ndecisecond5\nNumber\n7\n\n\nSS\ncentisecond5\nNumber\n70\n\n\nSSS\nmillisecond5\nNumber\n700\n\n\nSSSS\n0.0001 秒5\nNumber\n7000\n\n\nSSSSS\n0.00001 秒5\nNumber\n70000\n\n\nSSSSSS\n0.000001 秒5\nNumber\n700000\n\n\nT\nISO8601 时间（最多为 HH:mm:ss.SSSZZZ）4\nText\n12:06:56.568+01:00\n\n\nTU\nISO8601 时间（类似于 T，但是 +00:00 时区用“Z”替换）4\nText\n12:06:56.568+01:00、15:42:12.000Z\n\n\nw\n一年中的某一周6\nNumber\n7、53\n\n\nww\n一年中的某一周6\nNumber\n07、53\n\n\nW\n一月中的某一周7\nNumber\n2\n\n\nyy\nyear8\nNumber\n06\n\n\nyyyy\nyear8\nNumber\n2006\n\n\nYY\n年：仅与年中的周结合使用6\nNumber\n06\n\n\nYYYY\n年：仅与年中的周结合使用6\nNumber\n2006\n\n\nzzz\n时区（简称）9\nText\nEST\n\n\nzzzz\n时区（全称）\nText\n东部标准时间\n\n\nZ\n时区（+&#x2F;-n）\nText\n+3\n\n\nZZ\n时区（+&#x2F;-nn）\nText\n+03\n\n\nZZZ\n时区（+&#x2F;-nn:nn）\nText\n+03:00\n\n\nZZZU\n时区（如同 ZZZ，“+00:00”由“Z”替换）\nText\n+03:00、Z\n\n\nZZZZ\n时区（GMT+&#x2F;-nn:nn）\nText\nGMT+03:00\n\n\nZZZZZ\n时区（如同 ZZZ，但是无冒号）（+&#x2F;-nnnn）\nText\n+0300\n\n\n‘\n文本的转义\n\n‘User text’\n\n\n“\n（两个单引号）转义文本中的单引号\n\n‘o”clock’\n\n\n日期时间对象的表示取决于指定的符号。\n\n文本：如果指定四个或四个以上的符号，将会显示完整的格式。 如果您指定的符号小于四个，将显示简短或缩略格式（如果存在）。 例如：EEEE生成Monday,EEE生成Mon.\n数字：数值型日期时间部分的字符数必须在相应的格式化符号的限制范围内。 请重复使用同一符号以指定所需的最小位数。 允许的最大位数是特定符号的上限。 例如，月份中的第几天的上限为 31; 因此，格式字符串d允许对值 2 或 21 进行解析，但不允许对值 32 或 210 进行解析。 在输出时，使用零将数字填充到指定长度。 年是特殊情况；见附注 8。 分数秒也是特殊情况；请参阅注释 5。\n容错日期时间检查：解析器将范围外的日期时间值转换为相应的范围内的值。 例如，将日期 2005-05-32 转换为 2005-06-01。 日期时间的输出始终遵循符号计数。 例如，以下项的格式化字符串:yyyy-MM-dd(其中 “-“ 是字段分隔符) 允许对一个或多个字符进行语法分析MM和dd。此转换允许输入无效的日期 (例如 2006-01-123 和 2006-011-12)。 值 2006-01-123 编写为日期 2006-05-03，而值 2006-011-12 编写为日期 2006-11-12。 时区格式符号 Z 出现的数目仅适用于输出 dateTime 格式。 空格被跳过。\n物理类型：如果将 dateTime 对象的物理类型属性指定为压缩十进制，那么只有表示数字的模式格式化符号有效；即，表中“表示”列中包含“数字”的那些符号有效。 在格式模式中不允许使用其他字符。 例如，yyyyMMdd 有效，但是 yyyyMMMdd 无效，这是因为 MM 是月份的数字表示，而 MMM 是月份的文本表示。\n模式中的任何不在 [‘a’..’z’] 和 [‘A’..’Z’] 范围内的字符被视为引用的文本。 例如，冒号 (:)、逗号 (,)、句点 (.)、数字符（井号 #）、@ 符以及空格等字符将出现在所生成的时间文本中，即使这些字符未括在单引号内也是如此。\n您可能创建产生不可预测结果的格式字符串；因此必须谨慎使用这些符号。 例如，如果您指定dMyyyy，是无法区分日，月和年的。dMyyyy指示集成节点至少有一个字符表示日期，至少有一个字符表示月份，有四个字符表示年份。 因此3112006可能解释为 2006 年 11 月 3 日或 2006 年 1 月 31 日。\n\n注: 以下注释适用于上一张表。\n\n“一周中的某一天”字段是一周中的数字偏移量并且会根据物理消息集属性一周中的第一天而改变。 例如，如果将物理消息集属性一周中的第一天设置为星期一，那么一周中的第三天就是星期三。\n\n12th 2006 年 7 月是 7 月的第二个星期三，可以表示为2006 July Wednesday 2使用格式字符串yyyy MMMM EEEE F。请注意，此格式不表示 2006 年 7 月 2 日的第 2 周的星期三，即 5th 2006 年 7 月; 此格式的字符串为yyyy MMMM EEEE W.\n\n如果使用冲突的 am&#x2F;pm 字段指定，24 小时字段可能产品有歧义的时间。\n\n请参阅 ISO8601， I 和 T 日期时间标记。\n\n小数秒由大写 S 表示。 长度必须与输入中格式符号数目隐式匹配。 格式字符串ss SSS或ss.SSS例如，表示秒数和毫秒数。 但是，格式字符串ss.sss表示重复字段 (以秒为单位); 周期 (.) 后的值将作为秒字段，而不是作为小数秒。 输出将截断到指定长度。\n\n一年的开始通常是在一周的某天。 如果这一周中的天数小于一年中第一个星期的天数物理消息集属性所指定的值，就认为这一周是上一年的最后一周；因此第一周从新年中的某天开始。 否则，就认为这一周是新年的第一周；在这种情况中，第一周在新年前的之几天开始。 例如， 2004 年第 1 周的星期一 (2004 年第 1 周的星期一，其中每年第 1 周 4 天数 &#x3D; 4 ，第 1 周的第 1 天 &#x3D; 星期一) 使用格式字符串YYYY ww EEEE实际上是 29th 2003 年 12 月。 如果您使用Y，一周中的第几天 (E) 和一年中的周 (w) 在必要时进行调整，以指示日期在上一年。\n如果使用小写字母y符号，那么不会进行调整，并且可能会对年底前后的日期产生不可预测的结果。 例如，如果将字符串 2002 01 Monday 格式化：\n\n2002 年第 1 周的星期一 (使用格式字符串)YYYY ww EEEE正确解释为31st December 2001\n2002 年第 1 周的星期一 (使用格式字符串)yyyy ww EEEE被错误地解释为30th December 2002\n\n使用Y与w仅当您未指定w，使用y.\n\n一月中的第一周和最后一周可能包含相邻月份的天数。 例如，周一 31st 2006 年 7 月可以表示为 _2006 年 8 月第一周的星期一_，即2006 08 1 Monday使用格式字符串yyyy MM W EEEE.\n\n年是作为特殊情况处理的：\n\n在输出时，如果计数为y是2，年份将截断为 2 位数字。 例如，如果yyyy生成2006,yy生成06.\n在输入时，对于 2 位数年份，将使用物理消息集属性世纪的开始采用 2 位数年份来确定世纪。 例如，如果将世纪的开始采用 2 位数年份设置为 53，97 年将是 1997，52 年是 2052，而 53 年是 1953。\n\n\n使用zzz选项可能有不明确的结果。 例如：BST可以解释为Bangladesh Standard Time或British Summer Time. 出于兼容性原因， IBM® Integration Bus 使用先前的解释。\n要避免这些问题，请使用zzzz具有明确定义的名称的选项; 例如，欧洲&#x2F;伦敦，亚洲&#x2F;达卡或美洲&#x2F;洛杉矶。\n\n秒s &amp;ss，必须在范围 0-59 内。 如果您需要构造一个 TIMESTAMP 来表示闰秒期间的时间，且要创建或强制转换的值要使用值 60 作为秒数，那么您必须在 ESQL 代码中处理此情况。 产品中的 CURRENT_ datetime 函数（例如 CURRENT_TIME）从不会生成秒值超出 0-59 范围的时间。\n\n\n","slug":"JAVA/时间格式化符号","date":"2022-09-20T17:06:36.000Z","categories_index":"Number,JAVA","tags_index":"Text,例如,时区","author_index":"dandeliono"},{"id":"7c8ff8b55627526d1305ad59c430dd58","title":"nginx--❤️图解及代码实现正向代理、反向代理及负载均衡","content":"nginx–❤️图解及代码实现正向代理、反向代理及负载均衡什么是nginx？\n\n\n\n\n\n\n\n\n\n【百度百科】Nginx (engine x) 是一个高性能的HTTP和反向代理web服务器…\n除了反向代理，nginx还支持正向代理、负载均衡以及基于SSL安全证书的HTTPS访问等功能特性~本文主要是介绍是如何配置nginx正向代理、反向代理及负载均衡，进入正文~\n什么是代理？来一张图了解下\n代理又分为正向代理和反向代理。\n1.1 什么是正向代理？先来看张图~\n【再举个栗子】 \n\n\n\n\n\n\n\n\n\n某同学喜欢面向搜索引擎编程，想通过 百度 搜索引擎查找一些学习资料，但是有些网站直接访问可能不太安全，会暴露自己的IP，同学比较苦恼，想着怎样才能使用百度 搜索自己想要的学习资料，又不会暴露自己的IP在网站上呢？这时我告诉该同学，我呢手上刚好有一台代理服务器，这台代理服务器通过nginx配置了正向代理转发http和https请求，你呢，只需要在自己的Windows本地电脑的网关配置一下这台代理服务器的IP和端口号，就能正常通过代理服务器访问到百度 并搜索相关的学习资料了，还不会暴露自己真实的IP~\n同学非常兴奋，说自己了解过nginx，但是啥是正向代理啊？\n\n\n\n\n\n\n\n\n\n正向代理，指的是通过代理服务器 代理浏览器/客户端去重定向请求访问到目标服务器 的一种代理服务。正向代理服务的特点是代理服务器 代理的对象是浏览器/客户端，也就是对于目标服务器 来说浏览器/客户端是隐藏的。\n同学觉得有点意思，于是又问，那nginx如何实现正向代理啊？\n1.2 如何实现nginx正向代理？（1）正向代理转发https说明\n在实现nginx正向代理之前，先说明一下，现在的网站基本上都是https，因此要实现nginx正向代理转发请求，除了要配置转发http80端口的请求，还有配置https443端口的请求~正向代理转发http请求很简单，不过正向代理转发https请求稍微有一点麻烦，目前网上的教程大多都是如下这样配置的（也不知道他们验证过没。。。）：\n一开始我也像上面那样配置，虽然http请求正常转发了，但是发现https并没有转成功，而是报错HTTP/1.1 400 Bad Request~后来才了解到，nginx官方并不支持直接转发https请求，但是阿里的一个大佬对nginx拓展了一个ngx_http_proxy_connect_module模块，并且在github上开源了 https://github.com/chobits/ngx_http_proxy_connect_module不过维护的ngx_http_proxy_connect_module模块的补丁也是有nginx版本限制的（目前维护了1.4.x~1.19.x版本），如README.md说明：\n如果你安装的nginx版本不在1.4.x~1.19.x这个范围内，那无法支持正向代理转发https请求。\n（2）安装nginx如果已安装nginx（可跳过），这里以1.9.2版本为例，使用root用户进行安装：\n12345$ cd /usr/nginx$ wget http://nginx.org/download/nginx-1.9.2.tar.gz$ tar -xzvf nginx-1.9.2.tar.gz$ cd /usr/nginx/nginx-1.9.2$ make &amp;&amp; make install \n\n这里安装nginx通过install进行编译安装，编译后默认安装目录为/usr/local/nginx，后续配置新模块ngx_http_proxy_connect_module还需要重新install编译一次~\n（3）下载新模块GitHub上下载ngx_http_proxy_connect_module的zip压缩包源码：https://github.com/chobits/ngx_http_proxy_connect_module\n（4）解压新模块源码将新模块ngx_http_proxy_connect_module源码压缩包上传到服务器/usr/nginx目录，并解压并重命名\n12345$ mkdir -p /usr/nginx$ cd /usr/nginx$ /usr/nginx$ unzip ngx_http_proxy_connect_module-master.zip$ mv ngx_http_proxy_connect_module-master ngx_http_proxy_connect_module \n\n（5）添加新模块到nginx使用root用户进入nginx的资源目录/usr/nginx/nginx-1.9.2，给nginx添加新模块ngx_http_proxy_connect_module和并重新编译nginx\n1234$ /usr/nginx/nginx-1.9.2$ patch -p1 &lt; /usr/nginx/ngx_http_proxy_connect_module/patch/proxy_connect.patch$ ./configure --add-module=/usr/nginx/ngx_http_proxy_connect_module$ make &amp;&amp; make install \n\n–说明：这里nginx-1.9.2版本对应proxy_connect.patch补丁，其他版本相关补丁支持版本，详情见GitHub~ https://github.com/chobits/ngx_http_proxy_connect_module\n使用root用户安装和编译新模块完成后，如果你希望后续不使用root用户运维，可以授权安装目录/usr/local/nginx给nginx用户或者其他普通用户~\n123chown -R nginx:nginx /usr/local/nginxchown root:root /usr/local/nginx/sbin/nginxchmod +s /usr/local/nginx/sbin/nginx \n\n–说明：二进制/usr/local/nginx/sbin/nginx文件需要重新归属为root用户，并且权限位添加s权限（权限位+s的二进制文件属于是管道文件，也就是普通用户也可执行该二进制文件，执行后产生的进程属于该文件的权限所有者，这里文件所有者是root）\n（6）修改nginx的配置修改nginx的配置分别添加http和https的server，其他配置保持不变~\n1vi /usr/local/nginx/conf/nginx.conf \n\n这两个server主要配置是对DNS解析和proxy_pass代理进行：\n1234567891011121314151617181920212223242526272829303132333435363738 server &#123;        resolver 114.114.114.114;        listen 80;        server_name  localhost;\t        location / &#123;        proxy_pass                 http://$host$request_uri;        proxy_set_header           HOST $host;        proxy_buffers              256 4k;        proxy_max_temp_file_size   0k;        proxy_connect_timeout      30;        proxy_send_timeout         60;        proxy_read_timeout         60;        proxy_next_upstream error  timeout invalid_header http_502;    &#125;&#125;server &#123;        resolver 114.114.114.114;        listen 443;\t        proxy_connect;    proxy_connect_allow            443 563;    proxy_connect_connect_timeout  10s;    proxy_connect_read_timeout     10s;    proxy_connect_send_timeout     10s;    location / &#123;        proxy_pass http://$host;        proxy_set_header Host $host;    &#125;&#125; \n\n– DNS说明：（国内外）目前比较主流的DNS：（国外）谷歌：8.8.8.8 developers.google.com（国外）OpenDNS：208.67.222.222 signup.opendns.com（国内）114：114.114.114.114 www.114dns.com（国内）腾讯：119.29.29.29 www.dnspod.cn（国内）阿里：223.5.5.5 alidns.com（国内）百度：180.76.76.76 dudns.baidu.com\n（7）检查和刷新nginx配置\n12/usr/local/nginx/sbin/nginx -t/usr/local/nginx/sbin/nginx -s reload \n\n1.3 客户端访问（验证正向代理）客户端要代理访问目标网站示例对象：http://www.baidu.com 和https://www.baidu.com\n（1）客户端–Windows浏览器访问首先在本地电脑的IE浏览器设置代理服务器和端口：IE-&gt;右上角 -&gt;工具 -&gt;Internet选项-&gt;连接-&gt;局域网（LAN）设置 -&gt;配置代理IP和端口\n浏览器访问http://www.baidu.com/ 和https://www.baidu.com/\n实时查看nginx日志\n1tail -f /usr/local/nginx/logs/access.log \n\n通过实时查看nginx的访问日志，可以看到Windows下设置代理IP和端口后，本地电脑访问的所有网页会通过代理服务器进行访问网页，实现了正向代理的功能，并且隐藏了用户自己真实的IP~\n（2）客户端–Linux代理访问Linux下也可以验证代理服务器是否能正常代理转发http和https请求~\n12curl http://www.baidu.com/ -v -x 127.0.0.1:80curl https://www.baidu.com/ -v -x 127.0.0.1:443 \n\nnginx正向代理转发https成功：\n2.1 什么是反向代理？先来一张图了解下~\n相比于正向代理，反向代理的定义如下：\n\n\n\n\n\n\n\n\n\n反向代理，指的是浏览器/客户端并不知道自己要访问具体哪台目标服务器，只知道去访问代理服务器 ，代理服务器再通过反向代理 +负载均衡实现请求分发到应用服务器的一种代理服务。反向代理服务的特点是代理服务器 代理的对象是应用服务器，也就是对于浏览器/客户端 来说应用服务器是隐藏的。\n2.2 nginx实现反向代理（1）修改nginx配置修改nginx配置vi /usr/local/nginx/conf/nginx.conf 在http模块中配置如下：\nnginx反向代理都是结合负载均衡来实现的，这里先直接提供反向代理+负载均衡的配置，负载均衡后一章节介绍~\n12345678910111213141516171819 upstream reverseProxyServer&#123;        server 应用服务器A的IP:8080 weight=10 max_fails=2 fail_timeout=10s;         server 应用服务器B的IP:8080 weight=5 fail_timeout=10s max_fails=2;        server 应用服务器C的IP:8080 weight=5 fail_timeout=10s max_fails=2;&#125;server &#123;        listen 80;        server_name  localhost;        location /appname &#123;        proxy_pass http://reverseProxyServer;    &#125;&#125; \n\n（2）检查和刷新nginx配置\n12/usr/local/nginx/sbin/nginx -t/usr/local/nginx/sbin/nginx -s reload \n\n（3）浏览器访问代理服务器部署了tomcat应用，访问tomcat静态页面验证一波~http://代理服务器IP:8080/appname/ReverseProxy1.html\n3.1 什么是负载均衡？再来看下前面介绍反向代理的这张图~\n可以看到负载均衡（Load Balance）的主要作用是采用了负载均衡算法将请求分发到集群模式中的应用服务器上，这样即使某个后台应用服务器挂了，其他应用服务器还可以正常接收请求，实现了高可用，并且集群模式模式下的应用服务器支持纵向扩展，可以应对业务快速增长带来的高并发应用场景~\n3.2 负载均衡算法有哪些？负载均衡算法常用的有轮询、权重和ip_hash算法，默认是轮询算法~\n（1）基于轮询的算法原理是每一个请求按时间顺序逐一被分发到不同的应用服务器，如果接收请求的应用服务器挂了，并且请求超过最大失败次数max_fails（1次），则在失效时间fail_timeout（10秒）内不会再转发请求到该节点~\n1234upstream defaultReverseProxyServer&#123;    server 192.168.0.1:8080;     server 192.168.0.2:8080; &#125; \n\n（2）基于权重的算法原理是每一个请求按权重被分发到不同的应用服务器，同样，如果接收请求的应用服务器挂了，并且请求超过最大失败次数max_fails（默认1次或可设置N次），则在失效时间fail_timeout（默认10秒，可设置N秒）内，不会再转发请求到该节点~\n1234upstream weightReverseProxyServer&#123;    server 192.168.0.1:8080 weight=10 max_fails=2 fail_timeout=5s;    server 192.168.0.2:8080 weight=5 max_fails=2 fail_timeout=5s;&#125; \n\n（3）基于ip_hash的算法原理是每一个请求按用户访问IP的哈希结果分配，如果请求来自同一个用户IP则固定这台IP访问一台应用服务器，该算法可以有效解决动态网页中存在的session共享问题。\n12345upstream ipHashReverseProxyServer&#123;    ip_hash;    server 192.168.0.1:8080;    server 192.168.0.2:8080;&#125; \n\n一般使用的是基于权重的算法，因为现在很多情况下都是集群部署，而且集群下的各个服务器资源大多都是不均匀的，资源高的则分配权重高一些，资源低的则分配权重低一些，这种情况使用基于权重的负载均衡算法，可以更高效的利用资源和提高并发处理能力~\n","slug":"MIDDLEWARE/nginx--❤️图解及代码实现正向代理、反向代理及负载均衡","date":"2022-09-15T10:07:03.000Z","categories_index":"nginx,MIDDLEWARE","tags_index":"https,com,http","author_index":"dandeliono"},{"id":"52d188bef2ce89a10077bd22ef3e8593","title":"在Ubuntu 18.04上安装和配置Squid代理","content":"在Ubuntu 18.04上安装和配置Squid代理Squid是功能齐全的缓存代理，支持流行的网络协议，例如HTTP，HTTPS，FTP等。 Squid主要用于通过缓存重复的请求，过滤Web流量和访问受地理限制的内容来提高Web服务器的性能。\n本教程将说明如何在Ubuntu 18.04上设置Squid代理以及配置Firefox和Google Chrome浏览器以使用Squid的步骤。包括使用apt命令安装Squid，配置Squid默认监听端口和ACL访问控制列表，Squid的身份认证，配置UFW防火墙以允许来自远程的连接，配置Chrome&#x2F;Firefox浏览器以使用Squid代理。\n在Ubuntu上安装SquidSquid包含在默认的Ubuntu 18.04存储库中。可使用apt命令安装Squid，请使用具有sudo权限的用户运行以下命令：\n12sudo apt updatesudo apt install squid\n\n安装完成后，Squid服务将自动启动。要验证安装并检查Squid服务是否正在运行，请键入以下命令，以显示服务状态：\n1sudo systemctl status squid\n\n1234● squid.service - LSB: Squid HTTP Proxy version 3.x   Loaded: loaded (/etc/init.d/squid; generated)   Active: active (running) since Thu 2019-06-27 11:45:17 UTC...\n\n配置Squid可以通过编辑/etc/squid/squid.conf文件来配置Squid。您还可以使用独立配置选项的文件，然后通过include指令将其包含在内。配置文件包含了描述每个配置选项注释。进行任何更改之前，最好使用cp命令备份原始配置文件：\n1sudo cp /etc/squid/squid.conf&#123;,.orginal&#125;\n\n要编辑/etc/squid/squid.conf文件，请在文本编辑器中打开文件：\n1sudo vim /etc/squid/squid.conf\n\n默认情况下，Squid在所有网络接口上监听端口3128。如果要更改端口并设置监听的接口&#x2F;网卡，请找到以http_port开头的行，并指定接口IP地址和新端口。如果未指定接口，Squid将在所有接口上侦听。\n12# Squid normally listens to port 3128http_port IP_ADDR:PORT\n\n&#x2F;etc&#x2F;squid&#x2F;squid.conf\n对于大多数用户来说，在所有接口和默认端口上运行Squid就可以。在Squid中，您可以使用访问控制列表ACL控制客户端如何访问Web资源。\n默认情况下，Squid仅允许从本地主机访问。如果所有将使用代理的客户端都具有静态IP地址，则可以创建一个ACL，其中将包含允许的IP。我们将创建一个新的专用文件来保存IP，而不是在主配置文件中添加IP地址：\n1192.168.33.1 \n\n&#x2F;etc&#x2F;squid&#x2F;allowed_ips.txt\n完成后，打开主配置文件并创建一个名为allowed_ips的新ACL，并允许使用http_access指令允许访问该ACL：\n123456 acl allowed_ips  src &quot;/etc/squid/allowed_ips.txt&quot;http_access allow localhosthttp_access allow allowed_ipshttp_access deny all\n\n&#x2F;etc&#x2F;squid&#x2F;squid.conf\nhttp_access规则的顺序很重要。确保在http_access deny all之前添加行http_access。\nhttp_access指令的工作方式与防火墙规则类似。 Squid从上到下读取规则，当一个规则匹配时，下面的规则将不进行处理。无论何时对配置文件进行更改，都需要重新启动Squid服务以使更改生效：\n1sudo systemctl restart squid\n\nSquid认证Squid可以使用不同的后端，包括对已认证的Samba，LDAP和HTTP基本身份验证。在本教程中，我们将配置Squid以使用基本身份验证。这是HTTP协议中内置的一种简单的身份验证方法。\n我们将使用openssl生成密码，并使用[tee](https://www.myfreax.com/linux-tee-command/)命令将username:password对追加到/etc/squid/htpasswd文件，如下所示：\n1printf &quot;USERNAME:$(openssl passwd -crypt PASSWORD)\\n&quot; | sudo tee -a /etc/squid/htpasswd\n\n让我们创建一个名为josh的用户，其密码设置为Sz$Zdg69：\n1printf &quot;josh:$(openssl passwd -crypt &#x27;Sz$Zdg69&#x27;)\\n&quot; | sudo tee -a /etc/squid/htpasswd\n\n1josh:RrvgO7NxY86VM\n\n现在已创建用户，下一步是启用HTTP基本身份验证并包含htpasswd文件。使用vim打开文件/etc/squid/squid.conf并添加以下内容：\n12345678910# ...auth_param basic program /usr/lib/squid3/basic_ncsa_auth /etc/squid/htpasswdauth_param basic realm proxyacl authenticated proxy_auth REQUIRED# ...#http_access allow localnethttp_access allow localhosthttp_access allow authenticated# And finally deny all other access to this proxyhttp_access deny all\n\n这将创建一个名为authenticated的新ACL，允许访问经过身份验证的用户。重新启动Squid服务：\n1sudo systemctl restart squid\n\n配置防火墙假设您正在使用UFW防火墙管理工具，则需要打开Squid端口。为此，请启用Squid配置文件，其中包含了默认Squid端口的规则。\n1sudo ufw allow &#x27;Squid&#x27;\n\n要验证状态类型：\n1sudo ufw status\n\n输出将类似于以下内容：\n12345678Status: activeTo                         Action      From--                         ------      ----22/tcp                     ALLOW       AnywhereSquid                      ALLOW       Anywhere            22/tcp (v6)                ALLOW       Anywhere (v6)             Squid (v6)                 ALLOW       Anywhere (v6)  \n\n如果Squid正在另一个非默认端口上运行，例如8888，则可以使用以下命令允许该端口上的流量：sudo ufw allow 8888/tcp。\n配置浏览器以使用Squid代理现在您已经设置了Squid，最后一步是配置您喜欢的浏览器以使用Squid代理。\nFirefox在右上角，单击汉堡包图标☰以打开Firefox的菜单，单击⚙ Preferences链接。向下滚动到Network Settings部分，然后单击Settings...按钮。\n将打开一个新窗口。选择Manual proxy configuration单选按钮，在HTTP Host字段中输入您的Squid服务器IP地址，在Port字段中输入3128。选中Use this proxy server for all protocols复选框。单击OK按钮保存设置。\n至此，您的Firefox已配置完毕，您可以通过Squid代理浏览Internet。要验证它，请打开google.com，键入what is my ip，您应该会看到自己的Squid服务器IP地址。\n要恢复为默认设置，请转到Network Settings，选择Use system proxy settings单选按钮并保存设置。有几个插件也可以帮助您配置Firefox的代理设置，例如 FoxyProxy。\nGoogle Chrome浏览器Google Chrome默认使用系统代理设置。您也可以使用SwitchyOmega之类的插件，也可以从命令行启动Chrome网络浏览器。无需更改操作系统代理设置。要使用新的配置文件启动Chrome并连接到Squid服务器，请使用以下命令：\nLinux\n123/usr/bin/google-chrome \\    --user-data-dir=&quot;$HOME/proxy-profile&quot; \\    --proxy-server=&quot;http://SQUID_IP:3128&quot;\n\nmacOS\n123&quot;/Applications/Google Chrome.app/Contents/MacOS/Google Chrome&quot; \\    --user-data-dir=&quot;$HOME/proxy-profile&quot; \\    --proxy-server=&quot;http://SQUID_IP:3128&quot;\n\nWindows\n123&quot;C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe&quot; ^    --user-data-dir=&quot;%USERPROFILE%\\proxy-profile&quot; ^    --proxy-server=&quot;http://SQUID_IP:3128&quot;\n\n如果配置文件不存在，则会自动创建。要确认代理服务器正常工作，请打开google.com，然后键入what is my ip。浏览器中显示的IP应该是Squid服务器的IP地址。\n结论您已了解如何在Ubuntu 18.04上安装Squid并配置浏览器以使用它。Squid是最流行的代理缓存服务器之一。它提高了Web服务器的速度，可以帮助您限制用户访问Internet。如果您遇到问题或有反馈，请在下面发表评论。\n","slug":"LINUX/在Ubuntu 18.04上安装和配置Squid代理","date":"2022-09-14T18:00:06.000Z","categories_index":"Squid,LINUX","tags_index":"https,com,squid","author_index":"dandeliono"},{"id":"629c251c0c483276f62c67636b6a5d8b","title":"Linux配置http代理","content":"Linux配置http代理我们可以在很多地方设置Proxy，生产环境中最常见的还是在应用中直接调用一些库来为应用设置Proxy，但在测试Proxy的时候，就需要用到系统全局的Proxy设置以及部分应用的Proxy常见的Proxy一般就两种，Socks和HTTP，HTTP是一种七层代理，而Socks则是封装过后的四层代理顾名思义，HTTP只能代理HTTP协议的流量，Socks则只接受Socks封装过的流量，对于大部分Web应用，我们会部署HTTP代理，因为如果经过了Socks封装，前置在应用和代理服务器之间的防火墙就无法看到URL了(你可能会奇怪防火墙为什么不是在最外面，事实上这里的防火墙专用于URL过滤，放在代理服务器外侧也可以，但如此一来，防火墙将看不到真正的源主机的地址，而只能看到代理服务器的地址了)\n而对于无法通过HTTP代理的协议，比如SSH和SFTP，就需要用到Socks代理了Windows以及部分浏览器(例如Firefox)可以设置Socks代理，Linux全局下似乎无法设置，但是一些其他方法可以不安装Socks客户端的情况下使用Socks代理在&#x2F;etc&#x2F;profile下增加下列配置即可设置HTTP&#x2F;HTTPS&#x2F;FTP代理\n如果要全局用户使用应用于所有的Shell，就需要修改 &#x2F;etc&#x2F;profile 文件\n代理服务开启\n设置全局代理，需要编辑profile文件\n12vi /etc/profile\n\n文末添加以下代理配置，参考代理是否需要用户名密码\n123456789export http_proxy=http://proxy_ip:protexport https_proxy=https://proxy_ip:prot export http_proxy=http://username:password@proxy_ip:protexport https_proxy=https://username:password@proxy_ip:portexport ftp_proxy=http://username:password@proxyserver:port\n\n如\n12345678910export http_proxy=http://192.168.64.1:1080export https_proxy=http://192.168.64.1:1080http_proxy=proxy.abc.com:8080  https_proxy=$http_proxy  ftp_proxy=user:password@proxy.abc.com:8080  no_proxy=*.abc.com,10.*.*.*,192.168.*.*,*.local,localhost,127.0.0.1  export http_proxy https_proxy ftp_proxy no_proxy  \n\n其中：\n\nhttp_proxy：http协议使用代理服务器地址；\nhttps_proxy：https协议使用安全代理地址；\nftp_proxy：ftp协议使用代理服务器地址；\nuser：代理使用的用户名；\npassword：代理使用用户名的密码；\nproxy.abc.com：代理地址，可以是IP，也可以是域名；\n8080：使用的端口；\nno_proxy：不使用代理的主机或IP。\n\n\n备注： \n\n\n\n环境变量\n描述\n值示例\n\n\n\nhttp_proxy\n为http变量设置代理；默认不填开头以http协议传输\n10.0.0.51:8080\n\n\nuser:&#x70;&#97;&#115;&#115;&#64;&#49;&#48;&#46;&#x30;&#x2e;&#x30;&#x2e;&#49;&#48;:8080\n\n\n\n\nsocks4:&#x2F;&#x2F;10.0.0.51:1080\n\n\n\n\nsocks5:&#x2F;&#x2F;192.168.1.1:1080\n\n\n\n\nhttps_proxy\n为https变量设置代理；\n同上\n\n\nftp_proxy\n为ftp变量设置代理；\n同上\n\n\nall_proxy\n全部变量设置代理，设置了这个时候上面的不用设置\n同上\n\n\nno_proxy\n无需代理的主机或域名；\n\n\n\n可以使用通配符；\n\n\n\n\n多个时使用“,”号分隔；\n*.aiezu.com,10.*.*.*,\n\n\n\n192.168.*.*,*.local,localhost,127.0.0.1\n\n\n\n\n1、在&#x2F;etc&#x2F;profile文件2、在&#x2F;.bashrc3、在&#x2F;.zshrc4、在&#x2F;etc&#x2F;profile.d&#x2F;文件夹下新建一个文件xxx.sh\n写入如下配置：\n123456export proxy=&quot;http://192.168.5.14:8118&quot;export http_proxy=$proxyexport https_proxy=$proxyexport ftp_proxy=$proxyexport no_proxy=&quot;localhost, 127.0.0.1, ::1&quot;\n\n而对于要取消设置可以使用如下命令，其实也就是取消环境变量的设置：\n12345unset http_proxyunset https_proxyunset ftp_proxyunset no_proxy\n\n此方法只适合配置http代理，使用socket代理上网的另有其他配置方法。\n生效配置文件\n123source /etc/profile. /etc/profile\n\n查看当前已设置代理\n123echo $http_proxyecho $https_proxy\n\n测试\n因为\n要配置subversion的代理服务器，需要修改$HOME&#x2F;.subversion&#x2F;servers文件，在此文件的[global]段加上：\n12345http-proxy-host = 192.168.1.1http-proxy-port = 8080 http-proxy-username = easwyhttp-proxy-password = 123456 \n\n现在svn就可以使用代理服务器访问版本库了。\n\n\n\n\n\n\n\n\n\n针对yum配置走代理：经过测试其实只要设置上面的变量之后已经可以走代理了，但如果要单独设置，可以设置如下文件的变量：\n12echo &quot;proxy=http://127.0.0.1:8080/&quot; &gt;&gt; /etc/yum.conf\n如果想让CentOS中的yum可以通过代理服务器更新程序，则需要修改文件&#x2F;etc&#x2F;yum.conf，在此文件中加上：\n12proxy=http://easwy:123456@192.168.1.1:8080\n","slug":"LINUX/Linux配置http代理","date":"2022-09-14T17:44:07.000Z","categories_index":"proxy,LINUX","tags_index":"HTTP,Socks,http","author_index":"dandeliono"},{"id":"c870bba49f8ac447777617e20cdfda53","title":"MySQL Binlog日志配置","content":"MySQL Binlog日志配置Binlog 记录模式Redo Log 是属于InnoDB引擎所特有的日志，而MySQL Server也有自己的日志，即 Binary log（二进制日志），简称Binlog。\nBinlog是记录所有数据库表结构变更以及表数据修改的二进制日志，不会记录SELECT和SHOW这类操作。\nBinlog日志是以事件形式记录，还包含语句所执行的消耗时间。\n开启Binlog日志有以下两个最重要的使用场景。\n12主从复制：在主库中开启Binlog功能，这样主库就可以把Binlog传递给从库，从库拿到Binlog后实现数据恢复达到主从数据一致性。 数据恢复：通过mysqlbinlog工具来恢复数据。\n\nBinlog文件名默认为“主机名_binlog-序列号”格式，例如oak_binlog-000001，也可以在配置文件中指定名称。\n文件记录模式有STATEMENT、ROW和MIXED三种，具体含义如下。\n\nROW（row-based replication, RBR）\n\n日志中会记录每一行数据被修改的情况，然后在slave端对相同的数据进行修改。\n123优点：能清楚记录每一个行数据的修改细节，能完全实现主从数据同步和数据的恢复。缺点：批量操作，会产生大量的日志，尤其是alter table会让日志暴涨。\n\n\nSTATMENT（statement-based replication, SBR）\n\n每一条被修改数据的SQL都会记录到master的Binlog中，slave在复制的时候SQL进程会解析成和原来master端执行过的相同的SQL再次执行。简称SQL语句复制。\n123优点：日志量小，减少磁盘IO，提升存储和恢复速度缺点：在某些情况下会导致主从数据不一致，比如last\\_insert\\_id()、now()等函数。\n\n\nMIXED（mixed-based replication, MBR）\n\n以上两种模式的混合使用，一般会使用 STATEMENT 模式保存binlog，对于STATEMENT模式无法复制的操作使用 ROW 模式保存binlog，MySQL会根据执行的SQL语句选择写入模式。\nBinlog 写入机制常用的log event有：Query event、Row event、Xid event等。binlog文件的内容就是各种Log event的集合。\n\n根据记录模式和操作触发event事件生成log event（事件触发执行机制）\n将事务执行过程中产生log event写入缓冲区，每个事务线程都有一个缓冲区\n\n\n\n\n\n\n\n\n\n\n1Log Event保存在一个binlog\\_cache\\_mngr数据结构中，在该结构中有两个缓冲区，一个是stmt\\_cache，用于存放不支持事务的信息；另一个是trx\\_cache，用于存放支持事务的信息。\n\n事务在提交阶段会将产生的log event写入到外部binlog文件中。\n\n\n\n\n\n\n\n\n\n\n1不同事务以串行方式将log event写入binlog文件中，所以一个事务包含的log event信息在binlog文件中是连续的，中间不会插入其他事务的log event。\nBinlog 文件操作Binlog状态查看\n12show variables like ;\n\n开启Binlog功能需要修改my.cnf或my.ini配置文件，在[mysqld]下面增加log_bin&#x3D;mysql_bin_log，重启MySQL服务。\n1234binlog-format\\=ROW log-bin\\=mysqlbinlog\n\n执行开启语句\n12set global log\\_bin\\=mysqllogbin;\n\n使用show binlog events命令\n123456789101112show binary logs; //等价于show master logs; show master status; show binlog events; show binlog events in &#x27;mysqlbinlog.000001&#x27;\\\\G;结果:     Log\\_name: mysql\\_bin.000001   //此条log存在那个文件中        Pos: 174                 //log在bin-log中的开始位置      Event\\_type: Intvar          //log的类型信息         Server\\_id: 1  //可以查看配置中的server\\_id,表示log是那个服务器产生      End\\_log\\_pos: 202          //log在bin-log中的结束位置         Info: INSERT\\_ID=2    //log的一些备注信息，可以直观的看出进行了什么操作 \n\n可以用mysql自带的工具mysqlbinlog\n12mysqlbinlog &quot;文件名&quot; mysqlbinlog &quot;文件名&quot; &gt; &quot;文件名比如:test.sql&quot;\n\n使用 binlog 恢复数据\n1234mysqlbinlog --start-datetime=&quot;2020-04-25 18:00:00&quot; --stop-datetime=&quot;2020-04-26 00:00:00&quot; mysqlbinlog.000002 | mysql -uroot -p1234 mysqlbinlog --start-position=154 --stop-position=957 mysqlbinlog.000002 | mysql -uroot -p1234\n\nmysqldump：定期全部备份数据库数据。mysqlbinlog: 可以做增量备份和恢复操作。\n删除Binlog文件\n1234purge binary logs to &#x27;mysqlbinlog.000001&#x27;; //删除指定文件 purge binary logs before &#x27;2020-04-28 00:00:00&#x27;; //删除指定时间之前的文件 reset master; //清除所有文件\n\n可以通过设置expire_logs_days参数来启动自动清理功能。默认值为0表示没启用。设置为1表示超出1天binlog文件会自动删除掉\nRedo Log和 Binlog 区别\nRedo Log是属于InnoDB引擎功能，Binlog是属于MySQL Server自带功能，并且是以二进制文件记录。\nRedo Log属于物理日志，记录该数据页更新状态内容，Binlog是逻辑日志，记录更新过程。\nRedo Log日志是循环写，日志空间大小是固定，Binlog是追加写入，写完一个写下一个，不会覆盖使用。\nRedo Log作为服务器异常宕机后事务数据自动恢复使用，Binlog可以作为主从复制和数据恢复使用。Binlog没有自动crash-safe能力。\n\n(crash-safe 即在 InnoDB 存储引擎中，事务提交过程中任何阶段，MySQL突然奔溃，重启后都能保证事务的完整性，已提交的数据不会丢失，未提交完整的数据会自动进行回滚。这个能力依赖的就是redo log和unod log两个日志。)\n","slug":"MIDDLEWARE/MySQL Binlog日志配置","date":"2022-08-09T17:05:23.000Z","categories_index":"Binlog,MIDDLEWARE","tags_index":"log,binlog,event","author_index":"dandeliono"},{"id":"d4735e073a9024218ffc41d269e2b60f","title":"订单拆分的算法工具","content":"订单拆分的算法工具本篇文章带你打造一个自适应场景的交易订单合单拆分通用算法方案，根据现有技术的痛点，我们支付的时候设计一种自适应场景的交易下单合单拆分通用算法的方案，可插拔的场景组件提升扩展性和通用性就很重要。\n现有业内或产品是否有类似的，现有技术是否有缺陷或不足或问题，我们支付的时候\n\n交易系统为满足企业生产、销售、服务等需要，信息化模型复杂；为应对运营、管理和决策从而业务变化快；需要应对多商业模式的场景，例如多供应商、自营模式、入驻模式等；运营系统多样性，如 O2O&#x2F;C2C&#x2F;B2C&#x2F;。\n随着业务不断迭代，每增加一个场景或者一个交易支付工具，交易模型就成笛卡尔积形式增长，下单、支付退款、结算业务将越来越难以维护和扩展。如果不能有效控制和管理，后续扩展都需要不断重复的进行原有工作，导致业务代码冗余，逻辑不清晰，不能有效的保证数据一致性，也会带来代码效率和开发效率等问题。代码可读性变差，同逻辑不同实现代码性能参差不齐的问题。\n\n针对以上问题，设计一种自适应场景的电商下单合单拆分通用算法的方案，设计根据下单场景自适应交易合单的拆分订单信息、结算信息，通过顺序表存储可插拔的场景组件提升扩展性和通用性，使用持久层提升代码执行逻辑效率和数据一致性。\n对于复杂的交易模型，此设计整体上分为 4 个模块——分摊模块、事件模块、服务模块、数据模\n1、数据模块将复杂的模型抽象化，保留交易类型、交易流水信息、订单商品信息，如需要扩展信息，可根据已知信息通过业务场景，在数据获取更多的信息。以统一的抽象数据模型应对多样的复杂的场景，场景动态扩展，程序通过策略选择，统一迭代来自适应复杂场景。\n\n2、事件模块 事件体系是交易系统内部抽象模型，定义了一个唯一的事件 ID，支持同步异步执行，具有配置持久化机制、自定义异常重试机制、自动触发机制、监控报警等特点，将复杂逻辑代码和交易业务代码解耦。\n\n3、服务模块将数据模块、交易拆分信息在提供服务之前完成持久化，提供统一通用的服务接口，避免数据不一致，提高下单、退款、清结算、金额&#x2F;数量分摊的性能与一致性。\n\n4、算法模块总体基于事件模块自动执行，将数据模块信息拆分，内置支付工具模块可支持配置的所有工具扁平化处理，过滤仅使用的工具。拆分主体根据配置规则，按照商品总金额占比模式拆分，对复杂交易模式进行统一集中处理，对防资损使用拆分金额&#x2F;数量恒等于交易金额&#x2F;数量的方式，对异常处理具有失败重试、失败监控告警、异常捕获等措施。最终通过此模块，服务各类复杂订单拆分、不同场景拆分的能力。\n\n5、整体设计​​\n整体业务处理流程图，首先我们拿到用户的行为，例如：下单使用了支付宝+商家券+满减活动+618 活动+花呗+银行卡，出去拆分下单落库，优惠券的核销使用，我们将拆分逻辑使用异步 API-Future 进行开发，首先将拆分事件定义为一个 event 事件，此事件保存在环形缓冲器（ring buffer）这么做为了可以循环执行添加的事件，无需阻塞，这个时候在事件里面，实现自适应的拆分逻辑，并将拆分逻辑提供对外的服务接口即可，当此事件执行异常时，会进行落库操作\n\n1、发布事件Disruptor 是一个高性能的异步处理框架，或者可以认为是线程间通信的高效低延时的内存消息组件，它最大特点是高性能，其 LMAX 架构可以获得每秒 6 百万订单，用 1 微秒的延迟获得吞吐量为 100K+。disruptor 设计了一种高效的替代方案，我们使用 disruptor.getRingBuffer(); 构件高并发容器\n123456789101112131415161718192021222324252627* 发布事件-异步执行     *     * @param orderEvent 事件     * @param isAsync    true-异步 ，false-同步     * @param context    上下文     */    @Override    public void publishEvent(OrderEventBean orderEvent, Object context, boolean isAsync) &#123;        try &#123;                                                if (isAsync) &#123;                RingBuffer&lt;OrderEventBean&gt; ringBuffer = disruptor.getRingBuffer();                ringBuffer.publishEvent((event, sequence, groupId, orderId) -&gt; ConvertBeanUtil.copyBeanProperties(orderEvent, event), orderEvent.getEventGroupId(), orderEvent.getOdrId());            &#125; else &#123;                                logger.info(&quot;publishEvent方法orderEvent=&#123;&#125;&quot;, orderEvent);                EventProcessorUtil.run(orderEvent);            &#125;        &#125; catch (Exception e1) &#123;            logger.error(&quot;publishEvent error e1 =&quot;, e1);        &#125;        logger.info(&quot;publishEvent end&quot;);    &#125;\n\n​​​2、逻辑模块计算分摊数量工具的核心逻辑，分摊逻辑：单笔子单 支付方式 支付金额 &#x2F; 该支付方式总订单积分支付金额 &#x3D; 数量，最后一笔子单使用减法 （订单总数 - 分摊总数） &#x3D; 剩余分摊数 &lt; 最后一笔分摊数 ？ 剩余分摊数 ： 最后一笔分摊数主要流程：1、先对 list&lt;…Detail&gt;根据优先级排序，2、按顺序分摊，不能超过剩余的总数量，最后一笔取剩余，退款不能超过交易，3、获取下单时的支付工具，4、计算\n123456789101112* 计算分摊数量的工具     * 分摊逻辑：单笔子单 支付方式 支付金额 / 该支付方式总订单积分支付金额 = 数量     * 最后一笔子单使用减法 （订单总数 - 分摊总数） = 剩余分摊数 &lt; 最后一笔分摊数 ？ 剩余分摊数 ： 最后一笔分摊数     */    @Override    public ForkQuantityDTO subOdrForkQuantityPayProcess(ForkQuantityDTO forkQuantityDTO) &#123;                                            &#125;\n\n1、对拆分和服务逻辑做了解耦，将拆分模块化，解决了不同业务分摊的数据一致性问题，提高代码的可维护性可扩展性\n2、自适应复杂的业务场景，支持对复杂场景配置和组装，实现新业务新需求时改动小，提高开发效率\n3、支持对其他业务提供服务，解决了业务逻辑处理异地处理\n","slug":"JAVA/订单拆分的算法工具","date":"2022-08-05T09:20:57.000Z","categories_index":"https,JAVA","tags_index":"com,raw,githubusercontent","author_index":"dandeliono"},{"id":"a5de914b5d23a430628c528e96e12efd","title":"iotop发现jdb2 sdb1-8 io使用过高解决办法","content":"iotop发现jdb2&#x2F;sdb1-8 io使用过高解决办法现象1234567891011121314\\[root@push-5\\-221 ~\\]# iotopotal DISK READ: 0.00 B/s | Total DISK WRITE: 6.26 M/s  TID  PRIO  USER     DISK READ  DISK WRITE  SWAPIN     IO\\&gt; COMMAND 795 be/3 root        0.00 B/s    0.00 B/s  0.00 % 95.80 % \\[jbd2/dm-0\\-8\\] 22952 be/4 rabbitmq    0.00 B/s    4.04 M/s  0.00 % 50.10 % beam.smp -W w -A 128 -P 10~inet\\_dist\\_listen\\_max 25672 8597 be/4 mysql       0.00 B/s   90.05 K/s  0.00 % 16.08 % mysqld --basedir=/usr --da~=/var/lib/mysql/mysql.sock 22960 be/4 rabbitmq    0.00 B/s   52.53 K/s  0.00 % 13.40 % beam.smp -W w -A 128 -P 10~inet\\_dist\\_listen\\_max 2567222955 be/4 rabbitmq    0.00 B/s  105.06 K/s  0.00 % 11.76 % beam.smp -W w -A 128 -P 10~inet\\_dist\\_listen\\_max 2567222948 be/4 rabbitmq    0.00 B/s   41.27 K/s  0.00 % 10.10 % beam.smp -W w -A 128 -P 10~inet\\_dist\\_listen\\_max 2567222947 be/4 rabbitmq    0.00 B/s   48.78 K/s  0.00 %  7.44 % beam.smp -W w -A 128 -P 10~inet\\_dist\\_listen\\_max 2567222921 be/4 rabbitmq    0.00 B/s   48.78 K/s  0.00 %  6.55 % beam.smp -W w -A 128 -P 10~inet\\_dist\\_listen\\_max 2567222946 be/4 rabbitmq    0.00 B/s   45.03 K/s  0.00 %  6.30 % beam.smp -W w -A 128 -P 10~inet\\_dist\\_listen\\_max 2567223039 be/4 rabbitmq    0.00 B/s   26.27 K/s  0.00 %  6.07 % beam.smp -W w -A 128 -P 10~inet\\_dist\\_listen\\_max 2567223002 be/4 rabbitmq    0.00 B/s   45.03 K/s  0.00 %  6.02 % beam.smp -W w -A 128 -P 10~inet\\_dist\\_listen\\_max 2567222964 be/4 rabbitmq    0.00 B/s   48.78 K/s  0.00 %  4.67 % beam.smp -W w -A 128 -P 10~inet\\_dist\\_listen\\_max 2567211655 be/4 mysql       0.00 B/s    0.00 B/s  0.00 %  2.77 % mysqld --basedir=/usr --da~=/var/lib/mysql/mysql.sock 30533 be/4 mysql       0.00 B/s    7.50 K/s  0.00 %  2.72 % mysqld --basedir=/usr --da~=/var/lib/mysql/mysql.sock 4458 be/4 mysql       0.00 B/s    3.75 K/s  0.00 %  1.87 % mysqld --basedir=/usr --da~=/var/lib/mysql/mysql.sock 6629 be/4 mysql       0.00 B/s    3.75 K/s  0.00 %  1.66 % mysqld --basedir=/usr --da~=/var/lib/mysql/mysql.sock 6116 be/4 mysql       0.00 B/s    3.75 K/s  0.00 %  1.62 % mysqld --basedir=/usr --da~=/var/lib/mysql/mysql.sock 19645 be/4 mysql       0.00 B/s    3.75 K/s  0.00 %  1.44 % mysqld --basedir=/usr --da~=/var/lib/mysql/mysql.sock 25633 be/4 mysql       0.00 B/s    3.75 K/s  0.00 %  1.22 % mysqld --basedir=/usr --da~=/var/lib/mysql/mysql.sock 8603 be/4 mysql       0.00 B/s    7.50 K/s  0.00 %  0.89 % mysqld --basedir=/usr --da~=/var/lib/mysql/mysql.sock 11659 be/4 mysql       0.00 B/s  180.11 K/s  0.00 %  0.43 % mysqld --basedir=/usr --da~=/var/lib/mysql/mysql.sock 22803 be/4 mysql       0.00 B/s   11.26 K/s  0.00 %  0.36 % mysqld --basedir=/usr --da~=/var/lib/mysql/mysql.sock 4470 be/4 mysql       0.00 B/s    3.75 K/s  0.00 %  0.03 % mysqld --basedir=/usr --da~=/var/lib/mysql/mysql.sock 8601 be/4 mysql       0.00 B/s    3.75 K/s  0.00 %  0.02 % mysqld --basedir=/usr --da~=/var/lib/mysql/mysql.sock\n\n发现[jbd2&#x2F;dm-0-8]这个进程占用 IO95%。\n解决进入 mysql ，查看 sync_binlog 变量设置\n1234567891011121314151617181920212223\\[root@hlsms-fensheng-4 ~\\]# mysql -uroot -pEnter password: Welcome to the MySQL monitor.  Commands end with ; or \\\\g.Your MySQL connection id is 2118462 Server version: 5.7.22 MySQL Community Server (GPL)Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &#x27;help;&#x27; or &#x27;\\\\h&#x27; for help. Type &#x27;\\\\c&#x27; to clear the current input statement.mysql\\&gt; show variables like &#x27;%sync\\_binlog%&#x27;; +---------------+-------+| Variable\\_name | Value |+---------------+-------+| sync\\_binlog   | 1     |+---------------+-------+1 row in set (0.00 sec)mysql\\&gt; \n\n当前值为 1，表示每次提交事务后，将 binlog_cache 中的数据强制写入磁盘。当设置为 “1” 的时候，是最安全但是性能损耗最大的设置。因为当设置为 1 的时候，即使系统 Crash，也最多丢失 binlog_cache 中未完成的一个事务\n当设置为 0 时，表示当事务提交之后，MySQL 不做 fsync 之类的磁盘同步指令刷新 binlog_cache 中的信息到磁盘，而让 Filesystem 自行决定什么时候来做同步，或者 cache 满了之后才同步到磁盘。\n在 MySQL 中系统默认的设置是 sync_binlog&#x3D;0，也就是不做任何强制性的磁盘刷新指令，这时候的性能是最好的，但是风险也是最大的。因为一旦系统 Crash，在 binlog_cache 中的所有 binlog 信息都会被丢失。\nsync_binlog&#x3D;n，当每进行 n 次事务提交之后，MySQL 将进行一次 fsync 之类的磁盘同步指令来将 binlog_cache 中的数据强制写入磁盘。\n所以 sync_binlog&#x3D;1，导致事务写入太频繁，从而出现[jbd2&#x2F;dm-0-8]这个进程占用 IO95%。\n因此将 sync_log 设置为 500\n123456789101112mysql&gt; set global sync\\_binlog=500;Query OK, 0 rows affected (0.00 sec)mysql\\&gt; show variables like &#x27;%sync\\_binlog%&#x27;; +---------------+-------+| Variable\\_name | Value |+---------------+-------+| sync\\_binlog   | 500   |+---------------+-------+1 row in set (0.00 sec)mysql\\&gt; \n\n设置innodb_flush_log_at_trx_commit 变量innodb_flush_log_at_trx_commit 是配置 MySql 日志何时写入硬盘的参数：\n0：log buffer 将每秒一次地写入 log file 中，并且 log file 的 flush(刷到磁盘) 操作同时进行。该模式下在事务提交的时候，不会主动触发写入磁盘的操作。\n1：每次事务提交时MySQL都会把 log buffer 的数据写入 log file，并且 flush(刷到磁盘) 中去，该模式为系统默认。\n2：每次事务提交时mysql都会把 log buffer 的数据写入 log file，但是 flush(刷到磁盘) 操作并不会同时进行。该模式下，MySQL 会每秒执行一次 flush(刷到磁盘) 操作\\\n一般设置为 2\n1234567891011121314151617mysql&gt; show variables like &#x27;%innodb\\_flush\\_log\\_at\\_trx\\_commit%&#x27;; +--------------------------------+-------+| Variable\\_name                  | Value |+--------------------------------+-------+| innodb\\_flush\\_log\\_at\\_trx\\_commit | 1     |+--------------------------------+-------+1 row in set (0.01 sec)mysql\\&gt; set global innodb\\_flush\\_log\\_at\\_trx\\_commit=2;Query OK, 0 rows affected (0.00 sec)mysql\\&gt; show variables like &#x27;%innodb\\_flush\\_log\\_at\\_trx\\_commit%&#x27;; +--------------------------------+-------+| Variable\\_name                  | Value |+--------------------------------+-------+| innodb\\_flush\\_log\\_at\\_trx\\_commit | 2     |+--------------------------------+-------+1 row in set (0.01 sec)\n\n验证再次查看 iotop\nTotal DISK READ: 0.00 B/s | Total DISK WRITE: 781.51 K/s \n  TID  PRIO  USER     DISK READ  DISK WRITE  SWAPIN     IO\\&gt; COMMAND 1458 be/3 root        0.00 B/s    0.00 B/s  0.00 %  6.97 % \\[jbd2/dm-2\\-8\\] 26496 be/4 mysql       0.00 B/s   14.47 K/s  0.00 %  4.02 % mysqld --basedir=/usr --datadir=~ocket=/home/data/mysql/mysql.sock 26500 be/4 mysql       0.00 B/s  665.73 K/s  0.00 %  2.42 % mysqld --basedir=/usr --datadir=~ocket=/home/data/mysql/mysql.sock 26498 be/4 mysql       0.00 B/s    0.00 B/s  0.00 %  1.52 % mysqld --basedir=/usr --datadir=~ocket=/home/data/mysql/mysql.sock 26505 be/4 mysql       0.00 B/s    0.00 B/s  0.00 %  0.17 % mysqld --basedir=/usr --datadir=~ocket=/home/data/mysql/mysql.sock 26499 be/4 mysql       0.00 B/s    0.00 B/s  0.00 %  0.00 % mysqld --basedir=/usr --datadir=~ocket=/home/data/mysql/mysql.sock 14668 be/4 mysql       0.00 B/s   21.71 K/s  0.00 %  0.00 % mysqld --basedir=/usr --datadir=~ocket=/home/data/mysql/mysql.sock 16970 be/4 mysql       0.00 B/s   18.09 K/s  0.00 %  0.00 % mysqld --basedir=/usr --datadir=~ocket=/home/data/mysql/mysql.sock\n\n[jbd2&#x2F;dm-2-8]明显降低\n","slug":"LINUX/iotop发现jdb2 sdb1-8 io使用过高解决办法","date":"2022-08-03T16:05:11.000Z","categories_index":"binlog,LINUX","tags_index":"cache,log,MySQL","author_index":"dandeliono"},{"id":"f4f318f964064d6dbbbf9f460a2ebe37","title":"Logstash性能调优","content":"Logstash性能调优详细调优参考\nInputs 和 Outputs 的性能当输入输出源的性能已经达到上限，那么性能瓶颈不在 Logstash，应优先对输入输出源的性能进行调优。\n系统性能指标：\nCPU\n\n确定 CPU 使用率是否过高，如果 CPU 过高则先查看 JVM 堆空间使用率部分，确认是否为 GC 频繁导致，如果 GC 正常，则可以通过调节 Logstash worker 相关配置来解决。\n\n\n内存\n\n由于 Logstash 运行在 JVM 上，因此注意调整 JVM 堆空间上限，以便其有足够的运行空间。另外注意 Logstash 所在机器上是否有其他应用占用了大量内存，导致 Logstash 内存磁盘交换频繁。\n\n\nI&#x2F;O 使用率1）磁盘 IO：磁盘 IO 饱和可能是因为使用了会导致磁盘 IO 饱和的创建（如 file output）, 另外 Logstash 中出现错误产生大量错误日志时也会导致磁盘 IO 饱和。Linux 下可以通过 iostat, dstat 等查看磁盘 IO 情况2）网络 IO：网络 IO 饱和一般发生在使用有大量网络操作的插件时。linux 下可以使用 dstat 或 iftop 等查看网络 IO 情况3）JVM 堆检查：```null1、如果 JVM 堆大小设置过小会导致 GC 频繁，从而导致 CPU 使用率过高2、快速验证这个问题的方法是 double 堆大小，看性能是否有提升。注意要给系统至少预留 1GB 的空间。3、为了精确查找问题可以使用 jmap 或 VisualVM。参考4、设置 Xms 和 Xmx 为相同值，防止堆大小在运行时调整，这个过程非常消耗性能。\n1234567894）Logstash worker设置：  worker相关配置在logstash.yml中，主要包括如下三个：*   pipeline.workers：      该参数用以指定Logstash中执行filter和output的线程数，当如果发现CPU使用率尚未达到上限，可以通过调整该参数，为Logstash提供更高的性能。建议将Worker数设置适当超过CPU核数可以减少IO等待时间对处理过程的影响。实际调优中可以先通过-w指定该参数，当确定好数值后再写入配置文件中。*   pipeline.batch.size:      该指标用于指定单个worker线程一次性执行flilter和output的event批量数。增大该值可以减少IO次数，提高处理速度，但是也以为这增加内存等资源的消耗。当与Elasticsearch联用时，该值可以用于指定Elasticsearch一次bluck操作的大小。*   pipeline.batch.delay:      该指标用于指定worker等待时间的超时时间，如果worker在该时间内没有等到pipeline.batch.size个事件，那么将直接开始执行filter和output而不再等待。\n\n\n","slug":"MIDDLEWARE/Logstash性能调优","date":"2022-08-03T15:53:32.000Z","categories_index":"Inputs,MIDDLEWARE","tags_index":"Outputs,的性能,系统性能指标","author_index":"dandeliono"},{"id":"36fd7c99867f3cc13d5235c8e240e440","title":"使用K3S+ Kilo部署安全多区云原生ARM集群，支持nginx-ingress及velero备份","content":"使用K3S+ Kilo部署安全多区云原生ARM集群，支持nginx-ingress及velero备份申请 Oracle甲骨文的Arm实例已经很久了，甲骨文的配置相当的高，4 核 24G 内存 100G 硬盘 5TB 流量，但一直没有正儿八经地用起来，属实是有些浪费资源了。于是前些天就把手上有的这些机子组了一个 ARM 集群，分属三个区域，分别为：春川 (Chuncheon)、东京 (Tokyo)、首尔 (Seoul)。由于三个机房是物理隔离开的，没有直接组在一起的链路，于是除了使用甲骨文官方的 VPN 之外，我们这次直接使用 Wireguard 将这三台机子组到一个局域网内，这样就方便组建集群了。\n先来看看夸张的 ARM 12 核心，70G 内存，158G 可用存储的 k3s 集群。\n\n\n\nOK～前情交待完毕，以下是这次部署所需达成的目标：\n\n前面提到，三个机器不是一个区域的，所以需要将三台主机使用 Wireguard 组成一个局域网\n先不考虑高可用问题，由首尔作为控制节点，与春川和东京共为工作节点提供服务\n不使用甲骨文的 LB，由主机自行提供服务，域名指向其中任意一台都可以访问，也可搭配智能解析系统\n需要在 lb 位置集成 modsecurity，过滤一些恶意请求\n不使用 traefik 作为前端暴露组件，需要使用 nginx ingress(因为里面自带集成了 modsecurity)\n需要一个好用的控制面板，kubectl 操作起来还得连接服务器，稍显麻烦。这里还是直接使用 rancher，习惯了。\n需要使用分布式存储，这里直接使用 rancher 的 Longhorn 进行部署\n需要部署数据备份组件，这里使用开源的[velero]，可以定期备份到远程的 S3 存储服务器，作为最后的灾备恢复选项。\n\n以下为实操步骤：\n注意：这里统一使用 ubuntu 20.04 的发行版，不建议用 ubuntu 22.04，最低版本为 18.04，kubernetes 使用 v1.22 稳定版，版本太新会有其他问题的\n注意 2：这里有两个概念，一个是公网 IP，一个是网卡 IP，在甲骨文、阿里云等等的云服务厂商那里，公网 IP 是不会直接给到实例中的系统的，而是给一个 nat 后的 IP，这个 IP 在外部是不能访问的。而公网 IP 是可以给外部访问的。因此，在进行接下来的配置中，要注意这点。wireguard 需要使用公网 IP 地址，而 k3s 里，则是使用网卡 IP 地址。\n使用 Wireguard 组局域网在本例中，我们需要组建一个局域网，但与网上搜索到的直接使用 wireguard 不同，我们需要借助 Kilo 对 wireguard 进行管理和连接，因此在这里只需要安装好 wireguard 即可，后续操作在 k3s 中进行。\n安装 wireguard由于使用 ubuntu 20.04 的版本，因此不用额外操作，直接在终端内分别为三个区域的主机安装 wireguard 即可，使用命令：\n12apt updateapt install wireguard\n安装部署 k3s server 节点1curl -sfL https://get.k3s.io | K3S_CLUSTER_INIT=true INSTALL_K3S_EXEC=&quot;server&quot; INSTALL_K3S_CHANNEL=v1.22 sh -s - --node-label &quot;topology.kubernetes.io/region=seoul&quot; --node-label &quot;master-node=true&quot; --tls-san &#123;&#123; server_public_ip &#125;&#125; --advertise-address &#123;&#123; server_public_ip &#125;&#125; --disable traefik --flannel-backend none --kube-proxy-arg &quot;metrics-bind-address=0.0.0.0&quot; --kube-apiserver-arg &quot;feature-gates=EphemeralContainers=true&quot;\n\n简要说明：\n\nK3S_CLUSTER_INIT&#x3D;true : 集群模式，会安装内置的 etcd，而不是 sqlite3；\nINSTALL_K3S_EXEC&#x3D;”server” : 指定为 server 节点\nINSTALL_K3S_CHANNEL&#x3D;v1.22 : 安装的 K3S 版本，这里选择 1.22 稳定版本，不选择太新的版本；\n–node-label “topology.kubernetes.io&#x2F;region&#x3D;seoul” : 设置当前服务器的 label 为首尔区域；\n–node-label “master-node&#x3D;true” : 设置为主节点；\n–tls-san :  改为当前服务器公网 IPv4 地址，–tls-san这个选项在 TLS 证书中增加了一个额外的主机名或 IP 作为备用名称，如果你想通过 IP 和主机名访问，可以多次指定，多个值之间用英文半角逗号分隔。注意：如果不指定 –tls-san ，可能会导致 kubectl 无法通过 server_public_ip 来访问集群；\n–advertise-address ： 改为当前服务器公网 IPv4 地址，用于 agent 连接 K3S Server 的 API，但是注意不要指定 –node-external-ip ，会导致无法获得用户真实 IP；\n–disable traefik : 关闭 traefik，后面安装 nginx-ingress\n–flannel-backend none : 关闭默认的 flannel CNI，后面安装 Kilo\n–kube-proxy-arg “metrics-bind-address&#x3D;0.0.0.0” : kube-proxy 参数，这个参数直接影响 Metrics 的获取；\n–kube-apiserver-arg “feature-gates&#x3D;EphemeralContainers&#x3D;true” :  （可选）启用 feature-gates：EphemeralContainers，方便在不重启 pod 的情况下附加 sidecar 进行调试。\n\n执行后，检查安装结果，可以看到节点已经可以找到了：\n123# kubectl get node NAME                     STATUS   ROLES                       AGE     VERSIONinstance-20190921-1325   NotReady    control-plane,etcd,master   0h01m   v1.22.12+k3s1\n可以看到节点状态为 NotReady ，这是正常的，因为我们还没有添加网络 CNI 网络组件，接下来就开始安装它。\n安装 Kilo CNI 网络组件网络拓扑然后我们需要指定第一台节点的拓扑，后续添加上来的 agent 节点会自动添加进来，但仍然建议对其网络进行拓扑定义，现在先定义第一台：\n123kubectl annotate nodes instance-20190921-1325 kilo.squat.ai/location=&quot;seoul&quot;kubectl annotate nodes instance-20190921-1325 kilo.squat.ai/force-endpoint=&quot;&#123;&#123; server_public_ip &#125;&#125;:51820&quot;kubectl annotate nodes instance-20190921-1325 kilo.squat.ai/persistent-keepalive=20\n简单说明：\n\nlocation : 对节点进行位置定义，方便维护\nforce-endpoint : 自动发现节点时，会有 endpoint 的定义，这个地址如果你是用内网 ipv4，就容易变成不可路由的地址，所以这里建议直接用绑定到网卡上的 IPv6 地址，进行覆盖定义\npersistent-keepalive : 是 wireguard 的配置选项，保活\n\n安装 Kilo CNI 组件12kubectl apply -f https://raw.githubusercontent.com/squat/kilo/main/manifests/crds.yamlwget https://raw.githubusercontent.com/squat/kilo/main/manifests/kilo-k3s.yaml\n在第二步使用 wget，是因为需要对 Kilo 的 DaemonSet 进行修改，在我们案例中，各区没有直接的网络连接，最好使用 fullmesh 模式，这部分的说明，参考：https://kilo.squat.ai/docs/topology/#full-mesh，添加–mesh-granularity&#x3D;full 到 kilo-k3s.yaml 守护程序 pod 模板中：\n1234567891011- name: kilo  image: squat/kilo  args:      - --kubeconfig=/etc/kubernetes/kubeconfig      - --hostname=$(NODE_NAME)      - --mesh-granularity=full  env:      - name: NODE_NAME        valueFrom:            fieldRef:                fieldPath: spec.nodeName\n然后执行安装\n1kubectl apply -f kilo-k3s.yaml\n验证 Kilo 安装\n在 kube-system 命名空间下的 pod 中，可以找到 koli 的 pod 已经成功运行，状态为 RUNNING\n执行 kubectl get node 时查看到的节点状态由 NotReady 变为 Ready\n在 k3s server 节点上，已经有 kilo 的相关 annotations，示例如下：12345678910111213141516171819apiVersion: v1kind: Nodemetadata:  annotations:    k3s.io/external-ip: &#123;&#123; server_public_ip &#125;&#125;    k3s.io/hostname: instance-20190921-1325    k3s.io/internal-ip: 10.0.0.170    ...    kilo.squat.ai/discovered-endpoints: &#x27;&#123;&quot;&#123;&#123; public_key_1 &#125;&#125;&quot;:&#123;&quot;IP&quot;:&quot;&#123;&#123; agent_public_ip_1 &#125;&#125;&quot;,&quot;Port&quot;:51820,&quot;Zone&quot;:&quot;&quot;&#125;,&quot;&#123;&#123; public_key_2 &#125;&#125;&quot;:&#123;&quot;IP&quot;:&quot;&#123;&#123; agent_public_ip_2 &#125;&#125;&quot;,&quot;Port&quot;:51820,&quot;Zone&quot;:&quot;&quot;&#125;&#125;&#x27;    kilo.squat.ai/endpoint: &#x27;&#123;&#123; server_public_ip &#125;&#125;:51820&#x27;    kilo.squat.ai/force-endpoint: &#x27;&#123;&#123; server_public_ip &#125;&#125;:51820&#x27;    kilo.squat.ai/granularity: location    kilo.squat.ai/internal-ip: 10.0.0.170/24    kilo.squat.ai/key: &#123;&#123; private_key &#125;&#125;    kilo.squat.ai/last-seen: &quot;1658647061&quot;    kilo.squat.ai/location: seoul    kilo.squat.ai/persistent-keepalive: &quot;20&quot;    kilo.squat.ai/wireguard-ip: 10.4.0.2/16...\n可以看到 discovered-endpoints 这个是内网自动发现到的节点\n\n安装部署 k3s agent 节点部署 agent 节点前，需要在 server 节点先获取到 token，后面添加 agent 节点时都需要带上，可以添加多台 agent 节点，这里以添加两台为例子。\n获取 token：\n1cat /var/lib/rancher/k3s/server/node-token\n执行命令添加 agent 节点\n1curl -sfL https://get.k3s.io  | INSTALL_K3S_EXEC=&quot;agent&quot; INSTALL_K3S_CHANNEL=v22 K3S_URL=https://&#123;&#123; server_public_ip &#125;&#125;:6443 K3S_TOKEN=&#123;&#123; server_token &#125;&#125; sh -s - --node-label &quot;topology.kubernetes.io/region=chuncheon&quot; --node-label &quot;worker-node=true&quot; --kube-proxy-arg &quot;metrics-bind-address=0&quot;\n简要说明：\n\nINSTALL_K3S_EXEC&#x3D;”agent” : 定义添加的是 agent 节点，将只作为工作节点加入集群，没有 etcd；\nINSTALL_K3S_CHANNEL&#x3D;v1.22 : 安装的 K3S 版本，这里选择 1.22 稳定版本，不选择太新的版本；\nK3S_URL=[https://&#123;&#123;](https://&#123;&#123;) server_public_ip &#125;&#125;:6443 : 指定 server 节点 api 访问地址\nK3S_TOKEN&#x3D; : 使用上一步获取到的 token 值\n–node-label “topology.kubernetes.io&#x2F;region&#x3D;chuncheon” : 设置当前服务器的 label 为春川区域；\n–node-label “worker-node&#x3D;true” : 设置为工作节点；\n–kube-proxy-arg “metrics-bind-address&#x3D;0.0.0.0” : kube-proxy 参数，这个参数直接影响 Metrics 的获取；\n\n执行后等待 1 分钟左右，回到 k3s server 节点，执行 kubectl get node，即可看到新添加的节点，同样，这里我们对添加进来的节点进行拓扑定义\n123kubectl annotate nodes instance-20190931-1637 kilo.squat.ai/location=&quot;chuncheon&quot;kubectl annotate nodes instance-20190931-1637 kilo.squat.ai/force-endpoint=&quot;&#123;&#123; agent_public_ip &#125;&#125;:51820&quot;kubectl annotate nodes instance-20190931-1637 kilo.squat.ai/persistent-keepalive=20\nKilo CNI 的网卡说明网卡列表\nkilo0：WireGuard VPN 网络，用以 Node 间组建 VPN 内网；如果同一个区域内存在 VPC，内网连接则优先使用 VPC 内网，只有走外部其他区域的链路才会走 WireGuard 加密网络；\nkube-bridge: 桥接网络，Node 内 Pod 的网卡与宿主机的网卡进行连接，如需跨网通信，则会路由到 WireGuard VPN 网络共享通信；\ntunl0：Bridge 模式下，多主机网络通信需要额外配置主机路由，或使用 overlay 网络。通过 Kilo 来自动配置。引用网上的一张图：\n\n\n路由表首尔机器上的路由表：\n12345678910# ip rdefault via 10.0.0.1 dev enp0s3 proto dhcp src 10.0.0.170 metric 100 10.0.0.0/24 dev enp0s3 proto kernel scope link src 10.0.0.170 10.0.0.98 via 10.4.0.1 dev kilo0 proto static onlink 10.0.0.246 via 10.4.0.3 dev kilo0 proto static onlink 10.4.0.0/16 dev kilo0 proto kernel scope link src 10.4.0.2 10.42.0.0/24 dev kube-bridge proto kernel scope link src 10.42.0.1 10.42.1.0/24 via 10.4.0.1 dev kilo0 proto static onlink 10.42.2.0/24 via 10.4.0.3 dev kilo0 proto static onlink 169.254.0.0/16 dev enp0s3 proto dhcp scope link src 10.0.0.170 metric 100\n春川的路由表：\n12345678910# ip rdefault via 10.0.0.1 dev enp0s3 proto dhcp src 10.0.0.98 metric 100 10.0.0.0/24 dev enp0s3 proto kernel scope link src 10.0.0.98 10.0.0.170 via 10.4.0.2 dev kilo0 proto static onlink 10.0.0.246 via 10.4.0.3 dev kilo0 proto static onlink 10.4.0.0/16 dev kilo0 proto kernel scope link src 10.4.0.1 10.42.0.0/24 via 10.4.0.2 dev kilo0 proto static onlink 10.42.1.0/24 dev kube-bridge proto kernel scope link src 10.42.1.1 10.42.2.0/24 via 10.4.0.3 dev kilo0 proto static onlink 169.254.0.0/16 dev enp0s3 proto dhcp scope link src 10.0.0.98 metric 100\n东京的路由表：\n12345678910# ip rdefault via 10.0.0.1 dev enp0s3 proto dhcp src 10.0.0.246 metric 100 10.0.0.0/24 dev enp0s3 proto kernel scope link src 10.0.0.246 10.0.0.98 via 10.4.0.1 dev kilo0 proto static onlink 10.0.0.170 via 10.4.0.2 dev kilo0 proto static onlink 10.4.0.0/16 dev kilo0 proto kernel scope link src 10.4.0.3 10.42.0.0/24 via 10.4.0.2 dev kilo0 proto static onlink 10.42.1.0/24 via 10.4.0.1 dev kilo0 proto static onlink 10.42.2.0/24 dev kube-bridge proto kernel scope link src 10.42.2.1 169.254.0.0/16 dev enp0s3 proto dhcp scope link src 10.0.0.246 metric 100\n在这里可以看到，这三台机器，已经通过各自的路由，分别都找到了对方。\n安装 helm 部署软件helm 常用于 k8s 里的软件部署，在 k3s 里同样需要安装，执行命令：\n1curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash\n安装部署 nginx-ingressnginx-ingress 直接使用 helm 进行安装，执行命令：\n123456helm upgrade --install ingress-nginx ingress-nginx   --repo https://kubernetes.github.io/ingress-nginx   --namespace ingress-nginx --create-namespace   --set controller.service.type=LoadBalancer   --set controller.ingressClassResource.default=true   --set controller.watchIngressWithoutClass=true\n安装部署 rancher 控制面板新版本 rancher 2.6 的安装与 2.5 之前不一样，2.6 版本依赖 helm，直接运行以下命令进行安装，需要提前准备的是管理后台域名的证书\n首先添加 helm 的 repo12helm repo add rancher-stable https://releases.rancher.com/server-charts/stablehelm repo update\n接着将域名证书放在 &#x2F; etc&#x2F;rancher&#x2F;cert 路径中12345# cd /tmp# tar -zxf cert.tgz# mkdir -p /etc/rancher/cert# mv fullchain.pem /etc/rancher/cert# mv private.pem /etc/rancher/cert\n然后创建 cattle-system 的命名空间和 tls 密钥12345kubectl create namespace cattle-systemkubectl -n cattle-system create secret tls tls-rancher-ingress   --cert=/etc/rancher/cert/fullchain.pem   --key=/etc/rancher/cert/private.pem\n最后安装 rancher 面板12345helm install rancher rancher-stable/rancher   --namespace cattle-system   --set hostname=cluster.domain.com   --set replicas=3   --set ingress.tls.source=secret\n等待 2 分钟后，执行命令获取登录信息，复制粘贴到浏览器打开进行初始化，访问前请先对域名进行解析。\n1echo https://cluster.domain.com/dashboard/?setup=$(kubectl get secret --namespace cattle-system bootstrap-secret -o go-template=&#x27;&#123;&#123;.data.bootstrapPassword|base64decode&#125;&#125;&#x27;)\n安装部署 Longhorn 分布式存储完成上步对 rancher 部署后，进入左下角带齿轮图标的 Cluster Tools，找到 Longhorn 点击安装，一路默认点下一步就好，等待安装完成就好。\n安装部署 velero 备份还原组件velero 是 k8s 下用于备份命名空间下容器的最佳利器，同样也适用于 k3s，以下为安装步骤\n安装 velero首先从官方 github 上下载软件进行安装，注意选择 arm64 的软件架构\n1234# cd /tmp# wget https://github.com/vmware-tanzu/velero/releases/download/v0/velero-v0-linux-armtar.gz# tar xvf velero-*-linux-*.tar.gz# cd velero-*-linux-*\n复制到运行环境中，检查版本是否为目标版本\n12345# cp velero /usr/local/bin/# velero version --client-onlyClient:\tVersion: v1.9.0\tGit commit: 6021f148c4d7721285e815a3e1af761262bff029\n配置命令行自动补全\n12# echo &#x27;source &lt;(velero completion bash)&#x27; &gt;&gt; ~/.bashrc# velero completion bash &gt; /etc/bash_completion.d/velero\n配置 velerovelero 一般使用是搭配 s3 存储，然后定义好备份时间后，就会在时间到达的时候，自动备份到指定的 s3 存储，所以在配置 velero 之前，请先准备一个 s3 存储服务，可以说 aws 的 s3，也可以是自己搭建的 minio 服务，首先定义好 access_key 和 secret_key：\n12345cat &gt; credentials-velero &lt;&lt; EOF[default]aws_access_key_id = fsjDhferKQCZnYtA9h8Faws_secret_access_key = Fw1Wch7fkS2iKS3S55j1LOxHindEllpOnjUIet2NEOF\n然后定义环境变量，为该 s3 存储的位置信息和存储桶信息 (以下值根据你的服务情况而定)\n123export S3_CONFIG_BUCKET=veleroexport S3_CONFIG_REGION=eu-sto-1export S3_CONFIG_S3URL=https://samaaws.com\n\n然后开始安装，在这里需要定义 k3s 的 kubeconfig 文件，我遵从 k8s 的使用习惯，是将 rancher k3s 的配置信息写进本地家路径下的. kube&#x2F;config 中，实际情况你需要根据情况而进行修改\n12345678velero --kubeconfig /root/.kube/config install     --provider aws     --plugins=velero/velero-plugin-for-aws:v0     --bucket $&#123;S3_CONFIG_BUCKET&#125;     --secret-file ./credentials-velero     --use-volume-snapshots=false     --use-restic     --backup-location-config region=$&#123;S3_CONFIG_REGION&#125;,s3ForcePathStyle=&quot;true&quot;,s3Url=$&#123;S3_CONFIG_S3URL&#125; --prefix velero\n验证安装结果，如果有报错，可以在日志中看到的，执行命令：\n1kubectl logs deployment/velero -n velero\n卸载 velero如果配置出错或不想使用 velero 进行备份了，可以非常方便地进行卸载操作，执行命令：\n1velero uninstall\n使用 velero 进行单次备份123velero backup create app-1   --include-namespaces app   --default-volumes-to-restic\n简要说明：\n\napp-1 为本次备份的名称，方便后续查找和恢复，唯一\n–include-namespaces app : velero 遵从按命名空间进行备份，所有位于该命名空间下的资料都会被当成一个备份条件进行备份\n–default-volumes-to-restic : 开启这个选项后，就可以备份到 pvc 里面的内容了，就不仅仅是 yaml 配置信息，是完整的备份。\n\ntips：可以随时查看备份的详情，例如在备份进行中时，可以使用命令查看进度\n1velero backup describe app-1\n使用 velero 进行循环定时备份1234velero schedule create app-12h   --include-namespaces app   --default-volumes-to-restic   --schedule=&quot;@every 12h&quot;\n简要说明：\n\n与单次备份不同，这里使用 schedule create，创建一个计划任务的方式创建备份任务\n–schedule : 定义了间隔时间，例如 @every 12h，代表间隔 12 小时备份一次，例如 @daily ，则为每日备份一次\n\n使用 velero 进行还原还原操作适用于回档或进行迁移，首先在集群 b 安装好运行环境和 velero 后，配置好 s3 的信息，就可以执行命令获取备份列表：\n1# velero backup get\n执行恢复：\n1velero restore create --from-backup app-1\n配置 ModSecurity在 nginx-ingress 中，默认已经包含 ModSecurity 的库，并也有 OWASP 的规则，只是默认情况下没有开启，我们可以在 ingress-nginx 的 configmaps 里，添加配置开启这部分的功能\n1kubectl patch configmaps -n ingress-nginx ingress-nginx-controller -p &#x27;&#123;&quot;data&quot;:&#123; &quot;brotli-level&quot;: &quot;4&quot;,&quot;compute-full-forwarded-for&quot;: &quot;true&quot;,&quot;enable-brotli&quot;: &quot;true&quot;,&quot;enable-modsecurity&quot;: &quot;true&quot;,&quot;enable-ocsp&quot;: &quot;true&quot;,&quot;enable-owasp-modsecurity-crs&quot;: &quot;true&quot;,&quot;enable-real-ip&quot;: &quot;true&quot;,&quot;forwarded-for-header&quot;: &quot;X-Forwarded-For&quot;,&quot;gzip-level&quot;: &quot;4&quot;,&quot;max-worker-connections&quot;: &quot;65535&quot;,&quot;modsecurity-snippet&quot;: &quot;SecRuleEngine On\\\\n&quot;,&quot;proxy-add-original-uri-header&quot;: &quot;true&quot;,&quot;proxy-body-size&quot;: &quot;0&quot;,&quot;ssl-buffer-size&quot;: &quot;16k&quot;,&quot;use-forwarded-headers&quot;: &quot;true&quot;,&quot;use-gzip&quot;: &quot;true&quot;,&quot;worker-cpu-affinity&quot;: &quot;auto&quot;,&quot;worker-processes&quot;: &quot;auto&quot;,&quot;worker-rlimit-nofile&quot;: &quot;300000&quot;&#125;&#125;&#x27;\n这里开启后会对所有 ingress 都起作用，如果不需要 modsecurity 的，注意可以不用理会这条，不必修改就好\n开启 modsecurity 之后的朋友，还需要对 rancher 的 ingress，关闭功能，防止一些莫名其妙的控制面板报错\n1kubectl patch ingress -n cattle-system rancher -p &#x27;&#123;&quot;metadata&quot;: &#123;&quot;annotations&quot;: &#123;&quot;nginx.ingress.kubernetes.io/enable-modsecurity&quot;: &quot;false&quot;,&quot;nginx.ingress.kubernetes.io/enable-owasp-core-rules&quot;: &quot;false&quot;,&quot;nginx.ingress.kubernetes.io/modsecurity-snippet&quot;: &quot;SecRuleEngine Off\\\\n&quot;&#125;&#125;&#125;&#x27;\n安装 Cert-manager 管理证书直接 kubectl 一把梭了，执行：\n1kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.8.2/cert-manager.yaml\n然后给需要分配证书的 ns 分配发行者，这里我们直接用 letsencrypt 提供的证书，太方便了\n1234567891011121314151617kubectl -n icodex create -f - &lt;&lt;EOFapiVersion: cert-manager.io/v1kind: Issuermetadata:  name: letsencrypt-prod  namespace: icodexspec:  acme:    email: icodex@msn.com    privateKeySecretRef:      name: letsencrypt-prod    server: https://acme-vapi.letsencrypt.org/directory    solvers:    - http01:        ingress:          class: nginxEOF\n然后给域名分配证书\n12345678910111213141516171819202122kubectl -n icodex create -f - &lt;&lt;EOFapiVersion: cert-manager.io/v1kind: Certificatemetadata:  name: web-domain-com  namespace: icodexspec:  secretName: web-domain-com-tls  duration: 2160h  renewBefore: 360h  commonName: web.domain.com  isCA: false  privateKey:    algorithm: RSA    encoding: PKCS1    size: 2048  dnsNames:  - web.domain.com  - app.domain.com  issuerRef:    name: letsencrypt-prodEOF\n等待数分钟后，或者通过命令查询证书分配情况，当看到 Type:Ready 时，即代表证书已经申请成功，回到 ingress 那里，给域名指定一个证书就可以了。\n至此安装完毕，所有 pod 如下：\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475# kubectl get po -ANAMESPACE                   NAME                                                     READY   STATUS    RESTARTS       AGEcattle-fleet-local-system   fleet-agent-57497ff7dc-b8kl5                             1/1     Running   0              3h12mcattle-fleet-system         fleet-controller-5746685958-szdjn                        1/1     Running   0              3h13mcattle-fleet-system         gitjob-cc9948fd7-xzb75                                   1/1     Running   0              3h13mcattle-monitoring-system    alertmanager-rancher-monitoring-alertmanager-0           2/2     Running   0              3h7mcattle-monitoring-system    prometheus-rancher-monitoring-prometheus-0               3/3     Running   0              3h7mcattle-monitoring-system    pushprox-k3s-server-client-5gck9                         1/1     Running   0              3h7mcattle-monitoring-system    pushprox-k3s-server-client-hhfj8                         1/1     Running   0              3h7mcattle-monitoring-system    pushprox-k3s-server-client-pgrbh                         1/1     Running   0              3h7mcattle-monitoring-system    pushprox-k3s-server-proxy-f4f5d4874-gfkmc                1/1     Running   0              3h7mcattle-monitoring-system    rancher-monitoring-grafana-57777cc795-2pzs6              3/3     Running   0              3h7mcattle-monitoring-system    rancher-monitoring-kube-state-metrics-5bc8bb48bd-jmb2j   1/1     Running   0              3h7mcattle-monitoring-system    rancher-monitoring-operator-f79dc4944-lg8m8              1/1     Running   0              3h7mcattle-monitoring-system    rancher-monitoring-prometheus-adapter-8846d4757-qcjcd    1/1     Running   0              3h7mcattle-monitoring-system    rancher-monitoring-prometheus-node-exporter-hq82r        1/1     Running   0              3h7mcattle-monitoring-system    rancher-monitoring-prometheus-node-exporter-mmp8n        1/1     Running   0              3h7mcattle-monitoring-system    rancher-monitoring-prometheus-node-exporter-z99jt        1/1     Running   0              3h7mcattle-system               rancher-7649d589ff-gj894                                 1/1     Running   0              3h14mcattle-system               rancher-7649d589ff-mm5jx                                 1/1     Running   0              3h14mcattle-system               rancher-7649d589ff-xfcm7                                 1/1     Running   0              3h14mcattle-system               rancher-webhook-6958cfcddf-xhqwk                         1/1     Running   0              3h12mcert-manager                cert-manager-66b646d76-qmxms                             1/1     Running   0              165mcert-manager                cert-manager-cainjector-59dc9659c7-tzhzm                 1/1     Running   0              165mcert-manager                cert-manager-webhook-7d8f555998-kh5zl                    1/1     Running   0              165micodex                      nginx-6dd7b9db9b-79rdp                                   1/1     Running   0              178micodex                      nginx-6dd7b9db9b-cs2tm                                   1/1     Running   0              178micodex                      nginx-6dd7b9db9b-w94d8                                   1/1     Running   0              178micodex                      php-fpm-5c5758b88-7tcqx                                  1/1     Running   0              178micodex                      php-fpm-5c5758b88-kw4pp                                  1/1     Running   0              178micodex                      php-fpm-5c5758b88-pgg7n                                  1/1     Running   0              178mingress-nginx               ingress-nginx-controller-5b7cf87848-dnbt8                1/1     Running   0              3h31mingress-nginx               svclb-ingress-nginx-controller-9fdqg                     2/2     Running   0              3h31mingress-nginx               svclb-ingress-nginx-controller-dglh6                     2/2     Running   0              3h31mingress-nginx               svclb-ingress-nginx-controller-szs9s                     2/2     Running   0              3h31mkube-system                 coredns-7796b77cd4-b5gwb                                 1/1     Running   0              5h16mkube-system                 kilo-5khs9                                               1/1     Running   0              5h5mkube-system                 kilo-8574c                                               1/1     Running   0              5h3mkube-system                 kilo-jqw5f                                               1/1     Running   0              5h12mkube-system                 local-path-provisioner-84bb864455-85dtg                  1/1     Running   0              5h16mkube-system                 metrics-server-ff9dbcb6c-jk5v2                           1/1     Running   0              5h16mlonghorn-system             csi-attacher-8b4cc9cf6-dj55n                             1/1     Running   0              3h8mlonghorn-system             csi-attacher-8b4cc9cf6-mj78z                             1/1     Running   0              3h8mlonghorn-system             csi-attacher-8b4cc9cf6-n8v7v                             1/1     Running   0              3h8mlonghorn-system             csi-provisioner-59b7b8b7b8-5skdn                         1/1     Running   0              3h8mlonghorn-system             csi-provisioner-59b7b8b7b8-tddz2                         1/1     Running   0              3h8mlonghorn-system             csi-provisioner-59b7b8b7b8-xqzjp                         1/1     Running   0              3h8mlonghorn-system             csi-resizer-68ccff94-gh9fx                               1/1     Running   0              3h8mlonghorn-system             csi-resizer-68ccff94-lxlst                               1/1     Running   0              3h8mlonghorn-system             csi-resizer-68ccff94-rkftz                               1/1     Running   0              3h8mlonghorn-system             csi-snapshotter-6d7d679c98-477s4                         1/1     Running   0              3h8mlonghorn-system             csi-snapshotter-6d7d679c98-bt9ww                         1/1     Running   0              3h8mlonghorn-system             csi-snapshotter-6d7d679c98-qkcff                         1/1     Running   0              3h8mlonghorn-system             engine-image-ei-d474e07c-2nk6r                           1/1     Running   0              3h8mlonghorn-system             engine-image-ei-d474e07c-jqsgl                           1/1     Running   0              3h8mlonghorn-system             engine-image-ei-d474e07c-vh48h                           1/1     Running   0              3h8mlonghorn-system             instance-manager-e-8b411f4a                              1/1     Running   0              3h8mlonghorn-system             instance-manager-e-da92c7e5                              1/1     Running   0              3h8mlonghorn-system             instance-manager-e-fb0588b7                              1/1     Running   0              3h8mlonghorn-system             instance-manager-r-5c615480                              1/1     Running   0              3h8mlonghorn-system             instance-manager-r-9e4dac9d                              1/1     Running   0              3h8mlonghorn-system             instance-manager-r-cfd922a1                              1/1     Running   0              3h8mlonghorn-system             longhorn-csi-plugin-fmc6b                                2/2     Running   0              3h8mlonghorn-system             longhorn-csi-plugin-g62hj                                2/2     Running   0              3h8mlonghorn-system             longhorn-csi-plugin-k7mlz                                2/2     Running   0              3h8mlonghorn-system             longhorn-driver-deployer-7ffd665594-2q8rm                1/1     Running   0              3h9mlonghorn-system             longhorn-manager-55hjs                                   1/1     Running   1 (3h8m ago)   3h9mlonghorn-system             longhorn-manager-mdtn2                                   1/1     Running   1 (3h8m ago)   3h9mlonghorn-system             longhorn-manager-twg2m                                   1/1     Running   0              3h9mlonghorn-system             longhorn-ui-556866b6bb-6wpb7                             1/1     Running   0              3h9mlonghorn-system             share-manager-pvc-7a347acd-b8e8-44df-a2c0-4b4ec54d84fc   1/1     Running   0              3h4mvelero                      restic-8qkjv                                             1/1     Running   0              156mvelero                      restic-b9nfn                                             1/1     Running   0              156mvelero                      restic-jmt9n                                             1/1     Running   0              156mvelero                      velero-79fd55b75d-q68lm                                  1/1     Running   0              156m\n最后是完成后的一些图片，目前已经稳定运行半个月。\n\n\n\n后记：Arm 架构的服务器在几个大厂的推动下，取得不错的成绩，在程序方面也有许多开发者做了这部分的兼容，使用起来是没有问题的。未来以 arm 为代表的精简指令集的 soc 必将在服务器领域取得不错的成绩的。\n","slug":"OCI/使用K3S+ Kilo部署安全多区云原生ARM集群，支持nginx-ingress及velero备份","date":"2022-07-30T12:39:51.000Z","categories_index":"velero,OCI","tags_index":"https,server,ingress","author_index":"dandeliono"},{"id":"fcfa6f5ea8dabe903d8fbcda021fdf5f","title":"应对掘金CDN开启防盗链 记一次爬取markdown图片的经历","content":"应对掘金CDN开启防盗链 记一次爬取markdown图片的经历使用 markdown 写文章有什么好处?\nmarkdown 是一种纯文本格式 (后缀.md), 写法简单, 不用考虑排版, 输出的文章样式简洁优雅\nmarkdown 自带开源属性, 一次书写后, 即可在任意支持 markdown 格式的平台发布 (国内支持的平台有, 掘金, 知乎 (以文档方式导入), 简书 (原本是最好用的, 最近在走下坡路))\n著名代码托管平台 github, 每个代码仓库的说明书README.md就是典型的 markdown 格式\n\n怎么办?我只好将 md 文档保存到本地, 然后根据 md 保存的源图片信息, 使用爬虫爬取图片到本地, 然后将图片上传到 github 仓库 (github 仓库支持图片上传, 而且不封外链), 将原图片信息替换为 github 仓库保存的图片信息\n首先在 github 新建一个名为 [test] 的仓库, 用来存储图片\n\n将仓库 clone 到本地 的 /Users/github文件夹\ncd &#x2F;Users&#x2F;githubgit clone https://github.com/test.git\n\n\n复制\n\n并保证 在此文件夹下, 有权限 push 到 github,\n将 github 已有的. md 文章对应的仓库下载到本地 (以星聚弃疗榜为例)git clone https://github.com/StarsAndClown.git\n\n复制\n\n编写 python 脚本 md_images_upload.py\n此脚本:\n\n能搜索当前目录下所有 md 文件, 将每个 md 中的图片爬取到本地, 存放到/Users/github/test/images目录;\n图片爬取完成后, 自动将/Users/github/test/images目录下的所有图片, push 到 Github\n使用 Github 中的新图片地址, 替换原图片地址\n大功告成123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175import osimport imghdrimport reimport requestsimport shutilimport gitimport hashlib## 用户名user_name = &quot;user&quot;;## 仓库名github_repository = &quot;test&quot;;## git仓库在本机的位置git_repository_folder = &quot;/Users/github/test&quot;## 存放图片的git文件夹路径git_images_folder = &quot;/Users/github/test/images&quot;## 设置忽略目录ignore_dir_list=[&quot;.git&quot;]# 设置用户代理头headers = &#123;    # 设置用户代理头(为狼披上羊皮)    &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36&quot;,&#125;# 根据输入的url输入md5命名def create_name(src_name):    src_name = src_name.encode(&quot;utf-8&quot;)    s = hashlib.md5()    s.update(src_name)    return s.hexdigest()# 获取当前目录下所有md文件def get_md_files(md_dir):    md_files = [];    for root, dirs, files in sorted(os.walk(md_dir)):        for file in files:            # 获取.md结尾的文件            if(file.endswith(&quot;.md&quot;)):                file_path = os.path.join(root, file)                print(file_path)                #忽略排除目录                need_append = 0                for ignore_dir in ignore_dir_list:                    if(ignore_dir in file_path.split(&quot;/&quot;) == True):                        need_append = 1                if(need_append == 0):                    md_files.append(file_path)    return md_files# 获取网络图片def get_http_image(image_url):    image_info = &#123;&quot;image_url&quot;: &quot;&quot;, &quot;new_image_url&quot;: &quot;&quot;&#125;    file_uuid_name = create_name(image_url)    image_data = requests.get(image_url, headers=headers).content    # 创建临时文件    tmp_new_image_path_and_name = os.path.join(git_images_folder, file_uuid_name)    with open(tmp_new_image_path_and_name, &quot;wb+&quot;) as f:        f.write(image_data)    img_type = imghdr.what(tmp_new_image_path_and_name)    if(img_type == None):        img_type = &quot;&quot;    else:        img_type = &quot;.&quot;+img_type    # 生成新的名字加后缀    new_image_path_and_name = tmp_new_image_path_and_name+img_type    # 重命名图片    os.rename(tmp_new_image_path_and_name, new_image_path_and_name)    new_image_url = &quot;https://raw.githubusercontent.com/&quot;+ user_name + &quot;/&quot; +github_repository+&quot;/master/&quot;+git_images_folder.split(&quot;/&quot;)[-1]+&quot;/&quot;+new_image_path_and_name.split(&quot;/&quot;)[-1]    image_info = &#123;        &quot;image_url&quot;: image_url,        &quot;new_image_url&quot;: new_image_url    &#125;    print(image_info)    return image_info# 获取本地图片def get_local_image(image_url):    image_info = &#123;&quot;image_url&quot;: &quot;&quot;, &quot;new_image_url&quot;: &quot;&quot;&#125;    try:        # 创建文件名        file_uuid_name = uuid.uuid4().hex        # 获取图片类型        img_type = image_url.split(&quot;.&quot;)[-1]        # 新的图片名和文件后缀        image_name = file_uuid_name+&quot;.&quot;+img_type        # 新的图片路径和名字        new_image_path_and_name = os.path.join(git_images_folder, image_name);        shutil.copy(image_url, new_image_path_and_name)        # 生成url        new_image_url = &quot;https://raw.githubusercontent.com/&quot;+ user_name + &quot;/&quot; +github_repository+&quot;/master/&quot;+git_images_folder.split(&quot;/&quot;)[-1]+&quot;/&quot;+new_image_path_and_name.split(&quot;/&quot;)[-1]        # 图片信息        image_info = &#123;            &quot;image_url&quot;: image_url,            &quot;new_image_url&quot;: new_image_url        &#125;        print(image_info)        return image_info    except Exception as e:        print(e)    return image_info    # 爬取单个md文件内的图片def get_images_from_md_file(md_file):    md_content = &quot;&quot;    image_info_list = []    with open(md_file, &quot;r+&quot;) as f:        md_content = f.read()        image_urls = re.findall(r&quot;!\\[.*?\\]\\((.*?)\\)&quot;, md_content)        for image_url in image_urls:            # 处理本地图片            if(image_url.startswith(&quot;http&quot;) == False):                image_info = get_local_image(image_url)                image_info_list.append(image_info)            # 处理网络图片            else:                # 不爬取svg                if(image_url.startswith(&quot;https://img.shields.io&quot;) == False):                    try:                        image_info = get_http_image(image_url)                        image_info_list.append(image_info)                    except Exception as e:                        print(image_url, &quot;无法爬取, 跳过!&quot;)                        pass        for image_info in image_info_list:            md_content = md_content.replace(image_info[&quot;image_url&quot;], image_info[&quot;new_image_url&quot;])        print(&quot;替换完成后::&quot;, md_content);        md_content = md_content    with open(md_file, &quot;w+&quot;) as f:        f.write(md_content)def git_push_to_origin():    # 通过git提交到github仓库    repo = git.Repo(git_repository_folder)    print(&quot;初始化成功&quot;, repo)    index = repo.index    index.add([&quot;images/&quot;])    print(&quot;add成功&quot;)    index.commit(&quot;新增图片1&quot;)    print(&quot;commit成功&quot;)    # 获取远程仓库    remote = repo.remote()    print(&quot;远程仓库&quot;, remote);    remote.push()    print(&quot;push成功&quot;)def main():    if(os.path.exists(git_images_folder)):        pass    else:        os.mkdir(git_images_folder)    # 获取本目录下所有md文件    md_files = get_md_files(&quot;./&quot;)    # 将md文件依次爬取    for md_file in md_files:      # 爬取单个md文件内的图片      get_images_from_md_file(md_file)        git_push_to_origin()    if __name__ == &quot;__main__&quot;:    main()\n\n\n\n\n\n\n\n\n\n\n几个优化点:\n\n支持 md 引用本地目录图片的爬取 (以后就可以在本地编写 markdown 文件了, 编写完成后, 运行上述脚本, 即可自动将 md 引用的本地图片上传到 github, 同时本地图片的引用地址被 github 在线图片地址所取代)\n为防止图片重名, 使用 uuid 重命名图片名称 (后面发现使用 uuid 会导致相同的网络图片反复爬取保存, 所以后面使用网络图片的 url 地址对应的 md5 码为新名称, 即可防止生成内容相同, 名称不同的图片)\n爬取本地图片, 依然使用 uuid 重名防止重复 (个人命名可能会反复使用001.png, 002.png等常用名称)\n对爬取的图片, 进行了类型判断, 自动补充图片扩展名\n\n使用方法\n安装 python3\n\n安装方法见 Python 数据挖掘 环境搭建\n\n将脚本md_images_upload.py放到/Users/github/test目录 (这里目录可以按照自己的来, 但脚本顶部的几行参数也要修改)\n\n\n\n\n在命令行安装相关依赖包\n12pip3 install requestspip3 install gitpython\n\n从命令行进入/Users/github/test\ncd &#x2F;Users&#x2F;github&#x2F;test\n\n\n复制\n\n运行脚本\npython3 md_images_upload.py\n\n\n复制\n\n这里我已经是第二次替换图片了, 所以上面的动图显示的原图片也是 GitHub 的图片, 说明脚本第一次已完全替换成功~\n图片又可以显示了\n","slug":"WORK/应对掘金CDN开启防盗链 记一次爬取markdown图片的经历","date":"2022-07-29T14:06:36.000Z","categories_index":"github,WORK","tags_index":"https,com,png","author_index":"dandeliono"},{"id":"c154332e2066fa4ee3ef3036412a15b5","title":"使用线程池时候当程序结束时候记得调用shutdown关闭线程池","content":"使用线程池时候当程序结束时候记得调用shutdown关闭线程池使用线程池时候当程序结束时候记得调用 shutdown 关闭线程池日常开发中为了便于线程的有效复用，线程池是经常会被用的工具，然而线程池使用完后如果不调用 shutdown 会导致线程池资源一直不会被释放。下面通过简单例子来说明该问题。\n1 问题复现下面通过一个例子说明当不调用线程池对象的 shutdown 方法后，当线程池里面的任务执行完毕后主线程这个 JVM 不会退出。\npublic class TestShutDown &#123;\n\n        static void asynExecuteOne() &#123;\n        ExecutorService executor = Executors.newSingleThreadExecutor();\n        executor.execute(new  Runnable() &#123;\n            public void run() &#123;\n                System.out.println(&quot;--async execute one ---&quot;);\n            &#125;\n        &#125;);\n    &#125;\n    \n    static void asynExecuteTwo() &#123;\n        ExecutorService executor = Executors.newSingleThreadExecutor();\n        executor.execute(new  Runnable() &#123;\n            public void run() &#123;\n                System.out.println(&quot;--async execute two ---&quot;);\n            &#125;\n        &#125;);\n    &#125;\n    \n\n    public static void main(String[] args) &#123;\n       \n        System.out.println(&quot;---sync execute---&quot;);\n       \n        asynExecuteOne();\n       \n        asynExecuteTwo();\n       \n        System.out.println(&quot;---execute over---&quot;);\n    &#125;\n&#125;\n\n如上代码主线程里面首先同步执行了操作（1）然后执行操作（2）（3），操作（2）（3）使用线程池的一个线程执行异步操作，我们期望当主线程和操操作（2）（3）执行完线程池里面的任务后整个 JVM 就会退出，但是执行结果却如下：\n\nimage.png\n右上角红色方块说明 JVM 进程还没有退出，Mac 上执行ps -eaf|grep java后发现 Java 进程还是存在的，这是什么情况那？修改操作（2）（3）在方法里面添加调用线程池的 shutdown 方法如下代码：\n static void asynExecuteOne() &#123;\n        ExecutorService executor = Executors.newSingleThreadExecutor();\n        executor.execute(new  Runnable() &#123;\n            public void run() &#123;\n                System.out.println(&quot;--async execute one ---&quot;);\n            &#125;\n        &#125;);\n        \n        executor.shutdown();\n    &#125;\n    \n    static void asynExecuteTwo() &#123;\n        ExecutorService executor = Executors.newSingleThreadExecutor();\n        executor.execute(new  Runnable() &#123;\n            public void run() &#123;\n                System.out.println(&quot;--async execute two ---&quot;);\n            &#125;\n        &#125;);\n        \n        executor.shutdown();\n    &#125;\n\n在执行就会发现 JVM 已经退出了，使用ps -eaf|grep java后发现 Java 进程以及不存在了，这说明只有调用了线程池的 shutdown 方法后当线程池任务执行完毕后线程池资源才会释放。\n2 问题分析下面看下为何如此那？大家或许还记得基础篇讲解的守护线程与用户线程吧，JVM 退出的条件是当前不存在用户线程，而线程池默认的 ThreadFactory 创建的线程是用户线程，\n static class DefaultThreadFactory implements ThreadFactory &#123;\n        ...\n        public Thread newThread(Runnable r) &#123;\n            Thread t = new Thread(group, r,\n                                  namePrefix + threadNumber.getAndIncrement(),\n                                  0);\n            if (t.isDaemon())\n                t.setDaemon(false);\n            if (t.getPriority() != Thread.NORM_PRIORITY)\n                t.setPriority(Thread.NORM_PRIORITY);\n            return t;\n        &#125;\n    &#125;\n\n如上代码可知线程池默认的线程工厂创建创建的都是用户线程。而线程池里面的核心线程是一直会存在的，如果没有任务则会阻塞，所以线程池里面的用户线程一直会存在. 而 shutdown 方法的作用就是让这些核心线程终止，下面在简单看下 shutdown 重要代码：\n public void shutdown() &#123;\n        final ReentrantLock mainLock = this.mainLock;\n        mainLock.lock();\n        try &#123;\n            ...\n            \n            advanceRunState(SHUTDOWN);\n            \n            interruptIdleWorkers();\n            ...\n        &#125; finally &#123;\n            mainLock.unlock();\n        &#125;\n           ...\n        &#125;\n\n可知 shutdown 里面设置了线程池状态为 SHUTDOWN，并且设置了所有工作线程的中断标志，那么下面在简单看下工作线程 Worker 里面是不是发现中断标志被设置了就会退出了。\n final void runWorker(Worker w) &#123;\n            ...\n            try &#123;\n            while (task != null || (task = getTask()) != null) &#123;\n               ...            \n            &#125;\n            ...\n          &#125; finally &#123;\n            ...\n        &#125;\n    &#125;\n\nprivate Runnable getTask() &#123;\n        boolean timedOut = false; \n\n        for (;;) &#123;\n            ...\n            \n            if (rs &gt;= SHUTDOWN &amp;&amp; (rs &gt;= STOP || workQueue.isEmpty())) &#123;\n                decrementWorkerCount();\n                return null;\n            &#125;\n            \n            try &#123;\n                \n                Runnable r = timed ?\n                    workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) :\n                    workQueue.take();\n                if (r != null)\n                    return r;\n                timedOut = true;\n            &#125; catch (InterruptedException retry) &#123;\n                timedOut = false;\n            &#125;\n        &#125;\n    &#125;\n\n如上代码正常情况下如果队列里面没有任务了，工作线程阻塞到代码（2）等待从工工作队列里面获取一个任务，这时候如果调用了线程池的 shutdown 命令而 shutdown 命令会中断所有工作线程，所以代码（2）会抛出处抛出 InterruptedException 异常而返回，而这个异常被 catch 了，所以继续执行代码（1），而 shutdown 时候设置了线程池的状态为 SHUTDOWN 所以 getTask 方法返回了 null，所以 runWorker 方法退出循环，该工作线程就退出了。\n3 总结本节通过一个简单的使用线程池异步执行任务案例介绍了线程池使用完后要如果不调用 shutdown 会导致线程池的线程资源一直不会被释放，然后通过源码分析了没有被释放的原因。所以日常开发中使用线程池的场景一定不要忘记了调用 shutdown 方法设置线程池状态和中断工作线程池\n","slug":"JAVA/使用线程池时候当程序结束时候记得调用shutdown关闭线程池","date":"2022-07-26T10:03:32.000Z","categories_index":"shutdown,JAVA","tags_index":"JVM,使用线程池时候当程序结束时候记得调用,关闭线程池","author_index":"dandeliono"},{"id":"77b291e312b6296807210b6244b44cc2","title":"nginx 反向代理和路径重写","content":"nginx 反向代理和路径重写nginx 反向代理路径问题http://abc.com:8080 写法和 http://abc.com:8080/ 写法的区别如下:\n123456789location /NginxTest/ &#123;    proxy_pass  http://abc.com:8080;&#125;location /NginxTest/ &#123;    proxy_pass  http://abc.com:8080/;&#125;\n\n上面两种配置，区别只在于 proxy_pass 转发的路径后是否带 “&#x2F;”\n针对情况 1:\n（带参数）访问 http://abc.com:8080/NginxTest/servlet/MyServlet?name=123333333 ，则被 nginx 代理后，请求路径便会访问 http://abc.com:8080/NginxTest/servlet/MyServlet?name=123333333\n（不带参）访问 http://abc.com/NginxTest/servlet/MyServlet ，则被 nginx 代理后，请求路径便会访问 http://abc.com:8080/NginxTest/servlet/MyServlet\n\n针对情况 2：\n访问 http://abc.com:8080/NginxTest/test.jsp，则被 nginx 代理后，请求路径会变为 http://abc.com:8080/test.jsp ，直接访问 server 的根资源。\n访问 http://abc.com:8080/NginxTest/NginxTest/NginxTest/servlet/MyServlet ，被 nginx 代理后，请求路径才会访问 http://abc.com:8080/NginxTest/servlet/MyServlet\n\n注意：上面两种访问路径的差别。修改配置后重启 nginx 代理就成功了\nNginx 地址重写Rewrite 的语法：12rewrite regex URL [flag];\n\nrewrite 是关键字，regex 是正则表达式，URL 是要替代的内容，[flag]是标记位的意思，它有以下几种值： \n\nlast: 相当于 Apache 的[L]标记，表示完成 rewrite\nbreak: 停止执行当前虚拟主机的后续 rewrite 指令集\nredirect: 返回 302 临时重定向，地址栏会显示跳转后的地址\npermanent: 返回 301 永久重定向，地址栏会显示跳转后的地址\n\n12345location /NginxTest/ &#123;    rewrite  ^/NginxTest/(.*)$  /$1  break;    proxy_pass  http://abc.com:8080;&#125;\n\n\n^~&#x2F;NginxTest&#x2F; 是一个匹配规则，用于拦截请求，匹配任何以 &#x2F;NginxTest&#x2F; 开头的地址，匹配符合以后就停止往下搜索正则；\nrewrite ^&#x2F;NginxTest&#x2F;(.*)$ &#x2F;$1 break；代表重写拦截进来的请求，并且只能对域名后边的除去传递的参数外的字符串起作用。例如：http://abc.com:8080/NginxTest/NginxTest/servlet/MyServlet?name=lovleovlove 重写，只对 &#x2F;NginxTest&#x2F;NginxTest&#x2F;servlet&#x2F;MyServlet 重写。\n\n\n访问地址：http://abc.com:8080/NginxTest/NginxTest/servlet/MyServlet?name=lovleovlove ， 实际访问的地址（重写地址）为 http://abc.com:8080/NginxTest/servlet/MyServlet?name=lovleovlove\n访问地址：http://abc.com:8080/NginxTest/NginxTest/servlet/MyServlet ，实际访问的地址（重写地址）为 http://abc.com:8080/NginxTest/servlet/MyServlet\n\n\nrewrite 后面的参数是一个简单的正则 ^&#x2F;NginxTest&#x2F;(.*)$ &#x2F;$1 ,$1 代表正则中的第一个 (),$2 代表第二个() 的值, 以此类推。break 代表匹配一个之后停止匹配。\n\n配置样例\n全局配置\n\n1234567891011121314151617181920212223242526http &#123;    include       /etc/nginx/mime.types;    default_type  application/octet-stream;    log_format  main  &#x27;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#x27;                      &#x27;$status $body_bytes_sent &quot;$http_referer&quot; &#x27;                      &#x27;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#x27;;    access_log  /var/log/nginx/access.log  main;    sendfile        on;    tcp_nopush      on;    tcp_nodelay     on;    keepalive_timeout  65;    gzip  on;    gzip_vary on;    gzip_proxied any;    gzip_comp_level 6;    gzip_buffers 16 8k;    gzip_http_version 1.1;    gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript image/svg+xml;    include /etc/nginx/conf.d/*.conf;&#125;\n\n\nhttp 请求的配置样例\n\n12345678910111213141516171819202122232425262728293031323334353637383940414243444546upstream gateway_upstream &#123;    server 127.0.0.1:8760 fail_timeout=0;&#125;upstream jenkins_upstream &#123;    server 127.0.0.1:8080 fail_timeout=0;&#125;server &#123;    listen       80;    server_name  localhost;                location / &#123;        root   /usr/share/nginx/html;        index  index.html index.htm;    &#125;                error_page   500 502 503 504  /50x.html;    location = /50x.html &#123;        root   /usr/share/nginx/html;    &#125;            location /jenkins &#123;        proxy_pass   http://jenkins_upstream/jenkins;        proxy_set_header Host $host;        proxy_set_header X-Real-IP $remote_addr;        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;    &#125;    location ^~ /api/ &#123;        proxy_pass   http://gateway_upstream/;        proxy_set_header Host $host;                    &#125;&#125;\n\n\nhttps 请求配置样例\n\n12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970server &#123;    listen      80;    listen    [::]:80;    server_name  abeille.top www.abeille.top;                  return 301 https://$host$request_uri;&#125;upstream jenkins_upstream &#123;    server 127.0.0.1:8080 fail_timeout=0;&#125;upstream gateway_upstream &#123;    server 127.0.0.1:8760 fail_timeout=0;&#125;server &#123;    listen         443 default ssl http2;    listen         [::]:443 default ssl http2;    server_name    abeille.top www.abeille.top;      ssl_certificate     302xxxw.abeille.top.pem;       ssl_certificate_key 302xxxw.abeille.top.key;       ssl_session_timeout 5m;    ssl_session_cache    shared:MozSSL:10m;          ssl_protocols TLSv1.3;    ssl_prefer_server_ciphers off;    add_header Strict-Transport-Security &quot;max-age=63072000&quot; always;        add_header X-Frame-Options DENY;                        add_header X-Content-Type-Options nosniff;                     add_header X-Xss-Protection 1;                                ssl_stapling on;    ssl_stapling_verify on;    error_page   500 502 503 504  /50x.html;        location = /50x.html &#123;        root   /usr/share/nginx/html;    &#125;    location ^~ /jenkins &#123;        proxy_pass              http://jenkins_upstream/jenkins;        proxy_set_header Host $host;        proxy_set_header X-Real-IP $remote_addr;        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;    &#125;    location ^~ /api/ &#123;                      proxy_pass       http://gateway_upstream/;        proxy_set_header Host $host;                proxy_set_header X-Real-IP $remote_addr;        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;    &#125;    location / &#123;        root   /usr/share/nginx/dist;        index  index.html index.htm;        try_files $uri $uri/ /index.html last;      &#125;&#125;\n","slug":"MIDDLEWARE/nginx 反向代理和路径重写","date":"2022-07-14T09:31:31.000Z","categories_index":"NginxTest,MIDDLEWARE","tags_index":"com,http,abc","author_index":"dandeliono"},{"id":"61ee30c1a55828f654b62f168fb5fa1d","title":"在Spring Boot启动时执行代码的几种方法","content":"在Spring Boot启动时执行代码的几种方法前言有时候我们需要在应用启动时执行一些代码片段，这些片段可能是仅仅是为了记录 log，也可能是在启动时检查与安装证书 ，诸如上述业务要求我们可能会经常碰到\nSpring Boot 提供了至少 5 种方式用于在应用启动时执行代码。我们应该如何选择？本文将会逐步解释与分析这几种不同方式\nCommandLineRunnerCommandLineRunner 是一个接口，通过实现它，我们可以在 Spring 应用成功启动之后 执行一些代码片段\n123456789101112131415@Slf4j@Component@Order(2)public class MyCommandLineRunner implements CommandLineRunner &#123;\t\t@Override\tpublic void run(String... args) throws Exception &#123;\t\tlog.info(&quot;MyCommandLineRunner order is 2&quot;);\t\tif (args.length &gt; 0)&#123;\t\t\tfor (int i = 0; i &lt; args.length; i++) &#123;\t\t\t\tlog.info(&quot;MyCommandLineRunner current parameter is: &#123;&#125;&quot;, args[i]);\t\t\t&#125;\t\t&#125;\t&#125;&#125;\n\n当 Spring Boot 在应用上下文中找到 CommandLineRunner bean，它将会在应用成功启动之后调用 run() 方法，并传递用于启动应用程序的命令行参数\n通过如下 maven 命令生成 jar 包:\n1mvn clean package\n\n通过终端命令启动应用，并传递参数:\n1java -jar springboot-application-startup-0.0.1-SNAPSHOT.jar --foo=bar --name=rgyb\n\n查看运行结果:\n\n到这里我们可以看出几个问题:\n\n命令行传入的参数并没有被解析，而只是显示出我们传入的字符串内容 --foo=bar，--name=rgyb，我们可以通过 ApplicationRunner 解析，我们稍后看\n在重写的 run() 方法上有 throws Exception 标记，Spring Boot 会将 CommandLineRunner 作为应用启动的一部分，如果运行 run() 方法时抛出 Exception，应用将会终止启动\n我们在类上添加了 @Order(2) 注解，当有多个 CommandLineRunner 时，将会按照 @Order 注解中的数字从小到大排序 (数字当然也可以用复数)\n\n\n\n\n\n\n\n\n\n\n⚠️不要使用 @Order 太多看到 order 这个 “黑科技” 我们会觉得它可以非常方便将启动逻辑按照指定顺序执行，但如果你这么写，说明多个代码片段是有相互依赖关系的，为了让我们的代码更好维护，我们应该减少这种依赖使用\n小结如果我们只是想简单的获取以空格分隔的命令行参数，那 MyCommandLineRunner 就足够使用了\nApplicationRunner上面提到，通过命令行启动并传递参数，MyCommandLineRunner 不能解析参数，如果要解析参数，那我们就要用到 ApplicationRunner 参数了\n1234567891011@Component@Slf4j@Order(1)public class MyApplicationRunner implements ApplicationRunner &#123;\t@Override\tpublic void run(ApplicationArguments args) throws Exception &#123;\t\tlog.info(&quot;MyApplicationRunner order is 1&quot;);\t\tlog.info(&quot;MyApplicationRunner Current parameter is &#123;&#125;:&quot;, args.getOptionValues(&quot;foo&quot;));\t&#125;&#125;\n\n重新打 jar 包，运行如下命令:\n1java -jar springboot-application-startup-0.0.1-SNAPSHOT.jar --foo=bar,rgyb\n\n运行结果如下:\n\n到这里我们可以看出:\n\n同 MyCommandLineRunner 相似，但 ApplicationRunner 可以通过 run 方法的 ApplicationArguments 对象解析出命令行参数，并且每个参数可以有多个值在里面，因为 getOptionValues 方法返回 List 数组\n在重写的 run() 方法上有 throws Exception 标记，Spring Boot 会将 CommandLineRunner 作为应用启动的一部分，如果运行 run() 方法时抛出 Exception，应用将会终止启动\nApplicationRunner 也可以使用 @Order 注解进行排序，从启动结果来看，它与 CommandLineRunner 共享 order 的顺序，稍后我们通过源码来验证这个结论\n\n小结如果我们想获取复杂的命令行参数时，我们可以使用 ApplicationRunner\nApplicationListener如果我们不需要获取命令行参数时，我们可以将启动逻辑绑定到 Spring 的 ApplicationReadyEvent 上\n12345678910@Slf4j@Component@Order(0)public class MyApplicationListener implements ApplicationListener&lt;ApplicationReadyEvent&gt; &#123;\t@Override\tpublic void onApplicationEvent(ApplicationReadyEvent applicationReadyEvent) &#123;\t\tlog.info(&quot;MyApplicationListener is started up&quot;);\t&#125;&#125;\n\n运行程序查看结果:\n\n到这我们可以看出:\n\nApplicationReadyEvent 当且仅当 在应用程序就绪之后才被触发，甚至是说上面的 Listener 要在本文说的所有解决方案都执行了之后才会被触发，最终结论请稍后看\n代码中我用 Order(0) 来标记，显然 ApplicationListener 也是可以用该注解进行排序的，按数字大小排序，应该是最先执行。但是，这个顺序仅用于同类型的 ApplicationListener 之间的排序，与前面提到的 ApplicationRunners 和 CommandLineRunners 的排序并不共享\n\n小结如果我们不需要获取命令行参数，我们可以通过 ApplicationListener&lt;ApplicationReadyEvent&gt; 创建一些全局的启动逻辑，我们还可以通过它获取 Spring Boot 支持的 configuration properties 环境变量参数\n\n如果你看过我之前写的 Spring Bean 生命周期三部曲:\n\nSpring Bean 生命周期之缘起\nSpring Bean 生命周期之缘尽\nSpring Aware 到底是什么？\n\n那么你会对下面两种方式非常熟悉了\n@PostConstruct创建启动逻辑的另一种简单解决方案是提供一种在 bean 创建期间由 Spring 调用的初始化方法。我们要做的就只是将 @PostConstruct 注解添加到方法中：\n12345678910@Component@Slf4j@DependsOn(&quot;myApplicationListener&quot;)public class MyPostConstructBean &#123;\t@PostConstruct\tpublic void testPostConstruct()&#123;\t\tlog.info(&quot;MyPostConstructBean&quot;);\t&#125;&#125;\n\n查看运行结果:\n\n从上面运行结果可以看出:\n\nSpring 创建完 bean 之后 (在启动之前)，便会立即调用 @PostConstruct 注解标记的方法，因此我们无法使用 @Order 注解对其进行自由排序，因为它可能依赖于 @Autowired 插入到我们 bean 中的其他 Spring bean。\n相反，它将在依赖于它的所有 bean 被初始化之后被调用，如果要添加人为的依赖关系并由此创建一个排序，则可以使用 @DependsOn 注解（虽然可以排序，但是不建议使用，理由和 @Order 一样）\n\n小结@PostConstruct 方法固有地绑定到现有的 Spring bean，因此应仅将其用于此单个 bean 的初始化逻辑；\nInitializingBean与 @PostConstruct 解决方案非常相似，我们可以实现 InitializingBean 接口，并让 Spring 调用某个初始化方法:\n12345678910@Component@Slf4jpublic class MyInitializingBean implements InitializingBean &#123;\t@Override\tpublic void afterPropertiesSet() throws Exception &#123;\t\tlog.info(&quot;MyInitializingBean.afterPropertiesSet()&quot;);\t&#125;&#125;\n\n查看运行结果:\n\n从上面的运行结果中，我们得到了和 @PostConstruct 一样的效果，但二者还是有差别的\n\n\n\n\n\n\n\n\n\n⚠️ @PostConstruct 和 afterPropertiesSet 区别\nafterPropertiesSet，顾名思义「在属性设置之后」，调用该方法时，该 bean 的所有属性已经被 Spring 填充。如果我们在某些属性上使用 @Autowired（常规操作应该使用构造函数注入），那么 Spring 将在调用afterPropertiesSet 之前将 bean 注入这些属性。但 @PostConstruct 并没有这些属性填充限制\n所以 InitializingBean.afterPropertiesSet 解决方案比使用 @PostConstruct 更安全，因为如果我们依赖尚未自动注入的 @Autowired 字段，则 @PostConstruct 方法可能会遇到 NullPointerExceptions\n\n小结如果我们使用构造函数注入，则这两种解决方案都是等效的\n源码分析请打开你的 IDE (重点代码已标记注释):\n\n\n\n\n\n\n\n\n\nMyCommandLineRunner 和 ApplicationRunner 是在何时被调用的呢？\n打开 SpringApplication.java 类，里面有 callRunners 方法\n123456789101112131415161718192021private void callRunners(ApplicationContext context, ApplicationArguments args) &#123;    List&lt;Object&gt; runners = new ArrayList&lt;&gt;();        runners.addAll(context.getBeansOfType(ApplicationRunner.class).values());        runners.addAll(context.getBeansOfType(CommandLineRunner.class).values());        AnnotationAwareOrderComparator.sort(runners);        for (Object runner : new LinkedHashSet&lt;&gt;(runners)) &#123;        if (runner instanceof ApplicationRunner) &#123;            callRunner((ApplicationRunner) runner, args);        &#125;        if (runner instanceof CommandLineRunner) &#123;            callRunner((CommandLineRunner) runner, args);        &#125;    &#125;&#125;\n\n强烈建议完整看一下 SpringApplication.java 的全部代码，Spring Boot 启动过程及原理都可以从这个类中找到一些答案\n灵魂追问\n上面程序运行结果， afterPropertiesSet 方法调用先于 @PostConstruct 方法，但这和我们在 Spring Bean 生命周期之缘起 中的调用顺序恰恰相反，你知道为什么吗？\nMyPostConstructBean 通过 @DependsOn(&quot;myApplicationListener&quot;) 依赖了 MyApplicationListener，为什么调用结果前者先与后者呢？\n为什么不建议 @Autowired 形式依赖注入\n\n在写 Spring Bean 生命周期时就有朋友问我与之相关的问题，显然他们在概念上有一些含混，所以，仔细理解上面的问题将会帮助你加深对 Spring Bean 生命周期的理解\nSpring Boot 应用启动执行代码概览图最后画一张图用来总结这几种方式\n\n","slug":"JAVA/在Spring Boot启动时执行代码的几种方法","date":"2022-07-11T15:58:46.000Z","categories_index":"Spring,JAVA","tags_index":"https,小结,PostConstruct","author_index":"dandeliono"},{"id":"7fb39e737e8e188c4592d757c3adaa67","title":"ElasticSearch-Aggregations-GroupBy-实现源码分析","content":"ElasticSearch-Aggregations-GroupBy-实现源码分析\n\n\n\n\n\n\n\n\n在前文 ElasticSearch Aggregations 分析 中，我们提及了 【Aggregation Bucket 的实现】，然而只是用文字简要描述了原理。今天我们会举个实际 groupBy 的例子进行剖析，让大家对 ElasticSearch Aggregations 的工作原理有更深入的理解\n准备工作\n为了方便调试，我对索引做了如下配置\n{  “mappings”: {“my_type”: {  “properties”: {    “newtype”: {      “type”:       “string”,      “index”:      “not_analyzed”    },    “num”: {      “type”:       “integer”    }  }}  },   “settings” : {    “index” : {        “number_of_shards” : 1,        “number_of_replicas” : 0    }}}\n\n\n这样只有一个分片，方便 IDE 的跟踪，也算是个看源码的技巧\n\n数据\n{“user” : “kimchy”,“post_date” : “2009-11-15T14:12:12”,“newtype”: “abc”,“message” : “trying out Elasticsearch”,“num” : 10}\n\n\n查询语句假定的查询如下：\n&#123;\n    &quot;from&quot;: 0,\n    &quot;size&quot;: 0,\n    &quot;_source&quot;: &#123;\n        &quot;includes&quot;: [\n            &quot;AVG&quot;\n        ],\n        &quot;excludes&quot;: []\n    &#125;,\n    &quot;aggregations&quot;: &#123;\n        &quot;newtype&quot;: &#123;\n            &quot;terms&quot;: &#123;\n                &quot;field&quot;: &quot;newtype&quot;,\n                &quot;size&quot;: 200\n            &#125;,\n            &quot;aggregations&quot;: &#123;\n                &quot;AVG(num)&quot;: &#123;\n                    &quot;avg&quot;: &#123;\n                        &quot;field&quot;: &quot;num&quot;\n                    &#125;\n                &#125;\n            &#125;\n        &#125;\n    &#125;\n&#125; \n\n其语义类似这个 sql 语句：\nSELECT avg(num) FROM twitter group by newtype \n\n也就是按 newtype 字段进行 group by, 然后对 num 求平均值。在我们实际的业务系统中，这种统计需求也是最多的。\nPhase 概念在查询过程中，ES 是将整个查询分成几个阶段的，大体如下：\n\nQueryPhase\nrescorePhase\nsuggestPhase\naggregationPhase\nFetchPhase\n\n对于全文检索，可能还有 DFSPhase。\n顺带提一点，Spark SQL + ES 的组合，最影响响应时间的地方其实是 Fetch original source 。\n而对于这些 Phase, 并不是一个链路的模式，而是在某个 Phase 调用另外一个 Phase。这个在源码中也很明显，我们看如下一段代码：\n //创建聚合需要的AggregationContext,\n     //里面包含了各个Aggregator\n      aggregationPhase.preProcess(searchContext);\n\n       //实际query,还有聚合操作其实是在这部完成的\n        boolean rescore = execute(searchContext, searchContext.searcher());\n\n        //如果是全文检索，并且需要打分\n        if (rescore) &#123; // only if we do a regular search\n            rescorePhase.execute(searchContext);\n        &#125;\n        suggestPhase.execute(searchContext);\n        //获取聚合结果\n        aggregationPhase.execute(searchContext);       \n        &#125; \n\nAggregation 的相关概念要了解具体是如何实现聚合功能的，则需要了解 ES 的 aggregator 相关的概念。大体有五个：\n\nAggregatorFactory （典型的工厂模式）负责创建 Aggregator 实例\nAggregator (负责提供 collector, 并且提供具体聚合逻辑的类)\nAggregations (聚合结果)\nPipelineAggregator (对聚合结果进一步处理)\nAggregator 的嵌套，比如 示例中的 AvgAggregator 就是根据 GlobalOrdinalsStringTermsAggregator 的以 bucket 为维度，对相关数据进行操作. 这种嵌套结构也是\nBucket 其实就是被 groupBy 字段的数字表示形式。用数字表示，可以节省对应字段列式存储的空间，并且提高性能。\n\nAggregations 实现的机制我们知道，无论检索亦或是聚合查询，本质上都需要转化到 Lucene 里的 Collector，以上面的案例为例, 其实由两个 Collector 完成最后的计算：\n\nTotalHitCountCollecotr\nGlobalOrdinalsStringTermsAggregator(里面还有个 Aggregator)\n\n因为我们没有定义过滤条件，所以最后的 Query 是个 MatchAllQuery，之后基于这个基础上，这两个 collector 完成对应的计算。通常，这两个 Collector 会被 wrap 成一个新的 MultiCollector ，最终传入 IndexSearcher 的 Collector 就是 MultiCollector。\n根据上面的分析，我们知道示例中的聚合计算完全由 GlobalOrdinalsStringTermsAggregator 负责。\n基于 DocValues 实现 groupBy 概览对于每一个 segment, 我们都会为每个列单独存储成一个文件，为了压缩，我们可能会将里面具体的值转换成数字，然后再形成一个字典和数字对应关系的文件。我们进行所谓的 groupBy 操作，以最后进行 Avg 为例子，其实就是维护了两个大数组，\nLongArray counts;//Long数组\nDoubleArray sums; //Double 数组 \n\ncounts 是 newtype(我们例子中被 groupby 的字段) 次数统计，对应的数组下标是 newtype(我们已经将 newtype 转化为数字表示了)。我们遍历文档的时候 (MatchAllQuery)，可以获取 doc, 然后根据 doc 到列存文件获取对应的 newtype, 然后给 counts 对应的 newtype +1。 这样我们就知道每个 newtype 出现的次数了。\n这里我们也可以看到，消耗内存的地方取决于 newtype 的数量 (distinct 后)，我们称之为基数。基数过高的话，是比较消耗内存的。\nsums 也是一样的，下标是 newtype 的值，而对应的值则是不断累加 num(我们例子中需要被 avg 的字段)。\n之后就可以遍历两个数组得到结果了，代码大体如下：\n//这里的owningBucketOrd 就是newtype 的数字化表示\npublic double metric(long owningBucketOrd) &#123;\n        if (valuesSource == null || owningBucketOrd &gt;= sums.size()) &#123;\n            return Double.NaN;\n        &#125;\n        return sums.get(owningBucketOrd) / counts.get(owningBucketOrd);\n    &#125; \n\nGlobalOrdinalsStringTermsAggregator&#x2F;AvgAggregator 组合实现GlobalOrdinalsStringTermsAggregator 首先要提供一个 Collector 给主流程，所以其提供了一个 newCollector 方法：\nprotected LeafBucketCollector newCollector(\n//DocValue 列式存储的一个API表现\nfinal RandomAccessOrds ords,\n//AvgAggregator提供的Collector\nfinal LeafBucketCollector sub) \n\n接着判定是不是只有一个列文件 (DocValues):\nfinal SortedDocValues singleValues = DocValues.unwrapSingleton(words);\n//如果singleValues!=null 则是一个，否则有多个列文件 \n\n如果是一个的话：\npublic void collect(int doc, long bucket) throws IOException &#123;\n                    assert bucket == 0;\n                    final int ord = singleValues.getOrd(doc);\n                    if (ord &gt;= 0) &#123;\n                        collectExistingBucket(sub, doc, ord);\n                    &#125;\n                &#125;\n//collectExistingBucket\n public final void collectExistingBucket(LeafBucketCollector subCollector, int doc, long bucketOrd) throws IOException &#123;\n        docCounts.increment(bucketOrd, 1);\n        subCollector.collect(doc, bucketOrd);\n    &#125; \n\n通过 doc 拿到 ord(newtype), 然后交给 Avg 的 collector 接着处理, 进入 AvgAggregator 里的 Collector 的 collect 逻辑：\npublic void collect(int doc, long bucket) throws IOException &#123;\n                counts = bigArrays.grow(counts, bucket + 1);\n                sums = bigArrays.grow(sums, bucket + 1);\n\n                values.setDocument(doc);\n                final int valueCount = values.count();\n                counts.increment(bucket, valueCount);\n                double sum = 0;\n                for (int i = 0; i &lt; valueCount; i++) &#123;\n                    sum += values.valueAt(i);\n                &#125;\n                sums.increment(bucket, sum);\n            &#125; \n\n这个和我上面的概述中描述是一致的。\n如果是多个 DocValues(此时索引还没有对那些 Segment 做合并)，这个时候会走下面的流程：\npublic void collect(int doc, long bucket) throws IOException &#123;\n                    assert bucket == 0;\n                    ords.setDocument(doc);\n                    final int numOrds = ords.cardinality();\n                    for (int i = 0; i &lt; numOrds; i++) &#123;\n                        final long globalOrd = ords.ordAt(i);\n                        collectExistingBucket(sub, doc, globalOrd);\n                    &#125;\n                &#125; \n\n这里的 ords 包括了多个 DocValues 文件, 然后做了全局映射，因为要把文件的下标做映射。为啥要有下标映射呢？因为多个列文件 (DocValues) 的全集才具有完整的 newtype，但是每个列文件都是从 0 开始递增的。现在要扩张到一个 global 的空间上。 ords.cardinality() 拿到了列文件 (DocValues) 的数目，然后对每个文件都处理一遍，通过 ords.ordAt(i) 拿到 newtype 的全局下标，这个时候就可以继续交给 Avg 完成了。\n到这个阶段，我们其实已经算好了每个 newtype 出现的次数，以及 num 的累计值，也就是我们前面提到的两个数组。\nBuildAggregation最终我们是要把这个数据输出输出的，不论是输出给别的 ES 节点，还是直接输出给调用方。所以有个 BuildAggregation 的过程，可以根据名字进行直观的了解。\n考虑到内存问题，ES 允许你设置一些 Threshhold, 然后通过 BucketPriorityQueue(优先队列) 来完成实际的数据收集以及排序 (默认按文档出现次数排序)。 里面的元素是 OrdBucket，OrdBucket 包含了几个值：\nglobalOrd： 全局下标\nbucketOrd： 在所属文件里的下标\ndocCount : 文档出现的次数 \n\n接着取出 topN 的对象，放到 InternalTerms.Bucket[] 数组中。然后遍历该数组，调用子 Aggregator 的 buildAggregation 方法，这里的子 Aggregator 是 AvgAggregator , 每个 Bucket(newtype) 就获取到一个 avg aggregations 了，该 aggregations 通过 InternalAggregations 包裹，InternalAggregations 包含了一个 reduce 方法，该方法会调用具体 InternalAggregation 的 doReduce 方法，比如 AvgAggregator 就有自己的 reduce 方法。说这个主要给下一小结做铺垫。\n最后会被包装成 StringTerms , 然后就可以序列化成 JSON 格式，基本就是你在接口上看到的样子了。\n多分片聚合结果合并前面我们讨论的，都是基于一个分片，但是最终是要把结果数据进行 Merge 的。 这个功能是由 SearchPhaseController 对象来完成，大体如下：\nsortedShardList = searchPhaseController.sortDocs(useScroll, firstResults);\n\nfinal InternalSearchResponse internalResponse = searchPhaseController.merge(sortedShardList, firstResults,\n                    firstResults, request); \n\n其中 merge 动作是按分类进行 merge 的，比如：\n\ncounter(计数, 譬如 total_hits)\nhits\naggregations\nsuggest\nprofile （性能相关的数据）\n\n这里我们只关注 aggregations 的 merge\n // merge addAggregation\n        InternalAggregations aggregations = null;\n        if (!queryResults.isEmpty()) &#123;\n            if (firstResult.aggregations() != null &amp;&amp; firstResult.aggregations().asList() != null) &#123;\n                List&lt;InternalAggregations&gt; aggregationsList = new ArrayList&lt;&gt;(queryResults.size());\n                for (AtomicArray.Entry&lt;? extends QuerySearchResultProvider&gt; entry : queryResults) &#123;\n                    aggregationsList.add((InternalAggregations) entry.value.queryResult().aggregations());\n                &#125;\n                aggregations = InternalAggregations.reduce(aggregationsList, new ReduceContext(bigArrays, scriptService, headersContext));\n            &#125;\n        &#125; \n\n代码有点长，核心是\nInternalAggregations.reduce(.....) \n\n里面实际的逻辑也是比较简单直观的。会调用 InternalTerms 的 reduce 方法做 merge, 但是不同的类型的 Aggregator 产生 Aggregations 合并逻辑是不一样的，所以会委托给对应实现。比如 GlobalOrdinalsStringTermsAggregator 则会委托给 InternalTerms 的 doReduce 方法，而如 AvgAggregator 会委托给 InternalAvg 的 doReduce。 这里就不展开。未来会单独出一片文章讲解。\n附录这里我们再额外讲讲 ValueSource (ES 对 FieldData&#x2F;DocValues 的抽象)。\n前文我们提到，大部分 Aggregator 都是依赖于 FieldData&#x2F;DocValues 来实现的，而 ValueSource 则是他们在 ES 里的表示。所以了解他们是很有必要的。ValuesSource 全类名是：\n org.elasticsearch.search.aggregations.support.ValuesSource \n\n该类就是 ES 为了管理 DocValues 而封装的。它是一个抽象类，内部还有很多实现类，Bytes,WithOrdinals,FieldData,Numeric,LongValues 等等。这些都是对特定类型 DocValues 类型的 ES 表示。\n按上面我们的查询示例来看，newtype 字段对应的是\n org.elasticsearch.search.aggregations.support.ValuesSource.Bytes.WithOrdinals.FieldData \n\n对象。这个对象是 ES 对 Lucene String 类型的 DocValues 的一个表示。 你会发现在 ValueSource 类里，有不同的 FieldData。不同的 FieldData 可能继承自不同基类从而表示不同类型的数据。在现在这个 FieldData 里面有一个对象：\nprotected final IndexOrdinalsFieldData indexFieldData; \n\n该对象在 newtype(我们示例中的字段) 是 String 类型的时候，对应的是实现类是\norg.elasticsearch.index.fielddata.plain.SortedSetDVOrdinalsIndexFieldData \n\n该对象的大体作用是，构建出 DocValue 的 ES 的 Wraper。\n具体代码如下：\n@Overridepublic AtomicOrdinalsFieldData load(LeafReaderContext context) &#123;    \nreturn new SortedSetDVBytesAtomicFieldData(\n   context.reader(),\n   fieldNames.indexName());\n&#125;\n//或者通过loadGlobal方法得到\n//org.elasticsearch.index.fielddata.ordinals.InternalGlobalOrdinalsIndexFieldData \n\n以第一种情况为例，上面的代码 new 了一个新的org.elasticsearch.index.fielddata.AtomicOrdinalsFieldData对象, 该对象的一个实现类是SortedSetDVBytesAtomicFieldData 。 这个对象和 Lucene 的 DocValues 完成最后的对接：\n @Override\n    public RandomAccessOrds getOrdinalsValues() &#123;\n        try &#123;\n            return FieldData.maybeSlowRandomAccessOrds(DocValues.getSortedSet(reader, field));\n        &#125; catch (IOException e) &#123;\n            throw new IllegalStateException(&quot;cannot load docvalues&quot;, e);\n        &#125;\n    &#125; \n\n我们看到，通过 Reader 获取到最后的列就是在该类里的 getOrdinalsValues 方法里实现的。\n该方法最后返回的 RandomAccessOrds 就是 Lucene 的 DocValues 实现了。\n分析了这么多，所有的逻辑就浓缩在getLeafCollector 的第一行代码上。globalOrds 的类型是 RandomAccessOrds，并且是直接和 Lucene 对应上了。\nglobalOrds = valuesSource.globalOrdinalsValues(cox); \n\ngetLeafCollector 最后 newCollector 的规则如下：\n protected LeafBucketCollector newCollector(final RandomAccessOrds ords, final LeafBucketCollector sub) &#123;\n        grow(ords.getValueCount());\n        final SortedDocValues singleValues = DocValues.unwrapSingleton(ords);\n        if (singleValues != null) &#123;\n            return new LeafBucketCollectorBase(sub, ords) &#123;\n                @Override\n                public void collect(int doc, long bucket) throws IOException &#123;\n                    assert bucket == 0;\n                    final int ord = singleValues.getOrd(doc);\n                    if (ord &gt;= 0) &#123;\n                        collectExistingBucket(sub, doc, ord);\n                    &#125;\n                &#125;\n            &#125;;\n        &#125; \n\n我们知道，在 Lucene 里，大部分文件都是不可更新的。一个段一旦生成后就是不可变的，新的数据或者删除数据都需要生成新的段。DocValues 的存储文件也是类似的。所以 DocValues.unwrapSingleton 其实就是做这个判定的，是不是有多个文件 。无论是否则不是都直接创建了一个匿名的 Collector。\n当个文件的很好理解，包含了索引中 newtype 字段所有的值，其下标获取也很自然。\n//singleValues其实就是前面的RandomAccessOrds。\nfinal int ord = singleValues.getOrd(doc); \n\n根据文档号获取值对应的位置，如果 ord &gt;&#x3D;0 则代表有值，否则代表没有值。\n如果有多个文件，则会返回如下的 Collecor:\nelse &#123;\n            return new LeafBucketCollectorBase(sub, ords) &#123;\n                @Override\n                public void collect(int doc, long bucket) throws IOException &#123;\n                    assert bucket == 0;\n                    ords.setDocument(doc);\n                    final int numOrds = ords.cardinality();\n                    for (int i = 0; i &lt; numOrds; i++) &#123;\n                        final long globalOrd = ords.ordAt(i);\n                        collectExistingBucket(sub, doc, globalOrd);\n                    &#125;\n                &#125;\n            &#125;; \n\n上面的代码可以保证多个文件最终合起来保持一个文件的序号。什么意思呢？比如 A 文件有一个文档，B 文件有一个，那么最终获取的 globalOrd 就是 0,1 而不会都是 0。此时的 ords 实现类 不是 SingletonSortedSetDocValues 而是\norg.elasticsearch.index.fielddata.ordinals.GlobalOrdinalMapping \n\n对象了。\n计数的方式两个都大体类似。\ndocCounts.increment(bucketOrd, 1); \n\n这里的 bucketOrd 其实就是前面的 ord&#x2F;globalOrd。所以整个计算就是填充 docCounts\n总结ES 的 Aggregation 机制还是挺复杂的。本文试图通过一个简单的 group by 的例子来完成对其机制的解释。其中 ValueSource 那层我目前也没没完全吃透，如有表述不合适的地方，欢迎大家指出。\n","slug":"MIDDLEWARE/ElasticSearch-Aggregations-GroupBy-实现源码分析","date":"2022-07-06T09:43:40.000Z","categories_index":"newtype,MIDDLEWARE","tags_index":"DocValues,Aggregator,Collector","author_index":"dandeliono"},{"id":"13e3d87bd156131d5858fa5b620de5c7","title":"Bulk异常引发的Elasticsearch内存泄漏","content":"Bulk异常引发的Elasticsearch内存泄漏运维线上 ES 集群时，偶然遇到内存泄露的问题，排查问题时看到了这篇文章，清晰明了，所以分享给大家，希望给大家问题排查提供一些思路。\n背景介绍前天公司度假部门一个线上 ElasticSearch 集群发出报警，有 Data Node 的 Heap 使用量持续超过 80% 警戒线。 收到报警邮件后，不敢怠慢，立即登陆监控系统查看集群状态。还好，所有的节点都在正常服务，只是有 2 个节点的 Heap 使用率非常高。此时，Old GC 一直在持续的触发，却无法回收内存。\n\n问题排查问题节点的 Heapsize 分配了 30GB，80% 的使用率约等于 24GB。 但集群的数据总量并不大，5 个节点所有索引文件加起来占用的磁盘空间还不到 10GB。\n\n查看各节点的 segment memory 和 cache 占用量也都非常小，是 MB 级别的。\n\n集群的 QPS 只有 30 上下，CPU 消耗 10% 都不到，各类 thread pool 的活动线程数量也都非常低。\n\n非常费解是什么东西占着 20 多 GB 的内存不释放？\n出现问题的集群 ES 版本是 5.3.2，而这个版本的稳定性在公司内部已经经过长时间的考验，做为稳定版本在线上进行了大规模部署。 其他一些读写负载非常高的集群也未曾出现过类似的状况，看来是遇到新问题了。\n查看问题节点 ES 的日志，除了看到一些 Bulk 异常以外，未见特别明显的其他和资源相关的错误:\n\n\n\n\n\n\n\n\n\n[2017-11-06T16:33:15,668][DEBUG][o.e.a.b.TransportShardBulkAction] [] [suggest-3][0] failed to execute bulk item (update) BulkShardRequest [[suggest-3][0]] containing [44204 ] requests org.elasticsearch.index.engine.DocumentMissingException: [type][Á∫≥Ê†ºÂ∞îÊûúÂæ∑_1198]: document missing at org.elasticsearch.action.update.UpdateHelper.prepare(UpdateHelper.java:92) ~[elasticsearch-5.3.2.jar:5.3.2] at org.elasticsearch.action.update.UpdateHelper.prepare(UpdateHelper.java:81) ~[elasticsearch-5.3.2.jar:5.3.2]\n和用户确认这些异常的原因，是因为写入程序会从数据源拿到数据后，根据 doc_id 对 ES 里的数据做 update。会有部分 doc_id 在 ES 里不存在的情况，但并不影响业务逻辑，因而 ES 记录的 document missing 异常应该可以忽略。\n至此别无他法，只能对 JVM 做 Dump 分析了。\nHeap Dump 分析用的工具是 Eclipse MAT，从这里下载的 Mac 版: Downloads 。 使用这个工具需要经过以下 2 个步骤:\n\n\n\n\n\n\n\n\n\n1. 获取二进制的 head dump 文件 jmap -dump:format&#x3D;b,file&#x3D;&#x2F;tmp&#x2F;es_heap.bin &lt;pid&gt; 其中 pid 是 ES JAVA 进程的进程号。\n2. 将生成的 dump 文件下载到本地开发机器，启动 MAT，从其 GUI 打开文件。\n要注意，MAT 本身也是 JAVA 应用，需要有 JDK 运行环境的支持。\nMAT 第一次打 dump 文件的时候，需要对其解析，生成多个索引。这个过程比较消耗 CPU 和内存，但一旦完成，之后再打开 dump 文件就很快，消耗很低。 对于这种 20 多 GB 的大文件，第一次解析的过程会非常缓慢，并且很可能因为开发机内存的较少而内存溢出。因此，我找了台大内存的服务器来做第一次的解析工作:\n1. 将 linux 版的 MAT 拷贝上去，解压缩后，修改配置文件 MemoryAnalyzer.ini，将内存设置为 20GB 左右:\n\n这样能保证解析的过程中不会内存溢出。\n2. 将 dump 文件拷贝上去，执行下面几个命令生成索引及 3 个分析报告:\n\n\n\n\n\n\n\n\n\nmat&#x2F;ParseHeapDump.sh es_heap.bin org.eclipse.mat.api:suspects\nmat&#x2F;ParseHeapDump.sh es_heap.bin org.eclipse.mat.api:overview\nmat&#x2F;ParseHeapDump.sh es_heap.bin org.eclipse.mat.api:top_components\n分析成功以后，会生成如下一堆索引文件 (.index) 和分析报告(.zip)\n\n将这些文件打包下载到本地机器上，用 MAT GUI 打开就可以分析了。\n在 MAT 里打开 dump 文件的时候，可以选择打开已经生成好的报告，比如 Leak suspects:\n\n通过 Leak Suspects，一眼看到这 20 多 GB 内存主要是被一堆 bulk 线程实例占用了，每个实例则占用了接近 1.5GB 的内存。\n\n进入 “dominator_tree”面板，按照”Retained Heap” 排序，可以看到多个 bulk 线程的内存占用都非常高。\n\n将其中一个 thread 的引用链条展开，看看这些线程是如何 Retain 这么多内存的，特别注意红圈部分:\n\n这个引用关系解读如下:\n\n\n\n\n\n\n\n\n\n1. 这个 bulk 线程的 thread local map 里保存了一个 log4j 的 MultableLogEvent 对象。\n2.MutablelogEvent 对象引用了 log4j 的 ParameterizedMessage 对象。\n3.ParameterizedMessage 引用了 bulkShardRequest 对象。\n4.bulkShardRequest 引用了 4 万多个 BulkitemRequest 对象。\n这样看下来，似乎是 log4j 的 logevent 对一个大的 bulk 请求对象有强引用而导致其无法被垃圾回收掉，产生内存泄漏。\n联想到 ES 日志里，有记录一些 document missing 的 bulk 异常，猜测是否在记录这些异常的时候产生的泄漏。\n问题复现为了验证猜测，我在本地开发机上，启动了一个单节点的 5.3.2 测试集群，用 bulk api 做批量的 update，并且有意为其中 1 个 update 请求设置不存在的 doc_id。\n为了便于测试，我在 ES 的配置文件 elasticsearch.yml 里添加了配置项 processors: 1。 这个配置项影响集群 thread_pool 的配置，bulk thread pool 的大小将减少为 1 个，这样可以更快速和便捷的做各类验证。\n启动集群，发送完 bulk 请求后，立即做一个 dump，重复之前的分析过程，问题得到了复现。\n这时候想，是否其他 bulk 异常也会引起同样的问题，比如写入的数据和 mapping 不匹配？ 测试了一下，问题果然还是会产生。再用不同的 bulk size 进行测试，发现无法回收的这段内存大小，取决于最后一次抛过异常的 bulk size 大小。至此，基本可以确定内存泄漏与 log4j 记录异常消息的逻辑有关系。\n为了搞清楚这个问题是否 5.3.2 独有，后续版本是否有修复，在最新的 5.6.3 上做了同样的测试，问题依旧，因此这应该是一个还未发现的深层 Bug.\n读源码查根源大致搞清楚问题查找的方向了，但根源还未找到，也就不知道如何修复和避免，只有去扒源码了。\n在 TransportShardBulkAction 第 209 行，找到了 ES 日志里抛异常的代码片段。\nif (isConflictException(failure)) &#123;\n\n复制代码\n这里看到了 ParameterizedMessage 实例化过程中，request 做为一个参数传入了。这里的 request 是一个 BulkShardRequest 对象，保存的是要写入到一个 shard 的一批 bulk item request。 这样以来，一个批次写入的请求数量越多，这个对象 retain 的内存就越多。 可问题是，为什么 logger.debug（）调用完毕以后，这个引用不会被释放？\n通过和之前 MAT 上的 dominator tree 仔细对比，可以看到 ParameterizedMessage 之所以无法释放，是因为被一个 MutableLogEvent 在引用，而这个 MutableLogEvent 被做为一个 thread local 存放起来了。 由于 ES 的 Bulk thread pool 是 fix size 的，也就是预先创建好，不会销毁和再创建。 那么这些 MutableLogEvent 对象由于是 thread local 的，只要线程没有销毁，就会对该线程实例一直全局存在，并且其还会一直引用最后一次处理过的 ParameterizedMessage。 所以在 ES 记录 bulk exception 这种比较大的请求情况下， 整个 request 对象会被 thread local 变量一直强引用无法释放，产生大量的内存泄漏。\n再继续挖一下 log4j 的源码，发现 MutableLogEvent 是在 org.apache.logging.log4j.core.impl.ReusableLogEventFactory 里做为 thread local 创建的。\npublic class ReusableLogEventFactory implements LogEventFactory &#123;    private static final ThreadNameCachingStrategy THREAD_NAME_CACHING_STRATEGY = ThreadNameCachingStrategy.create();    private static final Clock CLOCK = ClockFactory.getClock();    private static ThreadLocal&lt;MutableLogEvent&gt; mutableLogEventThreadLocal = new ThreadLocal&lt;&gt;();\n\n复制代码\n而 org.apache.logging.log4j.core.config.LoggerConfig 则根据一个常数 ENABLE_THREADLOCALS 的值来决定用哪个 LogEventFactory。\n       if (LOG_EVENT_FACTORY == null) &#123;\n\n复制代码\n继续深挖，在 org.apache.logging.log4j.util.Constants 里看到，log4j 会根据运行环境判断是否是 WEB 应用，如果不是，就从系统参数 log4j2.enable.threadlocals 读取这个常量，如果没有设置，则默认值是 true。\npublic static final boolean ENABLE_THREADLOCALS = !IS_WEB_APP &amp;&amp; PropertiesUtil.getProperties().getBooleanProperty(            &quot;log4j2.enable.threadlocals&quot;, true);\n\n复制代码\n由于 ES 不是一个 web 应用，导致 log4j 选择使用了 ReusableLogEventFactory，因而使用了 thread_local 来创建 MutableLogEvent 对象，最终在 ES 记录 bulk exception 这个特殊场景下产生非常显著的内存泄漏。\n再问一个问题，为何 log4j 要将 logevent 做为 thread local 创建？ 跑到 log4j 的官网去扒了一下文档，在这里 Garbage-free Steady State Logging 找到了合理的解释。 原来为了减少记录日志过程中的反复创建的对象数量，减轻 GC 压力从而提高性能，log4j 有很多地方使用了 thread_local 来重用变量。 但使用 thread local 字段装载非 JDK 类，可能会产生内存泄漏问题，特别是对于 web 应用。 因此才会在启动的时候判断运行环境，对于 web 应用会禁用 thread local 类型的变量。\nThreadLocal fields holding non-JDK classes can cause memory leaks in web applications when the application server&#39;s thread pool continues to reference these fields after the web application is undeployed. To avoid causing memory leaks, Log4j will not use these ThreadLocals when it detects that it is used in a web application (when the javax.servlet.Servlet class is in the classpath, or when system property log4j2.is.webapp is set to &quot;true&quot;).\n\n复制代码\n参考上面的文档后，也为 ES 找到了规避这个问题的措施： 在 ES 的 JVM 配置文件 jvm.options 里，添加一个 log4j 的系统变量 - Dlog4j2.enable.threadlocals&#x3D;false，禁用掉 thread local 即可。 经过测试，该选项可以有效避开这个内存泄漏问题。\n这个问题 Github 上也提交了 Issue，对应的链接是: Memory leak upon partial TransportShardBulkAction failure  \n结束ES 的确是非常复杂的一个系统，包含非常多的模块和第三方组件，可以支持很多想象不到的用例场景，但一些边缘场景可能会引发一些难以排查的问题。完备的监控体系和一个经验丰富的支撑团队对于提升业务开发人员使用 ES 开发的效率、提升业务的稳定性是非常重要的！\n","slug":"MIDDLEWARE/Bulk异常引发的Elasticsearch内存泄漏","date":"2022-07-06T09:41:21.000Z","categories_index":"thread,MIDDLEWARE","tags_index":"https,log,bulk","author_index":"dandeliono"},{"id":"067b852fcc8abb83b4af6ed8c83266b0","title":"Spring Boot 集成 Kafka","content":"Spring Boot 集成 KafkaMaven 依赖1234&lt;dependency&gt;    &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt;    &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt;&lt;/dependency&gt;\n\n配置属性12345678910111213141516171819202122232425262728293031323334353637spring:  kafka:        bootstrap-servers: localhost:9092,localhost:9093,localhost:9094        producer:            batch-size: 16384            buffer-memory: 33554432            retries: 0                              acks: 1      key-serializer: org.apache.kafka.common.serialization.StringSerializer      value-serializer: org.apache.kafka.common.serialization.StringSerializer      properties:                        linger.ms: 0        consumer:            group-id: group-x            enable-auto-commit: true            auto-commit-interval: 5000                        auto-offset-reset: earliest      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer\n\n生产方123456789101112131415161718192021222324252627282930313233343536@Datapublic class Message &#123;    private Long id;    private String msg;    private Date sendTime;&#125;@Configurationpublic class KafkaConfiguration &#123;    public static final String TOPIC = &quot;topic&quot;;        @Bean    public NewTopic topic() &#123;        return TopicBuilder.name(TOPIC).partitions(1).replicas(1).build();    &#125;&#125;@Slf4j@Componentpublic class MessageSender &#123;    private final KafkaTemplate&lt;Object, Object&gt; kafkaTemplate;    public MessageSender(KafkaTemplate&lt;Object, Object&gt; kafkaTemplate) &#123;        this.kafkaTemplate = kafkaTemplate;    &#125;        public void send(Message message) &#123;        kafkaTemplate.send(TOPIC, message);    &#125;&#125;\n\n消费方1234567891011121314151617181920212223242526272829303132333435363738394041@Configurationpublic class KafkaConfiguration &#123;    public static final String TOPIC = &quot;topic&quot;;        @Bean    public NewTopic topic() &#123;        return TopicBuilder.name(TOPIC).partitions(1).replicas(1).build();    &#125;&#125;@Slf4j@Componentpublic class MessageReceiver &#123;        @KafkaListener(id = &quot;group-2-1&quot;, topics = &#123;TOPIC&#125;)    public void receive(ConsumerRecord&lt;Object, Object&gt; record, Acknowledgment acknowledgment) &#123;        Optional&lt;Object&gt; messageOptional = Optional.ofNullable(record.value());        if (messageOptional.isPresent()) &#123;            Object message = messageOptional.get();            log.info(&quot;Receive message: &#123;&#125;&quot;, message);        &#125;                acknowledgment.acknowledge();    &#125;        @KafkaListener(id = &quot;group-2-2&quot;, topics = &#123;TOPIC&#125;)    public void receive(String message, Acknowledgment acknowledgment) &#123;        log.info(&quot;Receive message: &#123;&#125;&quot;, message);        acknowledgment.acknowledge();    &#125;&#125;\n\n高级配置序列化 &#x2F; 反序列化当我们发送对象类型消息数据时，在消费者方默认接收到的数据类型是字符串，如果我们希望接收到的是对象类型，可以在生产者 &#x2F; 消费者两方都添加一点配置。\n\n生产方\n1234567spring:  kafka:        producer:      properties:                spring.json.type.mapping: message:samples.dto.Message\n消费方\n1234567891011121314151617181920@Configurationpublic class KafkaConfiguration &#123;        @Bean    public RecordMessageConverter converter() &#123;        DefaultJackson2JavaTypeMapper typeMapper = new DefaultJackson2JavaTypeMapper();        typeMapper.setTypePrecedence(Jackson2JavaTypeMapper.TypePrecedence.TYPE_ID);        typeMapper.addTrustedPackages(&quot;samples.dto&quot;);        Map&lt;String, Class&lt;?&gt;&gt; mappings = new HashMap&lt;&gt;(10);        mappings.put(&quot;message&quot;, Message.class);        typeMapper.setIdClassMapping(mappings);        StringJsonMessageConverter converter = new StringJsonMessageConverter();        converter.setTypeMapper(typeMapper);        return converter;    &#125;&#125;\n\n之后，我们就能够像下面这样接收 Message 参数的消息了：\n12345678910@KafkaListener(id = &quot;group-2-3&quot;, topics = &#123;TOPIC&#125;)public void receive(Message message, Acknowledgment acknowledgment) &#123;    log.info(&quot;Receive message: &#123;&#125;&quot;, message);        acknowledgment.acknowledge();&#125;\n\n在后续演示代码中，默认都会配置序列化 &#x2F; 反序列化，就不再一一列举了。\n获取消息发送结果在发送消息之后，我们可以获取到消息发送结果，既可以以同步方式获取结果，也可以以异步方式获取结果。\n\n同步方式\n123456789101112public void sendAndGetResultSync(Message message) &#123;    ListenableFuture&lt;SendResult&lt;Object, Object&gt;&gt; future = kafkaTemplate.send(TOPIC, message);    try &#123;        SendResult&lt;Object, Object&gt; result = future.get();        log.info(&quot;Sync get send result success, result: &#123;&#125;&quot;, result);    &#125; catch (Throwable e) &#123;        log.error(&quot;Sync get send result failure&quot;, e);    &#125;&#125;\n异步方式\n123456789101112131415161718public void sendAndGetResultAsync(Message message) &#123;    kafkaTemplate.send(TOPIC, message).addCallback(new ListenableFutureCallback&lt;SendResult&lt;Object, Object&gt;&gt;() &#123;        @Override        public void onSuccess(SendResult&lt;Object, Object&gt; result) &#123;            log.info(&quot;Async get send result success, result: &#123;&#125;&quot;, result);        &#125;        @Override        public void onFailure(Throwable e) &#123;            log.error(&quot;Async get send result failure&quot;, e);        &#125;    &#125;);&#125;\n\n使用事务开启事务当我们在生产者方配置了属性 transaction-id-prefix 后，Spring 会自动帮我们开启事务。不过开启事务之后，retries 属性需要设置为大于 0，acks 属性需要设置为 all 或 -1。另外，我们还需要将消费者方的 isolation.level 设置为 read_committed，这样对于未提交事务的消息，消费者就不会读取到。\n123456789101112131415161718spring:  kafka:        producer:            retries: 3                              acks: -1            transaction-id-prefix: tx.        consumer:      properties:                isolation.level: read_committed\n\n使用事务有两种方式Spring 提供了两种方式使用事务：调用 KafkaTemplate 的 executeInTransaction 方法，或使用 @Transactional。\n\n调用 KafkaTemplate 的 executeInTransaction 方法\n123456789101112public void sendInTransactionByMethod(Message message) &#123;    kafkaTemplate.executeInTransaction(kafkaTemplate -&gt; &#123;        kafkaTemplate.send(TOPIC, message);        log.info(&quot;Send in transaction by method success&quot;);                return null;            &#125;);&#125;\n使用 @Transactional\n123456789@Transactional(rollbackFor = Exception.class)public void sendInTransactionByAnnotation(Message message) &#123;    kafkaTemplate.send(TOPIC, message);    log.info(&quot;Send in transaction by annotation success&quot;);    &#125;\n\n需要注意的是，在生产者开启事务之后，所有发送消息的地方都必须放在事务中执行。\n转发消息消费者在收到消息并对消息进行处理之后，可以再将新的消息发送出去。在消费方，我们既可以使用 kafkaTemplate.send 实现手动发送消息，也可以使用 @Send 实现自动发送消息。\n\n生产方\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172@Configurationpublic class KafkaConfiguration &#123;    public static final String TOPIC_RESEND = &quot;topic-resend&quot;;    public static final String TOPIC_RESEND_NEXT = &quot;topic-resend-next&quot;;    @Bean    public NewTopic topicResend() &#123;        return TopicBuilder.name(TOPIC_RESEND).partitions(1).replicas(1).build();    &#125;    @Bean    public NewTopic topicResendNext() &#123;        return TopicBuilder.name(TOPIC_RESEND_NEXT).partitions(1).replicas(1).build();    &#125;&#125;@Slf4j@Componentpublic class MessageReceiver &#123;        @KafkaListener(id = &quot;group-1-1&quot;, topics = &#123;TOPIC_RESEND_NEXT&#125;)    public void receive(ConsumerRecord&lt;Object, Object&gt; record, Acknowledgment acknowledgment) &#123;        Optional&lt;Object&gt; messageOptional = Optional.ofNullable(record.value());        if (messageOptional.isPresent()) &#123;            Object message = messageOptional.get();            log.info(&quot;Receive message: &#123;&#125;&quot;, message);        &#125;        acknowledgment.acknowledge();    &#125;        @KafkaListener(id = &quot;group-1-2&quot;, topics = &#123;TOPIC_RESEND_NEXT&#125;)    public void receive(String msg, Acknowledgment acknowledgment) &#123;        log.info(&quot;Receive msg: &#123;&#125;&quot;, msg);        acknowledgment.acknowledge();    &#125;&#125;@Slf4j@Componentpublic class MessageReceiver &#123;        @KafkaListener(id = &quot;group-1-1&quot;, topics = &#123;TOPIC_RESEND_NEXT&#125;)    public void receive(ConsumerRecord&lt;Object, Object&gt; record, Acknowledgment acknowledgment) &#123;        Optional&lt;Object&gt; messageOptional = Optional.ofNullable(record.value());        if (messageOptional.isPresent()) &#123;            Object message = messageOptional.get();            log.info(&quot;Receive message: &#123;&#125;&quot;, message);        &#125;        acknowledgment.acknowledge();    &#125;        @KafkaListener(id = &quot;group-1-2&quot;, topics = &#123;TOPIC_RESEND_NEXT&#125;)    public void receive(String msg, Acknowledgment acknowledgment) &#123;        log.info(&quot;Receive msg: &#123;&#125;&quot;, msg);        acknowledgment.acknowledge();    &#125;&#125;\n消费方\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748@Configurationpublic class KafkaConfiguration &#123;    public static final String TOPIC_RESEND = &quot;topic-resend&quot;;    public static final String TOPIC_RESEND_NEXT = &quot;topic-resend-next&quot;;    @Bean    public NewTopic topicResend() &#123;        return TopicBuilder.name(TOPIC_RESEND).partitions(1).replicas(1).build();    &#125;    @Bean    public NewTopic topicResendNext() &#123;        return TopicBuilder.name(TOPIC_RESEND_NEXT).partitions(1).replicas(1).build();    &#125;&#125;@Slf4j@Componentpublic class MessageReceiver &#123;    private final KafkaTemplate&lt;Object, Object&gt; kafkaTemplate;    @SuppressWarnings(&quot;SpringJavaInjectionPointsAutowiringInspection&quot;)    public MessageReceiver(KafkaTemplate&lt;Object, Object&gt; kafkaTemplate) &#123;        this.kafkaTemplate = kafkaTemplate;    &#125;        @SendTo    @KafkaListener(id = &quot;group-2-4&quot;, topics = &#123;TOPIC_RESEND&#125;)    public void receiveAndResendManual(Message message, Acknowledgment acknowledgment) &#123;        log.info(&quot;Receive message: &#123;&#125;&quot;, message);        acknowledgment.acknowledge();        kafkaTemplate.send(TOPIC_RESEND_NEXT, message.getMsg());    &#125;        @SendTo(TOPIC_RESEND_NEXT)    @KafkaListener(id = &quot;group-2-5&quot;, topics = &#123;TOPIC_RESEND&#125;)    public String receiveAndResendAuto(Message message, Acknowledgment acknowledgment) &#123;        log.info(&quot;Receive message: &#123;&#125;&quot;, message);        acknowledgment.acknowledge();        return message.getMsg();    &#125;&#125;\n\n获取消息回复生产者在发送消息之后，可以同时等待获取消费者接收并处理消息之后的回复，就像传统的 RPC 交互那样，要实现这个功能，我们需要使用 ReplyingKafkaTemplate。ReplyingKafkaTemplate 是 KafkaTemplate 的一个子类，它除了继承父类的方法，还新增了方法 sendAndReceive ，该方法实现了消息发送 &#x2F; 回复的语义。Spring Boot 没有提供开箱即用的 ReplyingKafkaTemplate，我们需要做些额外的配置。\n\n生产方\n1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374@Configurationpublic class KafkaConfiguration &#123;    public static final String TOPIC_RECEIVE = &quot;topic-receive&quot;;    public static final String TOPIC_REPLIES = &quot;replies&quot;;    public static final String GROUP_REPLIES = &quot;repliesGroup&quot;;    @Bean    public NewTopic topicReceive() &#123;        return TopicBuilder.name(TOPIC_RECEIVE).partitions(1).replicas(1).build();    &#125;    @Bean    public NewTopic topicReplies() &#123;        return TopicBuilder.name(TOPIC_REPLIES).partitions(1).replicas(1).build();    &#125;    @Bean    public ConcurrentMessageListenerContainer&lt;Object, Object&gt; concurrentMessageListenerContainer(            ConcurrentKafkaListenerContainerFactory&lt;Object, Object&gt; containerFactory) &#123;        ConcurrentMessageListenerContainer&lt;Object, Object&gt; container =                containerFactory.createContainer(TOPIC_REPLIES);        container.getContainerProperties().setGroupId(GROUP_REPLIES);        container.setAutoStartup(false);        return container;    &#125;    @Bean    public ReplyingKafkaTemplate&lt;Object, Object, Object&gt; replyingKafkaTemplate(            ProducerFactory&lt;Object, Object&gt; producerFactory,            ConcurrentMessageListenerContainer&lt;Object, Object&gt; concurrentMessageListenerContainer) &#123;        ReplyingKafkaTemplate&lt;Object, Object, Object&gt; kafkaTemplate =                new ReplyingKafkaTemplate&lt;&gt;(producerFactory, concurrentMessageListenerContainer);                return kafkaTemplate;    &#125;        @Bean    public KafkaTemplate&lt;Object, Object&gt; kafkaTemplate(ProducerFactory&lt;Object, Object&gt; producerFactory) &#123;        return new KafkaTemplate&lt;&gt;(producerFactory);    &#125;&#125;@Slf4j@Componentpublic class MessageSender &#123;        private final ReplyingKafkaTemplate&lt;Object, Object, Object&gt; replyingKafkaTemplate;    public MessageSender(ReplyingKafkaTemplate&lt;Object, Object, Object&gt; replyingKafkaTemplate) &#123;        this.replyingKafkaTemplate = replyingKafkaTemplate;    &#125;        public void sendAndReceive(Message message) &#123;        ProducerRecord&lt;Object, Object&gt; producerRecord = new ProducerRecord&lt;&gt;(TOPIC_RECEIVE, message);                RequestReplyFuture&lt;Object, Object, Object&gt; replyFuture = replyingKafkaTemplate.sendAndReceive(producerRecord);        ConsumerRecord&lt;Object, Object&gt; consumerRecord;        try &#123;            consumerRecord = replyFuture.get();            log.info(&quot;Receive reply success, result: &#123;&#125;&quot;, consumerRecord.value());        &#125; catch (InterruptedException | ExecutionException e) &#123;            log.error(&quot;Receive reply failure&quot;, e);        &#125;    &#125;&#125;\n消费方\n12345678910111213141516171819202122232425@Configurationpublic class KafkaConfiguration &#123;    public static final String TOPIC_RECEIVE = &quot;topic-receive&quot;;    @Bean    public NewTopic topicReceive() &#123;        return TopicBuilder.name(TOPIC_RECEIVE).partitions(1).replicas(1).build();    &#125;&#125;@Slf4j@Componentpublic class MessageReceiver &#123;        @SendTo    @KafkaListener(id = &quot;group-2-6&quot;, topics = &#123;TOPIC_RECEIVE&#125;)    public String receiveAndReply(Message message, Acknowledgment acknowledgment) &#123;        log.info(&quot;Receive message: &#123;&#125;&quot;, message);        acknowledgment.acknowledge();        return &quot;successful&quot;;    &#125;&#125;\n\n多方法处理消息组合使用 @KafkaListener 和 @KafkaHandler，能够让我们在传递消息时，根据转换后的消息有效负载类型来确定调用哪个方法。\n\n生产方\n123456789101112131415161718192021222324252627282930313233@Configurationpublic class KafkaConfiguration &#123;    public static final String TOPIC_MULTIPLE = &quot;topic-multiple&quot;;    @Bean    public NewTopic topicMultiple() &#123;        return TopicBuilder.name(TOPIC_MULTIPLE).partitions(1).replicas(1).build();    &#125;&#125;@Slf4j@Componentpublic class MessageSender &#123;    private final KafkaTemplate&lt;Object, Object&gt; kafkaTemplate;    public MessageSender(KafkaTemplate&lt;Object, Object&gt; kafkaTemplate) &#123;        this.kafkaTemplate = kafkaTemplate;    &#125;        public void sendToMultipleHandlers(String msg) &#123;        kafkaTemplate.send(TOPIC_MULTIPLE, msg);    &#125;        public void sendToMultipleHandlers(Message message) &#123;        kafkaTemplate.send(TOPIC_MULTIPLE, message);    &#125;&#125;\n消费方\n123456789101112131415161718192021222324252627282930313233343536373839404142@Configurationpublic class KafkaConfiguration &#123;    public static final String TOPIC_MULTIPLE = &quot;topic-multiple&quot;;    @Bean    public NewTopic topicMultiple() &#123;        return TopicBuilder.name(TOPIC_MULTIPLE).partitions(1).replicas(1).build();    &#125;&#125;@Slf4j@KafkaListener(id = &quot;group-2-7&quot;, topics = &#123;TOPIC_MULTIPLE&#125;)@Componentpublic class MessageReceiverMultipleMethods &#123;        @KafkaHandler    public void handlerStr(String message, Acknowledgment acknowledgment) &#123;        log.info(&quot;Receive message: &#123;&#125;, type of String&quot;, message);        acknowledgment.acknowledge();    &#125;        @KafkaHandler    public void handlerMessage(Message message, Acknowledgment acknowledgment) &#123;        log.info(&quot;Receive message: &#123;&#125;, type of Message&quot;, message);        acknowledgment.acknowledge();    &#125;        @KafkaHandler(isDefault = true)    public void handlerUnknown(Object message, Acknowledgment acknowledgment) &#123;        log.info(&quot;Receive message: &#123;&#125;, type of Unknown&quot;, message);        acknowledgment.acknowledge();    &#125;&#125;\n\n手动提交 offset （ack）默认情况下，Kafka 会自动帮我们提交 offset，但是这样做容易导致消息重复消费或消失丢失：\n\n在消费者收到消息之后，且 kafka 未自动提交 offset 之前，broker 宕机了，然后重启 broker，此时消费者会从原来的 offset 开始消费，于是出现了重复消费；\n在消费者收到消息之后，且消费者还没有处理完消息时，由于自动提交的间隔时间到了，于是 kafka 自动提交了 offset，但是之后消费者又挂掉了，那么当消费者重启之后，会从下一个 offset 开始消费，这样前面的消息就丢失了。我们可以改为使用手动提交 offset，只需要做两处调整：\n\n\n修改 application.yml\n12345678spring:  kafka:        consumer:      enable-auto-commit: false    listener:            ack-mode: manual\n消息处理完成之后，调用 Acknowledgment 的 acknowledge 方法\n123456789101112131415@Slf4j@Componentpublic class MessageReceiver &#123;    @KafkaListener(id = &quot;group-2-1&quot;, topics = &#123;&quot;topic-1&quot;&#125;)    public void receive(ConsumerRecord&lt;Object, Object&gt; record, Acknowledgment acknowledgment) &#123;        log.info(&quot;receive record: &#123;&#125;&quot;, record);        Optional&lt;Object&gt; messageOptional = Optional.ofNullable(record.value());        if (messageOptional.isPresent()) &#123;            Object message = messageOptional.get();            log.info(&quot;receive message: &#123;&#125;&quot;, message);        &#125;        acknowledgment.acknowledge();    &#125;&#125;\n\n异常处理对于消费者在处理消息过程中抛出的异常，我们可以设置 errorHandler，然后在 errorHandler 中统一处理。\n123456789101112131415161718192021222324@KafkaListener(id = &quot;group-2-8&quot;, topics = &#123;TOPIC_EXCEPTION&#125;, errorHandler = &quot;customErrorHandler&quot;)public void receiveWithException(Message message, Acknowledgment acknowledgment) &#123;    log.info(&quot;Receive message: &#123;&#125;&quot;, message);    throw new RuntimeException(&quot;error&quot;);&#125;@Slf4j@Service(&quot;customErrorHandler&quot;)public class CustomKafkaListenerErrorHandler implements KafkaListenerErrorHandler &#123;    @Override    public Object handleError(Message&lt;?&gt; message, ListenerExecutionFailedException exception) &#123;        log.error(&quot;Handle message with exception, message: &#123;&#125;&quot;, message.getPayload().toString());        return null;    &#125;    @Override    public Object handleError(Message&lt;?&gt; message, ListenerExecutionFailedException exception, Consumer&lt;?, ?&gt; consumer) &#123;        log.error(&quot;Handle message with exception, message: &#123;&#125;&quot;, message.getPayload().toString());        return null;    &#125;&#125;\n\n并发接收消息12345678 @KafkaListener(id = &quot;group-2-9&quot;, topics = &#123;TOPIC_CONCURRENT&#125;, concurrency = &quot;3&quot;)public void receiveConcurrent(Message message, Acknowledgment acknowledgment) &#123;    log.info(&quot;Receive message: &#123;&#125;&quot;, message);    acknowledgment.acknowledge();&#125;\n\n暂停与恢复消费通过使用 KafkaListenerEndpointRegistry，我们可以动态的暂停与恢复消费者消费消息。\n1234567891011121314151617181920212223242526272829private final KafkaListenerEndpointRegistry kafkaListenerEndpointRegistry;public MessageSender(KafkaListenerEndpointRegistry kafkaListenerEndpointRegistry) &#123;    this.kafkaListenerEndpointRegistry = kafkaListenerEndpointRegistry;&#125;public void startListener(String listenerId) &#123;    kafkaListenerEndpointRegistry.getListenerContainer(listenerId).start();&#125;public void stopListener(String listenerId) &#123;    kafkaListenerEndpointRegistry.getListenerContainer(listenerId).pause();&#125;public void resumeListener(String listenerId) &#123;    kafkaListenerEndpointRegistry.getListenerContainer(listenerId).resume();&#125;\n\n消息重试与死信队列当消费者在处理消息过程中发生异常时，我们可以进行多次重试，如果最终还是存在异常，我们可以将消息发送到预定的 Topic，即死信队列中。\n1234567891011121314151617181920212223@Beanpublic ConcurrentKafkaListenerContainerFactory&lt;Object, Object&gt; kafkaListenerContainerFactory(        ConcurrentKafkaListenerContainerFactoryConfigurer configurer,        ConsumerFactory&lt;Object, Object&gt; consumerFactory,        KafkaTemplate&lt;Object, Object&gt; kafkaTemplate) &#123;    ConcurrentKafkaListenerContainerFactory&lt;Object, Object&gt; factory = new ConcurrentKafkaListenerContainerFactory&lt;&gt;();    configurer.configure(factory, consumerFactory);    factory.setErrorHandler(new SeekToCurrentErrorHandler(new DeadLetterPublishingRecoverer(kafkaTemplate), new FixedBackOff(0L, 3L)));    return factory;&#125;@KafkaListener(id = &quot;group-2-10-2&quot;, topics = &#123;TOPIC_DEAD_LETTER + &quot;.DLT&quot;&#125;)public void receiveByDeadLetter(ConsumerRecord&lt;Object, Object&gt; record, Acknowledgment acknowledgment) &#123;    log.info(&quot;Receive message from DLT, message: &#123;&#125;&quot;, record.value());    acknowledgment.acknowledge();&#125;\n\n拦截器Apache Kafka 提供了一种向生产者和消费者添加拦截器的机制。下面我们将演示如何在 Spring Boot 中配置拦截器。\n\n生产方\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051@Configurationpublic class KafkaConfiguration &#123;    @Bean    public ProducerFactory&lt;Object, Object&gt; kafkaProducerFactory(KafkaProperties properties, CustomComponent customComponent) &#123;        Map&lt;String, Object&gt; producerProperties = properties.buildProducerProperties();        producerProperties.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, CustomProducerInterceptor.class.getName());        producerProperties.put(&quot;custom.component&quot;, customComponent);        DefaultKafkaProducerFactory&lt;Object, Object&gt; factory = new DefaultKafkaProducerFactory&lt;&gt;(producerProperties);        String transactionIdPrefix = properties.getProducer().getTransactionIdPrefix();        if (transactionIdPrefix != null) &#123;            factory.setTransactionIdPrefix(transactionIdPrefix);        &#125;        return factory;    &#125;&#125;@Slf4j@Componentpublic class CustomComponent &#123;    public void doSomething() &#123;        log.info(&quot;Do something&quot;);    &#125;&#125;@Slf4jpublic class CustomProducerInterceptor implements ProducerInterceptor&lt;Object, Object&gt; &#123;    private CustomComponent customComponent;    @Override    public ProducerRecord&lt;Object, Object&gt; onSend(ProducerRecord&lt;Object, Object&gt; record) &#123;        log.info(&quot;Before send message, do your own business&quot;);        customComponent.doSomething();        return record;    &#125;    @Override    public void onAcknowledgement(RecordMetadata metadata, Exception exception) &#123;    &#125;    @Override    public void close() &#123;    &#125;    @Override    public void configure(Map&lt;String, ?&gt; configs) &#123;        this.customComponent = (CustomComponent) configs.get(&quot;custom.component&quot;);    &#125;&#125;\n消费方\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051@Configurationpublic class KafkaConfiguration &#123;    @Bean    public ProducerFactory&lt;Object, Object&gt; kafkaProducerFactory(KafkaProperties properties, CustomComponent customComponent) &#123;        Map&lt;String, Object&gt; producerProperties = properties.buildProducerProperties();        producerProperties.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, CustomProducerInterceptor.class.getName());        producerProperties.put(&quot;custom.component&quot;, customComponent);        DefaultKafkaProducerFactory&lt;Object, Object&gt; factory = new DefaultKafkaProducerFactory&lt;&gt;(producerProperties);        String transactionIdPrefix = properties.getProducer().getTransactionIdPrefix();        if (transactionIdPrefix != null) &#123;            factory.setTransactionIdPrefix(transactionIdPrefix);        &#125;        return factory;    &#125;&#125;@Slf4j@Componentpublic class CustomComponent &#123;    public void doSomething() &#123;        log.info(&quot;Do something&quot;);    &#125;&#125;@Slf4jpublic class CustomProducerInterceptor implements ProducerInterceptor&lt;Object, Object&gt; &#123;    private CustomComponent customComponent;    @Override    public ProducerRecord&lt;Object, Object&gt; onSend(ProducerRecord&lt;Object, Object&gt; record) &#123;        log.info(&quot;Before send message, do your own business&quot;);        customComponent.doSomething();        return record;    &#125;    @Override    public void onAcknowledgement(RecordMetadata metadata, Exception exception) &#123;    &#125;    @Override    public void close() &#123;    &#125;    @Override    public void configure(Map&lt;String, ?&gt; configs) &#123;        this.customComponent = (CustomComponent) configs.get(&quot;custom.component&quot;);    &#125;&#125;\n\n","slug":"JAVA/Spring Boot 集成 Kafka","date":"2022-04-16T18:36:54.000Z","categories_index":"生产方,JAVA","tags_index":"消费方,offset,使用事务","author_index":"dandeliono"},{"id":"5d405dae2a57ea9119868cf9afedbd64","title":"Linux PageCache详解","content":"Linux PageCache详解应用程序要存储或访问数据时，只需读或者写” 文件” 的一维地址空间即可，而这个地址空间与存储设备上存储块之间的对应关系则由操作系统维护。说白了，文件就是基于内核态 Page Cache 的一层抽象。\n相关场景\n服务器的 load 飙高；\n服务器的 I&#x2F;O 吞吐飙高；\n业务响应时延出现大的毛刺；\n业务平均访问时延明显增加。\n\n什么是 page cachepage cache 是内存管理的内存，属于内核不属于用户。 \n查看方式\n&#x2F;proc&#x2F;meminfo\nfree 命令\nvmstat 命令\n\npage cache 指标说明通过 &#x2F; proc&#x2F;meminfo 查看内存信息如下：\n12345678910111213141516171819202122232425262728293031323334353637383940414243444546MemTotal:        2046920 kBMemFree:          375284 kBMemAvailable:    1013780 kBBuffers:          142100 kBCached:           668196 kBSwapCached:            0 kBActive:           959184 kBInactive:         279700 kBActive(anon):     491680 kBInactive(anon):    46776 kBActive(file):     467504 kBInactive(file):   232924 kBUnevictable:           0 kBMlocked:               0 kBSwapTotal:             0 kBSwapFree:              0 kBDirty:                 0 kBWriteback:             0 kBAnonPages:        428608 kBMapped:            37768 kBShmem:            109868 kBSlab:             138120 kBSReclaimable:     126188 kBSUnreclaim:        11932 kBKernelStack:        2480 kBPageTables:         4572 kBNFS_Unstable:          0 kBBounce:                0 kBWritebackTmp:          0 kBCommitLimit:      891364 kBCommitted_AS:    1419192 kBVmallocTotal:   34359738367 kBVmallocUsed:        9136 kBVmallocChunk:   34359724540 kBHardwareCorrupted:     0 kBAnonHugePages:    169984 kBCmaTotal:              0 kBCmaFree:               0 kBHugePages_Total:     129HugePages_Free:      129HugePages_Rsvd:        0HugePages_Surp:        0Hugepagesize:       2048 kBDirectMap4k:       53104 kBDirectMap2M:     2043904 kBDirectMap1G:           0 kB\n\n通过计算发现：\nBuffers + Cached + SwapCached &#x3D; Active(file) + Inactive(file) + Shmem + SwapCached\n在 Page Cache 中，Active(file)+Inactive(file) 是 File-backed page（与文件对应的内存页），平时用的 mmap() 内存映射方式和 buffered I&#x2F;O 来消耗的内存就属于这部分，这部分在真实的生产环境上也最容易产生问题。\nSwapCached 说明 (生产环境中不建议开启，防止 IO 引起性能抖动)SwapCached 是在打开了 Swap 分区后，把 Inactive(anon)+Active(anon) 这两项里的匿名页给交换到磁盘（swap out），然后再读入到内存（swap in）后分配的内存。由于读入到内存后原来的 Swap File 还在，所以 SwapCached 也可以认为是 File-backed page，即属于 Page Cache。\nShmem\nShmem 是指匿名共享映射这种方式分配的内存（free 命令中 shared 这一项）\n进程使用 mmap(MAP_ANON|MAP_SHARED) 的方式申请内存\ntmpfs： 磁盘的速度是远远低于内存的，有些应用程序为了提升性能，会避免将一些无需持续化存储的数据写入到磁盘，而是把这部分临时数据写入到内存中，然后定期或者在不需要这部分数据时，清理掉这部分内容来释放出内存。在这种需求下，就产生了一种特殊的 Shmem：tmpfs\n\nfrem 命令的说明数据来源于 &#x2F; proc&#x2F;meminfo\n1234free -k              total        used        free      shared  buff/cache   availableMem:        2046920      732896      377464      109860      936560     1016044Swap:             0           0           0\n\n通过源码可知：buff&#x2F;cache &#x3D; Buffers + Cached + SReclaimable\nSReclaimable 是指可以被回收的内核内存，包括 dentry 和 inode。 \n缓存的具体含义官方定义123456789Buffers %lu    Relatively temporary storage for raw disk blocks that shouldn&#x27;t get tremendously large (20MB or so).Cached %lu   In-memory cache for files read from the disk (the page cache).  Doesn&#x27;t include SwapCached.SReclaimable %lu (since Linux 2.6.19)    Part of Slab, that might be reclaimed, such as caches.    SUnreclaim %lu (since Linux 2.6.19)    Part of Slab, that cannot be reclaimed on memory pressure.\n\n具体解释\nBuffers 是对原始磁盘块的临时存储，也就是用来缓存磁盘的数据，通常不会特别大（20MB 左右）。这样，内核就可以把分散的写集中起来，统一优化磁盘的写入，比如可以把多次小的写合并成单次大的写等等。\nCached 是从磁盘读取文件的页缓存，也就是用来缓存从文件读取的数据。这样，下次访问这些文件数据时，就可以直接从内存中快速获取，而不需要再次访问缓慢的磁盘。\nSReclaimable 是 Slab 的一部分。Slab 包括两部分，其中的可回收部分，用 SReclaimable 记录；而不可回收部分，用 SUnreclaim 记录。\nBuffer 是对磁盘数据的缓存，而 Cache 是文件数据的缓存，它们既会用在读请求中，也会用在写请求中\n\nPageCache 数据结构\n内存管理系统与 Page Cache 交互，负责维护每项 Page Cache 的分配和回收，同时在使用 memory map 方式访问时负责建立映射；\nVFS 与 Page Cache 交互，负责 Page Cache 与用户空间的数据交换，即文件读写；\n具体文件系统则一般只与 Buffer Cache 交互，它们负责在外围存储设备和 Buffer Cache 之间交换数据。\n一个 Page Cache 包含多个 Buffer Cache，一个 Buffer Cache 与一个磁盘块一一对应；假定了 Page 的大小是 4K，则文件的每个 4K 的数据块最多只能对应一个 Page Cache 项，它通过一个是 radix tree 来管理文件块和 page cache 的映射关系，Radix tree 是一种搜索树，Linux 内核利用这个数据结构来通过文件内偏移快速定位 Cache 项。\n\n为什么使用 page cache减少 I&#x2F;O，提升应用的 I&#x2F;O 速度\n\n具体文件系统： ，如 ext2&#x2F;ext3、jfs、ntfs 等，负责在文件 Cache 和存储设备之间交换数据\n虚拟文件系统 VFS：  负责在应用程序和文件 Cache 之间通过 read&#x2F;write 等接口交换数据\n内存管理系统：  负责文件 Cache 的分配和回收\n虚拟内存管理系统 (VMM)：  则允许应用程序和文件 Cache 之间通过 memory map 的方式交换数据\n在 Linux 系统中，文件 Cache 是内存管理系统、文件系统以及应用程序之间的一个联系枢纽。\n\npage cache 的产生产生方式\nBuffered I&#x2F;O（标准 I&#x2F;O）如：read&#x2F;write&#x2F;sendfile 等；\nMemory-Mapped I&#x2F;O（存储映射 I&#x2F;O）如：mmap；\nsendfile 和 mmap 都是零拷贝的实现方案。\n\n产生方式的区别\n标准 I&#x2F;O 是写的 (write(2)) 用户缓冲区 (Userpace Page 对应的内存)，然后再将用户缓冲区里的数据拷贝到内核缓冲区 (Pagecache Page 对应的内存)；如果是读的 (read(2)) 话则是先从内核缓冲区拷贝到用户缓冲区，再从用户缓冲区读数据，也就是 buffer 和文件内容不存在任何映射关系。\n对于存储映射 I&#x2F;O 而言，则是直接将 Pagecache Page 给映射到用户地址空间，用户直接读写 Pagecache Page 中内容。\n\n常规文件读写FileChannel#read，FileChannel#write，共涉及四次上下文切换（内核态和用户态的切换，包括 read 调用，read 返回，write 调用，write 返回）和四次数据拷贝。\n脏页123456cat /proc/vmstat | egrep &quot;dirty|writeback&quot;nr_dirty 25nr_writeback 0nr_writeback_temp 0nr_dirty_threshold 94000nr_dirty_background_threshold 31333\n\nnr_dirty 表示当前系统中积压了多少脏页，nr_writeback 则表示有多少脏页正在回写到磁盘中，他们两个的单位都是 Page(4KB)。\nmmap\n文件（page cache）直接映射到用户虚拟地址空间，内核态和用户态共享一片 page cache，避免了一次数据拷贝\n建立 mmap 之后，并不会立马加载数据到内存，只有真正使用数据时，才会引发缺页异常并加载数据到内存\n\nmemory map 具体步骤如下\n应用程序调用 mmap（图中 1），先到内核中\n后调用 do_mmap_pgoff（图中 2），该函数从应用程序的地址空间中分配一段区域作为映射的内存地址，并使用一个 VMA（vm_area_struct）结构代表该区域，\n之后就返回到应用程序（图中 3）\n当应用程序访问 mmap 所返回的地址指针时（图中 4），由于虚实映射尚未建立，会触发缺页中断（图中 5）。之后系统会调用缺页中断处理函数（图中 6），在缺页中断处理函数中，内核通过相应区域的 VMA 结构判断出该区域属于文件映射，于是调用具体文件系统的接口读入相应的 Page Cache 项（图中 7、8、9），并填写相应的虚实映射表。\n经过这些步骤之后，应用程序就可以正常访问相应的内存区域了。\n\nsendfile\n使用 sendfile 的方式避免了用户空间与内核空间的交互，复制次数减少到三次，内核态与用户态切换减少到两次。\n在 Linux 内核 2.4 及后期版本中，针对套接字缓冲区描述符做了相应调整，DMA 自带了收集功能，对于用户方面，用法还是一样。内部只把包含数据位置和长度信息的描述符追加到套接字缓冲区，DMA 引擎直接把数据从内核缓冲区传到协议引擎，从而消除了最后一次 CPU 参与的拷贝动作。\n\n顺序读写文件预读文件的预读机制，它是一种将磁盘块预读到 page cache 的机制, 执行步骤如下：\n\n对于每个文件的第一个读请求，系统读入所请求的页面并读入紧随其后的少数几个页面 (不少于一个页面，通常是三个页面)，这时的预读称为同步预读。\n对于第二次读请求，如果所读页面不在 Cache 中，即不在前次预读的 group 中，则表明文件访问不是顺序访问，系统继续采用同步预读；如果所读页面在 Cache 中，则表明前次预读命中，操作系统把预读 group 扩大一倍，并让底层文件系统读入 group 中剩下尚不在 Cache 中的文件数据块，这时的预读称为异步预读。\n无论第二次读请求是否命中，系统都要更新当前预读 group 的大小。\n系统中定义了一个 window，它包括前一次预读的 group 和本次预读的 group。任何接下来的读请求都会处于两种情况之一：\n第一种情况是所请求的页面处于预读 window 中，这时继续进行异步预读并更新相应的 window 和 group；\n第二种情况是所请求的页面处于预读 window 之外，这时系统就要进行同步预读并重置相应的 window 和 group。\n\n\n\n图中 group 指一次读入 page cached 的集合；window 包括前一次预读的 group 和本次预读的 group；浅灰色代表要用户想要查找的 page cache，深灰色代表命中的 page。\n顺序读写高效的原因以顺序读为例，当用户发起一个 fileChannel.read(4kb) 之后，实际发生了两件事\n\n操作系统从磁盘加载了 16kb 进入 PageCache，这被称为预读\n操作通从 PageCache 拷贝 4kb 进入用户内存\n当用户继续访问接下来的 [4kb,16kb] 的磁盘内容时，便是直接从 PageCache 去访问了\n\npage cache 的消亡page cache 的回收主要是针对 free 命令中的 buff&#x2F;cache 中的这些就是 “活着” 的 Page Cache。回收的过程如下图所示：\n回收的方式主要是两种：直接回收和后台回收, 具体的回收行为，可以使用以下命令查看：\n12345678910111213141516sar -B 1Linux 3.10.0-1062.9.1.el7.x86_64 (instance-gctg007a) \t08/18/2020 \t_x86_64_\t(1 CPU)07:34:59 PM  pgpgin/s pgpgout/s   fault/s  majflt/s  pgfree/s pgscank/s pgscand/s pgsteal/s    %vmeff07:35:00 PM      0.00      0.00     66.00      0.00     58.00      0.00      0.00      0.00      0.0007:35:01 PM      0.00     25.53   1893.62      0.00    608.51      0.00      0.00      0.00      0.0007:35:02 PM      0.00      0.00    648.48      0.00    280.81      0.00      0.00      0.00      0.0007:35:03 PM      0.00      0.00     17.00      0.00     19.00      0.00      0.00      0.00      0.0007:35:04 PM      0.00      0.00   1096.04      0.00    295.05      0.00      0.00      0.00      0.0007:35:05 PM      0.00      0.00     17.00      0.00     23.00      0.00      0.00      0.00      0.0007:35:06 PM      0.00     52.00     17.00      0.00     19.00      0.00      0.00      0.00      0.0007:35:07 PM      0.00      0.00     17.00      0.00     44.00      0.00      0.00      0.00      0.0007:35:08 PM      0.00      0.00     18.00      0.00     19.00      0.00      0.00      0.00      0.0007:35:09 PM      0.00      0.00     18.18      0.00     19.19      0.00      0.00      0.00      0.0007:35:10 PM      0.00      0.00     17.00      0.00     19.00      0.00      0.00      0.00      0.0007:35:11 PM      0.00      0.00     17.00      0.00     19.00      0.00      0.00      0.00      0.00\n\n\npgscank&#x2F;s : kswapd(后台回收线程) 每秒扫描的 page 个数。\npgscand&#x2F;s: Application 在内存申请过程中每秒直接扫描的 page 个数。\npgsteal&#x2F;s: 扫描的 page 中每秒被回收的个数。\n%vmeff: pgsteal&#x2F;(pgscank+pgscand), 回收效率，越接近 100 说明系统越安全，越接近 0 说明系统内存压力越大。\n\nsar -B 与 &#x2F; proc&#x2F;vmstat 比对\n\n\nsar -B\n&#x2F;proc&#x2F;vmstat\n\n\n\npgscank\npgscan_kswapd\n\n\npgscand\npgscan_direct\n\n\npgsteal\npgsteal_kswapd+pgsteal_direct\n\n\n其他DMA（Direct Memory Access, 直接存储器访问）\nDMA 的出现就是为了解决批量数据的输入 &#x2F; 输出问题。DMA 是指外部设备不通过 CPU 而直接与系统内存交换数据的接口技术\nDMA 控制器需要具备的功能：\n能向 CPU 发出系统保持信号，提出总线接管请求\n当 CPU 同意接管请求之后，对总线的控制交给 DMA\n能对存储器寻址及能修改地址指针，实现对内存的读写\n能决定本次 DMA 传送的字节数，判断 DMA 传送是否借宿\n发送 DMA 结束信号，使 CPU 恢复正常工作状态\n\n\n\n堆外内存堆内存与堆外内存的关系\n\n\n\n堆内内存\n堆外内存\n\n\n\n底层实现\n数组，JVM 内存\nunsafe.allocateMemory(size) 返回直接内存\n\n\n分配大小限制\n-Xms-Xmx，\n\n\n\n数组大小，当前 JVM free memory 大于 1.5G 时，ByteBuffer.allocate(900M) 会报错\n-XX:MaxDirectMemorySize 参数从 JVM 层面限制，同时受到机器虚拟内存的限制\n\n\n\n垃圾回收\n\n当前 DirectByteBuffer 不再被使用时，会触发内部 cleaner 的钩子\n\n\n保险起见，可以考虑手动回收((DirectBuffer)buffer).cleaner().clean() || 内存复制 | 堆内内存 — 堆外内存 —pageCache | 堆外内存 –pageCache |\n最佳实践\n当需要申请大块的内存时，堆内内存会受到限制，只能分配堆外内存。\n堆外内存适用于生命周期中等或较长的对象。(如果是生命周期较短的对象，在 YGC 的时候就被回收了，就不存在大内存且生命周期较长的对象在 FGC 对应用造成的性能影响)。\n堆内内存刷盘的过程中，还需要复制一份到堆外内存，这部分内容可以在 FileChannel 的实现源码中看到细节\n使用 HeapByteBuffer 读写都会经过 DirectByteBuffer，写入数据的流转方式其实是：HeapByteBuffer -&gt; DirectByteBuffer -&gt; PageCache -&gt; Disk，读取数据的流转方式正好相反。\n使用 HeapByteBuffer 读写会申请一块跟线程绑定的 DirectByteBuffer。这意味着，线程越多，临时 DirectByteBuffer 就越会占用越多的空间。\n\n\n堆外内存就是把内存对象分配在 Java 虚拟机堆以外的内存，这些内存直接受操作系统管理（而不是虚拟机），这样做的结果就是能够在一定程度上减少垃圾回收对应用程序造成的影响。\n内存回收流程\n\nPageCache 内存回收回收过程在内存紧张的时候会触发内存回收，内存回收会尝试去回收 reclaimable（可被回收）的内存。包括PageCache 以及 reclaimable kernel memory（比如 slab）。\n避免 PageCache 回收出现的性能问题memory cgroup protection\nmemory.max：memory cgroup 内的进程最多能够分配的内存，如果不设置的话，就默认不做内存大小的限制\nmemory.high：当 memory cgroup 内进程的内存使用量超过了该值后就会立即被回收掉，目的是为了尽快的回收掉不活跃的 Page Cache。\nmemory.low：用来保护重要数据的，当 memory cgroup 内进程的内存使用量低于了该值后，在内存紧张触发回收后就会先去回收不属于该 memory cgroup 的 Page Cache，等到其他的 Page Cache 都被回收掉后再来回收这些 Page Cache。\nmemory.min：用来保护重要数据的，只不过与 memoy.low 有所不同的是，当 memory cgroup 内进程的内存使用量低于该值后，即使其他不在该 memory cgroup 内的 Page Cache 都被回收完了也不会去回收这些 Page Cache。\n总结：如果你想要保护你的 Page Cache 不被回收，你就可以考虑将你的业务进程放在一个 memory cgroup 中，然后设置 memory.{min,low} 来进行保护；与之相反，如果你想要尽快释放你的 Page Cache，那你可以考虑设置 memory.high 来及时的释放掉不活跃的 Page Cache。\n\n出现 load 过高的原因直接内存回收引起内存回收过程后台回收原理：\n通过调整参数 vm.min_free_kbytes 来提高后台进程回收频率。\n123cat /proc/sys/vm/min_free_kbytesvi /etc/sysctl.conf vm.min_free_kbytes=524288 sysctl -p\n\n通过调整内存水位，在一定程度上保障了应用的内存申请，但是同时也带来了一定的内存浪费，因为系统始终要保障有这么多的 free 内存，这就压缩了 Page Cache 的空间。调整的效果你可以通过 &#x2F;proc&#x2F;zoneinfo 来观察\n系统中脏页积压过多内存申请过程解决方法设置配置：&#x2F;proc&#x2F;vmstat\n12345vm.dirty_background_bytes = 0vm.dirty_background_ratio = 10vm.dirty_bytes = 0vm.dirty_expire_centisecs = 3000vm.dirty_ratio = 20\n\n系统 numa 策略配置不当内存泄漏OOM KILL 逻辑可以调整 oom_score_adj 来防止进程被杀掉（不建议配置）\n如何观察内核内存泄漏\n如果 &#x2F;proc&#x2F;meminfo 中内核内存（比如 VmallocUsed 和 SUnreclaim）太大，那很有可能发生了内核内存泄漏\n周期性地观察 VmallocUsed 和 SUnreclaim 的变化，如果它们持续增长而不下降，也可能是发生了内核内存泄漏\n通过 &#x2F;proc&#x2F;vmallocinfo 来看到该模块的内存使用情况\nkmemleak 内核内存分析工具\n\n排查思路\n\n\n&#x2F;proc&#x2F;meminfo\n含义以及排查思路\n\n\n\nActive(anon)\n在 active anon lru 的 page，与下一项相互转换\n\n\nInactive(anon)\n在 inactive anonlru 的 page，可以交换到 swap 分区，（active anno 也是）但是不能回收\n\n\n程序使用 malloc() 或 mmap() 匿名方式申请并且写后的内存。如果过大，排除思路：  \n\n使用 top 找出内存消耗最大的进程  \n\n使用 pmap 分析该进程  \n\n如果没有任何进程内存开销大，则重点排除 tmpfs || Unevictable | 在系统内存紧张时不能被回收，主要组成：1.ram disk 或 ramfs 消耗的内存  \n\n以 SHM_LOCK 方式申请的 Shmem  \n\n使用 mlock() 序列函数来管理的内存 || Mlocked | 属于 Unevictable 的一种，重点排除 mlock() 方式包含的内存 || AnonPages | AnonPages！&#x3D;Active（anon）+Inactive（anon）因为 shmem（包括 tmpfs）虽然属于 active（anon）或 Inactive（anon），但是他们有自己的内存文件，所以不属于 AnonPagesactive anon 和 Inactive anon 表示不可回收但是可以被交换到 swap 分区的内存AnonPages 没有对应文件的内存排除 malloc() 方式申请的内存或 mmap（PROT_WRITE,MAP_ANON|MAP_PRIVATE）方式申请的内存 || Mapped | 使用 mmap（2）申请，没有被 unmap 的内存；unmap 包含主动调用 unmap（2）以及内核内存回收时的 unmap排查 mmap（）申请的内存 || Shmem | 共享内存，特别注意 tmpfs，排查思路  \n\n使用 top 找出 shr 最大的进程  \n\n使用 pmap 分析该进程  \n\n如果没有任何进程消耗 shr 内存，则重点排查 tmpfs || Slab | 分为可被回收（SReclaimable）和不可以被回收（SUnreclaim），其中不可被回收的 slab 如果发生泄漏，比如 kmalloc 申请的内存没有释放，排查思路  \n\n使用 slaptop 分析 slab 最大的数据  \n\n排查驱动程序以 kmalloc(）方式申请的内存 || VmallocUsed | 通过 vmalloc 分配的内核内存，可以使用 &#x2F; proc&#x2F;vmallocinfo，来判断哪些驱动程序以 vmalloc 方式申请的内存较多可以尝试卸载驱动，释放内存 |\n\n应用程序可以通过 malloc() 和 free() 在用户态申请和释放内存，与之对应，可以通过 kmalloc()&#x2F;kfree() 以及 vmalloc()&#x2F;vfree() 在内核态申请和释放内存\n\nvmalloc 申请的内存会体现在 VmallocUsed 这一项中，即已使用的 Vmalloc 区大小；而 kmalloc 申请的内存则是体现在 Slab 这一项中，它又分为两部分，其中 SReclaimable 是指在内存紧张的时候可以被回收的内存，而 SUnreclaim 则是不可以被回收只能主动释放的内存。\n\n\n其他清理缓存 buffer&#x2F;cache运行 sync 将 dirty 的内容写回硬盘通过修改 proc 系统的 drop_caches 清理 free 的 cache12345echo 3 &gt; /proc/sys/vm/drop_caches说明echo 1 &gt; /proc/sys/vm/drop_caches:表示清除pagecache。 echo 2 &gt; /proc/sys/vm/drop_caches:表示清除回收slab分配器中的对象（包括目录项缓存和inode缓存）。slab分配器是内核中管理内存的一种机制，其中很多缓存数据实现都是用的pagecache。 echo 3 &gt; /proc/sys/vm/drop_caches:表示清除pagecache和slab分配器中的缓存对象。\n\n可以通过/proc/vmstat文件判断是否执行过 drop_caches：\n123[root@instance-gctg007a ~]drop_pagecache 0drop_slab 0\n\n可以调用 crond 定时任务：每 10 分钟执行一次1*/10 * * * * sync;echo 3 &gt; /proc/sys/vm/drop_caches;  \n\n重要配置参数&#x2F;proc&#x2F;sys&#x2F;vm&#x2F;dirty_ratio（同步刷盘）这个参数控制文件系统的文件系统写缓冲区的大小，单位是百分比，表示系统内存的百分比，表示当写缓冲使用到系统内存多少的时候，开始向磁盘写出数据。增大之会使用更多系统内存用于磁盘写缓冲，也可以极大提高系统的写性能。但是，当你需要持续、恒定的写入场合时，应该降低其数值，一般启动上缺省是 10。设 1 加速程序速度\n&#x2F;proc&#x2F;sys&#x2F;vm&#x2F;dirty_background_ratio（异步刷盘）这个参数控制文件系统的 pdflush 进程，在何时刷新磁盘。单位是百分比，表示系统内存的百分比，意思是当写缓冲使用到系统内存多少的时 候，pdflush 开始向磁盘写出数据。增大之会使用更多系统内存用于磁盘写缓冲，也可以极大提高系统的写性能。但是，当你需要持续、恒定的写入场合时， 应该降低其数值，一般启动上缺省是 5\n&#x2F;proc&#x2F;sys&#x2F;vm&#x2F;dirty_writeback_centisecs这个参数控制内核的脏数据刷新进程 pdflush 的运行间隔。单位是 1&#x2F;100 秒。缺省数值是 500，也就是 5 秒。如果你的系统是持续地写入动作，那么实际上还是降低这个数值比较好，这样可以把尖峰的写操作削平成多次写操\n&#x2F;proc&#x2F;sys&#x2F;vm&#x2F;dirty_expire_centisecs这个参数声明 Linux 内核写缓冲区里面的数据多 “旧” 了之后，pdflush 进程就开始考虑写到磁盘中去。单位是 1&#x2F;100 秒。缺省是 30000，也就是 30 秒的数据就算旧了，将会刷新磁盘。对于特别重载的写操作来说，这个值适当缩小也是好的，但也不能缩小太多，因为缩小太多也会导致 IO 提高太快。建议设置为 1500，也就是 15 秒算旧。\n&#x2F;proc&#x2F;sys&#x2F;vm&#x2F;drop_caches释放已经使用的 cache\n&#x2F;proc&#x2F;sys&#x2F;vm&#x2F;page_cluster该文件表示在写一次到 swap 区的时候写入的页面数量，0 表示 1 页，1 表示 2 页，2 表示 4 页。\n&#x2F;proc&#x2F;sys&#x2F;vm&#x2F;swapiness该文件表示系统进行交换行为的程度，数值（0-100）越高，越可能发生磁盘交换。\n&#x2F;proc&#x2F;sys&#x2F;vm&#x2F;vfs_cache_pressure该文件表示内核回收用于 directory 和 inode cache 内存的倾向\n参考\nproc 帮助手册\n常用性能分析工具\nNIO 进阶篇：Page Cache、零拷贝、顺序读写、堆外内存\n面试官：RocketMQ 如何基于 mmap+page cache 实现磁盘文件的高性能读写？\n文件系统缓存 dirty_ratio 与 dirty_background_ratio 两个参数区别\nPageCache 系列之五 统一缓存之 PageCache\n\n","slug":"LINUX/Linux PageCache详解","date":"2022-04-12T14:38:01.000Z","categories_index":"proc,LINUX","tags_index":"Cache,cache,page","author_index":"dandeliono"},{"id":"19b24bca918ce369e190f8fc79735fce","title":"Advanced tuning finding and fixing slow Elasticsearch queries","content":"Advanced tuning: finding and fixing slow Elasticsearch queriesElasticsearch 是一个非常灵活且功能丰富的应用程序，它提供了许多不同的数据查询方法。但是，您是否遇到过查询速度低于预期的情况？对于像 Elasticsearch 这样的分布式系统，可能会有各种影响查询性能的因素，包括负载平衡器设置、网络延迟（带宽、网卡 &#x2F; 驱动程序）等在内的各种外部因素。\n在本篇博文中，我将讨论导致慢查询的原因，以及如何在 Elasticsearch 的上下文中识别它们。本文主要源于一些常见的排查方法展开论述，因此，可能需要您对 ElasticSearch 的工作原理非常熟悉。\nElasticsearch 慢查询的常见原因在我们研究一些更棘手的情况之前，让我们先从一些导致慢查询的最常见原因及其解决方案入手。\n症状：非活动状态下资源利用率也很高每个分片都消耗资源（CPU &#x2F; 内存）。即使没有索引 &#x2F; 搜索请求，分片的存在也会产生集群开销。\n问题集群中的分片太多，以至于任何查询的执行速度看起来都很慢。一个好的经验法则是：确保对于每个节点上已配置的每个 GB 堆，将非冻结的分片数量保持在 20 以下。\n解决方案减少分片计数，实施冻结索引和 &#x2F; 或添加附加节点来实现负载平衡。考虑结合使用 Elasticsearch 中的热 &#x2F; 温架构（非常适合基于时间的索引）以及滚动 &#x2F; 收缩功能，以高效管理分片计数。要想顺利完成部署，最好先执行适当的容量计划，以帮助确定适合每个搜索用例的最佳分片数。\n症状：线程池存在大量的 “rejected”（拒绝）根据上次集群重新启动的累积值看，搜索线程池显示 “rejected” 计数在持续增加。\nGET /\\_cat/thread\\_pool/search?v&amp;h\\=node\\_name,name,active,rejected,completed\n\n响应类似如下：\nnode\\_name             name   active rejected completed\ninstance\\-0000000001 search 0  10  0 instance\\-0000000002 search 0  20  0 instance\\-0000000003 search 0  30  0\n\n问题查询面向的分片太多，超过了集群中的核心数。这会在搜索线程池中造成排队任务，从而导致搜索拒绝。另一个常见原因是磁盘 I&#x2F;O 速度慢，导致搜索排队或在某些情况下 CPU 完全饱和。 \n解决方案创建索引时采用 1 个主分片: 1 个副本分片 (1P:1R) 模型。使用索引模板是一个在创建索引时部署此设置的好方法。（Elasticsearch 7.0 或更高版本将默认 1P:1R）。Elasticsearch 5.1 或更高版本支持搜索任务取消，这对于取消任务管理 API 中出现的慢查询任务非常有用。若要改进磁盘 I&#x2F;O，请查看存储建议，并确保使用推荐的硬件以获得最佳性能。\n症状：高 CPU 使用率和索引延迟指标相关性表明，当集群不堪重负时，CPU 使用率和索引延迟都会很高。\n问题集群索引量大会影响搜索性能。\n解决方案将 index.refresh_ interval 的值（从文档被索引到其变为可见的时间间隔）增加到 30 秒，通常有助于提高索引性能。实际业务情境中可能会有所不同，因此测试是关键。这可以确保分片不必因为每 1 秒默认创建一个新分段而造成工作负载增大。\n对于索引量大的用例，请查看索引调优建议，以优化索引和搜索性能。\n症状：副本分片增加后延迟增大在副本分片计数增加（例如，从 1 增加到 2）后，可以观察到查询延迟。如果存在较多的数据，那么缓存的数据将很快被逐出，从而导致操作系统页面错误增加。\n问题文件系统缓存没有足够的内存来缓存索引中经常查询的部分。Elasticsearch 的 查询缓存实现了 LRU 逐出策略：当缓存变满时，将逐出最近使用最少的数据，以便为新数据让路。\n解决方案为文件系统缓存留出至少 50% 的物理 RAM。内存越多，缓存的空间就越大，尤其是当集群遇到 I&#x2F;O 问题时。 假设堆大小已正确配置，任何剩余的可用于文件系统缓存的物理 RAM 都会大大提高搜索性能。\n例如，128GB 的 RAM 服务器留出 30GB 的内存用于堆，剩余内存用于文件系统缓存（有时称为 OS 缓存），假设操作系统缓存了最近访问的 4KB 数据块，这样，如果您一次又一次地读取相同的文件，那么您不必花很长时间到磁盘上读，而是会从内存中直接读取。\n除了文件系统缓存，Elasticsearch 还使用查询缓存和请求缓存来提高搜索速度。 所有这些缓存都可以使用搜索请求首选项进行优化，以便每次都将某些搜索请求路由到同一组分片，而不是在不同的可用副本之间进行交替。这将更好地利用请求缓存、节点查询缓存和文件系统缓存。\n症状：共享资源时利用率高操作系统显示出持续的高 CPU &#x2F; 磁盘 I&#x2F;O 利用率。停止第三方应用程序后，可以看到性能会提高。\n问题其他进程（例如 Logstash）和 Elasticsearch 本身之间存在资源（CPU 和 &#x2F; 或磁盘 I&#x2F;O）争用。\n解决方案避免在共享硬件上与其他资源密集型应用程序一起运行 ElasticSearch。\n症状：聚合多个高度唯一字段时堆利用率高当查询包含高度唯一值（如 ID、用户名、电子邮件地址等）的聚合字段时，性能不佳。在堆转储分析期间发现：使用 “search”、“buckets”、“aggregation” 等术语的 Java 对象消耗了大量的堆空间。\n问题聚合在高基数字段上运行，需要大量资源来提取许多存储桶。 此外，还可能存在涉及嵌套字段和 &#x2F; 或联接字段的嵌套聚合。\n解决方案若要改进高基数术语聚合的性能，请阅读我的咨询团队同事的这篇博文：https://www.elastic.co/cn/blog/improving-the-performance-of-high-cardinality-terms-aggregations-in-elasticsearch\n有关进一步的调整，请查看我们关于嵌套字段和联接字段的建议，以更好地提高聚合性能。\n偶发慢查询一般来说，可以采用一些索引调优&#x2F;搜索调优建议来解决偶尔或间歇出现的慢查询问题。偶发的慢查询应该与这些监测指标中的一个或多个密切相关：\n\nCPU 负载\n索引吞吐量\n搜索吞吐量\n垃圾收集 (GC) 活动\n搜索线程池队列大小\n\nElasticsearch 还有另一个有用的功能，称为自适应副本选择 (ARS)，它允许协调节点了解数据节点上的负载，并允许它选择最佳分片副本来执行搜索，从而提高搜索吞吐量并降低延迟。通过在查询期间更均匀地分配负载，ARS 对于偶发的减速有很大帮助。在 Elasticsearch 7.0 及更高版本中，默认情况下将启用 ARS。\n持续性慢查询对于持续性的慢查询，我们可以尝试逐个删除查询中的功能，并检查查询是否仍然慢。查找最简单查询以再现性能问题，有助于隔离和识别问题：\n\n没有高亮显示，它是否仍然很慢？\n没有聚合，它是否仍然很慢？\n如果大小设为 0，它是否仍然慢？（当大小设为 0 时，Elasticsearch 会缓存搜索请求的结果，以便更快地进行搜索）\n\n一些“搜索调优” 的建议是否有用？\n在故障排除期间，执行以下操作通常很有用：\n\n在启用配置文件的情况下，获取查询响应。\n通过在 while(true) 循环中运行的查询，收集节点的热点线程输出。这有助于了解 CPU 时间的使用情况。\n使用这个用户友好版本的配置文件 API 来配置查询。\n\n如果查询来自 Kibana 可视化，则使用可视化侦查面板（Kibana 版本 6.3 及更高版本）或仪表板检查面板（Kibana 版本 6.4 及更高版本）来查看并导出实际的查询请求，并将其导入到配置文件 API 中以做进一步分析。\n捕获慢查询或耗费资源的查询有时，在像 Elasticsearch 这样的分布式应用程序中，当同时处理不同的请求 &#x2F; 线程时，很难捕获慢查询或耗费资源的查询。如果对运行耗费资源查询的用户不加以控制，情况就会变得愈加复杂，这些查询会降低集群性能（例如，较长的垃圾收集 (GC) 周期），甚至更糟糕地会出现内存不足 (OOM) 的情况。\n在 Elasticsearch version 7.0 中，我们引入了一种新的内容熔断策略，用于在保留内存时测量实际堆内存的使用情况。这个新策略可提高节点对耗费资源的查询导致集群过载的弹性支持，并且在默认情况下处于启用状态，并可使用新的集群设置 indices.breaker.total.use_real_memory 对其进行控制。\n但是，我们应该注意，这些都是尽力而为；对于以上内容未涉及的情况，最好在 OOM 崩溃后收集堆转储，或从运行的 JVM 中收集堆转储，以更好地了解根本原因。\nElasticsearch 还有另一个保护设置（最大存储桶软限制），用于防止集群出现 OOM。当超过存储桶数量（在 7.0 版中默认为 10,000）时（例如，当运行多层聚合时），此最大存储桶聚合设置将停止执行并使搜索请求失败。\n为了进一步识别潜在的耗费资源查询，我们可以设置断路器设置 (indices.breaker.request.limit)，从一个低阈值开始隔离查询，并逐渐向上移动阈值，以将范围缩小到特定的查询。\nslowlogs此外，还可以通过在 Elasticsearch 中启用 slowlogs 来识别运行缓慢的查询。slowlogs 专门用于分片级别，这意味着仅适用于数据节点。仅协调 &#x2F; 客户端节点不具备慢日志分析功能，因为它们不保存数据（索引 &#x2F; 分片）。\nslowlogs 有助于回答如下问题：\n\n查询花费了多长时间？\n查询请求正文的内容是什么？\n\nslowlog 输出示例：\n\\[2019\\-02\\-11T16:47:39,882\\]\\[TRACE\\]\\[index.search.slowlog.query\\]  \\[2g1yKIZ\\]  \\[logstash\\-20190211\\]\\[4\\] took\\[10.4s\\], took\\_millis\\[10459\\], total\\_hits\\[16160\\], types\\[\\], stats\\[\\], search\\_type\\[QUERY\\_THEN\\_FETCH\\], total\\_shards\\[10\\], source\\[&#123;&quot;size&quot;:0,&quot;query&quot;:&#123;&quot;bool&quot;:&#123;&quot;must&quot;:\\[&#123;&quot;range&quot;:&#123;&quot;timestamp&quot;:&#123;&quot;from&quot;:1549266459837,&quot;to&quot;:1549871259837,&quot;include\\_lower&quot;:true,  &quot;include\\_upper&quot;:true,&quot;format&quot;:&quot;epoch\\_millis&quot;,&quot;boost&quot;:1.0&#125;&#125;&#125;\\],&quot;adjust\\_pure\\_negative&quot;:true,&quot;boost&quot;:1.0&#125;&#125;,&quot;\\_source&quot;:&#123;&quot;includes&quot;:\\[\\],&quot;excludes&quot;:\\[\\]&#125;,&quot;stored\\_fields&quot;:&quot;\\*&quot;,&quot;docvalue\\_fields&quot;:  \\[&#123;&quot;field&quot;:&quot;timestamp&quot;,&quot;format&quot;:&quot;date\\_time&quot;&#125;,&#123;&quot;field&quot;:&quot;utc\\_time&quot;,&quot;format&quot;:&quot;date\\_time&quot;&#125;\\],&quot;script\\_fields&quot;:&#123;&quot;hour\\_of\\_day&quot;:&#123;&quot;script&quot;:&#123;&quot;source&quot;:&quot;doc\\[&#39;timestamp&#39;\\].value.getHourOfDay()&quot;,  &quot;lang&quot;:&quot;painless&quot;&#125;,&quot;ignore\\_failure&quot;:false&#125;&#125;,&quot;aggregations&quot;:&#123;&quot;maxAgg&quot;:&#123;&quot;max&quot;:&#123;&quot;field&quot;:&quot;bytes&quot;&#125;&#125;,&quot;minAgg&quot;:&#123;&quot;min&quot;:&#123;&quot;field&quot;:&quot;bytes&quot;&#125;&#125;&#125;&#125;\\], id\\[\\]\\],\n\nslowlog 消息拆解：\n| 拆分项 | 说明 || [2019-02-11T16:47:39,882] | 查询日期 || [TRACE] | 日志级别 || [index.search.slowlog.query] | 搜索 slowlog 的查询阶段 || [2g1yKIZ] | 节点名称 || [logstash-20190211] | 索引名称 || [4] | 查询执行的分片序号 || took[10.4s] | 在分片 [4] 上花费的处理时间。注意：在查看 slowlog 时，我们需要避免将来自不同分片的所有时间相加，因为每个分片可能并行执行。 || took_millis[10459] | 花费的时间（毫秒） || total_hits[16160] | 总命中数 || search_type[QUERY_THEN_FETCH] | 搜索类型 || total_shards[10] | 索引的总分片数 || source[] | 执行的查询正文 |\n审计日志黄金级或白金级订阅的客户（包括 Elastic 安全功能），可以启用审计日志来捕获有关查询的更多详细信息。（用户可以开始为期 30 天的试用来测试 Elastic 安全功能。） 审计日志有助于回答以下问题：\n\n查询是何时发生的？\n谁执行了查询？\n查询的内容是什么？\n\n由于默认的审计设置相当繁琐，我们需要调整设置：\n\n启用安全审计日志：  在 elasticsearch.yml 中设置 xpack.security.audit.enabled: true。\n在安全审计输出中启用日志或索引：  在 elasticsearch.yml 中设置 xpack.security.audit.outputs:[logfile, index]。备注： \nxpack.security.audit.outputs 设置仅适用于版本 6.0-6.2 和 5.x。版本 7.0 不再接受此设置，并且当 xpack.security.audit.enabled 设置为 true 时，默认为 json 输出 (_audit.json)。\n出于排除故障的目的，我们建议选择 logfile 而不是索引，因为审计日志的冗长可能会对安全索引超出其预期大小的集群性能造成不必要的压力。\n审计模式可能非常冗长，因此，请在完成故障排除后将其关闭。\n\n\n在事件列表中包含 authentication_success 访问：  在 elasticsearch.yml 中设置 xpack.security.audit.logfile.events.include: authentication_success。备注： \n默认事件中不包括此设置。此设置会覆盖默认设置。\n如果需要再添加一个事件（而不是替换），请先写入现有的默认事件列表，然后在最后一个条目后添加上述设置。\n在审计事件中输出请求正文：  在 elasticsearch.yml 中设置 xpack.security.audit.logfile.events.emit_request_body: true。\n\n\n\n借助这些设置，您就可以像下面这样监测用户查询。\n\n_用户：_louisong\n查询时间： 2019-05-01T19:26:53,554 (UTC)\n查询端点：_msearch（通常表示它是从 Kibana 可视化 &#x2F; 仪表板发出的查询）\n_查询主体：_从以下日志中的 &quot;request.body&quot;: 开始：```{“@timestamp”:”2019-05-01T19:26:53,554”,  “node.id”:”Z1z_64sIRcy4dW2eqyqzMw”,”event.type”:”rest”,”event.action”:”authentication_success”,”user.name”:”louisong”,”origin.type”:”rest”,”origin.address”:”127.0.0.1:51426”,”realm”:”default_native”,”url.path”:”&#x2F;_msearch”,”url.query”:”rest_total_hits_as_int&#x3D;true&amp;ignore_throttled&#x3D;true”,”request.method”:”POST”,”request.body”:”{\\“index\\“:\\“*\\“,\\“ignore_unavailable\\“:true,\\“preference\\“:1556709820160}\\n{\\“aggs\\“:{\\“2\\“:{\\“terms\\“:{\\“field\\“:\\“actions\\“,\\“size\\“:5,\\“order\\“:{\\“_count\\“:\\“desc\\“},\\“missing\\“:\\“__missing__\\“}}},\\“size\\“:0,\\“_source\\“:{\\“excludes\\“:[]},\\“stored_fields\\“:[\\“*\\“],\\“script_fields\\“:{},\\“docvalue_fields\\“:[{\\“field\\“:\\“access_token.user_token.expiration_time\\“,\\“format\\“:\\“date_time\\“},{\\“field\\“:\\“canvas-workpad.@created\\“,\\“format\\“:\\“date_time\\“},{\\“field\\“:\\“canvas-workpad.@timestamp\\“,\\“format\\“:\\“date_time\\“},{\\“field\\“:\\“creation_time\\“,\\“format\\“:\\“date_time\\“},{\\“field\\“:\\“expiration_time\\“,\\“format\\“:\\“date_time\\“},{\\“field\\“:\\“maps-telemetry.timeCaptured\\“,\\“format\\“:\\“date_time\\“},{\\“field\\“:\\“task.runAt\\“,\\“format\\“:\\“date_time\\“},{\\“field\\“:\\“task.scheduledAt\\“,\\“format\\“:\\“date_time\\“},{\\“field\\“:\\“updated_at\\“,\\“format\\“:\\“date_time\\“},{\\“field\\“:\\“url.accessDate\\“,\\“format\\“:\\“date_time\\“},{\\“field\\“:\\“url.createDate\\“,\\“format\\“:\\“date_time\\“}],\\“query\\“:{\\“bool\\“:{\\“must\\“:[{\\“range\\“:{\\“canvas-workpad.@timestamp\\“:{\\“format\\“:\\“strict_date_optional_time\\“,\\“gte\\“:\\“2019-05-01T11:11:53.498Z\\“,\\“lte\\“:\\“2019-05-01T11:26:53.498Z\\“}}}],\\“filter\\“:[{\\“match_all\\“:{}},{\\“match_all\\“:{}}],\\“should\\“:[],\\“must_not\\“:[]}},\\“timeout\\“:\\“30000ms\\“}\\n”,”request.id”:”qrktsPxyST2nVh29GG7tog”}  1\n\n结论在本文中，我们讨论了慢查询的常见原因以及相应的解决方案。我们还讨论了识别持续性慢查询和偶发慢查询的不同方法。通常会将慢查询视为需要解决的更广泛集群性能问题的典型症状。 https://www.elastic.co/cn/blog/advanced-tuning-finding-and-fixing-slow-elasticsearch-queries\n","slug":"MIDDLEWARE/Advanced tuning finding and fixing slow Elasticsearch queries","date":"2022-03-29T10:28:38.000Z","categories_index":"elastic,MIDDLEWARE","tags_index":"https,www,elasticsearch","author_index":"dandeliono"},{"id":"0ba37cbc11858729ececbe4090fe5335","title":"Homeassistant - 使用小爱音箱控制 Hass 设备","content":"Homeassistant - 使用小爱音箱控制 Hass 设备使用小爱音箱 + 小爱开放平台技能开发 + NodeRed 控制 Homeassistant 的设备。\n很多内容基于网上大神的基础总结而来。\n小爱开放平台配置参考视频：https://www.bilibili.com/video/BV1Xy4y1i79c\n注意的是，\n\n修改 调用名称, 语料后可能会有些延迟才生效？最好把一个一个的下一步都点一遍。\n需要提供 https 的调用网址，参考：https://www.bilibili.com/video/BV19h411f7dp 以及之前的文章有说配置 https （实际就是起一个 https 服务，可以非 443 端口，可以 Nginx 代理，也可以程序自己启动 https 服务。用端口映射过去就能访问）。\n官网网页提供调试。调试实际就是通过调用名称激活 &#x2F; 启动自定义命令，然后自定义命令只要命中语料，就会调用自己提供的 https 接口服务。大佬提供的 conversation 只是帮我们启动了一个服务并内置了一些命令解析。\n\n小爱自定义技能接入到 NodeRed\nNodeRed - 节点管理 - 安装 node-red-contrib-home-assistant-websocket - node-red-contrib-xiaoai-tts （已安装直接下一步）\n添加节点 - events:state 监听状态改变 - 设置属性：Entity ID 为 conversation.voice （语音助手的实体 ID）\n添加节点 - switch - 通过 payload 自定义后续事件\n为了防止这种情况： state change 后，我们的智能设备实际没有变化，我们又发请求 change 同样的 state，此时 state 没有变化就监听不到。\n所以在监听状态改变后，增加节点 - 延迟 2s - call API - 设置属性：HTTP POST，path:&#x2F;api&#x2F;states&#x2F;conversation.voice, data: {“state”:”waiting”}\n也就是每次状态改变后我们把状态重置一下，下一次下命令就肯定会 state change 了。\n\n\n\nNodeRed\n","slug":"OTHER/Homeassistant - 使用小爱音箱控制 Hass 设备","date":"2022-03-28T20:30:27.000Z","categories_index":"https,OTHER","tags_index":"com,NodeRed,state","author_index":"dandeliono"},{"id":"8d8408765c8cd5cad402c3f0cebc116d","title":"利用云同步盘实现多台电脑部分配置同步问题","content":"利用云同步盘实现多台电脑部分配置同步问题我们经常会在公司，个人甚至更多环境不同的电脑进行切换，当然，也存在重装系统、换电脑等问题。而在每次更换之后，都会进行一次环境配置。或者在某台电脑上修改了部分配置而其他电脑又需要再次配置一次。\n尤其是类似于 ssh 密钥等配置，每次不光是重新生成一份就行了，还需要去各平台新增，及其麻烦。\n当然，使用云盘拷贝备份的方式可行，但是每次修改都需要再次备份，会存在遗忘的情况，有办法让配置自动同步么？\n答案是有的！\n我们可以利用云同步盘（如 OneDrive）让配置文件自动同步，同时利用文件链接如 window 的 mklink 创建文件链接。实际文件存放在云同步盘中，而系统配置中的文件使用链接的方式。\n这样不管是修改哪个文件，源文件都会被修改然后被云同步盘自动同步。\n例 ssh 下的 config 文件创建文件链接（windows）：\n12mklink config %USERPROFILE%/.ssh/config\n\n这样我们不管是修改云盘中的文件还是 C 盘下的文件，源文件都会被修改同步。\n但是这样我们需要在每台电脑上都进行一次文件链接创建。\n所以这里提供一个脚本直接进行链接创建，想来看看目录结构以及说明：\n每个目录对应系统中某个文件夹，具体对应的文件夹根目录为当前文件中的 .localpath 的内容。映射时会把当前目录下的所有文件分别链接到 .localpath 所指定的目录并且一一对应。\n-- settings/\n  -- userprofile/ # C 盘目录下 user/用户名 下面配置\n    -- .ssh/\n      -- config\n      -- id_rsa\n      -- id_rsa.pub\n    -- .gitconfig\n    -- .npmrc\n    -- .yarnrc\n    -- .localpath # 文件内容：%USERPROFILE%，即系统盘中的&quot;user/用户名&quot;目录\n  -- others/ # 其他配置\n    -- .localpath # others 文件所对应的系统目录位置\n  -- init.bat # 创建软链\n复制代码\n\n这样在执行 settings&#x2F;init.bat 就会吧 userprofile 文件夹下面的所有文件分别映射到 %USERPROFILE% 对应的文件下，即创建 .npmrc、.yarnrc、.gitconfig、以及 .ssh 下的所有配置进行映射同步。\n这样无论 修改 哪里的文件，同步盘中的内容都会跟着更新并同步。同时另外一台有做链接映射的电脑打开的时候同步盘自动更新后，系统配置文件也就自动更新了！\n而在重装系统、新增电脑的机器上把同步盘装上同步下来后执行 init.bat 就 OK 了。\n当然，如果是跨平台的操作系统，可以提供更多的配置，如 init.sh 等。\n这里附以下 init.bat 文件：\n123456789101112131415161718192021222324252627@echo offsetlocal enabledelayedexpansion @echo off :: 同步设置的根路径set SyncSettingsPath=%OneDrive%\\settings:: .localpath的文件名set LocalPathName=.localpath:: 遍历当前目录所有文件夹for /f %%n in (&#x27;dir %SyncSettingsPath% /b /a:d&#x27;) do (  if exist %SyncSettingsPath%\\%%n\\%LocalPathName% (    :: 读取.localpath    for /f %%i in (&#x27;type %SyncSettingsPath%\\%%n\\%LocalPathName%&#x27;) do set LocalPathContent=%%i    :: 转换.localpath内容中的变量    for /f %%i in (&#x27;echo !LocalPathContent!&#x27;) do (      :: 递归遍历当前文件夹下非.localpath的文件      for /f %%f in (&#x27;dir %SyncSettingsPath%\\%%n /s /b /a:-d ^| find /v &quot;%LocalPathName%&quot;&#x27;) do (        set CurrentSyncFilePath=%%f        set CurrentLocalFilePath=!CurrentSyncFilePath:%SyncSettingsPath%\\%%n=%%i!        if exist !CurrentLocalFilePath! del !CurrentLocalFilePath!        mklink !CurrentLocalFilePath! !CurrentSyncFilePath!      )    )  ))\n","slug":"WORK/利用云同步盘实现多台电脑部分配置同步问题","date":"2022-03-24T23:43:47.000Z","categories_index":"init,WORK","tags_index":"当然,ssh,bat","author_index":"dandeliono"},{"id":"83fcc4ed8f1c6107bdf89c34a8b59a09","title":"Nginx 服务器应用详解","content":"Nginx 服务器应用详解一、前言Nginx作为一个异步框架的 Web服务器，也可以用作反向代理，负载平衡器 和 HTTP缓存，下面本篇文章就来介绍一下Nginx反向代理、SSL及域名配置。\n基于公司中标某公司项目，作为项目产品组一员，前期负责服务器环境搭建部分。技术架构如下：\n\n其中，主要实现点如下：\n\n\n\n\n\n\n\n\n\n\n基于外网 APP 访问内网 Web 服务需求，应用nginx反向代理解决。\n基于内网 Web 服务访问外网 Internet 服务需求，应用nginx正向代理解决。\n随着访问并发量的增加，反向代理服务器会逐渐成为整个系统架构中的瓶颈点，容易出现单点故障，故考虑做方向代理服务器集群。\n同时考虑到网络安全，对报文进行SSL传输加密处理。\n外网 APP 通过域名方式访问反向代理服务器，故需要申请外网域名。\n\n二、环境搭建2.1 Nginx 安装上传Nginx安装包到服务器\ntar -zxvf nginx1.16.1.tar.gz\n\n复制代码\n2.2 Nginx 配置\n将nginx_config.zip解压。\n  unzip nginx_config.zip -d nginx_config&#x2F;\n\n\n复制代码\n\n将解压后的nginx.conf和conf.d/default.conf替换至服务器/etc/nginx/中的相应文件。\n\n将 &#x2F; dtap 文件夹中的aimonitor-front的前端文件夹压缩，并复制至 nginx 安装目录（/usr/share/nginx/html/），改名为AImonitor  \n  tar -cz aimonitor-front -f aimonitor.tar.gz\n\n\n复制代码\n\n修改nginx.conf和conf.d/default.conf中的 IP 信息。将 22.188.15.205 改为服务器地址。\n启动nginx\n\n如果启动成功，效果如下\n\n  systemctl restart nginx（重启）\n\n复制代码\n当修改了配置文件使用：\n建议不要停止再重启，以防报错。\n二、反向代理配置2.1 什么是反向代理?反向代理和正向代理的区别就是：正向代理代理客户端，反向代理代理服务器。 \n反向代理（Reverse Proxy），其实客户端对代理是无感知的，因为客户端不需要任何配置就可以访问，我们只需要将请求发送到反向代理服务器，由反向代理服务器去选择目标服务器获取数据后，再返回给客户端，此时反向代理服务器和目标服务器对外就是一个服务器，暴露的是代理服务器地址，隐藏了真实服务器 IP 地址。\n2.2 反向代理的好处\n\n\n\n\n\n\n\n\n\n\n保护了真实的 web 服务器，web 服务器对外不可见，外网只能看到反向代理服务器，而反向代理服务器上并没有真实数据，因此，保证了 web 服务器的资源安全。\n反向代理为基础产生了动静资源分离以及负载均衡的方式，减轻 web 服务器的负担，加速了对网站访问速度。\n节约了有限的 IP 地址资源，企业内所有的网站共享一个在 Internet 中注册的 IP 地址，这些服务器分配私有地址，采用虚拟主机的方式对外提供服务；\n\n\n2.3 http 反向代理配置信息nginx反向代理http配置步骤如下：\n\nvi /etc/nginx/nginx.conf 新增如下配置信息：\nserver {\n\n\n复制代码\n\nnginx -s reload重新加载配置。\n\n三、反向代理集群搭建 (待补充)四、正向代理配置4.1 什么是正向代理?前面提到过，正向代理代理客户端，反向代理代理服务器。\n理解正向代理、反向代理这两种代理的关键在于代理服务器所代理的对象是什么，正向代理代理的是客户端，我们需要在客户端进行一些代理的设置。而反向代理代理的是服务器，作为客户端的我们是无法感知到服务器的真实存在的。\n\n4.2 http 正向代理配置信息正向代理：如果把局域网外的Internet想象成一个巨大的资源库，则局域网中的客户端要访问Internet，则需要通过代理服务器来访问，这种代理服务就称为正向代理。\nNginx配置如下：\nserver &#123;\n\n复制代码\nNginx 正向代理配置说明：\n\n配置 DNS 解析 IP 地址，比如 Google Public DNS，以及超时时间（5 秒）。\n  resolver 8.8.8.8;\n\n\n复制代码\n\n配置正向代理参数，均是由 Nginx 变量组成。其中 proxy_set_header 部分的配置，是为了解决如果 URL 中带 “.“（点）后 Nginx 503 错误。\n  proxy_pass $scheme:\n\n\n复制代码\n\n配置缓存大小，关闭磁盘缓存读写减少I/O，以及代理连接超时时间。\n  proxy_buffers 256 4k;\n\n\n复制代码\n\n配置代理服务器 Http 状态缓存时间。\n  proxy_cache_valid 200 302 10m;\n\n\n复制代码\n注意⚠️：nginx 正向代理不支持代理 Https 网站。\n因为 Nginx 不支持 CONNECT，所以无法正向代理 Https 网站。如果访问 Https 网站，比如：https://www.google.com，Nginx access.log 日志如下：\n1\n\n复制代码\n配置完nginx之后， nginx -s reload重新加载配置。\n如果出现80端口被占用，Lsof -i :80 查看 80 端口的pid进程号，然后用Kill -9 + 进程号，删除进程，再重启服务。\n查看dns方法\n4.3 正向代理使用在需要访问外网的机器上（Linux 系统）执行以下操作之一即可：\ngedit ~/.bashrc  \n\n复制代码\n其中，yourproxyaddress 也就是你的Nginx服务器的 ip 了，proxyport就是上面配置中的 80，可以根据自己的需要修改。\n4.4 Nginx 支持正向代理 httpsnginx本身是不支持https协议请求转发，为了让nginx能达到这一效果需要借助第三方模块ngx_http_proxy_connect_module。首先下载这一模块到服务器，然后准备nginx环境。\n1\n\n复制代码\n编译安装成功后，配置nginx正向代理：\n1\n\n复制代码\n当配置文件配置好之后保存即可，重启nginx，进行测试：\nnginx -s reload重新加载配置。\n去内网服务器里curl，可以在环境变量里添加代理：\n...\n\n复制代码\n\n另一种方式不用加环境变量，临时代理\n#curl -i  --proxy 代理IP：端口      要访问域名\n\n复制代码\n4.4 正向、反向代理对比通过四、五章节的讲解，正向代理代理客户端，反向代理代理服务器。可以看到反向代理服务器对客户端透明，客户端无需任何配置，只需访问反向代理服务器 IP，即可实现访问内网服务。Nginx 反向代理服务器只需配置相应的代理规则即可。\n正向代理代理客户端，需要在作为 Nginx 正向代理服务器中配置相应的代理规则，配置方式与反向代理基本一致，同一服务器既作为反向代理服务器、又作为正向代理服务器，是由监听端口转发。Nginx 作为正向代理服务器无法代理 Https，若需支持的话，需要额外安装第三方模块ngx_http_proxy_connect_module。\n五、SSL 配置5.1 敲黑板随着互联网的快速发展，给我们的生活带来便利的同时，也伴随着网络钓鱼、信息泄露、网络诈骗等网络安全事件的频繁发生，企业网站被钓鱼网站仿冒，遭受经济损失，影响品牌形象。\n如果网站不使用SSL证书，数据以HTTP明文传输，极容易被第三方监听、截取和篡改，而导致用户信息被泄露，给在线用户带来直接损失。通过部署SSL证书后能确保信息传输的安全性，可防止信息泄露。\nSSL(Secure Sockets Layer安全套接字协议), 及其继任者传输层安全（Transport Layer Security，TLS）是为网络通信提供安全及数据完整性的一种安全协议。TLS与SSL在传输层与应用层之间对网络连接进行加密。\nSSL证书是数字证书 (数字证书包括：SSL 证书、客户端证书、代码签名证书等) 的一种，因为配置在服务器上也称为服务器 SSL 证书。SSL 证书就是遵守SSL协议，由受信任的数字证书颁发机构CA(如：沃通 CA) 在验证服务器身份后颁发的一种数字证书，它使用ssl协议在浏览器和web server之间建立一条安全通道，数据信息在client与server之间的安全传输.。\n**SSL 协议 **的特点： \n\n\n\n\n\n\n\n\n\n\nSSL 协议可用于保护正常运行于TCP之上的任何应用协议，如HTTP、FTP、SMTP或Telnet的通信，最常见的是用SSL来保护HTTP的通信。\nSSL 协议的优点在于它是与应用层协议无关的。高层的应用协议（如HTTP、FTP、Telnet等）能透明地建立于SSL 协议之上。\nSSL 协议在应用层协议之前就已经完成加密算法、通信密钥的协商以及服务器的认证工作。在此之后应用层协议所传送的数据都会被加密，从而保证通信的安全性。\n\n**SSL**的功能 ： \n\n\n\n\n\n\n\n\n\n\n客户对服务器的身份认证。SSL服务器允许客户的浏览器使用标准的公钥加密技术和一些可靠的认证中心（CA）的证书，来确认服务器的合法性。\n服务器对客户的身份认证。也可通过公钥技术和证书进行认证，也可通过用户名，password 来认证。\n建立服务器与客户之间安全的数据通道。SSL要求客户与服务器之间所有发送的数据都被发送端加密、接收端解密，同时还检查数据的完整性。\n\n使用**SSL 证书 **的优势： \n\n\n\n\n\n\n\n\n\n\nSSL 证书用于实现数据加密传输，防止数据被泄露和篡改；\n用于认证服务器身份，防范被钓鱼网站攻击；\n用于消除浏览器地址栏发出的 “不安全” 警告；\n提高SEO搜索引擎排名；\n提升用户对网站的信任；\n有助于提高网站的在线销售业绩；\n\n配置SSL模块首先需要CA证书，CA证书可以自己手动颁发也可以在阿里云申请。\n默认情况下ssl模块并未被安装，如果要使用该模块则需要在编译nginx时指定–with-http_ssl_module参数。\n5.2 CA 证书生成本小节主要讲述如何通过证书签名生成 CA 证书。\n首先确认nginx服务器已经安装openssl和nginx已经编译ssl的模块。\n\n生成秘钥和 CA 证书步骤：\n\n\n\n\n\n\n\n\n\n\n步骤 1：生成 key 秘钥\n步骤 2：生成证书签名请求文件（csr 文件）\n步骤 3：生成证书签名文件（CA 文件）\n\n5.2.1 生成 key 秘钥首先，创建一个用来存放秘钥的文件夹。\n\n输入加密算法：openssl genrsa -idea -out jesonc.key 1024  \n回车，会让输入密码，这里设置为 123456，完成后会生成一个.key的文件\n\n5.2.2 生成证书签名请求文件（csr 文件）openssl req -new -key jesonc.key -out jesonc.csr\n\n复制代码\n\n\n5.2.3 生成证书签名文件（CA 文件）打包：openssl x509 -req -days 3650 -in jesonc.csr -signkey jesonc.key -out jesonc.crt  \n\n5.3 https 反向代理配置信息https 默认采用 SHA-1 算法，非常脆弱。我们可以使用迪菲 - 赫尔曼密钥交换。\n我们在 /conf/ssl 目录下生成 dhparam.pem 文件\nopenssl dhparam -out dhparam.pem 2048\n\n复制代码\n下面的配置 ssl_protocols 和 ssl_ciphers 是用来限制连接只包含 SSL/TLS 的加強版本和算法。\n进入/etc/nginx/conf.d/  \nserver\n\n复制代码\n如果要实现http强制跳转到https, 可以添加下面配置段:\nserver &#123;\n\n复制代码\n\n\n\n\n\n\n\n\n\n注意⚠️：网上也有许多使用 rewrite 来重定向，但是 return 指令简单高效，建议尽量使用 return。\n配置好之后，nginx -s reload重新加载配置。\n确认已启用 443 的监听：netstat -luntp|grep 443  \n\n上面server部分第 2-5 项是关键。这些配置放在 server 块就可以对其中的所有 location 生效了，并且同时支持 http 和 https 。或者把 http 和 https 分开配置也很常见。\n5.3.1 合并证书配置文件和Apache配置不同，Nginx需要将服务器证书和 ca 证书链合并到一个文件中，作为 ssl_certificate 配置内容。例如，按照证书链从下向上的顺序，我有三个证书：\n\nssl.crt（自己域名的服务器证书）\nsub.class1.server.ca.pem（startssl 的一类证书）\nca.pem（startssl 的根证书）把它们的内容按顺序连接到的一个文件中，每个内容另起一行，中间没有空行或空格。\n\n5.3.2 避免启动时输入密码配好之后，启动nginx 会要你输入密钥的密码。这是因为 ssl_certificate_key 配置对应的文件（也就是 startssl 给你的私钥文件）内容是加密的，需要输入你创建这个时设置的密码才能解密。这样私钥虽然很安全，但是每次重启服务都要输入一次密码也太麻烦了。其实，只要证书改为解密了的内容，就可以避免每次输入密码。用如下命令即可：openssl rsa -in ssl.key -out newssl.key 输入密码，就生成了解密后的私钥内容，使用这个就 OK 了。\n但是就像前面说的，一定要在服务器上保护好它，例如：\nchmod 400 ssl.key （仅root可读）\n\n复制代码\n5.3.3 优化 SSL 配置SSL 很消耗 CPU 资源，尤其是在建立连接的握手阶段。一是通过开启 keepalive 可以重用连接。二是可以重用和共享ssl session，见上面ssl_session相关配置。\n会话存储在工作人员之间共享的 SSL 会话缓存中，并由 ssl_session_cache 参数配置。1 兆字节的缓存包含大约 4000 个会话。默认缓存超时为 5 分钟。它可以通过使用增加 ssl_session_timeout 指令。\n六、域名申请（待补充）七、拓展阅读\n\n\n\n\n\n\n\n\n\nnginx -s stop快速停止\nnginx -s quit完整停止\nnginx -s reload重新加载配置\nps -ef | grep nginx查看进程 id\nkill [option] 进程 id option: (1) -9 强制杀死; (2)TERM 或 INT 快速停止服务; (3)QUIT 平缓停止\nnginx -v查看版本\nnginx -t检查配置文件关闭：nginx -s stop -c /etc/nginx/nginx.conf启动：nginx -c /etc/nginx/nginx.confhttps://xie.infoq.cn/article/0a047242abb928ca99cf6bf32\n\n","slug":"MIDDLEWARE/Nginx 服务器应用详解","date":"2022-02-26T11:17:14.000Z","categories_index":"nginx,MIDDLEWARE","tags_index":"https,SSL,复制代码","author_index":"dandeliono"},{"id":"f4563b96f26978111a4862bba321db8f","title":"深入理解“软中断”","content":"深入理解“软中断”前言\n软中断（softirq）导致 CPU 使用率升高也是最常见的一种性能问题\n所以软中断这个硬骨头必须啃下去！\n\n回忆下什么是中断\n中断是系统用来响应硬件设备请求的一种机制\n它会打断进程的正常调度和执行\n然后调用内核中的中断处理程序来响应硬件设备的请求\n\n场景类比，加深印象比如说你订了一份外卖，但是不确定外卖什么时候送到，也没有别的方法了解外卖的进度， 但是，配送员送外卖是不等人的，到了你这儿没人取的话，就直接走人了；所以你只能苦苦等着，时不时去门口看看外卖送到没，而不能干其他事情；不过呢，如果在订外卖的时候，你就跟配送员约定好，让他送到后给你打个电话，那你就不用苦苦等待了，就可以去忙别的事情，直到电话一响，接电话、取外卖就可以了、\n\n打电话： 其实就是一个中断，没接到电话的时候，你可以做其他的事情\n只有接到了电话（也就是发生中断），你才要进行另一个动作：取外卖\n\n中断的优势一种异步的事件处理机制，可以提高系统的并发处理能力\n中断运行时间短\n由于中断处理程序会打断其他进程的运行，为了减少对正常进程运行调度的影响，中断处理程序就需要尽可能快地运行\n如果中断要处理的事情很多，中断服务程序就有可能要运行很长时间\n\n中断处理程序在响应中断会临时关闭中断。这就会导致上一次中断处理完成之前，其他中断都不能响应，也就是说中断有可能会丢失\n响应中断场景类比假如你订了 2 份外卖，一份主食和一份饮料，并且是由 2 个不同的配送员来配送。这次你不用时时等待着，两份外卖都约定了电话取外卖的方式。但是，问题又来了，当第一份外卖送到时，配送员给你打了个长长的电话，商量发票的处理方式。与此同时，第 二个配送员也到了，也想给你打电话。 但是很明显，因为电话占线（也就是关闭了中断响应），第二个配送员的电话是打不通的。 所以，第二个配送员很可能试几次后就走掉了（也就是丢失了一次中断）\n软中断中断处理过程分割\n为了解决中断处理程序执行过长和中断丢失的问题，Linux 会将中断处理过程分成两个阶段，也就是上半部和下半部\n上半部： 快速处理中断，它在中断禁止模式下运行，主要处理跟硬件紧密相关的或时间敏感的工作\n下半部： 延迟处理上半部未完成的工作，通常以内核线程的方式运行\n\n承上启下\n上面说到的响应中断场景\n上半部就是你接听电话，告诉配送员你已经知道了，其他事儿见面再说，然后电话就可以挂断了\n下半部才是取外卖的动作，以及见面后商量发票处理的动 作。\n\n网卡接收数据包的栗子网卡接收到数据包后，会通过硬件中断的方式，通知内核有新的数据到了。这时，内核就应该调用中断处理程序来响应它\n上半部\n快速处理\n首先，要把网卡的数据读到内存中\n然后，更新一下硬件寄存器的状态（表示数据已经读好了）\n最后，再发送一个软中断信号，通知下半部做进一步的处理\n\n下半部\n被软中断信号唤醒\n需要从内存中找到网络数据，再按照网络协议栈，对数据进行逐层解析和处理，直到把它送给应用程序\n\n总结上半部\n直接处理硬件请求，也就是硬中断\n特点： 快速执行\n会打断 CPU 正在执行的任务，然后立即执行中断处理程序\n\n下半部\n由内核触发，也就是软中断\n特点： 延迟执行\n以内核线程的方式执行，并且每个 CPU 都对应一个软中断内核线程，名字为 “ksoftirqd&#x2F;CPU 编号”，比如说， 0 号 CPU 对应的软中断内核线程的名字就是 ksoftirqd&#x2F;0\n不只包括了硬件设备中断处理程序的下半部，一些内核自定义的事件也属于软中断，网络收发、定时、调度、RCU 锁等各种类型\n内核调度和 RCU 锁（Read-Copy Update）， RCU 是 Linux 内核中最常用的锁之一\n\n查看软中断和内核线程proc 文件系统它是一种内核空间和用户空间进行通信的机制，可以用来查看内核的数据结构，或者用来动态修改内核的配置\n\n &#x2F;proc&#x2F;softirqs ：提供了软中断的运行情况\n &#x2F;proc&#x2F;interrupts ：提供了硬中断的运行情况\n\n查看软中断文件内容$ cat /proc/softirqs CPU0 CPU1 HI: 0 0 TIMER: 811613 1972736 NET\\_TX: 49 7 NET\\_RX: 1136736 1506885 BLOCK: 0 0 IRQ\\_POLL: 0 0 TASKLET: 304787 3691 SCHED: 689718 1897539 HRTIMER: 0 0 RCU: 1330771 1354737\n\n注意软中断的类型\n从第一列可以看出，软中断包括了 10 个类别\n比如： NET_RX 表示网络接收中断，而 NET_TX 表示网络发送中断\n\n注意同一种软中断在不同 CPU 上的分布情况\n也就是同一行的内容\n正常情况 下，同一种中断在不同 CPU 上的累积次数应该差不多\n比如： 上面的，NET_RX 在 CPU0 和 CPU1 上的中断次数基本是同一个数量级，相差不大\n\nTASKLET\nTASKLET 在不同 CPU 上的分布并不均匀\nTASKLET 是最常用的软中断实现机制，每个 TASKLET 只运行一次就会结束 ，并且只在调用它的函数所在的 CPU 上运行\n存在的问题： 由于只在一个 CPU 上运行导致的调度不均衡，再比如因为不能在多个 CPU 上并行运行带来了性能限制\n\n查看软中断线程\n\n\n注意，这些线程的名字外面都有中括号，这说明 ps 无法获取它们的命令行参数 （cmline）\n一般来说，ps 的输出中，名字括在中括号里的，一般都是内核线程https://www.cnblogs.com/poloyy/p/13435519.html\n\n","slug":"LINUX/深入理解“软中断”","date":"2022-02-02T19:27:00.000Z","categories_index":"CPU,LINUX","tags_index":"软中断,也就是,上半部","author_index":"dandeliono"},{"id":"f8276dfd473ae590df0c6e191d789800","title":"软中断导致 CPU 使用率过高的案例","content":"软中断导致 CPU 使用率过高的案例前言软中断基本原理，可参考这篇博客：https://www.cnblogs.com/poloyy/p/13435519.html\n中断\n一种异步的事件处理机制，用来提供系统的并发处理能力\n当中断事件发生，会触发执行中断处理程序\n中断处理程序分为上半部和下半部\n上半部： 硬中断，快速处理中断\n下半部： 软中断，用来异步处理上半部未完成的工作\n\n软中断\n每个 CPU 都对应一个软中断内核线程，名字是 ksoftirqd&#x2F;CPU 编号\n当软中断事件的频率过高时，内核线程也会因为 CPU 使用率过高而导致软中断处理不及时，进而引发网络收发延迟，调度缓慢等性能问题\n\n软中断频率过高案例系统配置Ubuntu 18.04， 2 CPU，2GB 内存，共两台虚拟机\n三个工具\nsar：是一个系统活动报告工具，既可以实时查看系统的当前活动，又可以配置保存和报告 历史统计数据。\nhping3：是一个可以构造 TCP&#x2F;IP 协议数据包的工具，可以对系统进行安全审计、防火墙 测试等。\ntcpdump：是一个常用的网络抓包工具，常用来分析各种网络问题\n\n虚拟机关系\n通过 docker 运行案例在 VM1 中执行命令\ndocker run \\-itd \\--name\\=nginx \\-p 80:80 nginx\n\n通过 curl 确认 Nginx 正常启动在 VM2 中执行命令\n12curl http://172.20.72.58/\n\n通过 hping3 模拟 Nginx 的客户端请求在 VM2 中执行命令\nhping3 \\-S \\-p 80 \\-i u100 172.20.72.58\n\n\n-S： 参数表示设置 TCP 协议的 SYN（同步序列号）\n-p： 表示目的端口为 80\n-i： u100 表示每隔 100 微秒发送一个网络帧\n\n回到 VM1感觉系统响应明显变慢了，即便只 是在终端中敲几个回车，都得很久才能得到响应\n分析系统为什么会响应变慢以下命令均在 VM1 中执行\n通过 top 命令查看系统资源使用情况\n\n系统 CPU 使用率（用户态 us 和内核态 sy ）并不高\n平均负载适中，只有 2 个 R 状态的进程，无僵尸进程\n但是软中断进程 1 号（ksoftirqd&#x2F;1）的 CPU 使用率偏高，而且处理软中断的 CPU 占比已达到 94\n此外，并无其他异常进程\n可以猜测，软中断就是罪魁祸首\n\n确认是什么类型的软中断观察 &#x2F;proc&#x2F;softirqs 文件的内容，就能知道各种软中断类型的次数\n这里的各类软中断次数，又是什么时间段里的次数呢？\n它是系统运行以来的累积中断次数\n所以直接查看文件内容，得到的只是累积中断次数，对这里的问题并没有直接参考意义\n中断次数的变化速率才是我们需要关注的\n\n通过 watch 动态查看命令输出结果因为我的机器是两核，如果直接读取 &#x2F;proc&#x2F;softirqs 会打印 128 核的信息，但对于我来说，只要看前面两核的信息足以，所以需要写提取关键数据\nwatch \\-d &quot;/bin/cat /proc/softirqs | /usr/bin/awk &#39;NR == 1&#123;printf \\\\&quot;%-15s %-15s %-15s\\\\n\\\\&quot;,\\\\&quot; \\\\&quot;,\\\\$1,\\\\$2&#125;; NR &gt; 1&#123;printf \\\\&quot;%-15s %-15s %-15s\\\\n\\\\&quot;,\\\\$1,\\\\$2,\\\\$3&#125;&#39;&quot;\n\n\n结果分析\nTIMER（定时中断）、 NET_RX（网络接收）、SCHED（内核调度）、RCU（RCU 锁）等这几个软中断都在不停变化\n而 NET_RX，就是网络数据包接收软中断的变化速率最快\n其他几种类型的软中断，是保证 Linux 调度、时钟、临界区保护这些正常工作所必需的，所以有变化时正常的\n\n通过 sar 查看系统的网络收发情况上面确认了从网络接收的软中断入手，所以第一步应该要看下系统的网络接收情况\nsar 的好处\n不仅可以观察网络收发的吞吐量（BPS，每秒收发的字节数）\n还可以观察网络收发的 PPS（每秒收发的网络帧数）\n\n执行 sar 命令sar \\-n DEV 1\n\n\n\n第二列：IFACE 表示网卡\n第三、四列：rxpck&#x2F;s 和 txpck&#x2F;s 分别表示每秒接收、发送的网络帧数【PPS】 \n第五、六列：rxkB&#x2F;s 和 txkB&#x2F;s 分别表示每秒接收、发送的千字节数【BPS】\n\n结果分析对网卡 ens33 来说\n\n每秒接收的网络帧数比较大，几乎达到 8w，而发送的网络帧数较小，只有接近 4w\n每秒接收的千字节数只有 4611 KB，发送的千字节数更小，只有 2314 KB\n\ndocker0 和 veth04076e3\n\n数据跟 ens33 基本一致只是发送和接收相反，发送的数据较大而接收的数据较小\n这是 Linux 内部网桥转发导致的，暂且不用深究，只要知道这是系统把 ens33 收到的包转发给 Nginx 服务即可\n\n异常点\n前面说到是网络数据包接收软中断的问题，那就重点看 ens33\n接收的 PPS 达到 8w，但接收的 BPS 只有 5k 不到，网络帧看起来是比较小的\n4611 * 1024 &#x2F; 78694 &#x3D; 64 字节，说明平均每个网络帧只有 60 字节，这显然是很小的网络帧，也就是常说的小包问题\n\n灵魂拷问如何知道这是一个什么样的网络帧，它又是从哪里发过来的呢？\n通过 tcpdump 抓取网络包已知条件Nginx 监听在 80 端口， 它所提供的 HTTP 服务是基于 TCP 协议的\n执行 tcpdump 命令tcpdump \\-i ens33 \\-n tcp port 80\n\n\n-i ens33： 只抓取 ens33 网卡\n-n： 不解析协议名和主机名\ntcp port 80： 表示只抓取 tcp 协议并且端口号为 80 的网络帧\n\n\n172.20.72.59.52195 &gt; 172.20.72.58.80\n表示网络帧从 172.20.72.59 的 52195 端口发 送到 172.20.72.58 的 80 端口\n也就是从运行 hping3 机器的 52195 端口发送网络帧， 目的为 Nginx 所在机器的 80 端口\n\nFlags [S]表示这是一个 SYN 包\n性能分析结果结合 sar 命令发现的 PPS 接近 4w 的现象，可以认为这就是从 172.20.72.59 这个地址发送过来的 SYN FLOOD 攻击\n解决 SYN FLOOD 问题从交换机或者硬件防火墙中封掉来源 IP，这样 SYN FLOOD 网络帧就不会发送到服务器中\n后续的期待至于 SYN FLOOD 的原理和更多解决思路在后面会讲到哦\n分析的整体思路\n系统出现卡顿，执行命令，响应也会变慢\n通过 top 查看系统资源情况\n发现 CPU 使用率（us 和 sy）均不高，平均负载适中，没有超 CPU 核数的运行状态的进程，也没有僵尸进程\n但是发现处理软中断的 CPU 占比（si）较高，在进程列表也可以看到软中断进程 CPU 使用率偏高，猜测是软中断导致系统变卡顿的主要原因\n通过 &#x2F;proc&#x2F;sorfirqs 查看软中断类型和变化频率，发现直接 cat 的话会打印 128 个核的信息，但只想要两个核的信息\n所以结合 awk 进行过滤，再通过 watch 命令可以动态输出查看结果\n发现有多个软中断类型在变化，重点是 NET_RX 变化频率超高，而且幅度也很大，它是网络数据包接收软中断，暂且认为它是问题根源\n既然跟网络有关系，可以先通过 sar 命令查看系统网络接收和发送的整体情况\n然后可以看到接收的 PPS 会比接收的 BPS 大很多，做下运算，发现网络帧会非常小，也就是常说的小包问题\n接下来，通过 tcpdump 抓取 80 端口的 tcp 协议网络包，会发现大量来自 VM2 发送的 SYN 包，结合 sar 命令，确认是 SYN FLOOD 攻击https://cloud.tencent.com/developer/article/1678685\n\n","slug":"LINUX/软中断导致 CPU 使用率过高的案例","date":"2022-02-02T19:25:37.000Z","categories_index":"CPU,LINUX","tags_index":"https,通过,SYN","author_index":"dandeliono"},{"id":"443ea60f9d53e9e33f4d61c5af59c5dc","title":"shell 语言","content":"shell 语言\n1. 简介\n1.1. 什么是 shell\n1.2. 什么是 shell 脚本\n1.3. Shell 环境\n1.4. 模式\n\n\n2. 基本语法\n2.1. 解释器\n2.2. 注释\n2.3. echo\n2.4. printf\n\n\n3. 变量\n3.1. 变量命名原则\n3.2. 声明变量\n3.3. 只读变量\n3.4. 删除变量\n3.5. 变量类型\n3.6. 变量示例源码\n\n\n4. 字符串\n4.1. 单引号和双引号\n4.2. 拼接字符串\n4.3. 获取字符串长度\n4.4. 截取子字符串\n4.5. 查找子字符串\n4.6. 字符串示例源码\n\n\n5. 数组\n5.1. 创建数组\n5.2. 访问数组元素\n5.3. 访问数组长度\n5.4. 向数组中添加元素\n5.5. 从数组中删除元素\n5.6. 数组示例源码\n\n\n6. 运算符\n6.1. 算术运算符\n6.2. 关系运算符\n6.3. 布尔运算符\n6.4. 逻辑运算符\n6.5. 字符串运算符\n6.6. 文件测试运算符\n\n\n7. 控制语句\n7.1. 条件语句\n7.2. 循环语句\n\n\n8. 函数\n8.1. 位置参数\n8.2. 函数处理参数\n\n\n9. Shell 扩展\n10. 流和重定向\n10.1. 输入、输出流\n10.2. 重定向\n10.3. /dev/null 文件\n\n\n11. Debug\n12. 更多内容\n\n1. 简介1.1. 什么是 shell\nShell 是一个用 C 语言编写的程序，它是用户使用 Linux 的桥梁。\nShell 既是一种命令语言，又是一种程序设计语言。\nShell 是指一种应用程序，这个应用程序提供了一个界面，用户通过这个界面访问 Linux 内核的服务。\n\nKen Thompson 的 sh 是第一种 Unix Shell，Windows Explorer 是一个典型的图形界面 Shell。\n1.2. 什么是 shell 脚本Shell 脚本（shell script），是一种为 shell 编写的脚本程序，一般文件后缀为 .sh。\n业界所说的 shell 通常都是指 shell 脚本，但 shell 和 shell script 是两个不同的概念。\n1.3. Shell 环境Shell 编程跟 java、php 编程一样，只要有一个能编写代码的文本编辑器和一个能解释执行的脚本解释器就可以了。\nShell 的解释器种类众多，常见的有：\n\nsh - 即 Bourne Shell。sh 是 Unix 标准默认的 shell。\nbash - 即 Bourne Again Shell。bash 是 Linux 标准默认的 shell。\nfish - 智能和用户友好的命令行 shell。\nxiki - 使 shell 控制台更友好，更强大。\nzsh - 功能强大的 shell 与脚本语言。\n\n指定脚本解释器在 shell 脚本，#! 告诉系统其后路径所指定的程序即是解释此脚本文件的 Shell 解释器。#! 被称作shebang（也称为 Hashbang ）。\n所以，你应该会在 shell 中，见到诸如以下的注释：\n\n指定 sh 解释器\n\n1#!/bin/sh\n\n\n指定 bash 解释器\n\n1#!/bin/bash\n\n\n\n\n\n\n\n\n\n\n注意\n上面的指定解释器的方式是比较常见的，但有时候，你可能也会看到下面的方式：\n1#!/usr/bin/env bash\n\n这样做的好处是，系统会自动在 PATH 环境变量中查找你指定的程序（本例中的bash）。相比第一种写法，你应该尽量用这种写法，因为程序的路径是不确定的。这样写还有一个好处，操作系统的PATH变量有可能被配置为指向程序的另一个版本。比如，安装完新版本的bash，我们可能将其路径添加到PATH中，来 “隐藏” 老版本。如果直接用#!/bin/bash，那么系统会选择老版本的bash来执行脚本，如果用#!/usr/bin/env bash，则会使用新版本。\n1.4. 模式shell 有交互和非交互两种模式。\n交互模式\n\n\n\n\n\n\n\n\n简单来说，你可以将 shell 的交互模式理解为执行命令行。\n看到形如下面的东西，说明 shell 处于交互模式下：\n1user@host:~$\n\n接着，便可以输入一系列 Linux 命令，比如 ls，grep，cd，mkdir，rm 等等。\n非交互模式\n\n\n\n\n\n\n\n\n简单来说，你可以将 shell 的非交互模式理解为执行 shell 脚本。\n在非交互模式下，shell 从文件或者管道中读取命令并执行。\n当 shell 解释器执行完文件中的最后一个命令，shell 进程终止，并回到父进程。\n可以使用下面的命令让 shell 以非交互模式运行：\n1234sh /path/to/script.shbash /path/to/script.shsource /path/to/script.sh./path/to/script.sh\n\n上面的例子中，script.sh是一个包含 shell 解释器可以识别并执行的命令的普通文本文件，sh和bash是 shell 解释器程序。你可以使用任何喜欢的编辑器创建script.sh（vim，nano，Sublime Text, Atom 等等）。\n其中，source /path/to/script.sh 和 ./path/to/script.sh 是等价的。\n除此之外，你还可以通过chmod命令给文件添加可执行的权限，来直接执行脚本文件：\n12chmod +x /path/to/script.sh /path/to/test.sh\n\n这种方式要求脚本文件的第一行必须指明运行该脚本的程序，比如：\n⌨️ 『示例源码』 helloworld.sh\n12#!/usr/bin/env bashecho &quot;Hello, world!&quot;\n\n上面的例子中，我们使用了一个很有用的命令echo来输出字符串到屏幕上。\n2. 基本语法2.1. 解释器前面虽然两次提到了#! ，但是本着重要的事情说三遍的精神，这里再强调一遍：\n在 shell 脚本，#! 告诉系统其后路径所指定的程序即是解释此脚本文件的 Shell 解释器。#! 被称作shebang（也称为 Hashbang ）。\n#! 决定了脚本可以像一个独立的可执行文件一样执行，而不用在终端之前输入sh, bash, python, php等。\n2.2. 注释注释可以说明你的代码是什么作用，以及为什么这样写。\nshell 语法中，注释是特殊的语句，会被 shell 解释器忽略。\n\n单行注释 - 以 # 开头，到行尾结束。\n多行注释 - 以 :&lt;&lt;EOF 开头，到 EOF 结束。\n\n⌨️ 『示例源码』 comment-demo.sh\n1234567891011121314:&lt;&lt;EOFecho &#x27;这是多行注释&#x27;echo &#x27;这是多行注释&#x27;echo &#x27;这是多行注释&#x27;EOF\n\n2.3. echoecho 用于字符串的输出。\n输出普通字符串：\n12echo &quot;hello, world&quot;\n\n输出含变量的字符串：\n12echo &quot;hello, \\&quot;zp\\&quot;&quot;\n\n输出含变量的字符串：\n123name=zpecho &quot;hello, \\&quot;$&#123;name&#125;\\&quot;&quot;\n\n输出含换行符的字符串：\n12345678echo &quot;YES\\nNO&quot;echo -e &quot;YES\\nNO&quot; \n\n输出含不换行符的字符串：\n12345678910echo &quot;YES&quot;echo &quot;NO&quot;echo -e &quot;YES\\c&quot; echo &quot;NO&quot;\n\n输出重定向至文件\n1echo &quot;test&quot; &gt; test.txt\n\n输出执行结果\n12echo `pwd`\n\n⌨️ 『示例源码』 echo-demo.sh\n2.4. printfprintf 用于格式化输出字符串。\n默认，printf 不会像 echo 一样自动添加换行符，如果需要换行可以手动添加 \\n。\n⌨️ 『示例源码』 printf-demo.sh\n12345678910111213141516171819202122232425262728293031323334353637383940printf &#x27;%d %s\\n&#x27; 1 &quot;abc&quot;printf &quot;%d %s\\n&quot; 1 &quot;abc&quot;printf %s abcdefprintf &quot;%s\\n&quot; abc defprintf &quot;%s %s %s\\n&quot; a b c d e f g h i jprintf &quot;%s and %d \\n&quot;printf &quot;%-10s %-8s %-4s\\n&quot; 姓名 性别 体重kgprintf &quot;%-10s %-8s %-4.2f\\n&quot; 郭靖 男 66.1234printf &quot;%-10s %-8s %-4.2f\\n&quot; 杨过 男 48.6543printf &quot;%-10s %-8s %-4.2f\\n&quot; 郭芙 女 47.9876\n\nprintf 的转义符\n\n\n序列\n说明\n\n\n\n\\a\n警告字符，通常为 ASCII 的 BEL 字符\n\n\n\\b\n后退\n\n\n\\c\n抑制（不显示）输出结果中任何结尾的换行字符（只在 %b 格式指示符控制下的参数字符串中有效），而且，任何留在参数里的字符、任何接下来的参数以及任何留在格式字符串中的字符，都被忽略\n\n\n\\f\n换页（formfeed）\n\n\n\\n\n换行\n\n\n\\r\n回车（Carriage return）\n\n\n\\t\n水平制表符\n\n\n\\v\n垂直制表符\n\n\n\\\\\n一个字面上的反斜杠字符\n\n\n\\ddd\n表示 1 到 3 位数八进制值的字符。仅在格式字符串中有效\n\n\n\\0ddd\n表示 1 到 3 位的八进制值字符\n\n\n3. 变量跟许多程序设计语言一样，你可以在 bash 中创建变量。\nBash 中没有数据类型，bash 中的变量可以保存一个数字、一个字符、一个字符串等等。同时无需提前声明变量，给变量赋值会直接创建变量。\n3.1. 变量命名原则\n命名只能使用英文字母，数字和下划线，首个字符不能以数字开头。\n中间不能有空格，可以使用下划线（_）。\n不能使用标点符号。\n不能使用 bash 里的关键字（可用 help 命令查看保留关键字）。\n\n3.2. 声明变量访问变量的语法形式为：$&#123;var&#125; 和 $var 。\n变量名外面的花括号是可选的，加不加都行，加花括号是为了帮助解释器识别变量的边界，所以推荐加花括号。\n123word=&quot;hello&quot;echo $&#123;word&#125;\n\n3.3. 只读变量使用 readonly 命令可以将变量定义为只读变量，只读变量的值不能被改变。\n1234rword=&quot;hello&quot;echo $&#123;rword&#125;readonly rword\n\n3.4. 删除变量使用 unset 命令可以删除变量。变量被删除后不能再次使用。unset 命令不能删除只读变量。\n1234567dword=&quot;hello&quot;  echo $&#123;dword&#125;  unset dword    echo $&#123;dword&#125;\n\n3.5. 变量类型\n局部变量 - 局部变量是仅在某个脚本内部有效的变量。它们不能被其他的程序和脚本访问。\n环境变量 - 环境变量是对当前 shell 会话内所有的程序或脚本都可见的变量。创建它们跟创建局部变量类似，但使用的是 export 关键字，shell 脚本也可以定义环境变量。\n\n常见的环境变量：\n\n\n\n变量\n描述\n\n\n\n$HOME\n当前用户的用户目录\n\n\n$PATH\n用分号分隔的目录列表，shell 会到这些目录中查找命令\n\n\n$PWD\n当前工作目录\n\n\n$RANDOM\n0 到 32767 之间的整数\n\n\n$UID\n数值类型，当前用户的用户 ID\n\n\n$PS1\n主要系统输入提示符\n\n\n$PS2\n次要系统输入提示符\n\n\n这里 有一张更全面的 Bash 环境变量列表。\n3.6. 变量示例源码⌨️ 『示例源码』 variable-demo.sh\n4. 字符串4.1. 单引号和双引号shell 字符串可以用单引号 &#39;&#39;，也可以用双引号 “”，也可以不用引号。\n\n单引号的特点\n单引号里不识别变量\n单引号里不能出现单独的单引号（使用转义符也不行），但可成对出现，作为字符串拼接使用。\n\n\n双引号的特点\n双引号里识别变量\n双引号里可以出现转义字符\n\n\n\n综上，推荐使用双引号。\n4.2. 拼接字符串123456789101112131415name1=&#x27;white&#x27;str1=&#x27;hello, &#x27;$&#123;name1&#125;&#x27;&#x27;str2=&#x27;hello, $&#123;name1&#125;&#x27;echo $&#123;str1&#125;_$&#123;str2&#125;name2=&quot;black&quot;str3=&quot;hello, &quot;$&#123;name2&#125;&quot;&quot;str4=&quot;hello, $&#123;name2&#125;&quot;echo $&#123;str3&#125;_$&#123;str4&#125;\n\n4.3. 获取字符串长度1234text=&quot;12345&quot;echo $&#123;#text&#125;\n\n4.4. 截取子字符串1234text=&quot;12345&quot;echo $&#123;text:2:2&#125;\n\n从第 3 个字符开始，截取 2 个字符\n4.5. 查找子字符串12345678#!/usr/bin/env bashtext=&quot;hello&quot;echo `expr index &quot;$&#123;text&#125;&quot; ll`\n\n查找 ll 子字符在 hello 字符串中的起始位置。\n4.6. 字符串示例源码⌨️ 『示例源码』 string-demo.sh\n5. 数组bash 只支持一维数组。\n数组下标从 0 开始，下标可以是整数或算术表达式，其值应大于或等于 0。\n5.1. 创建数组123nums=([2]=2 [0]=0 [1]=1)colors=(red yellow &quot;dark blue&quot;)\n\n5.2. 访问数组元素\n访问数组的单个元素：\n\n12echo $&#123;nums[1]&#125;\n\n\n访问数组的所有元素：\n\n12345echo $&#123;colors[*]&#125;echo $&#123;colors[@]&#125;\n\n上面两行有很重要（也很微妙）的区别：\n为了将数组中每个元素单独一行输出，我们用 printf 命令：\n123456printf &quot;+ %s\\n&quot; $&#123;colors[*]&#125;\n\n为什么dark和blue各占了一行？尝试用引号包起来：\n123printf &quot;+ %s\\n&quot; &quot;$&#123;colors[*]&#125;&quot;\n\n现在所有的元素都在一行输出 —— 这不是我们想要的！让我们试试$&#123;colors[@]&#125;\n12345printf &quot;+ %s\\n&quot; &quot;$&#123;colors[@]&#125;&quot;\n\n在引号内，$&#123;colors[@]&#125;将数组中的每个元素扩展为一个单独的参数；数组元素中的空格得以保留。\n\n访问数组的部分元素：\n\n123echo $&#123;nums[@]:0:2&#125;\n\n在上面的例子中，$&#123;array[@]&#125; 扩展为整个数组，:0:2取出了数组中从 0 开始，长度为 2 的元素。\n5.3. 访问数组长度123echo $&#123;#nums[*]&#125;\n\n5.4. 向数组中添加元素向数组中添加元素也非常简单：\n1234colors=(white &quot;$&#123;colors[@]&#125;&quot; green black)echo $&#123;colors[@]&#125;\n\n上面的例子中，$&#123;colors[@]&#125; 扩展为整个数组，并被置换到复合赋值语句中，接着，对数组colors的赋值覆盖了它原来的值。\n5.5. 从数组中删除元素用unset命令来从数组中删除一个元素：\n1234unset nums[0]echo $&#123;nums[@]&#125;\n\n5.6. 数组示例源码⌨️ 『示例源码』 array-demo.sh\n6. 运算符6.1. 算术运算符下表列出了常用的算术运算符，假定变量 x 为 10，变量 y 为 20：\n\n\n\n运算符\n说明\n举例\n\n\n\n+\n加法\nexpr $x + $y 结果为 30。\n\n\n-\n减法\nexpr $x - $y 结果为 -10。\n\n\n*\n乘法\nexpr $x * $y 结果为 200。\n\n\n&#x2F;\n除法\nexpr $y / $x 结果为 2。\n\n\n%\n取余\nexpr $y % $x 结果为 0。\n\n\n\\&#x3D;\n赋值\nx=$y 将把变量 y 的值赋给 x。\n\n\n\\&#x3D;&#x3D;\n相等。用于比较两个数字，相同则返回 true。\n[$x == $y] 返回 false。\n\n\n!&#x3D;\n不相等。用于比较两个数字，不相同则返回 true。\n[$x != $y] 返回 true。\n\n\n注意： 条件表达式要放在方括号之间，并且要有空格，例如: [$x==$y] 是错误的，必须写成 [$x == $y]。\n⌨️ 『示例源码』 operator-demo.sh\n1234567891011121314151617181920212223242526272829303132333435363738x=10y=20echo &quot;x=$&#123;x&#125;, y=$&#123;y&#125;&quot;val=`expr $&#123;x&#125; + $&#123;y&#125;`echo &quot;$&#123;x&#125; + $&#123;y&#125; = $val&quot;val=`expr $&#123;x&#125; - $&#123;y&#125;`echo &quot;$&#123;x&#125; - $&#123;y&#125; = $val&quot;val=`expr $&#123;x&#125; \\* $&#123;y&#125;`echo &quot;$&#123;x&#125; * $&#123;y&#125; = $val&quot;val=`expr $&#123;y&#125; / $&#123;x&#125;`echo &quot;$&#123;y&#125; / $&#123;x&#125; = $val&quot;val=`expr $&#123;y&#125; % $&#123;x&#125;`echo &quot;$&#123;y&#125; % $&#123;x&#125; = $val&quot;if [[ $&#123;x&#125; == $&#123;y&#125; ]]then  echo &quot;$&#123;x&#125; = $&#123;y&#125;&quot;fiif [[ $&#123;x&#125; != $&#123;y&#125; ]]then  echo &quot;$&#123;x&#125; != $&#123;y&#125;&quot;fi\n\n6.2. 关系运算符关系运算符只支持数字，不支持字符串，除非字符串的值是数字。\n下表列出了常用的关系运算符，假定变量 x 为 10，变量 y 为 20：\n\n\n\n运算符\n说明\n举例\n\n\n\n-eq\n检测两个数是否相等，相等返回 true。\n[$a -eq $b]返回 false。\n\n\n-ne\n检测两个数是否相等，不相等返回 true。\n[$a -ne $b] 返回 true。\n\n\n-gt\n检测左边的数是否大于右边的，如果是，则返回 true。\n[$a -gt $b] 返回 false。\n\n\n-lt\n检测左边的数是否小于右边的，如果是，则返回 true。\n[$a -lt $b] 返回 true。\n\n\n-ge\n检测左边的数是否大于等于右边的，如果是，则返回 true。\n[$a -ge $b] 返回 false。\n\n\n-le\n检测左边的数是否小于等于右边的，如果是，则返回 true。\n[$a -le $b]返回 true。\n\n\n⌨️ 『示例源码』 operator-demo2.sh\n1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950x=10y=20echo &quot;x=$&#123;x&#125;, y=$&#123;y&#125;&quot;if [[ $&#123;x&#125; -eq $&#123;y&#125; ]]; then   echo &quot;$&#123;x&#125; -eq $&#123;y&#125; : x 等于 y&quot;else   echo &quot;$&#123;x&#125; -eq $&#123;y&#125;: x 不等于 y&quot;fiif [[ $&#123;x&#125; -ne $&#123;y&#125; ]]; then   echo &quot;$&#123;x&#125; -ne $&#123;y&#125;: x 不等于 y&quot;else   echo &quot;$&#123;x&#125; -ne $&#123;y&#125;: x 等于 y&quot;fiif [[ $&#123;x&#125; -gt $&#123;y&#125; ]]; then   echo &quot;$&#123;x&#125; -gt $&#123;y&#125;: x 大于 y&quot;else   echo &quot;$&#123;x&#125; -gt $&#123;y&#125;: x 不大于 y&quot;fiif [[ $&#123;x&#125; -lt $&#123;y&#125; ]]; then   echo &quot;$&#123;x&#125; -lt $&#123;y&#125;: x 小于 y&quot;else   echo &quot;$&#123;x&#125; -lt $&#123;y&#125;: x 不小于 y&quot;fiif [[ $&#123;x&#125; -ge $&#123;y&#125; ]]; then   echo &quot;$&#123;x&#125; -ge $&#123;y&#125;: x 大于或等于 y&quot;else   echo &quot;$&#123;x&#125; -ge $&#123;y&#125;: x 小于 y&quot;fiif [[ $&#123;x&#125; -le $&#123;y&#125; ]]; then   echo &quot;$&#123;x&#125; -le $&#123;y&#125;: x 小于或等于 y&quot;else   echo &quot;$&#123;x&#125; -le $&#123;y&#125;: x 大于 y&quot;fi\n\n6.3. 布尔运算符下表列出了常用的布尔运算符，假定变量 x 为 10，变量 y 为 20：\n\n\n\n运算符\n说明\n举例\n\n\n\n!\n非运算，表达式为 true 则返回 false，否则返回 true。\n[! false] 返回 true。\n\n\n-o\n或运算，有一个表达式为 true 则返回 true。\n[$a -lt 20 -o $b -gt 100] 返回 true。\n\n\n-a\n与运算，两个表达式都为 true 才返回 true。\n[$a -lt 20 -a $b -gt 100] 返回 false。\n\n\n⌨️ 『示例源码』 operator-demo3.sh\n123456789101112131415161718192021222324252627282930313233343536x=10y=20echo &quot;x=$&#123;x&#125;, y=$&#123;y&#125;&quot;if [[ $&#123;x&#125; != $&#123;y&#125; ]]; then   echo &quot;$&#123;x&#125; != $&#123;y&#125; : x 不等于 y&quot;else   echo &quot;$&#123;x&#125; != $&#123;y&#125;: x 等于 y&quot;fiif [[ $&#123;x&#125; -lt 100 &amp;&amp; $&#123;y&#125; -gt 15 ]]; then   echo &quot;$&#123;x&#125; 小于 100 且 $&#123;y&#125; 大于 15 : 返回 true&quot;else   echo &quot;$&#123;x&#125; 小于 100 且 $&#123;y&#125; 大于 15 : 返回 false&quot;fiif [[ $&#123;x&#125; -lt 100 || $&#123;y&#125; -gt 100 ]]; then   echo &quot;$&#123;x&#125; 小于 100 或 $&#123;y&#125; 大于 100 : 返回 true&quot;else   echo &quot;$&#123;x&#125; 小于 100 或 $&#123;y&#125; 大于 100 : 返回 false&quot;fiif [[ $&#123;x&#125; -lt 5 || $&#123;y&#125; -gt 100 ]]; then   echo &quot;$&#123;x&#125; 小于 5 或 $&#123;y&#125; 大于 100 : 返回 true&quot;else   echo &quot;$&#123;x&#125; 小于 5 或 $&#123;y&#125; 大于 100 : 返回 false&quot;fi\n\n6.4. 逻辑运算符以下介绍 Shell 的逻辑运算符，假定变量 x 为 10，变量 y 为 20:\n\n\n\n运算符\n说明\n举例\n\n\n\n&amp;&amp;\n逻辑的 AND\n[[$&#123;x&#125; -lt 100 &amp;&amp; $&#123;y&#125; -gt 100 ]] 返回 false\n\n\n`\n\n`\n\n\n⌨️ 『示例源码』 operator-demo4.sh\n123456789101112131415161718192021222324x=10y=20echo &quot;x=$&#123;x&#125;, y=$&#123;y&#125;&quot;if [[ $&#123;x&#125; -lt 100 &amp;&amp; $&#123;y&#125; -gt 100 ]]then   echo &quot;$&#123;x&#125; -lt 100 &amp;&amp; $&#123;y&#125; -gt 100 返回 true&quot;else   echo &quot;$&#123;x&#125; -lt 100 &amp;&amp; $&#123;y&#125; -gt 100 返回 false&quot;fiif [[ $&#123;x&#125; -lt 100 || $&#123;y&#125; -gt 100 ]]then   echo &quot;$&#123;x&#125; -lt 100 || $&#123;y&#125; -gt 100 返回 true&quot;else   echo &quot;$&#123;x&#125; -lt 100 || $&#123;y&#125; -gt 100 返回 false&quot;fi\n\n6.5. 字符串运算符下表列出了常用的字符串运算符，假定变量 a 为 “abc”，变量 b 为 “efg”：\n\n\n\n运算符\n说明\n举例\n\n\n\n=\n检测两个字符串是否相等，相等返回 true。\n[$a = $b] 返回 false。\n\n\n!=\n检测两个字符串是否相等，不相等返回 true。\n[$a != $b] 返回 true。\n\n\n-z\n检测字符串长度是否为 0，为 0 返回 true。\n[-z $a] 返回 false。\n\n\n-n\n检测字符串长度是否为 0，不为 0 返回 true。\n[-n $a] 返回 true。\n\n\nstr\n检测字符串是否为空，不为空返回 true。\n[$a] 返回 true。\n\n\n⌨️ 『示例源码』 operator-demo5.sh\n1234567891011121314151617181920212223242526272829303132333435363738394041424344x=&quot;abc&quot;y=&quot;xyz&quot;echo &quot;x=$&#123;x&#125;, y=$&#123;y&#125;&quot;if [[ $&#123;x&#125; = $&#123;y&#125; ]]; then   echo &quot;$&#123;x&#125; = $&#123;y&#125; : x 等于 y&quot;else   echo &quot;$&#123;x&#125; = $&#123;y&#125;: x 不等于 y&quot;fiif [[ $&#123;x&#125; != $&#123;y&#125; ]]; then   echo &quot;$&#123;x&#125; != $&#123;y&#125; : x 不等于 y&quot;else   echo &quot;$&#123;x&#125; != $&#123;y&#125;: x 等于 y&quot;fiif [[ -z $&#123;x&#125; ]]; then   echo &quot;-z $&#123;x&#125; : 字符串长度为 0&quot;else   echo &quot;-z $&#123;x&#125; : 字符串长度不为 0&quot;fiif [[ -n &quot;$&#123;x&#125;&quot; ]]; then   echo &quot;-n $&#123;x&#125; : 字符串长度不为 0&quot;else   echo &quot;-n $&#123;x&#125; : 字符串长度为 0&quot;fiif [[ $&#123;x&#125; ]]; then   echo &quot;$&#123;x&#125; : 字符串不为空&quot;else   echo &quot;$&#123;x&#125; : 字符串为空&quot;fi\n\n6.6. 文件测试运算符文件测试运算符用于检测 Unix 文件的各种属性。\n属性检测描述如下：\n\n\n\n操作符\n说明\n举例\n\n\n\n-b file\n检测文件是否是块设备文件，如果是，则返回 true。\n[-b $file] 返回 false。\n\n\n-c file\n检测文件是否是字符设备文件，如果是，则返回 true。\n[-c $file] 返回 false。\n\n\n-d file\n检测文件是否是目录，如果是，则返回 true。\n[-d $file] 返回 false。\n\n\n-f file\n检测文件是否是普通文件（既不是目录，也不是设备文件），如果是，则返回 true。\n[-f $file] 返回 true。\n\n\n-g file\n检测文件是否设置了 SGID 位，如果是，则返回 true。\n[-g $file] 返回 false。\n\n\n-k file\n检测文件是否设置了粘着位 (Sticky Bit)，如果是，则返回 true。\n[-k $file]返回 false。\n\n\n-p file\n检测文件是否是有名管道，如果是，则返回 true。\n[-p $file] 返回 false。\n\n\n-u file\n检测文件是否设置了 SUID 位，如果是，则返回 true。\n[-u $file] 返回 false。\n\n\n-r file\n检测文件是否可读，如果是，则返回 true。\n[-r $file] 返回 true。\n\n\n-w file\n检测文件是否可写，如果是，则返回 true。\n[-w $file] 返回 true。\n\n\n-x file\n检测文件是否可执行，如果是，则返回 true。\n[-x $file] 返回 true。\n\n\n-s file\n检测文件是否为空（文件大小是否大于 0），不为空返回 true。\n[-s $file] 返回 true。\n\n\n-e file\n检测文件（包括目录）是否存在，如果是，则返回 true。\n[-e $file] 返回 true。\n\n\n⌨️ 『示例源码』 operator-demo6.sh\n1234567891011121314151617181920212223242526272829303132333435363738394041424344454647file=&quot;/etc/hosts&quot;if [[ -r $&#123;file&#125; ]]; then   echo &quot;$&#123;file&#125; 文件可读&quot;else   echo &quot;$&#123;file&#125; 文件不可读&quot;fiif [[ -w $&#123;file&#125; ]]; then   echo &quot;$&#123;file&#125; 文件可写&quot;else   echo &quot;$&#123;file&#125; 文件不可写&quot;fiif [[ -x $&#123;file&#125; ]]; then   echo &quot;$&#123;file&#125; 文件可执行&quot;else   echo &quot;$&#123;file&#125; 文件不可执行&quot;fiif [[ -f $&#123;file&#125; ]]; then   echo &quot;$&#123;file&#125; 文件为普通文件&quot;else   echo &quot;$&#123;file&#125; 文件为特殊文件&quot;fiif [[ -d $&#123;file&#125; ]]; then   echo &quot;$&#123;file&#125; 文件是个目录&quot;else   echo &quot;$&#123;file&#125; 文件不是个目录&quot;fiif [[ -s $&#123;file&#125; ]]; then   echo &quot;$&#123;file&#125; 文件不为空&quot;else   echo &quot;$&#123;file&#125; 文件为空&quot;fiif [[ -e $&#123;file&#125; ]]; then   echo &quot;$&#123;file&#125; 文件存在&quot;else   echo &quot;$&#123;file&#125; 文件不存在&quot;fi\n\n7. 控制语句7.1. 条件语句跟其它程序设计语言一样，Bash 中的条件语句让我们可以决定一个操作是否被执行。结果取决于一个包在[[]]里的表达式。\n由[[]]（sh中是[ ]）包起来的表达式被称作 检测命令 或 基元。这些表达式帮助我们检测一个条件的结果。这里可以找到有关bash 中单双中括号区别的答案。\n共有两个不同的条件表达式：if和case。\nif（1）if 语句\nif在使用上跟其它语言相同。如果中括号里的表达式为真，那么then和fi之间的代码会被执行。fi标志着条件代码块的结束。\n12345678910if [[ 1 -eq 1 ]]; then echo &quot;1 -eq 1 result is: true&quot;; fiif [[ &quot;abc&quot; -eq &quot;abc&quot; ]]then  echo &quot;&quot;abc&quot; -eq &quot;abc&quot; result is: true&quot;fi\n\n（2）if else 语句\n同样，我们可以使用if..else语句，例如：\n123456if [[ 2 -ne 1 ]]; then  echo &quot;true&quot;else  echo &quot;false&quot;fi\n\n（3）if elif else 语句\n有些时候，if..else不能满足我们的要求。别忘了if..elif..else，使用起来也很方便。\n12345678910x=10y=20if [[ $&#123;x&#125; &gt; $&#123;y&#125; ]]; then   echo &quot;$&#123;x&#125; &gt; $&#123;y&#125;&quot;elif [[ $&#123;x&#125; &lt; $&#123;y&#125; ]]; then   echo &quot;$&#123;x&#125; &lt; $&#123;y&#125;&quot;else   echo &quot;$&#123;x&#125; = $&#123;y&#125;&quot;fi\n\n⌨️ 『示例源码』 if-demo.sh\ncase如果你需要面对很多情况，分别要采取不同的措施，那么使用case会比嵌套的if更有用。使用case来解决复杂的条件判断，看起来像下面这样：\n⌨️ 『示例源码』 case-demo.sh\n12345678910111213141516171819202122execcase $&#123;oper&#125; in  &quot;+&quot;)    val=`expr $&#123;x&#125; + $&#123;y&#125;`    echo &quot;$&#123;x&#125; + $&#123;y&#125; = $&#123;val&#125;&quot;  ;;  &quot;-&quot;)    val=`expr $&#123;x&#125; - $&#123;y&#125;`    echo &quot;$&#123;x&#125; - $&#123;y&#125; = $&#123;val&#125;&quot;  ;;  &quot;*&quot;)    val=`expr $&#123;x&#125; \\* $&#123;y&#125;`    echo &quot;$&#123;x&#125; * $&#123;y&#125; = $&#123;val&#125;&quot;  ;;  &quot;/&quot;)    val=`expr $&#123;x&#125; / $&#123;y&#125;`    echo &quot;$&#123;x&#125; / $&#123;y&#125; = $&#123;val&#125;&quot;  ;;  *)    echo &quot;Unknown oper!&quot;  ;;esac\n\n每种情况都是匹配了某个模式的表达式。|用来分割多个模式，)用来结束一个模式序列。第一个匹配上的模式对应的命令将会被执行。*代表任何不匹配以上给定模式的模式。命令块儿之间要用;;分隔。\n7.2. 循环语句循环其实不足为奇。跟其它程序设计语言一样，bash 中的循环也是只要控制条件为真就一直迭代执行的代码块。\nBash 中有四种循环：for，while，until和select。\nfor循环for与它在 C 语言中的姊妹非常像。看起来是这样：\n1234for arg in elem1 elem2 ... elemNdo  done\n\n在每次循环的过程中，arg依次被赋值为从elem1到elemN。这些值还可以是通配符或者大括号扩展。\n当然，我们还可以把for循环写在一行，但这要求do之前要有一个分号，就像下面这样：\n1for i in &#123;1..5&#125;; do echo $i; done\n\n还有，如果你觉得for..in..do对你来说有点奇怪，那么你也可以像 C 语言那样使用for，比如：\n123for (( i = 0; i &lt; 10; i++ )); do  echo $idone\n\n当我们想对一个目录下的所有文件做同样的操作时，for就很方便了。举个例子，如果我们想把所有的.bash文件移动到script文件夹中，并给它们可执行权限，我们的脚本可以这样写：\n12345DIR=/home/zpfor FILE in $&#123;DIR&#125;/*.sh; do  mv &quot;$FILE&quot; &quot;$&#123;DIR&#125;/scripts&quot;done\n\n⌨️ 『示例源码』 for-demo.sh\nwhile循环while循环检测一个条件，只要这个条件为 _真_，就执行一段命令。被检测的条件跟if..then中使用的基元并无二异。因此一个while循环看起来会是这样：\n1234while [[ condition ]]do  done\n\n跟for循环一样，如果我们把do和被检测的条件写到一行，那么必须要在do之前加一个分号。\n比如下面这个例子：\n1234567891011121314151617x=0while [[ $&#123;x&#125; -lt 10 ]]; do  echo $((x * x))  x=$((x + 1))done\n\n⌨️ 『示例源码』 while-demo.sh\nuntil循环until循环跟while循环正好相反。它跟while一样也需要检测一个测试条件，但不同的是，只要该条件为 假 就一直执行循环：\n1234567891011x=0until [[ $&#123;x&#125; -ge 5 ]]; do  echo $&#123;x&#125;  x=`expr $&#123;x&#125; + 1`done\n\n⌨️ 『示例源码』 until-demo.sh\nselect循环select循环帮助我们组织一个用户菜单。它的语法几乎跟for循环一致：\n1234select answer in elem1 elem2 ... elemNdo  done\n\nselect会打印elem1..elemN以及它们的序列号到屏幕上，之后会提示用户输入。通常看到的是$?（PS3变量）。用户的选择结果会被保存到answer中。如果answer是一个在1..N之间的数字，那么语句会被执行，紧接着会进行下一次迭代 —— 如果不想这样的话我们可以使用break语句。\n一个可能的实例可能会是这样：\n1234567891011121314#!/usr/bin/env bashPS3=&quot;Choose the package manager: &quot;select ITEM in bower npm gem pipdoecho -n &quot;Enter the package name: &quot; &amp;&amp; read PACKAGEcase $&#123;ITEM&#125; in  bower) bower install $&#123;PACKAGE&#125; ;;  npm) npm install $&#123;PACKAGE&#125; ;;  gem) gem install $&#123;PACKAGE&#125; ;;  pip) pip install $&#123;PACKAGE&#125; ;;esacbreak done\n\n这个例子，先询问用户他想使用什么包管理器。接着，又询问了想安装什么包，最后执行安装操作。\n运行这个脚本，会得到如下输出：\n1234567$ ./my_script1) bower2) npm3) gem4) pipChoose the package manager: 2Enter the package name: gitbook-cli\n\n⌨️ 『示例源码』 select-demo.sh\nbreak 和 continue如果想提前结束一个循环或跳过某次循环执行，可以使用 shell 的break和continue语句来实现。它们可以在任何循环中使用。\n\n\n\n\n\n\n\n\n\nbreak语句用来提前结束当前循环。\ncontinue语句用来跳过某次迭代。\n⌨️ 『示例源码』 break-demo.sh\n12345678910i=1while [[ $&#123;i&#125; -lt 10 ]]; do  if [[ $((i % 3)) -eq 0 ]] &amp;&amp; [[ $((i % 2)) -eq 0 ]]; then    echo $&#123;i&#125;    break;  fi  i=`expr $&#123;i&#125; + 1`done\n\n⌨️ 『示例源码』 continue-demo.sh\n12345678910111213for (( i = 0; i &lt; 10; i ++ )); do  if [[ $((i % 2)) -eq 0 ]]; then    continue;  fi  echo $&#123;i&#125;done\n\n8. 函数bash 函数定义语法如下：\n1234[ function ] funname [()] &#123;    action;    [return int;]&#125;\n\n\n\n\n\n\n\n\n\n\n💡 说明：\n\n函数定义时，function 关键字可有可无。\n函数返回值 - return 返回函数返回值，返回值类型只能为整数（0-255）。如果不加 return 语句，shell 默认将以最后一条命令的运行结果，作为函数返回值。\n函数返回值在调用该函数后通过 $? 来获得。\n所有函数在使用前必须定义。这意味着必须将函数放在脚本开始部分，直至 shell 解释器首次发现它时，才可以使用。调用函数仅使用其函数名即可。\n\n⌨️ 『示例源码』 function-demo.sh\n1234567891011121314151617181920212223242526272829303132#!/usr/bin/env bashcalc()&#123;  PS3=&quot;choose the oper: &quot;  select oper in + - \\* /   do  echo -n &quot;enter first num: &quot; &amp;&amp; read x   echo -n &quot;enter second num: &quot; &amp;&amp; read y   exec  case $&#123;oper&#125; in    &quot;+&quot;)      return $(($&#123;x&#125; + $&#123;y&#125;))    ;;    &quot;-&quot;)      return $(($&#123;x&#125; - $&#123;y&#125;))    ;;    &quot;*&quot;)      return $(($&#123;x&#125; * $&#123;y&#125;))    ;;    &quot;/&quot;)      return $(($&#123;x&#125; / $&#123;y&#125;))    ;;    *)      echo &quot;$&#123;oper&#125; is not support!&quot;      return 0    ;;  esac  break  done&#125;calcecho &quot;the result is: $?&quot; \n\n执行结果：\n123456789$ ./function-demo.sh1) +2) -3) *4) /choose the oper: 3enter first num: 10enter second num: 10the result is: 100\n\n8.1. 位置参数位置参数是在调用一个函数并传给它参数时创建的变量。\n位置参数变量表：\n\n\n\n变量\n描述\n\n\n\n$0\n脚本名称\n\n\n$1 … $9\n第 1 个到第 9 个参数列表\n\n\n$&#123;10&#125; … $&#123;N&#125;\n第 10 个到 N 个参数列表\n\n\n$* or $@\n除了$0外的所有位置参数\n\n\n$#\n不包括$0在内的位置参数的个数\n\n\n$FUNCNAME\n函数名称（仅在函数内部有值）\n\n\n⌨️ 『示例源码』 function-demo2.sh\n1234567891011121314151617181920212223#!/usr/bin/env bashx=0if [[ -n $1 ]]; then  echo &quot;第一个参数为：$1&quot;  x=$1else  echo &quot;第一个参数为空&quot;fiy=0if [[ -n $2 ]]; then  echo &quot;第二个参数为：$2&quot;  y=$2else  echo &quot;第二个参数为空&quot;fiparamsFunction()&#123;  echo &quot;函数第一个入参：$1&quot;  echo &quot;函数第二个入参：$2&quot;&#125;paramsFunction $&#123;x&#125; $&#123;y&#125;\n\n执行结果：\n1234567891011$ ./function-demo2.sh第一个参数为空第二个参数为空函数第一个入参：0函数第二个入参：0$ ./function-demo2.sh 10 20第一个参数为：10第二个参数为：20函数第一个入参：10函数第二个入参：20\n\n执行 ./variable-demo4.sh hello world ，然后在脚本中通过 $1、$2 … 读取第 1 个参数、第 2 个参数。。。\n8.2. 函数处理参数另外，还有几个特殊字符用来处理参数：\n\n\n\n参数处理\n说明\n\n\n\n$#\n返回参数个数\n\n\n$*\n返回所有参数\n\n\n$$\n脚本运行的当前进程 ID 号\n\n\n$!\n后台运行的最后一个进程的 ID 号\n\n\n$@\n返回所有参数\n\n\n$-\n返回 Shell 使用的当前选项，与 set 命令功能相同。\n\n\n$?\n函数返回值\n\n\n⌨️ 『示例源码』 function-demo3.sh\n12345678910111213141516171819202122232425262728293031323334runner() &#123;  return 0&#125;name=zpparamsFunction()&#123;  echo &quot;函数第一个入参：$1&quot;  echo &quot;函数第二个入参：$2&quot;  echo &quot;传递到脚本的参数个数：$#&quot;  echo &quot;所有参数：&quot;  printf &quot;+ %s\\n&quot; &quot;$*&quot;  echo &quot;脚本运行的当前进程 ID 号：$$&quot;  echo &quot;后台运行的最后一个进程的 ID 号：$!&quot;  echo &quot;所有参数：&quot;  printf &quot;+ %s\\n&quot; &quot;$@&quot;  echo &quot;Shell 使用的当前选项：$-&quot;  runner  echo &quot;runner 函数的返回值：$?&quot;&#125;paramsFunction 1 &quot;abc&quot; &quot;hello, \\&quot;zp\\&quot;&quot;\n\n9. Shell 扩展扩展 发生在一行命令被分成一个个的 记号（tokens） 之后。换言之，扩展是一种执行数学运算的机制，还可以用来保存命令的执行结果，等等。\n感兴趣的话可以阅读关于 shell 扩展的更多细节。\n大括号扩展大括号扩展让生成任意的字符串成为可能。它跟 文件名扩展 很类似，举个例子：\n1echo beg&#123;i,a,u&#125;n \n\n大括号扩展还可以用来创建一个可被循环迭代的区间。\n12echo &#123;0..5&#125; echo &#123;00..8..2&#125; \n\n命令置换命令置换允许我们对一个命令求值，并将其值置换到另一个命令或者变量赋值表达式中。当一个命令被``或$()包围时，命令置换将会执行。举个例子：\n12345now=`date +%T`now=$(date +%T)echo $now \n\n算数扩展在 bash 中，执行算数运算是非常方便的。算数表达式必须包在$(())中。算数扩展的格式为：\n12result=$(( ((10 + 5*3) - 7) / 2 ))echo $result \n\n在算数表达式中，使用变量无需带上$前缀：\n12345x=4y=7echo $(( x + y ))     echo $(( ++x + y++ )) echo $(( x + y ))     \n\n单引号和双引号单引号和双引号之间有很重要的区别。在双引号中，变量引用或者命令置换是会被展开的。在单引号中是不会的。举个例子：\n12echo &quot;Your home: $HOME&quot; echo &#x27;Your home: $HOME&#x27; \n\n当局部变量和环境变量包含空格时，它们在引号中的扩展要格外注意。随便举个例子，假如我们用echo来输出用户的输入：\n123INPUT=&quot;A string  with   strange    whitespace.&quot;echo $INPUT   echo &quot;$INPUT&quot; \n\n调用第一个echo时给了它 5 个单独的参数 —— $INPUT 被分成了单独的词，echo在每个词之间打印了一个空格。第二种情况，调用echo时只给了它一个参数（整个 $INPUT 的值，包括其中的空格）。\n来看一个更严肃的例子：\n123FILE=&quot;Favorite Things.txt&quot;cat $FILE   cat &quot;$FILE&quot; \n\n尽管这个问题可以通过把 FILE 重命名成Favorite-Things.txt来解决，但是，假如这个值来自某个环境变量，来自一个位置参数，或者来自其它命令（find, cat, 等等）呢。因此，如果输入 可能 包含空格，务必要用引号把表达式包起来。\n10. 流和重定向Bash 有很强大的工具来处理程序之间的协同工作。使用流，我们能将一个程序的输出发送到另一个程序或文件，因此，我们能方便地记录日志或做一些其它我们想做的事。\n管道给了我们创建传送带的机会，控制程序的执行成为可能。\n学习如何使用这些强大的、高级的工具是非常非常重要的。\n10.1. 输入、输出流Bash 接收输入，并以字符序列或 字符流 的形式产生输出。这些流能被重定向到文件或另一个流中。\n有三个文件描述符：\n\n\n\n代码\n描述符\n描述\n\n\n\n0\nstdin\n标准输入\n\n\n1\nstdout\n标准输出\n\n\n2\nstderr\n标准错误输出\n\n\n10.2. 重定向重定向让我们可以控制一个命令的输入来自哪里，输出结果到什么地方。这些运算符在控制流的重定向时会被用到：\n\n\n\nOperator\nDescription\n\n\n\n&gt;\n重定向输出\n\n\n&amp;&gt;\n重定向输出和错误输出\n\n\n&amp;&gt;&gt;\n以附加的形式重定向输出和错误输出\n\n\n&lt;\n重定向输入\n\n\n&lt;&lt;\nHere 文档 语法\n\n\n&lt;&lt;&lt;\nHere 字符串\n\n\n以下是一些使用重定向的例子：\n1234567891011ls -l &gt; list.txtls -a &gt;&gt; list.txtgrep da * 2&gt; errors.txtless &lt; errors.txt\n\n10.3. /dev/null 文件如果希望执行某个命令，但又不希望在屏幕上显示输出结果，那么可以将输出重定向到 &#x2F;dev&#x2F;null：\n1$ command &gt; /dev/null\n\n&#x2F;dev&#x2F;null 是一个特殊的文件，写入到它的内容都会被丢弃；如果尝试从该文件读取内容，那么什么也读不到。但是 &#x2F;dev&#x2F;null 文件非常有用，将命令的输出重定向到它，会起到 “禁止输出” 的效果。\n如果希望屏蔽 stdout 和 stderr，可以这样写：\n1$ command &gt; /dev/null 2&gt;&amp;1\n\n11. Debugshell 提供了用于 debug 脚本的工具。\n如果想采用 debug 模式运行某脚本，可以在其 shebang 中使用一个特殊的选项：\n1#!/bin/bash options\n\noptions 是一些可以改变 shell 行为的选项。下表是一些可能对你有用的选项：\n\n\n\nShort\nName\nDescription\n\n\n\n-f\nnoglob\n禁止文件名展开（globbing）\n\n\n-i\ninteractive\n让脚本以 交互 模式运行\n\n\n-n\nnoexec\n读取命令，但不执行（语法检查）\n\n\n-t\n—\n执行完第一条命令后退出\n\n\n-v\nverbose\n在执行每条命令前，向stderr输出该命令\n\n\n-x\nxtrace\n在执行每条命令前，向stderr输出该命令以及该命令的扩展参数\n\n\n举个例子，如果我们在脚本中指定了-x例如：\n12345#!/bin/bash -xfor (( i = 0; i &lt; 3; i++ )); do  echo $idone\n\n这会向stdout打印出变量的值和一些其它有用的信息：\n123456789101112131415$ ./my_script+ (( i = 0 ))+ (( i &lt; 3 ))+ echo 00+ (( i++  ))+ (( i &lt; 3 ))+ echo 11+ (( i++  ))+ (( i &lt; 3 ))+ echo 22+ (( i++  ))+ (( i &lt; 3 ))\n\n有时我们值需要 debug 脚本的一部分。这种情况下，使用set命令会很方便。这个命令可以启用或禁用选项。使用-启用选项，+禁用选项：\n⌨️ 『示例源码』 debug-demo.sh\n123456789101112131415161718192021222324set -xfor (( i = 0; i &lt; 3; i++ )); do  printf $&#123;i&#125;doneset +xfor i in &#123;1..5&#125;; do printf $&#123;i&#125;; doneprintf &quot;\\n&quot;\n\n https://www.cnblogs.com/jingmoxukong/p/7867397.html\n","slug":"LINUX/shell 语言","date":"2022-01-26T18:01:10.000Z","categories_index":"shell,LINUX","tags_index":"https,demo,true","author_index":"dandeliono"},{"id":"83a96f58bfaa8c716d7a61d4a063cd85","title":"Zookeeper开源客户端ZKClient和Curator简介","content":"Zookeeper开源客户端ZKClient和Curator简介Zookeeper 客户端提供了基本的操作，比如，创建会话、创建节点、读取节点、更新数据、删除节点和检查节点是否存在等。但对于开发人员来说，Zookeeper 提供的基本操纵还是有一些不足之处。\nZookeeper API 不足之处\nZookeeper 的 Watcher 是一次性的，每次触发之后都需要重新进行注册； \nSession 超时之后没有实现重连机制； \n异常处理繁琐，Zookeeper 提供了很多异常，对于开发人员来说可能根本不知道该如何处理这些异常信息；\n只提供了简单的 byte[]数组的接口，没有提供针对对象级别的序列化； \n创建节点时如果节点存在抛出异常，需要自行检查节点是否存在； \n删除节点无法实现级联删除；\n\nZkClient 简介ZkClient 是一个开源客户端，在 Zookeeper 原生 API 接口的基础上进行了包装，更便于开发人员使用。内部实现了 Session 超时重连，Watcher 反复注册等功能。像 dubbo 等框架对其也进行了集成使用。\n虽然 ZkClient 对原生 API 进行了封装，但也有它自身的不足之处：\n\n几乎没有参考文档；\n异常处理简化（抛出 RuntimeException）；\n重试机制比较难用；\n没有提供各种使用场景的实现；\n\nCurator 简介Curator 是 Netflix 公司开源的一套 Zookeeper 客户端框架，和 ZkClient 一样，解决了非常底层的细节开发工作，包括连接重连、反复注册 Watcher 和 NodeExistsException 异常等。目前已经成为 Apache 的顶级项目。另外还提供了一套易用性和可读性更强的 Fluent 风格的客户端 API 框架。\n除此之外，Curator 中还提供了 Zookeeper 各种应用场景（Recipe，如共享锁服务、Master 选举机制和分布式计算器等）的抽象封装。 https://cloud.tencent.com/developer/article/1015372?from=article.detail.1050407\n","slug":"JAVA/Zookeeper开源客户端ZKClient和Curator简介","date":"2022-01-20T22:39:09.000Z","categories_index":"Zookeeper,JAVA","tags_index":"Curator,API,ZkClient","author_index":"dandeliono"},{"id":"2781e4b82126a200e2952c3286dd1c9d","title":"[Elasticsearch] 向已存在的索引中添加自定义filter analyzer","content":"[Elasticsearch] 向已存在的索引中添加自定义filter&#x2F;analyzer问题描述随着应用的不断升级，索引中的类型也会越来越多，新增加的类型中势必会使用到一些自定义的 Analyzer。但是通过_settings 端点的更新 API 不能直接在已经存在的索引上使用。在 sense 中进行更新时会抛出异常：\n12345678910111213141516171819202122232425262728PUT /symbol&#123;  &quot;settings&quot;: &#123;    &quot;analysis&quot;: &#123;      &quot;filter&quot;: &#123;        &quot;edgengram&quot;: &#123;           &quot;type&quot;: &quot;edgeNGram&quot;,           &quot;min_gram&quot;: &quot;1&quot;,           &quot;max_gram&quot;: &quot;255&quot; &#125;      &#125;,      &quot;analyzer&quot;: &#123;        &quot;symbol_analyzer&quot;: &#123;          &quot;type&quot;: &quot;custom&quot;,          &quot;char_filter&quot;: [],          &quot;tokenizer&quot;: &quot;standard&quot;,          &quot;filter&quot;: [ &quot;lowercase&quot;, &quot;word_delimiter&quot; ] &#125;,        &quot;back_edge_ngram_analyzer&quot;: &#123;          &quot;type&quot;: &quot;custom&quot;,          &quot;char_filter&quot;: [],          &quot;tokenizer&quot;: &quot;whitespace&quot;,          &quot;filter&quot;: [ &quot;reverse&quot;, &quot;edgengram&quot;, &quot;reverse&quot; ] &#125;      &#125;    &#125;  &#125;&#125;```上例中，我们希望向名为symbol的索引中添加一个filter和两个analyzers。但是会抛出如下的错误信息：\n\n{   “error”: “IndexAlreadyExistsException[[symbol] already exists]”,”status”: 400}```\n提示我们该索引已经存在了，无法添加。\n\n解决方案最直观的解决方案是首先备份该索引中已经存在的数据，然后删除它再重建该索引。这种方式比较暴力，当索引中已经存在相当多的数据时，不建议这样做。\n另外一种方案是使用_open 和_close 这一对端点，首先将目标索引关闭，执行需要的更新操作，然后再打开该索引。\n12345678910111213POST /symbol/_closePUT /symbol/_settings&#123;  &quot;settings&quot;: &#123;    ....      &#125;&#125;POST /symbol/_open```这样就避免了需要重建索引的麻烦。有了新添加的filter和analyzer，就可以根据需要再对types中的mappings进行更新了。  [https://blog.csdn.net/dm_vincent/article/details/46996021](https://blog.csdn.net/dm_vincent/article/details/46996021)\n","slug":"MIDDLEWARE/[Elasticsearch] 向已存在的索引中添加自定义filter analyzer","date":"2021-12-02T15:03:01.000Z","categories_index":"symbol,MIDDLEWARE","tags_index":"settings,filter,POST","author_index":"dandeliono"},{"id":"a94b0501505e8b946dbe5622c7a78b56","title":"Linux 命令行编辑快捷键","content":"Linux 命令行编辑快捷键Linux 命令行编辑快捷键初学者在 Linux 命令窗口（终端）敲命令时，肯定觉得通过输入一串一串的字符的方式来控制计算是效率很低。 但是 Linux 命令解释器（Shell）是有很多快捷键的，熟练掌握可以极大的提高操作效率。 下面列出最常用的快捷键，这还不是完全版。\n\n命令行快捷键：\n常用：\n\nCtrl L ：清屏\nCtrl M ：等效于回车\nCtrl C : 中断正在当前正在执行的程序\n\n\n历史命令：\n\nCtrl P : 上一条命令，可以一直按表示一直往前翻\nCtrl N : 下一条命令\nCtrl R，再按历史命令中出现过的字符串：按字符串寻找历史命令（重度推荐）\n\n\n命令行编辑：\n\nTab : 自动补齐（重度推荐）\nCtrl A ： 移动光标到命令行首\nCtrl E : 移动光标到命令行尾\nCtrl B : 光标后退\nCtrl F : 光标前进\nAlt F : 光标前进一个单词\nAlt B : 光标后退一格单词\nCtrl ] : 从当前光标往后搜索字符串，用于快速移动到该字符串\nCtrl Alt ] : 从当前光标往前搜索字符串，用于快速移动到该字符串\nCtrl H : 删除光标的前一个字符\nCtrl D : 删除当前光标所在字符\nCtrl K ：删除光标之后所有字符\nCtrl U : 清空当前键入的命令\nCtrl W : 删除光标前的单词 (Word, 不包含空格的字符串)\n**Ctrl \\ ** : 删除光标前的所有空白字符\nCtrl Y : 粘贴Ctrl W或Ctrl K删除的内容\nAlt . : 粘贴上一条命令的最后一个参数（很有用）\nAlt [0-9] Alt . 粘贴上一条命令的第[0-9]个参数\nAlt [0-9] Alt . Alt. 粘贴上上一条命令的第[0-9]个参数\nCtrl X Ctrl E : 调出系统默认编辑器编辑当前输入的命令，退出编辑器时，命令执行\n\n\n其他：   *   Ctrl Z : 把当前进程放到后台（之后可用’’fg’’命令回到前台）   *   Shift Insert : 粘贴（相当于Windows的Ctrl V）   *   在命令行窗口选中即复制   *   在命令行窗口中键即粘贴，可用Shift Insert代替   *   Ctrl PageUp : 屏幕输出向上翻页   *   Ctrl PageDown : 屏幕输出向下翻页https://gist.github.com/zhulianhua/befb8f61db8c72b4763d\n\n\n\n\n","slug":"LINUX/Linux 命令行编辑快捷键","date":"2021-11-26T16:59:24.000Z","categories_index":"Linux,LINUX","tags_index":"命令行编辑快捷键,初学者在,命令窗口","author_index":"dandeliono"},{"id":"f086d547e55f7aeb852f476ef6c33b8e","title":"Elasticsearch系统常见问题","content":"Elasticsearch系统常见问题如何查看 Es 安装了哪些插件可以使用下面这个 API，会列出每个节点安装的插件列表。\n线程池队列满导致错误在这种场景下 ES 抛出的异常是\n\n\n\n\n\n\n\n\n\nrejected execution of org.elasticsearch.transport.TransportService$4@c8998f4 \non EsThreadPoolExecutor[bulk, queue capacity = 50, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@553aee29\n[Running, pool size = 4, active threads = 4, queued tasks = 50, completed tasks = 0]]\n\nES 内部有很多线程池，比如 index，search，bulk 是我们能够看到的 3 个典型的线程池，如果系统的压力特别大，后台线程处理不过来的时候，用户发起的任务会在线程池的队列里堆积，如果达到队列的上限就会抛出对应的异常，遇到这种错误需要做以下两步：\n\n检查系统的 CPU 和 IO 的利用情况，如果系统的 IO 和 CPU 的利用率比较高，这说明系统遇到资源瓶颈了，已经不能通过优化系统的参数来避免这种错误发生了，云上的用户可以在 ES 的控制台查看 ES 的 CPU 利用率，也可以通过 ES 自带的命令来查看，通过下面这个命令可以看到 ES 各个节点的 CPU 利用率以及负载： GET &#x2F;_cat&#x2F;nodes?v\n\n如果资源没有问题，那么检查当先线程池的配置，比如上面这个错误就需要检查 bulk 的线程池的配置，在 sense 里执行以下命令： GET &#x2F;_cluster&#x2F;settings\n结果如下\n &quot;thread_pool&quot;: &#123;\n       &quot;bulk&quot;: &#123;\n          &quot;type&quot;: &quot;fixed&quot;,\n          &quot;size&quot;: &quot;4&quot;,\n          &quot;queue_size&quot;: &quot;50&quot;\n       &#125;\n    &#125;\n\n这个结果表示处理 bulk 任务的线程池有 4 个执行线程，队列数为 50. 根据我们的经验看，这个值还是比较小的，所以可以直接用以下操作处理：\n\n\nES 5.5.0 版本：\nPUT /_cluster/settings\n&#123;\n    &quot;persistent&quot;: &#123;\n        &quot;thread_pool.bulk.size&quot;: 32,\n        &quot;thread_pool.bulk.queue_size&quot;: 300\n    &#125;\n&#125;\n\nES 6.5.3+ 集群：\nPUT /_cluster/settings\n&#123;\n    &quot;persistent&quot;: &#123;\n        &quot;thread_pool.write.size&quot;: 32,\n        &quot;thread_pool.write.queue_size&quot;: 300\n    &#125;\n&#125;\n\nToo Many Open Files 的错误在 es 的日志中如果出现这个错误，一般都是打开的文件太多了，ES 建议文件句柄的限制至少为 65536 个，用户可以通过修改 &#x2F;etc&#x2F;security&#x2F;limits.conf 来修改，或者用 ulimit 这个命令来修改。 es 里每个 shard 都是一个单独的 lucene index writer，每个 shard 由多个 segment 组成，每个 segment 有多个文件，所以打开的文件的数目 &#x3D; shard 数目 segment 数目 每个 segment 包含的文件数量，所以我们建议一个物理机节点上 shard 的数目在 1000 个左右，不建议有太多的 shard。另外 lucene 使用 compound file 格式也能有效的减少每个 segment 里的文件的数量。\nEs 中一个分片一般设置多大ES 的每个分片（shard）都是 lucene 的一个 index，而 lucene 的一个 index 只能存储 20 亿个文档，所以一个分片也只能最多存储 20 亿个文档。 另外，我们也建议一个分片的大小在 10G-50G 之间，太大的话查询时会比较慢，另外在做副本修复的时，耗时比较多；分片太小的话，会导致一个索引的分片数目很多，查询时带来的 fanout 太大。\n当集群为 red 或者 yellow 的时候怎么办集群为 RED 表示集群中有 primary shard 没有分配，yellow 表示有 replica 没有分配，我们建议你用下面这个 API 来看 shard 为什么没有被分配到某个节点上。 GET _cluster&#x2F;allocation&#x2F;explain\n根据我们的使用经验，有以下几种情况导致 shard 没有被分配：\n\n没有节点上有存储空间能够放下这个 shard。\n如果 shard 是 replica，那么可能是 primary shard 未分配或者处于 initializing 状态。\n\n分片长时间处于未分配状态 ES 内部会对一个 unassigned 分片尝试 5 次进行分配, 失败后不再尝试进行分配，这时候需要调用进行手动控制集群处理 unassigned 分片：\nPOST /_cluster/reroute?retry_failed=true\n\n如何 cancel 掉慢查询用户发送一个查询可能导致一个集群非常慢，CPU 利用率非常高，所以用户有的时候想把占用资源非常多的查询 cancel 掉。在 ES 5.0 之后 es 提供了 cancel 查询的命令。 es 内部把所有的执行任务都封装成了 task，可以通过 task api 来查看一个节点在执行的 task 任务列表，也可以使用 task api 来取消 task。比如我们要查询所有在执行 search 类型的 task，可以使用如下 API： GET &#x2F;_tasks?actions&#x3D;*search\n取消所有在执行的 search 任务：\nPOST _tasks/_cancel?actions=*search\n\n更多的使用方式可以参考官方网站的介绍。\nPageCache 在查询中的作用很大我们建议如果条件可以的话应该给 ES 留尽量多的 pagecache，这能极大的优化我们的查询速度，如果 pagecache 不足够多，那么 ES 每次查询【fetch 文档，拿 posting list】都会读取磁盘，此时系统就会变慢。 用户可以使用 iostat 来查看一下系统的 IO 信息，也可在 GET _nodes&#x2F;state 返回的信息里搜索 “io_stats” 查看。如果 iops 比较高的话，说明系统的 io 比较高了，可能就是 pagecache 小的原因。\n禁用权限验证有的时候业务系统原有的 ES 服务没有权限验证，但是云上的 ES 服务是有权限验证的，当业务系统迁移的时候不希望改代码，那么可以先把权限验证关掉，这样就能平滑迁移了。操作的方式是：\nPUT /_cluster/settings\n&#123;\n    &quot;persistent&quot;: &#123;\n        &quot;simpleauth.enable&quot;:false\n    &#125;\n&#125;\n\n支持的 Client 的类型目前我们云上的产品只支持基于 http 的 restful api，不支持基于 tcp 的 transport client 这种 api。 这个设置主要原因是 transport client 跟集群运行的版本深度绑定，当集群升级的时候需要前端业务也跟着升级才可以。\nEs 是否支持 Spark 和 Hadoop 来写入或者读取数据支持，需要到 es 官方网站下载 es-hadoop 包放到 spark 或者 hadoop 中就可以用 spark 或者 hadoop 读写 es 了。\nJVM FULL GC 的几种情形Scroll 导致 FullGC\n一些用户使用 scroll 做分页查询或者用 scroll 导出数据的时候，经常把 scroll 的超时时间设置的比较长，比如设置为 1 天，在这种情况下 es 后端会为这个 scroll 一直保存对应的 search context，每个 search context 都对应了 lucene 的 searcher，此时 searcher 一直不释放导致 lucene merge 完的文件也不删除，一些 leafreader， fst 等都长期在 JVM 里导致最终随着 search context 越来越多导致了 FullGC。用户可以使用以下 2 个 API 来查看和清除这些 Context。\nGET /_nodes/stats/indices/search\nDELETE /_search/scroll/_all\n\n查询导致 FullGC\n用户在查询时将结果集的 from+size 设置的太大，比如 size&#x3D;Integer.MAX_VALUE 导致的，目前 ES 会根据设置的这个 from+size 开辟一个 priority queue，当并发量大时，内存会分配不过来这么多非常大的 queue，导致 FULL GC，甚至 OOM。\naggregation 导致 FullGC\n用户在执行类似 terms agg 时，如果不同的值非常多，最终会导致产生很多 bucket，比如几千万个 bucket，这些 bucket 也会在内存里，最终导致 fullgc。\n如何提升导入性能减少副本数，增加 refresh 间隔\nPUT /index_name/_settings\n&#123;\n      &quot;index.number_of_replicas&quot;: 0,\n         &quot;index.refresh_interval&quot;: &quot;10s&quot;\n&#125; \n\nES 的多副本机制在写入时会向多个副本都发送原始的 json 文档，然后在多个副本上分别进行分词，建立索引等操作。由于导入是 CPU 密集型操作，所以把 replica 数目改成 0，可以减少 CPU 使用率，当导入完毕后，把 replica 数目改回，这样就是直接拷贝物理文件了，速度会比较快。\nrefresh interal 是用来控制多久把内存里的数据刷出 segment 的，es 会对刷出的 segment 进行 merge，如果 merge 不过来 es 会阻止写入。所以把 refresh interval 调大，也可以把刷出的 segment 变大，降低 merge 的频率，提升导入性能。 增大 index 的导入速度限制\nPUT /_cluster/settings\n&#123;\n      &quot;persistent&quot; : &#123;\n             &quot;indices.store.throttle.max_bytes_per_sec&quot; : &quot;200mb&quot;\n      &#125;\n&#125;\n\nES 在写入数据的时候会有速度的限制，防止占用过多的磁盘 IO，如果集群的导入比较大而查询比较少，那么可以把这个速度限制调大。\n集群配置问题\n需要使用 oracle JDK 1.8 以上版本。\n\n设置最大文件数：\n 修改 /etc/security/limits.conf ： \n\n  *　　soft　　nofile　　65536\n\n  *　　hard　　nofile　　65536\n\n\n增加 mmap counts ： 修改 &#x2F;etc&#x2F;sysctl.conf ：\n vm.max_map_count=262144\n    \n        然后执行： sysctl -p\n\n\n\n集群重启问题在一些情况下（比如修改配置文件），我们希望重启集群，重启集群可以是一台台的重启，也可以是整个集群重启，重启 Es 的时候，可能会引起数据的重分布，下面就这两种情况分别介绍如何重启服务。\n整个集群重启\n\n把整个集群设置为只读状态\n PUT /_cluster/settings\n        &#123;\n            &quot;persistent&quot;: &#123;\n                &quot;cluster.blocks.read_only&quot;:true\n            &#125;\n        &#125;\n        12345678910111213141516171819202122232425-   把节点内存的数据全部 flush 到硬盘上-   把所有的 es 节点重启-   当集群 green 之后，把集群修改为可写入状态         PUT /_cluster/settings        \t&#123;        \t    &quot;persistent&quot;: &#123;        \t        &quot;cluster.blocks.read_only&quot;:false        \t    &#125;        \t&#125;    **一台台重启**这种方式重启服务不会中断，适用于线上服务。-   禁止分片分配，这样我们关闭一台 Es 服务的时候，分片不会重分布。 `PUT /_cluster/settings &#123;&quot;transient&quot; : &#123; &quot;cluster.routing.allocation.enable&quot; : &quot;none&quot;&#125; &#125;`-   关闭单个节点，修改配置或者替换 jar 包，启动节点-   开启分片重分布         PUT /_cluster/settings        \t&#123;            \t&quot;transient&quot; : &#123;                \t&quot;cluster.routing.allocation.enable&quot; : &quot;all&quot;            \t&#125;        \t&#125;\n\n\n等待集群 green 后，重复执行 1-3 步，直到所有的节点都修改完配置为止。\n\n\n禁用_field_names_field_names字段是 Elasticsearch 的内部的一个元数据字段。该字段会索引文档内的每个字段的名字 (除了字段值为 null 的字段名字); 这个字段存在的意义主要是执行 Elasticsarech exists query，Elasticsearch 对该字段只做了索引处理，没有存储该字段，6.3 版本以后该字段只会索引没有禁掉 doc_value 和 norms 的字段，建议业务不使用exists 查询的情况下 disable 该字段，能够少量的减少倒排表的占用的存储空间，可能会适当增强 pagecache 的利用\n PUT index\n &#123;\n   &quot;mappings&quot;: &#123;\n     &quot;_doc&quot;: &#123;\n       &quot;_field_names&quot;: &#123;\n         &quot;enabled&quot;: false\n       &#125;\n     &#125;\n   &#125;\n &#125;\n\n导入数据发现越来越慢的几种情形导入数据中包含 update\nEs 的 update 实际上是先读取数据然后更改后，再写入的，当写入的数据越来越多的时候，读取数据就会比较慢，写入也就逐渐变慢了。\n控制 index 在节点上的数量默认情况下，ES 集群会尽可能将所有节点上的 index 和 shard 的数量进行 balance，但是一些特殊情况下，可能会造成某一个 index 的 shard 过多的集中在少量的节点上，这时候可以通过设置集群中每个节点存放 index shard 的个数：\nPUT &#123;index名字&#125;/_settings\n&#123;\n   &quot;index.routing.allocation.total_shards_per_node&quot;: 10\n&#125;\n\n控制索引的分片数量和副本数量不修改参数的情况下，一个 index 一共有 5 个分片，2 个副本（包括主分片）, 可以通过修改 index 的参数来控制：\nPUT /&#123;index名字&#125;\n&#123;\n    &quot;settings&quot;: &#123;\n        &quot;number_of_shards&quot;: 20,\n        &quot;number_of_replicas&quot;: 2\n    &#125;\n&#125;\n\nnumber_of_shards: 分片个数，创建完index后不可修改，需在创建的时候指定\nnumber_of_replicas： 副本个数，不包括主分片\n\n当集群处于恢复状态的时候，恢复速度可能会比较慢当前正在恢复的索引分片可以通过\nGET /_recovery?active_only=true \n\n查看，默认情况一个节点同时恢复的个数为 4，包括 2 个作为 source 节点，2 个作为 target 节点，当分片个数非常多的时候可能会恢复的很慢，恢复的时候默认是有限速的最大 40mb, 这时候可以通过设置集群参数：\ncurl -XPUT &quot;host:port/_cluster/settings&quot; -d&#39;\n&#123;\n    &quot;transient&quot;: &#123;\n        &quot;cluster.routing.allocation.node_concurrent_recoveries&quot;: 8,\n        &quot;indices.recovery.max_bytes_per_sec&quot;: &quot;120mb&quot;\n    &#125;\n&#125;&#39;\n\nindices.recovery.max_bytes_per_sec： 节点恢复的最大带宽，这个设置应该小于当前网络带宽，避免影响其他网络服务\ncluster.routing.allocation.node_concurrent_recoveries： 节点作为source node或target node同时恢复的最大个数\n\n磁盘满了之后如何恢复当 Es 的 DataNode 的磁盘使用率达到一定的阈值（95%）之后，Es 会阻止继续写入，Es 会在所有的 Index 上加一个 block，当用户继续写入的时候会收到以下错误\ncluster_block_exception [FORBIDDEN/12/index read-only / allow delete (api)];\n\n此时用户必须释放磁盘空间才能解决问题，释放磁盘空间有 2 种办法：\n\n删除不用的 Index\n降低 Index 的副本的数目，比如把 replica 数目从 2 降到 1.\n\n当释放完毕磁盘空间之后，Es 并不会自动把 block 去掉，此时用户仍然无法写入数据，需要执行以下命令：\ncurl -XPUT &quot;host:port/_all/_settings&quot; -d &#39;\n&#123;\n    &quot;index.blocks.read_only_allow_delete&quot;: null\n&#125;&#39;\n\n https://cloud.baidu.com/doc/BES/s/Zjyzk4k1w\n","slug":"MIDDLEWARE/Elasticsearch系统常见问题","date":"2021-11-17T10:00:10.000Z","categories_index":"shard,MIDDLEWARE","tags_index":"search,index,segment","author_index":"dandeliono"},{"id":"96cd93f7633628e16d6fdcc58320aac4","title":"docker-compose部署flink集群","content":"docker-compose部署flink集群flink 集群部署\n拉取 flink 镜像\n\n12docker pull flink\n\n\n自定义创建目录（例如 &#x2F; usr&#x2F;local&#x2F;flink&#x2F;flink-docker）\n\n12mdkir /usr/local/flink/flink-docker\n\n\n在上述目录，创建文件 docker-compose.yml\n\n12345678910111213141516171819202122232425262728293031version: &quot;2.1&quot;services:  jobmanager:    image: flink    expose:      - &quot;6123&quot;    ports:      - &quot;8081:8081&quot;    command: jobmanager    environment:      - JOB_MANAGER_RPC_ADDRESS=jobmanager    volumes:      - /usr/local/flink/flink-docker/conf/job/flink-conf.yaml:/opt/flink/conf/flink-conf.yaml    restart: always   taskmanager:    image: flink    expose:      - &quot;6121&quot;      - &quot;6122&quot;    depends_on:      - jobmanager    command: taskmanager    links:      - &quot;jobmanager:jobmanager&quot;    environment:      - JOB_MANAGER_RPC_ADDRESS=jobmanager    volumes:      - /usr/local/flink/flink-docker/conf/task/flink-conf.yaml:/opt/flink/conf/flink-conf.yaml    restart: always\n\n\n自定义创建 flink 挂载配置目录（例如 &#x2F; usr&#x2F;local&#x2F;flink&#x2F;flink-docker&#x2F;conf）\n\n12mkdir /usr/local/flink/flink-docker/conf/job\n\n12mkdir /usr/local/flink/flink-docker/conf/task\n\n\n将 flink-conf.yaml 分别放入上述两个文件中\n\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258#################################################################################  Licensed to the Apache Software Foundation (ASF) under one#  or more contributor license agreements.  See the NOTICE file#  distributed with this work for additional information#  regarding copyright ownership.  The ASF licenses this file#  to you under the Apache License, Version 2.0 (the#  &quot;License&quot;); you may not use this file except in compliance#  with the License.  You may obtain a copy of the License at##      http://www.apache.org/licenses/LICENSE-2.0##  Unless required by applicable law or agreed to in writing, software#  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.#  See the License for the specific language governing permissions and# limitations under the License.#################################################################################==============================================================================# Common#==============================================================================# The external address of the host on which the JobManager runs and can be# reached by the TaskManagers and any clients which want to connect. This setting# is only used in Standalone mode and may be overwritten on the JobManager side# by specifying the --host &lt;hostname&gt; parameter of the bin/jobmanager.sh executable.# In high availability mode, if you use the bin/start-cluster.sh script and setup# the conf/masters file, this will be taken care of automatically. Yarn/Mesos# automatically configure the host name based on the hostname of the node where the# JobManager runs.jobmanager.rpc.address: jobmanager# The RPC port where the JobManager is reachable.jobmanager.rpc.port: 6123# The total process memory size for the JobManager.## Note this accounts for all memory usage within the JobManager process, including JVM metaspace and other overhead.jobmanager.memory.process.size: 4096m# The total process memory size for the TaskManager.## Note this accounts for all memory usage within the TaskManager process, including JVM metaspace and other overhead.taskmanager.memory.process.size: 16384m# To exclude JVM metaspace and overhead, please, use total Flink memory size instead of &#x27;taskmanager.memory.process.size&#x27;.# It is not recommended to set both &#x27;taskmanager.memory.process.size&#x27; and Flink memory.## taskmanager.memory.flink.size: 1280m# The number of task slots that each TaskManager offers. Each slot runs one parallel pipeline.taskmanager.numberOfTaskSlots: 10# The parallelism used for programs that did not specify and other parallelism.parallelism.default: 1# The default file system scheme and authority.# # By default file paths without scheme are interpreted relative to the local# root file system &#x27;file:///&#x27;. Use this to override the default and interpret# relative paths relative to a different file system,# for example &#x27;hdfs://mynamenode:12345&#x27;## fs.default-scheme#==============================================================================# High Availability#==============================================================================# The high-availability mode. Possible options are &#x27;NONE&#x27; or &#x27;zookeeper&#x27;.## high-availability: zookeeper# The path where metadata for master recovery is persisted. While ZooKeeper stores# the small ground truth for checkpoint and leader election, this location stores# the larger objects, like persisted dataflow graphs.# # Must be a durable file system that is accessible from all nodes# (like HDFS, S3, Ceph, nfs, ...) ## high-availability.storageDir: hdfs:///flink/ha/# The list of ZooKeeper quorum peers that coordinate the high-availability# setup. This must be a list of the form:# &quot;host1:clientPort,host2:clientPort,...&quot; (default clientPort: 2181)## high-availability.zookeeper.quorum: localhost:2181# ACL options are based on https://zookeeper.apache.org/doc/r3.1.2/zookeeperProgrammers.html#sc_BuiltinACLSchemes# It can be either &quot;creator&quot; (ZOO_CREATE_ALL_ACL) or &quot;open&quot; (ZOO_OPEN_ACL_UNSAFE)# The default value is &quot;open&quot; and it can be changed to &quot;creator&quot; if ZK security is enabled## high-availability.zookeeper.client.acl: open#==============================================================================# Fault tolerance and checkpointing#==============================================================================# The backend that will be used to store operator state checkpoints if# checkpointing is enabled.## Supported backends are &#x27;jobmanager&#x27;, &#x27;filesystem&#x27;, &#x27;rocksdb&#x27;, or the# &lt;class-name-of-factory&gt;.## state.backend: filesystem# Directory for checkpoints filesystem, when using any of the default bundled# state backends.## state.checkpoints.dir: hdfs://namenode-host:port/flink-checkpoints# Default target directory for savepoints, optional.##state.savepoints.dir: file:/tmp/savepoint# Flag to enable/disable incremental checkpoints for backends that# support incremental checkpoints (like the RocksDB state backend). ## state.backend.incremental: false# The failover strategy, i.e., how the job computation recovers from task failures.# Only restart tasks that may have been affected by the task failure, which typically includes# downstream tasks and potentially upstream tasks if their produced data is no longer available for consumption.jobmanager.execution.failover-strategy: region#==============================================================================# Rest &amp; web frontend#==============================================================================# The port to which the REST client connects to. If rest.bind-port has# not been specified, then the server will bind to this port as well.##rest.port: 8081# The address to which the REST client will connect to##rest.address: 0.0.0.0# Port range for the REST and web server to bind to.##rest.bind-port: 8080-8090# The address that the REST &amp; web server binds to##rest.bind-address: 0.0.0.0# Flag to specify whether job submission is enabled from the web-based# runtime monitor. Uncomment to disable.#web.submit.enable: false#==============================================================================# Advanced#==============================================================================# Override the directories for temporary files. If not specified, the# system-specific Java temporary directory (java.io.tmpdir property) is taken.## For framework setups on Yarn or Mesos, Flink will automatically pick up the# containers&#x27; temp directories without any need for configuration.## Add a delimited list for multiple directories, using the system directory# delimiter (colon &#x27;:&#x27; on unix) or a comma, e.g.:#     /data1/tmp:/data2/tmp:/data3/tmp## Note: Each directory entry is read from and written to by a different I/O# thread. You can include the same directory multiple times in order to create# multiple I/O threads against that directory. This is for example relevant for# high-throughput RAIDs.## io.tmp.dirs: /tmp# The classloading resolve order. Possible values are &#x27;child-first&#x27; (Flink&#x27;s default)# and &#x27;parent-first&#x27; (Java&#x27;s default).## Child first classloading allows users to use different dependency/library# versions in their application than those in the classpath. Switching back# to &#x27;parent-first&#x27; may help with debugging dependency issues.## classloader.resolve-order: child-first# The amount of memory going to the network stack. These numbers usually need # no tuning. Adjusting them may be necessary in case of an &quot;Insufficient number# of network buffers&quot; error. The default min is 64MB, the default max is 1GB.# # taskmanager.memory.network.fraction: 0.1# taskmanager.memory.network.min: 64mb# taskmanager.memory.network.max: 1gb#==============================================================================# Flink Cluster Security Configuration#==============================================================================# Kerberos authentication for various components - Hadoop, ZooKeeper, and connectors -# may be enabled in four steps:# 1. configure the local krb5.conf file# 2. provide Kerberos credentials (either a keytab or a ticket cache w/ kinit)# 3. make the credentials available to various JAAS login contexts# 4. configure the connector to use JAAS/SASL# The below configure how Kerberos credentials are provided. A keytab will be used instead of# a ticket cache if the keytab path and principal are set.# security.kerberos.login.use-ticket-cache: true# security.kerberos.login.keytab: /path/to/kerberos/keytab# security.kerberos.login.principal: flink-user# The configuration below defines which JAAS login contexts# security.kerberos.login.contexts: Client,KafkaClient#==============================================================================# ZK Security Configuration#==============================================================================# Below configurations are applicable if ZK ensemble is configured for security# Override below configuration to provide custom ZK service name if configured# zookeeper.sasl.service-name: zookeeper# The configuration below must match one of the values set in &quot;security.kerberos.login.contexts&quot;# zookeeper.sasl.login-context-name: Client#==============================================================================# HistoryServer#==============================================================================# The HistoryServer is started and stopped via bin/historyserver.sh (start|stop)# Directory to upload completed jobs to. Add this directory to the list of# monitored directories of the HistoryServer as well (see below).#jobmanager.archive.fs.dir: hdfs:///completed-jobs/# The address under which the web-based HistoryServer listens.#historyserver.web.address: 0.0.0.0# The port under which the web-based HistoryServer listens.#historyserver.web.port: 8082# Comma separated list of directories to monitor for completed jobs.#historyserver.archive.fs.dir: hdfs:///completed-jobs/# Interval in milliseconds for refreshing the monitored directories.#historyserver.archive.fs.refresh-interval: 10000blob.server.port: 6124query.server.port: 6125\n\n\n切换到之前自定义的 docker-compose 目录\n\n12cd /usr/local/flink/flink-docker\n\n\n拉起 flink 集群\n\n12docker-compose up -d\n\n\n查看日志, 确定是否启动成功\n\n12docker-compose logs\n\n\n若想配置多个 task-manager\n\n12docker-compose scale taskmanager=2\n\n\n都配置好后，就可以访问 ip:8081 进行查看了\n\n","slug":"OCI/docker-compose部署flink集群","date":"2021-11-13T11:41:36.000Z","categories_index":"flink,OCI","tags_index":"docker,compose,集群","author_index":"dandeliono"},{"id":"bec62911910e4566d1eb24b159beea23","title":"如何优雅地记录操作日志？","content":"如何优雅地记录操作日志？操作日志几乎存在于每个系统中，而这些系统都有记录操作日志的一套 API。操作日志和系统日志不一样，操作日志必须要做到简单易懂。所以如何让操作日志不和业务逻辑耦合，如何让操作日志的内容易于理解，让操作日志的接入更加简单？上面这些都是本文要回答的问题，主要围绕着如何 “优雅” 地记录操作日志展开描述。\n1. 操作日志的使用场景\n例子\n系统日志和操作日志的区别\n系统日志：系统日志主要是为开发排查问题提供依据，一般打印在日志文件中；系统日志的可读性要求没那么高，日志中会包含代码的信息，比如在某个类的某一行打印了一个日志。\n操作日志：主要是对某个对象进行新增操作或者修改操作后记录下这个新增或者修改，操作日志要求可读性比较强，因为它主要是给用户看的，比如订单的物流信息，用户需要知道在什么时间发生了什么事情。再比如，客服对工单的处理记录信息。\n操作日志的记录格式大概分为下面几种： * 单纯的文字记录，比如：2021-09-16 10:00 订单创建。 * 简单的动态的文本记录，比如：2021-09-16 10:00 订单创建，订单号：NO.11089999，其中涉及变量订单号 “NO.11089999”。 * 修改类型的文本，包含修改前和修改后的值，比如：2021-09-16 10:00 用户小明修改了订单的配送地址：从 “金灿灿小区” 修改到 “银盏盏小区” ，其中涉及变量配送的原地址“金灿灿小区” 和新地址“银盏盏小区”。 * 修改表单，一次会修改多个字段。\n2. 实现方式2.1 使用 Canal 监听数据库记录操作日志Canal 是一款基于 MySQL 数据库增量日志解析，提供增量数据订阅和消费的开源组件，通过采用监听数据库 Binlog 的方式，这样可以从底层知道是哪些数据做了修改，然后根据更改的数据记录操作日志。\n这种方式的优点是和业务逻辑完全分离。缺点也很明显，局限性太高，只能针对数据库的更改做操作日志记录，如果修改涉及到其他团队的 RPC 的调用，就没办法监听数据库了，举个例子：给用户发送通知，通知服务一般都是公司内部的公共组件，这时候只能在调用 RPC 的时候手工记录发送通知的操作日志了。\n2.2 通过日志文件的方式记录123log.info(&quot;订单创建&quot;)log.info(&quot;订单已经创建，订单编号:&#123;&#125;&quot;, orderNo)log.info(&quot;修改了订单的配送地址：从“&#123;&#125;”修改到“&#123;&#125;”， &quot;金灿灿小区&quot;, &quot;银盏盏小区&quot;)\n\n这种方式的操作记录需要解决三个问题。\n问题一：操作人如何记录\n借助 SLF4J 中的 MDC 工具类，把操作人放在日志中，然后在日志中统一打印出来。首先在用户的拦截器中把用户的标识 Put 到 MDC 中。\n12345678910111213141516@Componentpublic class UserInterceptor extends HandlerInterceptorAdapter &#123;  @Override  public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123;        String userNo = getUserNo(request);        MDC.put(&quot;userId&quot;, userNo);    return super.preHandle(request, response, handler);  &#125;  private String getUserNo(HttpServletRequest request) &#123;        return null;  &#125;&#125;\n\n其次，把 userId 格式化到日志中，使用 %X{userId} 可以取到 MDC 中用户标识。\n1&lt;pattern&gt;&quot;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; %t %-5level %X&#123;userId&#125; %logger&#123;30&#125;.%method:%L - %msg%n&quot;&lt;/pattern&gt;\n\n问题二：操作日志如何和系统日志区分开\n通过配置 Log 的配置文件，把有关操作日志的 Log 单独放到一日志文件中。\n12345678910111213141516171819202122232425//不同业务日志记录到不同的文件&lt;appender name=&quot;businessLogAppender&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt;    &lt;File&gt;logs/business.log&lt;/File&gt;    &lt;append&gt;true&lt;/append&gt;    &lt;filter class=&quot;ch.qos.logback.classic.filter.LevelFilter&quot;&gt;        &lt;level&gt;INFO&lt;/level&gt;        &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt;        &lt;onMismatch&gt;DENY&lt;/onMismatch&gt;    &lt;/filter&gt;    &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt;        &lt;fileNamePattern&gt;logs/业务A.%d.%i.log&lt;/fileNamePattern&gt;        &lt;maxHistory&gt;90&lt;/maxHistory&gt;        &lt;timeBasedFileNamingAndTriggeringPolicy class=&quot;ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP&quot;&gt;            &lt;maxFileSize&gt;10MB&lt;/maxFileSize&gt;        &lt;/timeBasedFileNamingAndTriggeringPolicy&gt;    &lt;/rollingPolicy&gt;    &lt;encoder&gt;        &lt;pattern&gt;&quot;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; %t %-5level %X&#123;userId&#125; %logger&#123;30&#125;.%method:%L - %msg%n&quot;&lt;/pattern&gt;        &lt;charset&gt;UTF-8&lt;/charset&gt;    &lt;/encoder&gt;&lt;/appender&gt;        &lt;logger name=&quot;businessLog&quot; additivity=&quot;false&quot; level=&quot;INFO&quot;&gt;    &lt;appender-ref ref=&quot;businessLogAppender&quot;/&gt;&lt;/logger&gt;\n\n然后在 Java 代码中单独的记录业务日志。\n12345private final Logger businessLog = LoggerFactory.getLogger(&quot;businessLog&quot;); businessLog.info(&quot;修改了配送地址&quot;);\n\n问题三：如何生成可读懂的日志文案\n可以采用 LogUtil 的方式，也可以采用切面的方式生成日志模板，后续内容将会进行介绍。这样就可以把日志单独保存在一个文件中，然后通过日志收集可以把日志保存在 Elasticsearch 或者数据库中，接下来看下如何生成可读的操作日志。\n2.3 通过 LogUtil 的方式记录日志1234LogUtil.log(orderNo, &quot;订单创建&quot;, &quot;小明&quot;)模板LogUtil.log(orderNo, &quot;订单创建，订单号&quot;+&quot;NO.11089999&quot;,  &quot;小明&quot;)String template = &quot;用户%s修改了订单的配送地址：从“%s”修改到“%s”&quot;LogUtil.log(orderNo, String.format(tempalte, &quot;小明&quot;, &quot;金灿灿小区&quot;, &quot;银盏盏小区&quot;),  &quot;小明&quot;)\n\n\n\n\n\n\n\n\n\n\n这里解释下为什么记录操作日志的时候都绑定了一个 OrderNo，因为操作日志记录的是：某一个 “时间”“谁” 对“什么”做了什么 “事情”。当查询业务的操作日志的时候，会查询针对这个订单的的所有操作，所以代码中加上了 OrderNo，记录操作日志的时候需要记录下操作人，所以传了操作人“小明” 进来。\n上面看起来问题并不大，在修改地址的业务逻辑方法中使用一行代码记录了操作日志，接下来再看一个更复杂的例子：\n12345678910111213private OnesIssueDO updateAddress(updateDeliveryRequest request) &#123;    DeliveryOrder deliveryOrder = deliveryQueryService.queryOldAddress(request.getDeliveryOrderNo());        doUpdate(request);    String logContent = getLogContent(request, deliveryOrder);    LogUtils.logRecord(request.getOrderNo(), logContent, request.getOperator);    return onesIssueDO;&#125;private String getLogContent(updateDeliveryRequest request, DeliveryOrder deliveryOrder) &#123;    String template = &quot;用户%s修改了订单的配送地址：从“%s”修改到“%s”&quot;;    return String.format(tempalte, request.getUserName(), deliveryOrder.getAddress(), request.getAddress);&#125;\n\n可以看到上面的例子使用了两个方法代码，外加一个 getLogContent 的函数实现了操作日志的记录。当业务变得复杂后，记录操作日志放在业务代码中会导致业务的逻辑比较繁杂，最后导致 LogUtils.logRecord() 方法的调用存在于很多业务的代码中，而且类似 getLogContent() 这样的方法也散落在各个业务类中，对于代码的可读性和可维护性来说是一个灾难。下面介绍下如何避免这个灾难。\n2.4 方法注解实现操作日志为了解决上面问题，一般采用 AOP 的方式记录日志，让操作日志和业务逻辑解耦，接下来看一个简单的 AOP 日志的例子。\n12345@LogRecord(content=&quot;修改了配送地址&quot;)public void modifyAddress(updateDeliveryRequest request)&#123;        doUpdate(request);&#125;\n\n我们可以在注解的操作日志上记录固定文案，这样业务逻辑和业务代码可以做到解耦，让我们的业务代码变得纯净起来。可能有同学注意到，上面的方式虽然解耦了操作日志的代码，但是记录的文案并不符合我们的预期，文案是静态的，没有包含动态的文案，因为我们需要记录的操作日志是： 用户 %s 修改了订单的配送地址，从 “%s” 修改到“%s”。接下来，我们介绍一下如何优雅地使用 AOP 生成动态的操作日志。\n3. 优雅地支持 AOP 生成动态的操作日志3.1 动态模板一提到动态模板，就会涉及到让变量通过占位符的方式解析模板，从而达到通过注解记录操作日志的目的。模板解析的方式有很多种，这里使用了 SpEL（Spring Expression Language，Spring 表达式语言）来实现。我们可以先写下期望的记录日志的方式，然后再看下能否实现这样的功能。\n12345@LogRecord(content = &quot;修改了订单的配送地址：从“#oldAddress”, 修改到“#request.address”&quot;)public void modifyAddress(updateDeliveryRequest request, String oldAddress)&#123;        doUpdate(request);&#125;\n\n通过 SpEL 表达式引用方法上的参数，可以让变量填充到模板中达到动态的操作日志文本内容。 但是现在还有几个问题需要解决： * 操作日志需要知道是哪个操作人修改的订单配送地址。 * 修改订单配送地址的操作日志需要绑定在配送的订单上，从而可以根据配送订单号查询出对这个配送订单的所有操作。 * 为了在注解上记录之前的配送地址是什么，在方法签名上添加了一个和业务无关的 oldAddress 的变量，这样就不优雅了。\n为了解决前两个问题，我们需要把期望的操作日志使用形式改成下面的方式：\n1234567@LogRecord(     content = &quot;修改了订单的配送地址：从“#oldAddress”, 修改到“#request.address”&quot;,     operator = &quot;#request.userName&quot;, bizNo=&quot;#request.deliveryOrderNo&quot;)public void modifyAddress(updateDeliveryRequest request, String oldAddress)&#123;        doUpdate(request);&#125;\n\n修改后的代码在注解上添加两个参数，一个是操作人，一个是操作日志需要绑定的对象。但是，在普通的 Web 应用中用户信息都是保存在一个线程上下文的静态方法中，所以 operator 一般是这样的写法（假定获取当前登陆用户的方式是 UserContext.getCurrentUser()）。\n1operator = &quot;#&#123;T(com.meituan.user.UserContext).getCurrentUser()&#125;&quot;\n\n这样的话，每个 @LogRecord 的注解上的操作人都是这么长一串。为了避免过多的重复代码，我们可以把注解上的 operator 参数设置为非必填，这样用户可以填写操作人。但是，如果用户不填写我们就取 UserContext 的 user（下文会介绍如何取 user ）。最后，最简单的日志变成了下面的形式：\n123456@LogRecord(content = &quot;修改了订单的配送地址：从“#oldAddress”, 修改到“#request.address”&quot;,            bizNo=&quot;#request.deliveryOrderNo&quot;)public void modifyAddress(updateDeliveryRequest request, String oldAddress)&#123;        doUpdate(request);&#125;\n\n接下来，我们需要解决第三个问题：为了记录业务操作记录添加了一个 oldAddress 变量，不管怎么样这都不是一个好的实现方式，所以接下来，我们需要把 oldAddress 变量从修改地址的方法签名上去掉。但是操作日志确实需要 oldAddress 变量，怎么办呢？\n要么和产品经理 PK 一下，让产品经理把文案从 “修改了订单的配送地址：从 xx 修改到 yy” 改为 “修改了订单的配送地址为：yy”。但是从用户体验上来看，第一种文案更人性化一些，显然我们不会 PK 成功的。那么我们就必须要把这个 oldAddress 查询出来然后供操作日志使用了。还有一种解决办法是：把这个参数放到操作日志的线程上下文中，供注解上的模板使用。我们按照这个思路再改下操作日志的实现代码。\n12345678@LogRecord(content = &quot;修改了订单的配送地址：从“#oldAddress”, 修改到“#request.address”&quot;,        bizNo=&quot;#request.deliveryOrderNo&quot;)public void modifyAddress(updateDeliveryRequest request)&#123;        LogRecordContext.putVariable(&quot;oldAddress&quot;, DeliveryService.queryOldAddress(request.getDeliveryOrderNo()));        doUpdate(request);&#125;\n\n这时候可以看到，LogRecordContext 解决了操作日志模板上使用方法参数以外变量的问题，同时避免了为了记录操作日志修改方法签名的设计。虽然已经比之前的代码好了些，但是依然需要在业务代码里面加了一行业务逻辑无关的代码，如果有 “强迫症” 的同学还可以继续往下看，接下来我们会讲解自定义函数的解决方案。下面再看另一个例子：\n12345678@LogRecord(content = &quot;修改了订单的配送员：从“#oldDeliveryUserId”, 修改到“#request.userId”&quot;,        bizNo=&quot;#request.deliveryOrderNo&quot;)public void modifyAddress(updateDeliveryRequest request)&#123;        LogRecordContext.putVariable(&quot;oldDeliveryUserId&quot;, DeliveryService.queryOldDeliveryUserId(request.getDeliveryOrderNo()));        doUpdate(request);&#125;\n\n这个操作日志的模板最后记录的内容是这样的格式：修改了订单的配送员：从 “10090”，修改到 “10099”，显然用户看到这样的操作日志是不明白的。用户对于用户 ID 是 10090 还是 10099 并不了解，用户期望看到的是：修改了订单的配送员：从 “张三（18910008888）”，修改到 “小明（13910006666）”。用户关心的是配送员的姓名和电话。但是我们方法中传递的参数只有配送员的 ID，没有配送员的姓名可电话。我们可以通过上面的方法，把用户的姓名和电话查询出来，然后通过 LogRecordContext 实现。\n但是，“强迫症” 是不期望操作日志的代码嵌入在业务逻辑中的。接下来，我们考虑另一种实现方式：自定义函数。如果我们可以通过自定义函数把用户 ID 转换为用户姓名和电话，那么就能解决这一问题，按照这个思路，我们把模板修改为下面的形式：\n12345678@LogRecord(content = &quot;修改了订单的配送员：从“&#123;deliveryUser&#123;#oldDeliveryUserId&#125;&#125;”, 修改到“&#123;deveryUser&#123;#request.userId&#125;&#125;”&quot;,        bizNo=&quot;#request.deliveryOrderNo&quot;)public void modifyAddress(updateDeliveryRequest request)&#123;        LogRecordContext.putVariable(&quot;oldDeliveryUserId&quot;, DeliveryService.queryOldDeliveryUserId(request.getDeliveryOrderNo()));        doUpdate(request);&#125;\n\n其中 deliveryUser 是自定义函数，使用大括号把 Spring 的 SpEL 表达式包裹起来，这样做的好处：一是把 SpEL（Spring Expression Language，Spring 表达式语言）和自定义函数区分开便于解析；二是如果模板中不需要 SpEL 表达式解析可以容易的识别出来，减少 SpEL 的解析提高性能。这时候我们发现上面代码还可以优化成下面的形式：\n123456@LogRecord(content = &quot;修改了订单的配送员：从“&#123;queryOldUser&#123;#request.deliveryOrderNo()&#125;&#125;”, 修改到“&#123;deveryUser&#123;#request.userId&#125;&#125;”&quot;,        bizNo=&quot;#request.deliveryOrderNo&quot;)public void modifyAddress(updateDeliveryRequest request)&#123;        doUpdate(request);&#125;\n\n这样就不需要在 modifyAddress 方法中通过 LogRecordContext.putVariable() 设置老的快递员了，通过直接新加一个自定义函数 queryOldUser() 参数把派送订单传递进去，就能查到之前的配送人了，只需要让方法的解析在 modifyAddress() 方法执行之前运行。这样的话，我们让业务代码又变得纯净了起来，同时也让 “强迫症” 不再感到难受了。\n4. 代码实现解析4.1 代码结构\n上面的操作日志主要是通过一个 AOP 拦截器实现的，整体主要分为 AOP 模块、日志解析模块、日志保存模块、Starter 模块；组件提供了 4 个扩展点，分别是：自定义函数、默认处理人、业务保存和查询；业务可以根据自己的业务特性定制符合自己业务的逻辑。\n4.2 模块介绍有了上面的分析，已经得出一种我们期望的操作日志记录的方式，那么接下来看看如何实现上面的逻辑。实现主要分为下面几个步骤： * AOP 拦截逻辑 * 解析逻辑 * 模板解析 * LogContext 逻辑 * 默认的 operator 逻辑 * 自定义函数逻辑 * 默认的日志持久化逻辑 * Starter 封装逻辑\n4.2.1 AOP 拦截逻辑这块逻辑主要是一个拦截器，针对 @LogRecord 注解分析出需要记录的操作日志，然后把操作日志持久化，这里把注解命名为 @LogRecordAnnotation。接下来，我们看下注解的定义：\n12345678910111213141516171819@Target(&#123;ElementType.METHOD&#125;)@Retention(RetentionPolicy.RUNTIME)@Inherited@Documentedpublic @interface LogRecordAnnotation &#123;    String success();    String fail() default &quot;&quot;;    String operator() default &quot;&quot;;    String bizNo();    String category() default &quot;&quot;;    String detail() default &quot;&quot;;    String condition() default &quot;&quot;;&#125;\n\n注解中除了上面提到参数外，还增加了 fail、category、detail、condition 等参数，这几个参数是为了满足特定的场景，后面还会给出具体的例子。\n\n\n\n参数名\n描述\n是否必填\n\n\n\nsuccess\n操作日志的文本模板\n是\n\n\nfail\n操作日志失败的文本版本\n否\n\n\noperator\n操作日志的执行人\n否\n\n\nbizNo\n操作日志绑定的业务对象标识\n是\n\n\ncategory\n操作日志的种类\n否\n\n\ndetail\n扩展参数，记录操作日志的修改详情\n否\n\n\ncondition\n记录日志的条件\n否\n\n\n为了保持简单，组件的必填参数就两个。业务中的 AOP 逻辑大部分是使用 @Aspect 注解实现的，但是基于注解的 AOP 在 Spring boot 1.5 中兼容性是有问题的，组件为了兼容 Spring boot1.5 的版本我们手工实现 Spring 的 AOP 逻辑。\n\n切面选择 AbstractBeanFactoryPointcutAdvisor 实现，切点是通过 StaticMethodMatcherPointcut 匹配包含 LogRecordAnnotation 注解的方法。通过实现 MethodInterceptor 接口实现操作日志的增强逻辑。\n下面是拦截器的切点逻辑：\n1234567891011121314public class LogRecordPointcut extends StaticMethodMatcherPointcut implements Serializable &#123;        private LogRecordOperationSource logRecordOperationSource;        @Override    public boolean matches(@NonNull Method method, @NonNull Class&lt;?&gt; targetClass) &#123;                  return !CollectionUtils.isEmpty(logRecordOperationSource.computeLogRecordOperations(method, targetClass));    &#125;    void setLogRecordOperationSource(LogRecordOperationSource logRecordOperationSource) &#123;        this.logRecordOperationSource = logRecordOperationSource;    &#125;&#125;\n\n切面的增强逻辑主要代码如下：\n12345678910111213141516171819202122232425262728293031323334353637383940414243@Overridepublic Object invoke(MethodInvocation invocation) throws Throwable &#123;    Method method = invocation.getMethod();        return execute(invocation, invocation.getThis(), method, invocation.getArguments());&#125;private Object execute(MethodInvocation invoker, Object target, Method method, Object[] args) throws Throwable &#123;    Class&lt;?&gt; targetClass = getTargetClass(target);    Object ret = null;    MethodExecuteResult methodExecuteResult = new MethodExecuteResult(true, null, &quot;&quot;);    LogRecordContext.putEmptySpan();    Collection&lt;LogRecordOps&gt; operations = new ArrayList&lt;&gt;();    Map&lt;String, String&gt; functionNameAndReturnMap = new HashMap&lt;&gt;();    try &#123;        operations = logRecordOperationSource.computeLogRecordOperations(method, targetClass);        List&lt;String&gt; spElTemplates = getBeforeExecuteFunctionTemplate(operations);                functionNameAndReturnMap = processBeforeExecuteFunctionTemplate(spElTemplates, targetClass, method, args);    &#125; catch (Exception e) &#123;        log.error(&quot;log record parse before function exception&quot;, e);    &#125;    try &#123;        ret = invoker.proceed();    &#125; catch (Exception e) &#123;        methodExecuteResult = new MethodExecuteResult(false, e, e.getMessage());    &#125;    try &#123;        if (!CollectionUtils.isEmpty(operations)) &#123;            recordExecute(ret, method, args, operations, targetClass,                    methodExecuteResult.isSuccess(), methodExecuteResult.getErrorMsg(), functionNameAndReturnMap);        &#125;    &#125; catch (Exception t) &#123;                log.error(&quot;log record parse exception&quot;, t);    &#125; finally &#123;        LogRecordContext.clear();    &#125;    if (methodExecuteResult.throwable != null) &#123;        throw methodExecuteResult.throwable;    &#125;    return ret;&#125;\n\n拦截逻辑的流程：\n\n可以看到，操作日志的记录持久化是在方法执行完之后执行的，当方法抛出异常之后会先捕获异常，等操作日志持久化完成后再抛出异常。在业务的方法执行之前，会对提前解析的自定义函数求值，解决了前面提到的需要查询修改之前的内容。\n4.2.2 解析逻辑模板解析\nSpring 3 提供了一个非常强大的功能：Spring EL，SpEL 在 Spring 产品中是作为表达式求值的核心基础模块，它本身是可以脱离 Spring 独立使用的。举个例子：\n1234567public static void main(String[] args) &#123;        SpelExpressionParser parser = new SpelExpressionParser();        Expression expression = parser.parseExpression(&quot;#root.purchaseName&quot;);        Order order = new Order();        order.setPurchaseName(&quot;张三&quot;);        System.out.println(expression.getValue(order));&#125;\n\n这个方法将打印 “张三”。LogRecord 解析的类图如下：\n\n解析核心类：LogRecordValueParser 里面封装了自定义函数和 SpEL 解析类 LogRecordExpressionEvaluator。\n12345678910public class LogRecordExpressionEvaluator extends CachedExpressionEvaluator &#123;    private Map&lt;ExpressionKey, Expression&gt; expressionCache = new ConcurrentHashMap&lt;&gt;(64);    private final Map&lt;AnnotatedElementKey, Method&gt; targetMethodCache = new ConcurrentHashMap&lt;&gt;(64);    public String parseExpression(String conditionExpression, AnnotatedElementKey methodKey, EvaluationContext evalContext) &#123;        return getExpression(this.expressionCache, methodKey, conditionExpression).getValue(evalContext, String.class);    &#125;&#125;\n\nLogRecordExpressionEvaluator 继承自 CachedExpressionEvaluator 类，这个类里面有两个 Map，一个是 expressionCache 一个是 targetMethodCache。在上面的例子中可以看到，SpEL 会解析成一个 Expression 表达式，然后根据传入的 Object 获取到对应的值，所以 expressionCache 是为了缓存方法、表达式和 SpEL 的 Expression 的对应关系，让方法注解上添加的 SpEL 表达式只解析一次。 下面的 targetMethodCache 是为了缓存传入到 Expression 表达式的 Object。核心的解析逻辑是上面最后一行代码。\n1getExpression(this.expressionCache, methodKey, conditionExpression).getValue(evalContext, String.class);\n\ngetExpression 方法会从 expressionCache 中获取到 @LogRecordAnnotation 注解上的表达式的解析 Expression 的实例，然后调用 getValue 方法，getValue 传入一个 evalContext 就是类似上面例子中的 order 对象。其中 Context 的实现将会在下文介绍。\n日志上下文实现\n下面的例子把变量放到了 LogRecordContext 中，然后 SpEL 表达式就可以顺利的解析方法上不存在的参数了，通过上面的 SpEL 的例子可以看出，要把方法的参数和 LogRecordContext 中的变量都放到 SpEL 的 getValue 方法的 Object 中才可以顺利的解析表达式的值。下面看下如何实现：\n12345678@LogRecord(content = &quot;修改了订单的配送员：从“&#123;deveryUser&#123;#oldDeliveryUserId&#125;&#125;”, 修改到“&#123;deveryUser&#123;#request.getUserId()&#125;&#125;”&quot;,            bizNo=&quot;#request.getDeliveryOrderNo()&quot;)public void modifyAddress(updateDeliveryRequest request)&#123;        LogRecordContext.putVariable(&quot;oldDeliveryUserId&quot;, DeliveryService.queryOldDeliveryUserId(request.getDeliveryOrderNo()));        doUpdate(request);&#125;\n\n在 LogRecordValueParser 中创建了一个 EvaluationContext，用来给 SpEL 解析方法参数和 Context 中的变量。相关代码如下：\n123EvaluationContext evaluationContext = expressionEvaluator.createEvaluationContext(method, args, targetClass, ret, errorMsg, beanFactory);\n\n在解析的时候调用 getValue 方法传入的参数 evalContext，就是上面这个 EvaluationContext 对象。下面是 LogRecordEvaluationContext 对象的继承体系：\n\nLogRecordEvaluationContext 做了三个事情： * 把方法的参数都放到 SpEL 解析的 RootObject 中。 * 把 LogRecordContext 中的变量都放到 RootObject 中。 * 把方法的返回值和 ErrorMsg 都放到 RootObject 中。\nLogRecordEvaluationContext 的代码如下：\n123456789101112131415161718public class LogRecordEvaluationContext extends MethodBasedEvaluationContext &#123;    public LogRecordEvaluationContext(Object rootObject, Method method, Object[] arguments,                                      ParameterNameDiscoverer parameterNameDiscoverer, Object ret, String errorMsg) &#123;              super(rootObject, method, arguments, parameterNameDiscoverer);               Map&lt;String, Object&gt; variables = LogRecordContext.getVariables();        if (variables != null &amp;&amp; variables.size() &gt; 0) &#123;            for (Map.Entry&lt;String, Object&gt; entry : variables.entrySet()) &#123;                setVariable(entry.getKey(), entry.getValue());            &#125;        &#125;                setVariable(&quot;_ret&quot;, ret);        setVariable(&quot;_errorMsg&quot;, errorMsg);    &#125;&#125;\n\n下面是 LogRecordContext 的实现，这个类里面通过一个 ThreadLocal 变量保持了一个栈，栈里面是个 Map，Map 对应了变量的名称和变量的值。\n12345public class LogRecordContext &#123;    private static final InheritableThreadLocal&lt;Stack&lt;Map&lt;String, Object&gt;&gt;&gt; variableMapStack = new InheritableThreadLocal&lt;&gt;();   &#125;\n\n上面使用了 InheritableThreadLocal，所以在线程池的场景下使用 LogRecordContext 会出现问题，如果支持线程池可以使用阿里巴巴开源的 TTL 框架。那这里为什么不直接设置一个 ThreadLocal&gt; 对象，而是要设置一个 Stack 结构呢？我们看一下这么做的原因是什么。\n12345678@LogRecord(content = &quot;修改了订单的配送员：从“&#123;deveryUser&#123;#oldDeliveryUserId&#125;&#125;”, 修改到“&#123;deveryUser&#123;#request.getUserId()&#125;&#125;”&quot;,        bizNo=&quot;#request.getDeliveryOrderNo()&quot;)public void modifyAddress(updateDeliveryRequest request)&#123;        LogRecordContext.putVariable(&quot;oldDeliveryUserId&quot;, DeliveryService.queryOldDeliveryUserId(request.getDeliveryOrderNo()));        doUpdate(request);&#125;\n\n上面代码的执行流程如下：\n\n看起来没有什么问题，但是使用 LogRecordAnnotation 的方法里面嵌套了另一个使用 LogRecordAnnotation 方法的时候，流程就变成下面的形式：\n\n可以看到，当方法二执行了释放变量后，继续执行方法一的 logRecord 逻辑，此时解析的时候 ThreadLocal&gt;的 Map 已经被释放掉，所以方法一就获取不到对应的变量了。方法一和方法二共用一个变量 Map 还有个问题是：如果方法二设置了和方法一相同的变量两个方法的变量就会被相互覆盖。所以最终 LogRecordContext 的变量的生命周期需要是下面的形式：\n\nLogRecordContext 每执行一个方法都会压栈一个 Map，方法执行完之后会 Pop 掉这个 Map，从而避免变量共享和覆盖问题。\n默认操作人逻辑\n在 LogRecordInterceptor 中 IOperatorGetService 接口，这个接口可以获取到当前的用户。下面是接口的定义：\n12345public interface IOperatorGetService &#123;        Operator getUser();&#125;\n\n下面给出了从用户上下文中获取用户的例子：\n1234567891011public class DefaultOperatorGetServiceImpl implements IOperatorGetService &#123;    @Override    public Operator getUser() &#123;             return Optional.ofNullable(UserUtils.getUser())                        .map(a -&gt; new Operator(a.getName(), a.getLogin()))                        .orElseThrow(()-&gt;new IllegalArgumentException(&quot;user is null&quot;));            &#125;&#125;\n\n组件在解析 operator 的时候，就判断注解上的 operator 是否是空，如果注解上没有指定，我们就从 IOperatorGetService 的 getUser 方法获取了。如果都获取不到，就会报错。\n123456789String realOperatorId = &quot;&quot;;if (StringUtils.isEmpty(operatorId)) &#123;    if (operatorGetService.getUser() == null || StringUtils.isEmpty(operatorGetService.getUser().getOperatorId())) &#123;        throw new IllegalArgumentException(&quot;user is null&quot;);    &#125;    realOperatorId = operatorGetService.getUser().getOperatorId();&#125; else &#123;    spElTemplates = Lists.newArrayList(bizKey, bizNo, action, operatorId, detail);&#125;\n\n自定义函数逻辑\n自定义函数的类图如下：\n\n下面是 IParseFunction 的接口定义：executeBefore 函数代表了自定义函数是否在业务代码执行之前解析，上面提到的查询修改之前的内容。\n12345678910public interface IParseFunction &#123;  default boolean executeBefore()&#123;    return false;  &#125;  String functionName();  String apply(String value);&#125;\n\nParseFunctionFactory 的代码比较简单，它的功能是把所有的 IParseFunction 注入到函数工厂中。\n12345678910111213141516171819202122232425public class ParseFunctionFactory &#123;  private Map&lt;String, IParseFunction&gt; allFunctionMap;  public ParseFunctionFactory(List&lt;IParseFunction&gt; parseFunctions) &#123;    if (CollectionUtils.isEmpty(parseFunctions)) &#123;      return;    &#125;    allFunctionMap = new HashMap&lt;&gt;();    for (IParseFunction parseFunction : parseFunctions) &#123;      if (StringUtils.isEmpty(parseFunction.functionName())) &#123;        continue;      &#125;      allFunctionMap.put(parseFunction.functionName(), parseFunction);    &#125;  &#125;  public IParseFunction getFunction(String functionName) &#123;    return allFunctionMap.get(functionName);  &#125;  public boolean isBeforeFunction(String functionName) &#123;    return allFunctionMap.get(functionName) != null &amp;&amp; allFunctionMap.get(functionName).executeBefore();  &#125;&#125;\n\nDefaultFunctionServiceImpl 的逻辑就是根据传入的函数名称 functionName 找到对应的 IParseFunction，然后把参数传入到 IParseFunction 的 apply 方法上最后返回函数的值。\n12345678910111213141516171819202122public class DefaultFunctionServiceImpl implements IFunctionService &#123;  private final ParseFunctionFactory parseFunctionFactory;  public DefaultFunctionServiceImpl(ParseFunctionFactory parseFunctionFactory) &#123;    this.parseFunctionFactory = parseFunctionFactory;  &#125;  @Override  public String apply(String functionName, String value) &#123;    IParseFunction function = parseFunctionFactory.getFunction(functionName);    if (function == null) &#123;      return value;    &#125;    return function.apply(value);  &#125;  @Override  public boolean beforeFunction(String functionName) &#123;    return parseFunctionFactory.isBeforeFunction(functionName);  &#125;&#125;\n\n4.2.3 日志持久化逻辑同样在 LogRecordInterceptor 的代码中引用了 ILogRecordService，这个 Service 主要包含了日志记录的接口。\n12345public interface ILogRecordService &#123;        void record(LogRecord logRecord);&#125;\n\n业务可以实现这个保存接口，然后把日志保存在任何存储介质上。这里给了一个 2.2 节介绍的通过 log.info 保存在日志文件中的例子，业务可以把保存设置成异步或者同步，可以和业务放在一个事务中保证操作日志和业务的一致性，也可以新开辟一个事务，保证日志的错误不影响业务的事务。业务可以保存在 Elasticsearch、数据库或者文件中，用户可以根据日志结构和日志的存储实现相应的查询逻辑。\n123456789@Slf4jpublic class DefaultLogRecordServiceImpl implements ILogRecordService &#123;    @Override    public void record(LogRecord logRecord) &#123;        log.info(&quot;【logRecord】log=&#123;&#125;&quot;, logRecord);    &#125;&#125;\n\n4.2.4 Starter 逻辑封装上面逻辑代码已经介绍完毕，那么接下来需要把这些组件组装起来，然后让用户去使用。在使用这个组件的时候只需要在 Springboot 的入口上添加一个注解 @EnableLogRecord(tenant &#x3D; “com.mzt.test”)。其中 tenant 代表租户，是为了多租户使用的。\n12345678910@SpringBootApplication(exclude = DataSourceAutoConfiguration.class)@EnableTransactionManagement@EnableLogRecord(tenant = &quot;com.mzt.test&quot;)public class Main &#123;    public static void main(String[] args) &#123;        SpringApplication.run(Main.class, args);    &#125;&#125;\n\n再看下 EnableLogRecord 的代码，代码中 Import 了 LogRecordConfigureSelector.class，在 LogRecordConfigureSelector 类中暴露了 LogRecordProxyAutoConfiguration 类。\n12345678910@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Import(LogRecordConfigureSelector.class)public @interface EnableLogRecord &#123;    String tenant();        AdviceMode mode() default AdviceMode.PROXY;&#125;\n\nLogRecordProxyAutoConfiguration 就是装配上面组件的核心类了，代码如下：\n1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374@Configuration@Slf4jpublic class LogRecordProxyAutoConfiguration implements ImportAware &#123;  private AnnotationAttributes enableLogRecord;  @Bean  @Role(BeanDefinition.ROLE_INFRASTRUCTURE)  public LogRecordOperationSource logRecordOperationSource() &#123;    return new LogRecordOperationSource();  &#125;  @Bean  @ConditionalOnMissingBean(IFunctionService.class)  public IFunctionService functionService(ParseFunctionFactory parseFunctionFactory) &#123;    return new DefaultFunctionServiceImpl(parseFunctionFactory);  &#125;  @Bean  public ParseFunctionFactory parseFunctionFactory(@Autowired List&lt;IParseFunction&gt; parseFunctions) &#123;    return new ParseFunctionFactory(parseFunctions);  &#125;  @Bean  @ConditionalOnMissingBean(IParseFunction.class)  public DefaultParseFunction parseFunction() &#123;    return new DefaultParseFunction();  &#125;  @Bean  @Role(BeanDefinition.ROLE_INFRASTRUCTURE)  public BeanFactoryLogRecordAdvisor logRecordAdvisor(IFunctionService functionService) &#123;    BeanFactoryLogRecordAdvisor advisor =            new BeanFactoryLogRecordAdvisor();    advisor.setLogRecordOperationSource(logRecordOperationSource());    advisor.setAdvice(logRecordInterceptor(functionService));    return advisor;  &#125;  @Bean  @Role(BeanDefinition.ROLE_INFRASTRUCTURE)  public LogRecordInterceptor logRecordInterceptor(IFunctionService functionService) &#123;    LogRecordInterceptor interceptor = new LogRecordInterceptor();    interceptor.setLogRecordOperationSource(logRecordOperationSource());    interceptor.setTenant(enableLogRecord.getString(&quot;tenant&quot;));    interceptor.setFunctionService(functionService);    return interceptor;  &#125;  @Bean  @ConditionalOnMissingBean(IOperatorGetService.class)  @Role(BeanDefinition.ROLE_APPLICATION)  public IOperatorGetService operatorGetService() &#123;    return new DefaultOperatorGetServiceImpl();  &#125;  @Bean  @ConditionalOnMissingBean(ILogRecordService.class)  @Role(BeanDefinition.ROLE_APPLICATION)  public ILogRecordService recordService() &#123;    return new DefaultLogRecordServiceImpl();  &#125;  @Override  public void setImportMetadata(AnnotationMetadata importMetadata) &#123;    this.enableLogRecord = AnnotationAttributes.fromMap(            importMetadata.getAnnotationAttributes(EnableLogRecord.class.getName(), false));    if (this.enableLogRecord == null) &#123;      log.info(&quot;@EnableCaching is not present on importing class&quot;);    &#125;  &#125;&#125;\n\n这个类继承 ImportAware 是为了拿到 EnableLogRecord 上的租户属性，这个类使用变量 logRecordAdvisor 和 logRecordInterceptor 装配了 AOP，同时把自定义函数注入到了 logRecordAdvisor 中。\n对外扩展类：分别是IOperatorGetService、ILogRecordService、IParseFunction。业务可以自己实现相应的接口，因为配置了 @ConditionalOnMissingBean，所以用户的实现类会覆盖组件内的默认实现。 https://tech.meituan.com/2021/09/16/operational-logbook.html\n","slug":"LINUX/如何优雅地记录操作日志？","date":"2021-11-03T14:13:48.000Z","categories_index":"SpEL,LINUX","tags_index":"https,com,AOP","author_index":"dandeliono"},{"id":"6302f7b0274d5a479bc686d3d275831e","title":"虚拟机深层系列「GC本质底层机制」SafePoint的深入分析和底层原理探究指南","content":"虚拟机深层系列「GC本质底层机制」SafePoint的深入分析和底层原理探究指南SafePoint 前提介绍在高度优化的现代 JVM 里，Safepoint 有几种不同的用法。GC safepoint 是最常见、大家听说得最多的，但还有 deoptimization safepoint 也很重要。\n在 HotSpot VM 里，这两种 Safepoint 目前实现在一起，但其实概念上它们俩没有直接联系，需要的数据不一样。\n无论是哪种 SafePoint，最简洁的定义是 “A point in program where the state of execution is known by the VM”。这里 “state of execution” 特意说得模糊，是因为不同种类的 safepoint 需要的数据不一样。 \nGC safepointGC Safepoint 需要知道在那个程序位置上，调用栈、寄存器等一些重要的数据区域里什么地方包含了 GC 管理的指针； 如果要触发一次 GC，那么 JVM 里的所有 Java 线程都必须到达 GC safepoint。\nDeoptimization safepointDeoptimization safepoint 需要知道在那个程序位置上，原本抽象概念上的 JVM 的执行状态（所有局部变量、临时变量、锁，等等）到底分配到了什么地方，是在栈帧￼的具体某个操作数栈 slot，还是在某个寄存器里。\n如果要执行一次 deoptimization，那么需要执行 deoptimization 的线程要在到达 deoptimization safepoint 之后才可以开始 deoptimize。\n\n\n\n\n\n\n\n\n\n不同 JVM 实现会选用不同的位置放置 safepoint。 \nHotSpotVM 的 SafePoint解释器里每条字节码的边界都可以是一个 safepoint，因为 HotSpot 的解释器总是能很容易的找出完整的 “state of execution”。\nJIT 编译的世界里，HotSpot 会在所有方法的临返回之前，以及所有非 counted loop 的循环的回跳之前放置 safepoint，(counted loop 则没有放置 safepoint）。\nHotSpot 的 JIT 编译器不但会生成机器码，还会额外在每个 safepoint 生成一些 “调试符号信息”，以便 VM 能找到所需的 “state of execution”。\nSafePoint 的存储信息为 GC SafePoint 生成的符号信息是 OopMap，指出栈上和寄存器里哪里有 GC 管理的指针；\n为 deoptimization SafePoint 生成的符号信息是 debugInfo，指出如果要把当前栈帧从 compiled frame 转换为 interpreted frame 的话，要从哪里把相应的局部变量、临时变量、锁等信息找出来。\n选择在 SafePoint 的位置地点\n挂在 safepoint 的调试符号信息要占用空间，如果允许每条机器码都可以是 safepoint 的话，需要存储的数据量会很大（当然这有办法解决，例如用 delta 存储和用压缩）\nsafepoint 会影响优化，特别是 deoptimization safepoint，会迫使 JVM 保留一些只有解释器可能需要的、JIT 编译器认定无用的变量的值。本来 JIT 编译器可能可以发现某些值不需要而消除它们对应的运算，如果在 safepoint 需要这些值的话那就只好保留了。这才是更重要的地方，所以要尽量少放置 safepoint。\n\n\n\n\n\n\n\n\n\n\n像 HotSpotVM 这样，在 Safepoint 会生成（polling 代码）主动请求询问 JVM 是否要进入 safepoint，polling 也有开销所以要尽量减少。\nNative 代码的特殊性当某个线程在执行 native 函数的时候。此时该线程在执行 JVM 管理之外的代码，不能对 JVM 的执行状态做任何修改，因而 JVM 要进入 safepoint 不需要关心它。\n\n\n\n\n\n\n\n\n\n所以也可以把正在执行 native 函数的线程看作 “已经进入了 safepoint”，或者把这种情况叫做 “在 safe-region 里”。\nJVM 外部要对 JVM 执行状态做修改必须要通过 JNI。所有能修改 JVM 执行状态的 JNI 函数在入口处都有 safepoint 检查，一旦 JVM 已经发出通知说此时应该已经到达 safepoint 就会在这些检查的地方停下来把控制权交给 JVM。\nJRockit 选择放置 safepoint 的地方在方法的入口以及循环末尾回跳之前，跟 HotSpot 略为不同。\nUseCountedLoopSafepoints:\n\n复制代码\n\n\n\n\n\n\n\n\n\n可以避免 GC 发生时，线程因长时间运行 counted loop，进入不到 safepoint，而引起 GC 的 STW 时间过长。\nJVM 参数-XX:+PrintSafepointStatistics -XX:PrintSafepointStatisticsCount=1  \n在控制台输出以下信息：\nvmop [threads: total initially_running wait_to_block]  [time: spin block sync cleanup vmop] page_trap_count  370337.312: GenCollectForAllocation     [  1070     2       3  ]   [ 8830   0 8831   1  24  ] \n\n复制代码\nYGC 所花费的时间非常短，主要时间花费在所有线程达到安全点并暂停。\nJVM 参数配置如下：-server -Xms8192M -Xmx8192M -Xmn1500M -Xss256k -XX:PermSize=256M -XX:MaxPermSize=256M -XX:SurvivorRatio=8 -XX:-UseBiasedLocking -XX:MonitorBound=16384 -XX:+UseSpinning -XX:PreBlockSpin=1 -XX:+UseParNewGC -XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=55 -XX:CMSMaxAbortablePrecleanTime=5 -XX:+CMSParallelRemarkEnabled -XX:+UseCMSCompactAtFullCollection -XX:CMSFullGCsBeforeCompaction=0 -XX:+CMSClassUnloadingEnabled -XX:+UseFastAccessorMethods -XX:+UseCMSInitiatingOccupancyOnly -XX:SoftRefLRUPolicyMSPerMB=0 -XX:+DisableExplicitGC -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/home/xmail/jvm_heap.dump -XX:+PrintGC -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCApplicationConcurrentTime -XX:+PrintSafepointStatistics -XX:PrintSafepointStatisticsCount=1  \n\n复制代码\n最有可能导致问题的是代码里有 Java 代码\nfor (int i = 0; i &lt; ...; i++) &#123; &#125; 或者类似的循环代码。\n\n复制代码\n\n\n\n\n\n\n\n\n\n这种循环称为 “counted loop”，就是有明确的循环计数器变量，而且该变量有明确的起始值、终止值、步进长度的循环。\n它有可能被优化为循环末尾没有 safepoint，于是如果这个循环的循环次数很多、循环体里又不调用别的方法或者是调用了方法但被内联进来了，就有可能会导致进入 safepoint 非常耗时。\n可惜的是现在没什么特别方便的办法直接指出是什么地方有这种循环。有的话，一种解决办法是把单层循环拆成等价的双重嵌套循环，这样其中一层循环末尾的 safepoint 就可能会留下来，减少进入 safepoint 的等待时间。\n如何判断内联方法从代码角度如何判断方法被内联进来了，主要是方法被 final 修饰。 final 是可以帮助 JIT 编译器做出内联的判断，但不是必要条件。\n\n-XX:+PrintCompilation -XX:+PrintInlining 来看内联状况\n-XX:+PrintSafepointStatistics -XX:PrintSafepointStatisticsCount&#x3D;1 ” 输出的结果 “[time: spin block sync cleanup vmop] ” 中 spin 是指什么呢？\nPrintSafepointStatics：打印出来的 spin 值指的是 SafepointSynchronize 在同步每个线程时做的自旋。\n\n\n\n\n\n\n\n\n\n\nthread locking &#x2F; biased locking 的 spin 完全没关系，自然设置那些参数也不会影响 safepoint 的自旋（UseSpinning 之类控制的是 thread locking 的自旋）。\nSafePoint 存在的目的？\n\n\n\n\n\n\n\n\n为什么把这些位置设置为 jvm 的安全点呢，主要目的就是避免程序长时间无法进入 safepoint，比如 JVM 在做 GC 之前要等所有的应用线程进入到安全点后 VM 线程才能分派 GC 任务 ，如果有线程一直没有进入到安全点，就会导致 GC 时 JVM 停顿时间延长。\n比如，写了一个超大的循环导致线程一直没有进入到安全点，GC 前停顿了 8 秒。 \n产生的日志信息基本上 STW 的原因都是 RevokeBias 或者 BulkRevokeBias。这个是撤销偏向锁操作，虽然每次暂停的时间很短，但是特别频繁出现也会很耗时。\nGC 如何找到不可用的对象编写代码的时候是可以知道对象不可用的，但对于程序来说，需要一定的方式来知晓，可用方法比如：编译分析，引用计数，和对象是否可达。\n可达性分析因而可达性分析，只需要找到直接可达的引用，直接可达的引用就是根引用，根引用的集合就是根的集合\n\n一个对象只要能够通过 mutator 触达，那么它就是 “活” 着的。\n如果 Mutator 栈的一个槽位包含了对象的引用，那么对象就是直接可触达。\n从直接可达对象可触达的对象必定也是可达的，\n\nmuator 线程分析\nmutator 的上下文就包含了直接可达的数据，所以要获取对象根集合就是要找到 mutator 上下文中的对象引用，\nmutator 的上下文指的就是它的栈、它的寄存器文件以及一些线程上特定的数据。\n\n静态数据全局数据本身也是直接可达的\n可达性分析为了确保能正确的决定对象是否存活，GC 需要获取 mutator 上下文的（当前）一致性快照，然后枚举所有的根对象。\n\n一致性指的是：快照的抽取就像只在一个时间点发生，来避免丢失一些活着的对象。\n\n如何获取 mutator 上下文的一致性快照一种简单的方式就是在跟引用的过程中暂停所有的线程。当 mutator 暂停了它的执行时，只有将所有引用信息保存在其上下文中，才能枚举根的集合，这意味着，mutator 需要能够告诉 JVM 哪些栈的槽位有用，哪些寄存器持有引用。\n\n\n\n\n\n\n\n\n\n如果 GC 能够准确的获取上述引用信息，它就称作精准根集合枚举。而无法获取就是不精准的。\n如何获取精准的引用信息枚举对于 java 来说，JIT 知晓所有的栈帧信息和寄存器的内容，当 JIT 编译一个方法时，对于每条指令，它都可以去保存根引用信息，保存意味着额外的存储空间，如果要存储所有的指令就显得花销太大，另外在真实的运行过程中也只有少数指令才会成为暂停点，因此 JIT 只需要保存这些指令点的信息就够了。而真正有机会成为暂停点的地方就称作 safe-points，即能够安全的枚举根集合的暂停点。\n如何保证 mutator 会在 safe-point 暂停当 GC 想要触发一次回收时，它会设置一个标志，mutator 则周期性的去检查 (poll) 这个标志，如果检查到了，就会立马暂停，这里的检查点 (poll points) 也是安全点，由 JIT 负责把 poll points 放到合适的位置，那些地方适合设置检查 GC 事件的标记\npolling point 插入的主要原则是:\npolling point 应该足够多，防止 GC 等一个 mutator 的暂停太长，导致其他 mutator 都走在等 GC 释放空间，程序整个等待过长\npolling point 不能太频繁导致运行时存储开销过大\npolling 本身也是有开销的，不能过多\n权衡下来只在必须和必要的地方加\n分配地址的时候强制添加，因为分配空间很有肯能导致回收，所以这里是一个安全点\n长时间的执行一般意味着循环和方法调用，所以方法调用和循环返回最好加上\n\n\n\n\n\n\n\n\n\n\n但是有时候并不是长时间的执行，而是长时间的空闲，比如 sleep、block，线程在执行其他的 native 函数，这些时候 JVM 无法掌控执行能力，也就无法响应 GC 事件。\nSafePoint 无法解决 sleep&#x2F;block 带来的问题，当这段时间内 JVM 要发起 GC 时，就不管没到安全点但是在安全区域的线程。在线程要离开安全区域时，要检查系统是否已经完成了 GC，故我们又定义了一个安全区域的概念.\nSafeRegion 的简介safe-region 是指代码快中没有用到会变异的部分，这样的代码块中，任何一个点都可以安全的枚举根。\n\n当进入到 safe-region 中时，mutator 会设置一个准备标记，在离开 safe-region 区域之前，会检查 GC 是否已经完成了回收，如果没有，那么就暂停执行，如果有，就可以直接离开 safe-region 区域，不需要暂停 mutator。\n关于 Java&#x2F;JVM 的 safepoint &#x2F; safe-region，代码的执行过程中，如果需要执行某些操作，比如 GC，deoptimize, 等等，必须知道当前程序所有线程运行到的地方，是否能够恰好满足我执行对应操作，而不会对应用程序本身造成损害，能够正确执行的地方就是 safepoint&#x2F;saferegion\n\n参考文献\nGC safe-point (or safepoint) and safe-region | Xiao-Feng Li  \nCliff Click 大神 safepoint：How does Java Both Optimize Hot Loops and Allow Debugging\n\n","slug":"JAVA/虚拟机深层系列「GC本质底层机制」SafePoint的深入分析和底层原理探究指南","date":"2021-10-24T13:41:42.000Z","categories_index":"safepoint,JAVA","tags_index":"JVM,mutator,safe","author_index":"dandeliono"},{"id":"ff376bbc0a7a05ebffda5869db6237b5","title":"字体ttc文件打包时损坏","content":"字体ttc文件打包时损坏\n\n\n\n\n\n\n\n\n在使用 springboot 时，在 resources 添加 .ttc 类型的字体集，本地 jar 启动没有丝毫问题，打成 war 之后，就出出现，字符集读取失败的错误，这是因为在打包过程中 *.ttc 文件被损坏了。\n解决办法：在 pom.xml 的 build 标签中添加如下插件，过滤相关文件\n&lt;plugin&gt;\n &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt;\n &lt;version&gt;2.7&lt;/version&gt;\n &lt;configuration&gt;\n   &lt;nonFilteredFileExtensions&gt;\n     &lt;nonFilteredFileExtension&gt;ttf&lt;/nonFilteredFileExtension&gt;\n     &lt;nonFilteredFileExtension&gt;ttc&lt;/nonFilteredFileExtension&gt;\n     &lt;nonFilteredFileExtension&gt;ftl&lt;/nonFilteredFileExtension&gt;\n     &lt;nonFilteredFileExtension&gt;html&lt;/nonFilteredFileExtension&gt;\n   &lt;/nonFilteredFileExtensions&gt;\n &lt;/configuration&gt;\n &lt;dependencies&gt;\n &lt;dependency&gt;\n   &lt;groupId&gt;org.apache.maven.shared&lt;/groupId&gt;\n   &lt;artifactId&gt;maven-filtering&lt;/artifactId&gt;\n   &lt;version&gt;1.3&lt;/version&gt;\n &lt;/dependency&gt;\n &lt;/dependencies&gt;\n&lt;/plugin&gt; \n\n https://www.sxkawzp.cn/archives/%E5%AD%97%E4%BD%93%E6%89%93%E5%8C%85%E6%97%B6%E6%8D%9F%E5%9D%8F#more\n","slug":"LINUX/字体ttc文件打包时损坏","date":"2021-10-08T09:24:25.000Z","categories_index":"ttc,LINUX","tags_index":"https,www,sxkawzp","author_index":"dandeliono"},{"id":"c656f0476e6a10c5f9016d53a17b0bf8","title":"JIT编译器（Just In Time编译器）","content":"JIT编译器（Just In Time编译器）前提概要\n我们都知道开发语言整体分为两类，一类是编译型语言，一类是解释型语言。那么你知道二者有何区别吗？编译器和解释器又有什么区别？\n这是为了兼顾启动效率和运行效率两个方面。Java 程序最初是通过解释器进行解释运行的，当虚拟机返现某个方法或代码块的运行特别频繁时，就会把这段代码标记为热点代码，为了提供热点代码的运行效率，在运行时，虚拟机就会把这些代码编译成与本地平台相关的机器码。并进行各种层次的优化。\n\n编译器和解释器\nJava 编译器（javac）的作用是将 java 源程序编译成中间代码字节码文件，是最基本的开发工具。 \nJava 解释器（java）（英语：Interpreter），又译为直译器，是一种电脑程序，能够把高级编程语言一行一行直接转译运行。解释器不会一次把整个程序转译出来，只像一位 “中间人”，每次运行程序时都要先转成另一种语言再作运行，因此解释器的程序运行速度比较缓慢。  它每转译一行程序叙述就立刻运行，然后再转译下一行，再运行，如此不停地进行下去。\n\n\n\n当程序需要首次启动和执行的时候，解释器可以首先发挥作用，一行一行直接转译运行，但效率低下。\n当多次调用方法或循环体时 JIT 编译器可以发挥作用，把越来越多的代码编译成本地机器码，之后可以获得更高的效率（占内存），此时就有了智能化的编译器 (JIT 编译器)\n\n解释器与编译器的交互：\n\n\n\n\n\n\n\n\n\nHotSpot 虚拟机中内置了两个即时编译器，分别称为 Client Complier 和 Server Complier，它会根据自身版本与宿主机器的硬件性能自动选择运行模式，用户也可以使用 “-client” 或 “-server” 参数去强制指定虚拟机运行在 Client 模式或 Server 模式\n什么是 JIT 编译器\n即时（Just-In-Time）编译器是 Java 运行时环境的一个组件，它可提高运行时 Java 应用程序的性能。JVM 中没有什么比编译器更能影响性能，而选择编译器是运行 Java 应用程序时做出的首要决定之一。 \n当编译器做的激进优化不成立，如载入了新类后类型继承结构出现变化。出现了罕见陷阱时能够进行逆优化退回到解释状态继续运行。\n\n\n解释器与编译器搭配使用的方式：\n\n\n\n\n\n\n\n\nHotSpot JVM 内置了两个编译器，各自是 Client Complier 和 Server Complier，虚拟机默认是 Client 模式。我们也能够通过。 \n\n-client：强制虚拟机运行 Client 模式\n-server：强制虚拟机运行 Server 模式\n默认（java -version 混合模式）\n\n\n\n\n\n\n\n\n\n\n而不管是 Client 模式还是 Server 模式，虚拟机都会运行在解释器和编译器配合使用的混合模式下。能够通过。 \n\n解释模式（java -Xint -version）强制虚拟机运行于解释模式，仅使用解释器方式执行。 \n\n编译模式（java -Xcomp -version）优先采用编译方式执行程序，但解释器要在编译无法进行的情况下介入执行过程。 \njava -version\n\n\n复制代码\njava -Xint -version\n\n复制代码\njava -Xcomp -version\n\n复制代码\n\n\n\n\n\n\n\n\n\nJava 功能 “一次编译，到处运行” 的关键是 bytecode。字节码转换为应用程序的机器指令的方式对应用程序的速度有很大的影响。这些字节码可以被解释，编译为本地代码，或者直接在指令集架构中符合字节码规范的处理器上执行。\n\n解释字节码的是 Java 虚拟机（JVM）的标准实现，这会使程序的执行速度变慢。为了提高性能，JIT 编译器在运行时与 JVM 交互，并将适当的字节码序列编译为本地机器代码。 \n使用 JIT 编译器时，硬件可以执行本机代码，而不是让 JVM 重复解释相同的字节码序列，并导致翻译过程相对冗长。这样可以提高执行速度，除非方法执行频率较低。 \nJIT 编译器编译字节码所花费的时间被添加到总体执行时间中，并且如果不频繁调用 JIT 编译的方法，则可能导致执行时间比用于执行字节码的解释器更长。 \n当将字节码编译为本地代码时，JIT 编译器会执行某些优化。 \n由于 JIT 编译器将一系列字节码转换为本机指令，因此它可以执行一些简单的优化。 \nJIT 编译器执行的一些常见优化操作包括数据分析，从堆栈操作到寄存器操作的转换，通过寄存器分配减少内存访问，消除常见子表达式等。\nJIT 编译器进行的优化程度越高，在执行阶段花费的时间越多。\n\n\n\n\n\n\n\n\n\n\n因此，JIT 编译器无法承担所有静态编译器所做的优化，这不仅是因为增加了执行时间的开销，而且还因为它只对程序进行了限制。 \n\n\nJIT 编译器默认情况下处于启用状态，并在调用 Java 方法时被激活。 \nJIT 编译器将该方法的字节码编译为本地机器代码，“即时” 编译以运行。 \n编译方法后，JVM 会直接调用该方法的已编译代码，而不是对其进行解释。\n\n\n\n\n\n\n\n\n\n\n从理论上讲，如果编译不需要处理器时间和内存使用量，则编译每种方法都可以使 Java 程序的速度接近本机应用程序的速度。\nJIT 编译确实需要处理器时间和内存使用率。JVM 首次启动时，将调用数千种方法。即使程序最终达到了非常好的峰值性能，编译所有这些方法也会严重影响启动时间。 \n不同应用程序的不同编译器\n\n\n\n\n\n\n\n\nJIT 编译器有两种形式，并且选择使用哪个编译器通常是运行应用程序时唯一需要进行的编译器调整。实际上，即使在安装 Java 之前，也要考虑知道要选择哪个编译器，因为不同的 Java 二进制文件包含不同的编译器。 \n客户端编译器\n\n\n\n\n\n\n\n\n著名的优化编译器是 C1，它是通过 - clientJVM 启动选项启用的编译器。顾名思义，C1 是客户端编译器。它是为客户端应用程序设计的，这些客户端应用程序具有较少的可用资源，并且在许多情况下对应用程序启动时间敏感。C1 使用性能计数器进行代码性能分析，以实现简单，相对无干扰的优化。\n服务器端编译器\n\n\n\n\n\n\n\n\n对于长时间运行的应用程序（例如服务器端企业 Java 应用程序），客户端编译器可能不够。可以使用类似 C2 的服务器端编译器。通常通过将 JVM 启动选项添加 - server 到启动命令行来启用 C2 。由于大多数服务器端程序预计将运行很长时间，因此启用 C2 意味着您将能够比使用运行时间短的轻量级客户端应用程序收集更多的性能分析数据。因此，您将能够应用更高级的优化技术和算法。\n分层编译为什么要进行分层编译\n\n\n\n\n\n\n\n\n这是由于编译器编译本机代码须要占用程序运行时间，要编译出优化程度更高的代码锁花费的时间可能更长，并且想要编译出优化程度更高的代码，解释器可能还要替编译器收集性能监控信息。这对解释运行的速度也有影响。为了在程序启动响应速度和运行效率之间寻找平衡点。因此採用分层编译的策略。 \n\n分层编译结合了客户端和服务器端编译。分层编译利用了 JVM 中客户端和服务器编译器的优势。\n客户端编译器在应用程序启动期间最活跃，并处理由较低的性能计数器阈值触发的优化。\n客户端编译器还会插入性能计数器，并为更高级的优化准备指令集，服务器端编译器将在稍后阶段解决这些问题。\n\n\n\n\n\n\n\n\n\n\n分层编译是一种非常节省资源的性能分析方法，因为编译器能够在影响较小的编译器活动期间收集数据，以后可以将其用于更高级的优化。与仅使用解释的代码配置文件计数器所获得的信息相比，这种方法还可以产生更多的信息。 \n分层策略例如以下所看到的：\n第 0 层：程序解释运行。解释器不开启性能监控功能，可触发第 1 层编译。\n第 1 层：即 C1 编译。将字节码编译为本地代码。进行简单和可靠的优化，如有必要将增加性能监控的逻辑。\n第 2 层：即 C2 编译，将字节码编译为本地代码，同一时候启用一些编译耗时较长的优化，甚至会依据性能监控信息进行一些不可靠的激进优化。\n\n代码优化\n当选择一种方法进行编译时，JVM 会将其字节码提供给即时编译器（JIT）。JIT 必须先了解字节码的语义和语法，然后才能正确编译该方法。 \n为了帮助 JIT 编译器分析该方法，首先将其字节码重新格式化为称为 trees，它比字节码更类似于机器代码。 \n然后对方法的树进行分析和优化。\n最后，将树转换为本地代码。 \nJIT 编译器可以使用多个编译线程来执行 JIT 编译任务，使用多个线程可以潜在地帮助 Java 应用程序更快地启动。\n\n\n\n\n\n\n\n\n\n\n编译线程的默认数量由 JVM 标识，并且取决于系统配置。如果生成的线程数不是最佳的，则可以使用该 XcompilationThreads 选项覆盖 JVM 决策。 \n编译包括以下阶段：内联\n\n\n\n\n\n\n\n\n内联是将较小方法的树合并或 “内联” 到其调用者的树中的过程。这样可以加速频繁执行的方法调用。 \n局部优化\n\n\n\n\n\n\n\n\n局部优化可以一次分析和改进一小部分代码。许多本地优化实现了经典静态编译器中使用的久经考验的技术。 \n控制流优化\n\n\n\n\n\n\n\n\n控制流优化分析方法（或方法的特定部分）内部的控制流，并重新排列代码路径以提高其效率。 \n全局优化\n\n\n\n\n\n\n\n\n全局优化可一次对整个方法起作用。它们更加 “昂贵”，需要大量的编译时间，但可以大大提高性能。 \n本机代码生成\n\n\n\n\n\n\n\n\n本机代码生成过程因平台架构而异。通常，在编译的此阶段，将方法的树转换为机器代码指令；根据架构特征执行一些小的优化。 \n编译对象\n\n\n\n\n\n\n\n\n编译对象即为会被编译优化的热点代码。有下面两类：\n\n被多次调用的方法\n被多次运行的循环体\n\n触发条件\n\n\n\n\n\n\n\n\n这就牵扯到触发条件这个概念，推断一段代码是否是热点代码。是否须要触发即时编译，这样的行为成为热点探测（Spot Dectection）。 \n热点探测有两种手段：基于采样的热点探测（Sample Based Hot Spot Dectection）\n\n\n\n\n\n\n\n\n虚拟机会周期性的检查各个线程的栈顶，假设发现某些方法常常性的出如今栈顶，那么这种方法就是热点方法。 \n基于计数器的热点探测（Counter Based Hot Spot Dectection）\n\n\n\n\n\n\n\n\n虚拟机会为每一个方法或代码块建立计数器，统计方法的运行次数。假设运行次数超过一定的阈值就觉得他是热点方法。 \n\n\n\n\n\n\n\n\n\nHotSpot JVM 使用另外一种方法基于计数器的热点探測方法。它为每一个方法准备了两类计数器： \n方法调用计数器\n\n\n\n\n\n\n\n\n这个阈值在 Client 模式下是 1500 次。在 Server 模式下是 10000 此，这个阈值能够通过參数**-XX:CompileThreshold**来人为设定。 \n\n方法调用次数统计的并非方法被调用的绝对次数，而是相对的运行频率，即一段时间内方法被调用的次数，当超过一定时间限度，假设方法的调用次数仍然不足以让它提交给即时编译器编译，那这种方法的调用计数器会被降低一半，这个过程被称为方法调用计数器的热度衰减（Counter Decay）。 \n而这段时间就称为此方法统计的半衰周期（Counter Half Life Time）。相同也能够使用參数**-XX:-UseCounterDecay**来关闭热度衰减。\n\n方法调用计数器触发即时编译的整个流程例如以下图所看到的：\n\n回边计数器什么是回边？\n\n\n\n\n\n\n\n\n在字节码遇到控制流向后跳转的指令称为回边（Back Edge）。 \n\n回边计数器是用来统计一个方法中循环体代码运行的次数，回边计数器的阈值能够通过參数-XX：OnStackReplacePercentage来调整。\n\n虚虚拟机运行在 Client 模式下，回边计数器阂值计算公式为：\n方法调用计数器闭值(CompileThreshold) xOSR比率(OnStackReplacePercentage) / 100\n\n复制代码\n当中 OnSlackReplacePercentage 默认值为 933，假设都取默认值，那 Client 模式虚拟机的回边计数器的阂值为 13995。\n虚拟机运行在 Server 模式下，回边计数器阂值的 itm 公式为：\n方法调用计数器阂值(CompileThreshold) x (OSR比率(OnStackReplacePercentage) - 解释器监控比率(InterpreterProffePercentage) / 100\n\n复制代码\n\n当中 OnSlackReplacePercentage 默认值为 140。 InterpreterProffePercentage 默认值为 33.\n假设都取默认值。BF Server 模式虚拟机回边计数器的阈值为 10700。\n\n回边计数器触发即时编译的流程例如以下图所看到的：\n\n回边计数器与方法调用计数器不同的是，回边计数器没有热度衰减，因此这个计数器统计的就是循环运行的绝对次数。\n编译流程在默认设置下，不管是方法调用产生的即时编译请求，还是 OSR 编译请求，虚拟机在代码编译器还未完毕之前，都仍然依照解释方式继续进行，而编译动作则在后台的编译线程中继续进行。也能够使用 - XX:-BackgroundCompilation 来禁止后台编译，则此时一旦遇到 JIT 编译，运行线程向虚拟机提交请求后会一直等待，直到编译完毕后再開始运行编译器输出的本地代码。\n那么在后台编译过程中，编译器做了什么事呢？\nClient Compiler 编译流程\n第一阶段：一个平台独立的前端将字节码构造成一种高级中间码表示（High Level Infermediate Representaion），HIR 使用静态单分配的形式来表示代码值，这能够使得一些的构造过程之中和之后进行的优化动作更 easy 实现，在此之前编译器会在字节码上完毕一部分基础优化，如方法内联、常量传播等。\n第二阶段：一个平台相关的后端从 HIR 中产生低级中间代码表示（Low Level Intermediate Representation），而在此之前会在 HIR 上完毕还有一些优化。如空值检查消除、范围检查消除等。以便让 HIR 达到更高效的代码表示形式。\n第三阶段：在平台相关的后端使用线性扫描算法（Linear Scan Register Allocation）在 LIR 上分配寄存器, 并在 LIR 上做窥孔优化（Peephole）优化，然后产生机器码。\n\n\n发布于: 2021 年 07 月 30 日阅读数: 806\n","slug":"JAVA/JIT编译器（Just In Time编译器）","date":"2021-10-07T22:17:49.000Z","categories_index":"JIT,JAVA","tags_index":"JVM,Java,Client","author_index":"dandeliono"},{"id":"e527db9dc4d4819a60eaa7ad7fd37a44","title":"Java线程的六种状态 Thread.State","content":"Java线程的六种状态 Thread.StateThread.State 是 Thread 中的一个内部类，表示了 Thread 的六种状态，还有，这个类是一个枚举类。\nNEW线程刚刚 new 出来，热乎的，还未启动\n代码示例public class ThreadStateNewExample &#123;\n    private static Object waiter = new Object();\n    \n    public static void main(String[] args)&#123;\n        Runnable waiting = () -&gt; &#123;\n            try&#123;\n                waiter.wait();\n            &#125;catch (InterruptedException e)&#123;\n                e.printStackTrace();\n            &#125;\n        &#125;;\n        Thread whoWillWait = new Thread(waiting);\n        System.out.printf(whoWillWait.getState().toString());\n    &#125;\n&#125;\n复制代码\n\n结果\nNEW\nProcess finished with exit code 0\n复制代码\n\nRUNNABLE说明线程已经就绪，可能正在执行某个任务，也可能在等待 CPU 资源\n代码示例public class ThreadStateRunnableExample &#123;\n    private static boolean flag = true;\n\n    public static void main(String[] args)&#123;\n        Runnable waiting = () -&gt; &#123;\n            //让程序空转，保持线程是runnable状态\n                do &#123; &#125;while (flag);\n        &#125;;\n        Thread thread = new Thread(waiting);\n        thread.start();\n        try &#123;\n            //主线程先睡3秒，让子线程先跑起来，然后输出线程状态\n            Thread.sleep(3000);\n            System.out.printf(thread.getState().toString());\n            //更改标志位，让子线程结束循环\n            flag = false;\n        &#125;catch (InterruptedException e)&#123;\n            e.printStackTrace();\n        &#125;\n        System.exit(1);\n    &#125;\n&#125;\n复制代码\n\n运行结果RUNNABLE\nProcess finished with exit code 1\n复制代码\n\nBLOCKED线程正在阻塞，等待一个 monitor lock。例如等待获取一个文件的锁\n代码示例public class ThreadStateBlockExample &#123;\n\n    private static boolean LOCK_FLAG = true;\n\n    public static void main(String[] args)&#123;\n        Runnable locker = ThreadStateBlockExample::locker;\n\n        Thread whoWillLockOthers = new Thread(locker);\n        /**\n         * 启动whoWillLockOthers线程，主线程睡2秒让子线程先运行\n         * 此时whoWillLockOthers获得锁，这时候其他线程需要等待\n         */\n        whoWillLockOthers.start();\n        try &#123;\n            Thread.sleep(2000);\n        &#125;catch (InterruptedException e)&#123;\n            e.printStackTrace();\n        &#125;\n        Thread whoWillBeLocked = new Thread(locker);\n        /**\n         * 启动whoWillBeLocked线程，主线程睡2秒让子线程先运行\n         * 因为locker方法是个死循环，所以whoWillBeLocked线程永远拿不到锁，就会进入BLOCKED状态\n         */\n        whoWillBeLocked.start();\n        try &#123;\n            Thread.sleep(2000);\n        &#125;catch (InterruptedException e)&#123;\n            e.printStackTrace();\n        &#125;\n        System.out.printf(&quot;whoWillBeLocked 当前状态为=&quot;+whoWillBeLocked.getState().toString()+&quot;&quot;);\n        System.exit(1);\n    &#125;\n\n\n    private static synchronized void locker()&#123;\n        do &#123;\n\n        &#125;while (LOCK_FLAG);\n    &#125;\n&#125;\n\n复制代码\n\n执行结果whoWillBeLocked 当前状态为=BLOCKED\nProcess finished with exit code -1\n复制代码\n\nWAITING代表线程正在等待中。一个线程如果调用下列方法，会导致线程状态变为WAITING：\n\nObject.wait with no timeout\nThread.join with no timeout\nLockSupport.park\n\n举个栗子：ThreadA 调用了 Object.wait() 方法，此时 ThreadA 状态为WAITING。ThreadA 会等待其他的线程调用 Object.notify() 或 Object.notifyAll 才会被唤醒，继续执行后面的逻辑\n\n\n\n\n\n\n\n\n\n注意！！！调用 wait() 和 notify() 此类方法必须先获得 Object 的锁，至于原理我们开新帖子去讲，本文主要介绍线程的状态和代码示例，有兴趣的同学也可以在掘金上搜搜其他作者关于锁和 Monitor 的文章。 \n代码示例public class ThreadStateWaitingExample &#123;\n    private static final Object LOCKER = new Object();\n\n    public static void main(String[] args) &#123;\n\n        Runnable waiting = () -&gt; &#123;\n            System.out.println(&quot;whoWillWait 开始等待 whoWillNotify&quot;);\n            waiting();\n            System.out.println(&quot;whoWillWait 等到了 whoWillNotify 的通知&quot;);\n        &#125;;\n        //创建一个线程调用waiter.wait()方法，让whoWillWait线程进入waiting状态\n        Thread whoWillWait = new Thread(waiting);\n        whoWillWait.start();\n        //主线程先睡2秒，让whoWillWait先执行\n        try &#123;\n            Thread.sleep(2000);\n        &#125; catch (InterruptedException e) &#123;\n            e.printStackTrace();\n        &#125;\n        System.out.println(&quot;whoWillWait当前的线程状态=&quot; + whoWillWait.getState().toString());\n\n\n        Runnable notify = () -&gt; &#123;\n            System.out.println(&quot;whoWillNotify 准备通知 whoWillWait&quot;);\n            notifying();\n        &#125;;\n        //创建一个线程调用waiter.notify()方法，唤醒whoWillWait\n        Thread whoWillNotify = new Thread(notify);\n        whoWillNotify.start();\n        //主线程先睡2秒，让whoWillNotify先执行\n        try &#123;\n            Thread.sleep(2000);\n        &#125; catch (InterruptedException e) &#123;\n            e.printStackTrace();\n        &#125;\n        System.out.println(&quot;唤醒后，whoWillWait当前的线程状态=&quot; + whoWillWait.getState().toString());\n        System.exit(1);\n    &#125;\n\n    private static void waiting() &#123;\n        synchronized (LOCKER) &#123;\n            try &#123;\n                LOCKER.wait();\n            &#125; catch (InterruptedException e) &#123;\n                e.printStackTrace();\n            &#125;\n        &#125;\n\n    &#125;\n\n    private static void notifying() &#123;\n        synchronized (LOCKER) &#123;\n            LOCKER.notify();\n            System.out.println(&quot;whoWillNotify 已经通知，即将离开同步代码块&quot;);\n        &#125;\n    &#125;\n&#125;\n复制代码\n\n执行结果whoWillWait 开始等待 whoWillNotify\nwhoWillWait当前的线程状态=WAITING\nwhoWillNotify 准备通知 whoWillWait\nwhoWillNotify 已经通知，即将离开同步代码块\nwhoWillWait 等到了 whoWillNotify 的通知\n唤醒后，whoWillWait当前的线程状态=TERMINATED\n\nProcess finished with exit code 1\n复制代码\n\nTIMED_WAITING线程正在等待其他线程的操作，直到超过指定的超时时间。线程在调用以下方法是会将状态改变为 TIMED_WAITING 状态:\n\nThread.sleep\nObject.wait with timeout\nThread.join with timeout\nLockSupport.parkNanos\nLockSupport.parkUntil\n\n代码示例public class ThreadStateTimedWaitingExample &#123;\n    private static final Object LOCKER = new Object();\n\n    public static void main(String[] args) &#123;\n\n        Runnable waiting = () -&gt; &#123;\n            System.out.println(&quot;whoWillWait 开始等待 2秒钟&quot;);\n            waiting();\n            System.out.println(&quot;whoWillWait 等待结束&quot;);\n        &#125;;\n        //创建一个线程调用waiter.wait()方法，让whoWillWait线程进入waiting状态\n        Thread whoWillWait = new Thread(waiting);\n        whoWillWait.start();\n        //主线程先睡1秒，让whoWillWait先执行\n        try &#123;\n            Thread.sleep(1000);\n        &#125; catch (InterruptedException e) &#123;\n            e.printStackTrace();\n        &#125;\n        System.out.println(&quot;whoWillWait当前的线程状态=&quot; + whoWillWait.getState().toString());\n\n\n        try &#123;\n            Thread.sleep(2000);\n        &#125; catch (InterruptedException e) &#123;\n            e.printStackTrace();\n        &#125;\n        System.out.println(&quot;whoWillWait当前的线程状态=&quot; + whoWillWait.getState().toString());\n        System.exit(1);\n    &#125;\n\n    private static void waiting() &#123;\n        synchronized (LOCKER) &#123;\n            try &#123;\n                LOCKER.wait(2000);\n            &#125; catch (InterruptedException e) &#123;\n                e.printStackTrace();\n            &#125;\n        &#125;\n\n    &#125;\n    \n&#125;\n复制代码\n\n执行结果whoWillWait 开始等待 2秒钟\nwhoWillWait当前的线程状态=TIMED_WAITING\nwhoWillWait 等待结束\nwhoWillWait当前的线程状态=TERMINATED\n\nProcess finished with exit code 1\n复制代码\n\nTERMINATED线程已经结束执行，属于退出状态\n代码示例public class ThreadStateTerminatedExample &#123;\n\n    public static void main(String[] args) &#123;\n\n        Runnable waiting = () -&gt; &#123;\n            System.out.println(&quot;随便执行一下，然后线程就会变为Terminated&quot;);\n        &#125;;\n        Thread terminate = new Thread(waiting);\n        terminate.start();\n        //主线程先睡1秒，让terminate先执行，一秒钟足够terminate执行完毕，然后线程就结束了\n        try &#123;\n            Thread.sleep(1000);\n        &#125; catch (InterruptedException e) &#123;\n            e.printStackTrace();\n        &#125;\n        System.out.println(&quot;terminate当前的线程状态=&quot; + terminate.getState().toString());\n\n        System.exit(1);\n    &#125;\n\n&#125;\n复制代码\n\n执行结果随便执行一下，然后线程就会变为Terminated\nterminate当前的线程状态=TERMINATED\n\nProcess finished with exit code 1\n复制代码\n\n\n\n\n\n\n\n\n\n\n\n线程在某个时间点只会拥有一种状态，这些状态都是虚拟机的线程状态，并不是操作系统的线程状态，这是两个概念，不要混淆。 https://juejin.cn/post/6844903968200458247\n","slug":"JAVA/Java线程的六种状态 Thread.State","date":"2021-09-09T09:20:36.000Z","categories_index":"Thread,JAVA","tags_index":"代码示例,Object,WAITING","author_index":"dandeliono"},{"id":"78e24240498cee987d87ef60883b2ac7","title":"Spring中classpath的使用问题","content":"Spring中classpath的使用问题在 Spring 的配置文件中，经常使用classpath：xxx.xxx来读取文件。对于 maven 项目，误区是读取的文件必须在resources下，其实并不是。\n实验对于 maven 管理的项目，我们分别从三个例子分析，读取的文件位置可以在什么地方。\n配置文件中加入以下配置\n123&lt;bean id=&quot;txt&quot; class=&quot;org.springframework.beans.factory.config.PropertiesFactoryBean&quot;&gt;\t&lt;property name=&quot;location&quot; value=&quot;classpath:/txt/readme.txt&quot;/&gt;&lt;/bean&gt;\n\n测试代码\n12@Resource(name=&quot;txt&quot;)protected Properties Properties;\n\n第一种情况（resources 下）在 resources 下，新建文件夹 txt，在 txt 中放入 readme.txt，执行测试代码可以正常输出 txt 中的 readme.txt。\n第二种情况（jar 包中）新建项目，文件夹 txt 中放入 readme.txt，打包生成. jar 文件，在测试项目中引入生成的 jar 包，执行测试代码获取到 jar 包中 readme.txt。\n第三种情况（resouces 和 jar 中都存在）当 resources 和 jar 包中都有这个文件的时候，默认只读取 resources 下的资源文件。\n后记 (12 月 4 日补充)对于不同项目来说，其 classpath 不同，上述结论不适用于所有项目。\nResource上述实验用到的PropertiesFactoryBean返回java.util.Properties，依赖注入时用到的setlocation(Resource location)，所以我们需要对Resource进行简单的研究。\nResource 接口Spring 的 Resource 接口抽象了对低级资源文件的访问，接口定义如下\n1234567891011121314151617public interface Resource extends InputStreamSource &#123;    boolean exists();    boolean isOpen();    URL getURL() throws IOException;    File getFile() throws IOException;    Resource createRelative(String relativePath) throws IOException;    String getFilename();    String getDescription();&#125;\n\n这个接口继承于 InputStreamSource 接口，接口定义如下\n123public interface InputStreamSource &#123;    InputStream getInputStream() throws IOException;&#125;\n\nResource 接口中的一些重要的方法如下：\n\ngetInputStream()：定位和打开资源文件，从资源文件中返回InputStream\nexists()：返回boolean，表示资源是否存在\nisOpen()：返回boolean，如果是true，资源就只能被访问一次，且需要被关闭。对于其一般的实现类，返回的是false，除了InputStreamResource\n**getDescription()**：返回对这个资源文件的描述\n\n其他的方法可以获得一个真正的URL对象或者File对象，代表资源文件。在 Spring 框架中，当一个资源文件被需要时，会大量地使用Resource接口作为参数类型。当然在我们自己的代码中，也可以使用它的实现类获取文件资源，是URL更有用的代替。\n内置的 Resource 实现类\nUrlResource：包含了java.net.URL，可以用 URL 访问对象，如Files，Http，Ftp，一些前缀被使用，用来表名 URL 的类型，如file:，http&amp;#x3A;，ftp:\nClassPathResource：这个类代表了一个应该从classpath获取的资源文件，使用classloader，或者class来载入资源\nFileSystemResource\nServletContextResource\nInputStreamResource\nByteArrayResource\n\n前两个实现类都可以直接用构造器显式创建，不过大部分情况下，是用 string 表示路径来调用 api 方法（bean 依赖注入），隐式创建，如果前缀是classpath，则创建ClassPathResource。\nClassPathResource现在可以知道上述实验是生成了ClassPathResource类，再看源码（代码中所用到的 path 为去掉 classpath: 前缀的路径）\n123public ClassPathResource(String path)&#123;\tthis(path, (ClassLoader)null);&#125;\n\n123456789public ClassPathResource(String path, ClassLoader classLoader) &#123;\tAssert.notNull(path, &quot;Path must not be null&quot;);\tString pathToUse = StringUtils.cleanPath(path);\tif (pathToUse.startsWith(&quot;/&quot;)) &#123;\t  pathToUse = pathToUse.substring(1);\t&#125;\tthis.path = pathToUse;\tthis.classLoader = (classLoader != null ? classLoader : ClassUtils.getDefaultClassLoader());&#125;\n\n1234567891011121314151617public InputStream getInputStream() throws IOException &#123;    InputStream is;    if (this.clazz != null) &#123;      is = this.clazz.getResourceAsStream(this.path);    &#125;    else&#123;      if (this.classLoader != null) &#123;        is = this.classLoader.getResourceAsStream(this.path);      &#125; else &#123;        is = ClassLoader.getSystemResourceAsStream(this.path);      &#125;    &#125;    if (is == null) &#123;      throw new FileNotFoundException(getDescription() + &quot; cannot be opened because it does not exist&quot;);    &#125;    return is;&#125;\n\n最终调用java.lang.ClassLoader类中的getResource方法，在 api 中关于查找顺序，有以下的描述\n\n\n\n\n\n\n\n\n\nThis method will first search the parent class loader for the resource; if the parent is null the path of the class loader built-in to the virtual machine is searched. That failing, this method will invoke findResource(String) to find the resource.\n此方法首先搜索资源的父类加载器；如果父类加载器为 null，则搜索的路径就是虚拟机的内置类加载器的路径。如果搜索失败，则此方法将调用 findResource(String) 来查找资源。\n","slug":"JAVA/Spring中classpath的使用问题","date":"2021-09-07T20:30:06.000Z","categories_index":"Resource,JAVA","tags_index":"jar,txt,resources","author_index":"dandeliono"},{"id":"bed926b9b5ce2da9bc2fbda11d1273e0","title":"Linux 快捷键","content":"Linux 快捷键最有用快捷键\nTab\n命令或路径的补全键。应该是 Linux 最常用的快捷键，它将节省你 Linux 命令行中的大量时间。只需要输入一个命令，文件名，目录名甚至是命令选项的开头，并敲击 tab 键。它将自动完成你输入的内容，或为你显示全部可能的结果，快速提升输入效率。如果你只记一个快捷键，这将是必选的一个\n移动光标快捷键\nCtrl + A\n光标回到命令行首。该快捷键将移动光标到所在行首。假设你在终端输入了一个很长的命令或路径，并且你想要回到它的开头， 使用方向键移动光标将花费大量时间。注意你无法使用鼠标移动光标到行首。\n\nCtrl + E\n光标回到命令行尾。这对快捷键与 Ctrl+A 相反。Ctrl+A 送光标到行首，反之 Ctrl+E 移动光标到行尾。\nCtrl – b\n往回 (左) 移动一个字符\nCtrl – f\n往后 (右) 移动一个字符\nAlt – b\n往回 (左) 移动一个单词\nAlt – f\n往后 (右) 移动一个单词\nCtrl – xx\n在命令行尾和光标之间移动\nM-b\n往回 (左) 移动一个单词\nM-f\n往后 (右) 移动一个单词\n控制快捷键\nCtrl + L\n清除屏幕所有内容，并在屏幕最上面开始一个新行，等同 clear 命令。可以使用 Ctrl+L 清空终端，代替输入 C-L-E-A-R。\nCtrl + Z\n暂停执行在终端运行的任务。该快捷键将正在运行的程序送到后台。通常，你可以在使用 &amp; 选项运行程序前之完成该操作，但是如果你忘记使用选项运行程序，就使用这对组合键。\nCtrl + S\n锁定终端，使之无法输入内容。\nCtrl + Q\n解锁执行 Ctrl+s 的锁定状态。\n编辑命令（复制、粘贴和清除快捷键）\nlinux 命令行下，ctrl+c&#x2F;ctrl+v 已经不是复制粘贴了，下面这两个代替它们：复制： ctrl+insert粘贴： shift+insert\n\nCtrl + U\n剪切（删除）光标处到行首的字符。输入了错误的命令， 代替用退格键来丢弃当前命令，使用 Linux 终端中的 Ctrl+U 快捷键。 该快捷键会擦除从当前光标位置到行首的全部内容。\n\nCtrl + K\n剪切（删除）光标处到行尾的字符。这对和 Ctrl+U 快捷键有点像。 唯一的不同在于不是行首，它擦除的是从当前光标位置到行尾的全部内容。\nCtrl + W\n剪切（删除）光标前的一个单词。如果你只需要删除一个单词，使用 Ctrl+W 快捷键，你可以擦除光标位置前的单词。 如果光标在一个单词本身上，它将擦除从光标位置到词首的全部字母。最好的方法是用它移动光标到要删除单词后的一个空格上， 然后使用 Ctrl+W 键盘快捷键。\nCtrl + Y\n粘贴 Ctrl+u，Ctrl+k，Ctrl+w 删除的文本。 如果你删除了错误的文本或需要在某处使用已擦除的文本，这将派上用场。\nCtrl + C\n中断终端正在执行的任务或者删除整行。这些是为了在终端上中断命令或进程该按的键。它将立刻终止运行的程序。如果你想要停止使用一个正在后台运行的程序，只需按下这对组合键。\nCtrl – H\n删除光标左方位置的字符（相当于退格键）。\n重复执行命令快捷键\nCtrl + D\n退出当前 Shell 命令行。如果你使用 SSH 连接，它将会关闭。 如果你直接使用一个终端，该应用将会立刻关闭。把它当成 “退出” 命令。\nCtrl + R\n搜索命令行使用过的历史命令记录。搜索历史命令，随着输入会显示历史命令中的一条匹配命令，Enter 键执行匹配命令；ESC 键在命令行显示而不执行匹配命令。\nCtrl + G\n从执行 Ctrl+R 的搜索历史命令模式退出。\nEsc+.(点)\n获取上一条命令的最后的部分（空格分隔）\n重复执行操作动作\nM – 操作次数 操作动作 ： 指定操作次数，重复执行指定的操作。\n查找历史命令\nCtrl + P\n显示当前命令的上一条历史命令。可以使用该快捷键来查看上一个命令。 可以反复按该键来返回到历史命令。 在很多终端里，使用 PgUp 键来实现相同的功能。\nCtrl + N\n显示当前命令的下一条历史命令。可以结合 Ctrl+P 使用该快捷键。Ctrl+N 显示下一个命令。 如果使用 Ctrl+P 查看上一条命令，可以使用 Ctrl+N 来回导航。 许多终端都把此快捷键映射到 PgDn 键。\nCtrl + R\n搜索命令行使用过的历史命令记录。搜索历史命令，随着输入会显示历史命令中的一条匹配命令，Enter 键执行匹配命令；ESC 键在命令行显示而不执行匹配命令。\nCtrl + G\n从执行 Ctrl+R 的搜索历史命令模式退出。\nBang(!) 命令 ! 号开头的快捷命令\n!!：执行上一条命令。\n!pw：执行最近以 pw 开头的命令。\n!pw:p：仅打印最近 pw 开头的命令，但不执行。\n!num：执行历史命令列表的第 num(数字) 条命令。\n^foo^bar ：把上一条命令里的 foo 替换为 bar，并执行。\n!wget ：执行最近的以 wget 开头的命令。\n!wget:p ：仅打印最近的以 wget 开头的命令，不执行。\n!$ ：上一条命令的最后一个参数， 与 Alt - . 和 $_ 相同，相当于 Esc+.(点)。\n!* ：上一条命令的所有参数\n!*:p ：打印上一条命令是所有参数，也即 !*的内容。\n^abc ：删除上一条命令中的 abc。\n^foo^bar ：将上一条命令中的 foo 替换为 bar\n^foo^bar^ ：将上一条命令中的 foo 替换为 bar\n!-n ：执行前 n 条命令，执行上一条命令：!-1， 执行前 5 条命令的格式是：!-5\nESC 相关\nEsc+.(点)\n获取上一条命令的最后的部分（空格分隔）*\nEsc+b\n移动到当前单词的开头\nEsc+f\n移动到当前单词的结尾\nEsc+t\n颠倒光标所在处及其相邻单词的位置\nCtrl + 左右键：在单词之间跳转\nAlt – d ：由光标位置开始，往右删除单词。往行尾删\nCtrl – k: 先按住 Ctrl 键，然后再按 k 键；\nAlt – k: 先按住 Alt 键，然后再按 k 键；\nM – k：先单击 Esc 键，然后再按 k 键。\n编辑命令\nCtrl – h ：删除光标左方位置的字符\nCtrl – d ：删除光标右方位置的字符（注意：当前命令行没有任何字符时，会销系统或结束终端）\nCtrl – w ：由光标位置开始，往左删除单词。往行首删\nAlt – d ：由光标位置开始，往右删除单词。往行尾删\nM – d ：由光标位置开始，删除单词，直到该单词结束。\nCtrl – k ：由光标所在位置开始，删除右方所有的字符，直到该行结束。\nCtrl – u ：由光标所在位置开始，删除左方所有的字符，直到该行开始。\nCtrl – y ：粘贴之前删除的内容到光标后。\nctrl – t ：交换光标处和之前两个字符的位置。\nAlt + . ：使用上一条命令的最后一个参数。\nCtrl – _ ：回复之前的状态。撤销操作。\nCtrl -a + Ctrl -k 或 Ctrl -e + Ctrl -u 或 Ctrl -k + Ctrl -u 组合可删除整行。\n控制命令\nCtrl – l ：清除屏幕，然后，在最上面重新显示目前光标所在的这一行的内容。\nCtrl – o ：执行当前命令，并选择上一条命令。\nCtrl – s ：阻止屏幕输出\nCtrl – q ：允许屏幕输出\nCtrl – c ：终止命令\nCtrl – z ：挂起命令 https://zhuanlan.zhihu.com/p/97260846\n","slug":"LINUX/Linux 快捷键","date":"2021-09-06T13:24:10.000Z","categories_index":"Ctrl,LINUX","tags_index":"Alt,Esc,foo","author_index":"dandeliono"},{"id":"d80305d117df28e684a0a0d26ded383b","title":"ZooKeeper报错Last transaction was partial","content":"ZooKeeper报错Last transaction was partial\n\n\n\n\n\n\n\n\nZookeeper 无法正常启动，报错 Last transaction was partial\n[2020-11-28 10:28:59,552] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)\n[2020-11-28 10:28:59,556] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)\n[2020-11-28 10:28:59,588] ERROR Last transaction was partial. (org.apache.zookeeper.server.persistence.Util)\n[2020-11-28 10:28:59,589] ERROR Unexpected exception, exiting abnormally (org.apache.zookeeper.server.ZooKeeperServerMain)\njava.io.EOFException\n    at java.io.DataInputStream.readInt(DataInputStream.java:392)\n    at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:63)\n    at org.apache.zookeeper.server.persistence.FileHeader.deserialize(FileHeader.java:66)\n    at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.inStreamCreated(FileTxnLog.java:588)\n    at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.createInputArchive(FileTxnLog.java:607)\n    at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.goToNextLog(FileTxnLog.java:573)\n    at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.next(FileTxnLog.java:653)\n    at org.apache.zookeeper.server.persistence.FileTxnSnapLog.fastForwardFromEdits(FileTxnSnapLog.java:219)\n    at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:176)\n    at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:217)\n    at org.apache.zookeeper.server.ZooKeeperServer.loadData(ZooKeeperServer.java:284)\n    at org.apache.zookeeper.server.ZooKeeperServer.startdata(ZooKeeperServer.java:407)\n    at org.apache.zookeeper.server.NIOServerCnxnFactory.startup(NIOServerCnxnFactory.java:118)\n    at org.apache.zookeeper.server.ZooKeeperServerMain.runFromConfig(ZooKeeperServerMain.java:122)\n    at org.apache.zookeeper.server.ZooKeeperServerMain.initializeAndRun(ZooKeeperServerMain.java:89)\n    at org.apache.zookeeper.server.ZooKeeperServerMain.main(ZooKeeperServerMain.java:55)\n    at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:119)\n    at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:81) \n\n（1）删除 ZooKeeper 安装目录下的 logs\n无效果\n（2）删除 ZooKeeper 的数据文件夹下大小为 0 的文件\n数据文件夹位置可以通过 zookeeper 的配置文件得到。\n# the directory where the snapshot is stored. dataDir=/tmp/zookeeper \n\n删除该[dataDir]/version-2/下大小为 0 的快照文件，即可正常启动。\n（1）磁盘满了\n首先使用du -h --max-depth=1来逐层分析磁盘满的根本原因，比如我最终定位到 docker 文件夹，是因为 Docker 容器过多，需要将停止的容器清除，即可释放磁盘\n（2）ZooKeeper 快照无法写入（待考证）\n由于磁盘满，所以 ZooKeeper 无法将快照写入到磁盘，导致了大小为 0 的快照文件\n下次启动时，如果从快照中读取数据，就会出现Last transaction was partial\n https://neyzoter.cn/2020/11/28/Zookeeper-Last-Transaction-Was-Partial/\n","slug":"MIDDLEWARE/ZooKeeper报错Last transaction was partial","date":"2021-09-06T13:17:16.000Z","categories_index":"ZooKeeper,MIDDLEWARE","tags_index":"Last,transaction,was","author_index":"dandeliono"},{"id":"48d20d4467bd70f47406b938972138eb","title":"JSQLParser的简单使用","content":"JSQLParser的简单使用这里简单记录一下最近了解到的 JSQLParser 这个 SQL 语句解析器的用法，方便有需要的时候参考。\n正文：import net.sf.jsqlparser.JSQLParserException;\nimport net.sf.jsqlparser.parser.CCJSqlParserUtil;\nimport net.sf.jsqlparser.schema.Table;\nimport net.sf.jsqlparser.statement.Statement;\nimport net.sf.jsqlparser.statement.select.*;\nimport net.sf.jsqlparser.util.TablesNamesFinder;\n\nimport java.util.ArrayList;\nimport java.util.List;\n\npublic class JSQLParserTest &#123;\n    public static void main(String[] args) &#123;\n        String sql = &quot;SELECT\\n&quot; +\n                &quot;  a.id,\\n&quot; +\n                &quot;  username,\\n&quot; +\n                &quot;  b.workflow_name,\\n&quot; +\n                &quot;  reviewok_time\\n&quot; +\n                &quot;FROM archer.sql_users a\\n&quot; +\n                &quot;  JOIN sql_workflow b ON a.username = b.engineer\\n&quot; +\n                &quot;  JOIN archive.table3 c ON a.username = c.col5;\\n&quot;;\n\n        try &#123;\n            Statement stmt = CCJSqlParserUtil.parse(sql);\n            Select selectStatement = (Select) stmt;\n\n            TablesNamesFinder tablesNamesFinder = new TablesNamesFinder();\n            List&lt;String&gt; tableList = tablesNamesFinder.getTableList(selectStatement);\n            for (String name: tableList) &#123;\n                System.out.println(&quot;table: &quot; + name);\n            &#125;\n            System.out.println();\n\n            List&lt;String&gt; columnList = new ArrayList&lt;String&gt;();\n            PlainSelect ps = (PlainSelect) selectStatement.getSelectBody();\n            List&lt;SelectItem&gt; selectItems = ps.getSelectItems();\n            // System.out.println(selectItems);\n            selectItems.stream().forEach(selectItem -&gt; columnList.add(selectItem.toString()));\n            for (String name: columnList) &#123;\n                System.out.println(&quot;column: &quot; + name);\n            &#125;\n            System.out.println();\n\n            FromItem fromItem = ps.getFromItem();\n            Table table = (Table) fromItem;\n            System.out.println(table.getName() + &quot;:\\t&quot; + table.getAlias());\n\n            List&lt;Join&gt; joins = ps.getJoins();\n            for (Join join : joins) &#123;\n                fromItem = join.getRightItem();\n                table = (Table) fromItem;\n                System.out.println(table.getName() + &quot;:\\t&quot; + table.getAlias());\n            &#125;\n\n        &#125; catch (JSQLParserException e) &#123;\n            e.printStackTrace();\n        &#125;\n    &#125;\n\n&#125;\n\n简单来说就是，JSQLParser 使用起来还算简单，但就上面的测试 SQL 来看，效果难说完美（表名提取那里会把 archer.sql_users 里面的「archer.」给略掉，只剩下 sql_users）。\n据说「General Sql Parser’s (GSP)」的效果很好，不过是付费的（想来在很多方面要想把一件事情做好，还是需要专门的人力支持、维护，付费是有付费的道理的），还没有机会测试，以后有机会了再试试效果。\n参考链接：https://github.com/JSQLParser/JSqlParser/wiki/Examples-of-SQL-parsing\nJsqlparser 使用https://www.yuech.net/2015/10/09/Jsqlparser%E4%BD%BF%E7%94%A8/\njava-How to parse sql columns with JDBC or jSqlParser ?https://www.bswen.com/2019/05/android-How-to-parse-sql-columns-with-JDBC-or-jSqlParser.html\n使用 java sql parser 插件 Jsqlparser 实例 (一)https://blog.csdn.net/u014297722/article/details/53256533\nParsing table and column names from SQL&#x2F;HQL Javahttps://stackoverflow.com/questions/40908062/parsing-table-and-column-names-from-sql-hql-java\nhttps://stackoverflow.com/questions/16768365/how-to-retrieve-table-and-column-names-from-sql-using-jsqlparse\nhttps://www.programcreek.com/java-api-examples/index.php?api=net.sf.jsqlparser.schema.Columnhttps://www.programcreek.com/java-api-examples/?api=net.sf.jsqlparser.statement.select.AllTableColumnshttps://vimsky.com/examples/detail/java-class-net.sf.jsqlparser.schema.Table.html\nhttps://stackoverflow.com/questions/37250588/jsqlparser-getting-table-name-from-column\nJSQLParser 来分析复杂 SQL，实现 UI 业务一次 SQL 搞定https://www.jianshu.com/p/f57bc22b5b32\nJPA 表租户 SQL 解析实现https://www.codenong.com/js8ad3b4d6bd43/\n几种基于 Java 的 SQL 解析工具的比较与调用https://blog.csdn.net/qq_21383435&#x2F;article&#x2F;details&#x2F;81984297\n基于 Java 的 SQL 解析工具的比较与学习https://blog.csdn.net/czq850114000/article/details/80844689 https://ixyzero.com/blog/archives/4806.html\n","slug":"JAVA/JSQLParser的简单使用","date":"2021-09-01T16:57:01.000Z","categories_index":"https,JAVA","tags_index":"com,net,www","author_index":"dandeliono"},{"id":"b7ecf3954ed2a73caa330011a27b5abb","title":"为什么 TCP 协议有 TIME_WAIT 状态","content":"为什么 TCP 协议有 TIME_WAIT 状态\n\n\n\n\n\n\n\n\n为什么这么设计（Why’s THE Design）是一系列关于计算机领域中程序设计决策的文章，我们在这个系列的每一篇文章中都会提出一个具体的问题并从不同的角度讨论这种设计的优缺点、对具体实现造成的影响。\n在这个系列前面的文章中，我们已经多次讨论 TCP 协议的设计原理，其中包括 TCP 协议的 三次握手、流量控制和重传机制、最大数据段 以及 粘包 等问题。本文将继续分析 TCP 协议的实现细节，今天要分析的问题是为什么 TCP 协议需要 TIME_WAIT 状态以及该状态的作用究竟是什么。\nTCP 协议中包含 11 种不同的状态，TCP 连接会根据发送或者接收到的消息转换状态，如下图所示的状态机展示了所有可能的转换，其中不仅包含了正常情况下的状态转换过程，还包含了异常状态下的状态转换：\n\n图 1 - TCP 协议状态\n使用 TCP 协议通信的双方会在关闭连接时触发 TIME_WAIT 状态，关闭连接的操作其实是告诉通信的另一方自己没有需要发送的数据，但是它仍然保持了接收对方数据的能力，一个常见的关闭连接过程如下1：\n\n当客户端没有待发送的数据时，它会向服务端发送 FIN 消息，发送消息后会进入 FIN_WAIT_1 状态；\n服务端接收到客户端的 FIN 消息后，会进入 CLOSE_WAIT 状态并向客户端发送 ACK 消息，客户端接收到 ACK 消息时会进入 FIN_WAIT_2 状态；\n当服务端没有待发送的数据时，服务端会向客户端发送 FIN 消息；\n客户端接收到 FIN 消息后，会进入 TIME_WAIT 状态并向服务端发送 ACK 消息，服务端收到后会进入 CLOSED 状态；\n客户端等待两个最大数据段生命周期（Maximum segment lifetime，MSL）2的时间后也会进入 CLOSED 状态；\n\n\n图 2 - TCP 关闭连接的过程\n从上述过程中，我们会发现 TIME_WAIT 仅在主动断开连接的一方出现，被动断开连接的一方会直接进入 CLOSED 状态，进入 TIME_WAIT 的客户端需要等待 2 MSL 才可以真正关闭连接。TCP 协议需要 TIME_WAIT 状态的原因和客户端需要等待两个 MSL 不能直接进入 CLOSED 状态的原因是一样的3：\n\n防止延迟的数据段被其他使用相同源地址、源端口、目的地址以及目的端口的 TCP 连接收到；\n保证 TCP 连接的远程被正确关闭，即等待被动关闭连接的一方收到 FIN 对应的 ACK 消息；\n\n上述两个原因都相对比较简单，我们来展开介绍这两个原因背后可能存在的一些问题。\n阻止延迟数据段每一个 TCP 数据段都包含唯一的序列号，这个序列号能够保证 TCP 协议的可靠性和顺序性，在不考虑序列号溢出归零的情况下，序列号唯一是 TCP 协议中的重要约定，一旦违反了这条规则，就可能造成令人困惑的现象和结果。为了保证新 TCP 连接的数据段不会与还在网络中传输的历史连接的数据段重复，TCP 连接在分配新的序列号之前需要至少静默数据段在网络中能够存活的最长时间，即 MSL4：\n\n\n\n\n\n\n\n\n\nTo be sure that a TCP does not create a segment that carries a sequence number which may be duplicated by an old segment remaining in the network, the TCP must keep quiet for a maximum segment lifetime (MSL) before assigning any sequence numbers upon starting up or recovering from a crash in which memory of sequence numbers in use was lost.\n\n图 3 - TIME-WAIT 较短导致的数据段延迟接收\n在如上图所示的 TCP 连接中，服务端发送的 SEQ = 301 消息由于网络延迟直到 TCP 连接关闭后也没有收到；当使用相同端口号的 TCP 连接被重用后，SEQ = 301 的消息才发送到客户端，然而这个过期的消息却可能被客户端正常接收，这就会带来比较严重的问题，所以我们在调整 TIME_WAIT 策略时要非常谨慎，必须清楚自己在干什么。\nRFC 793 中虽然指出了 TCP 连接需要在 TIME_WAIT 中等待 2 倍的 MSL，但是并没有解释清楚这里的两倍是从何而来，比较合理的解释是 — 网络中可能存在来自发起方的数据段，当这些发起方的数据段被服务端处理后又会向客户端发送响应，所以一来一回需要等待 2 倍的时间5。\nRFC 793 文档将 MSL 的时间设置为 120 秒，即两分钟，然而这并不是一个经过严密推断的数值，而是工程上的选择，如果根据服务历史上的经验要求我们改变操作系统的设置，也是没有任何问题的；实际上，较早版本的 Linux 就开始将 TIME_WAIT 的等待时间 TCP_TIMEWAIT_LEN 设置成 60 秒，以便更快地复用 TCP 连接资源：\n在 Linux 上，客户端的可以使用端口号 32,768 ~ 61,000，总共 28,232 个端口号与远程服务器建立连接，应用程序可以在将近 3 万的端口号中任意选择一个：\n但是如果主机在过去一分钟时间内与目标主机的特定端口创建的 TCP 连接数超过 28,232，那么再创建新的 TCP 连接就会发生错误，也就是说如果我们不调整主机的配置，那么每秒能够建立的最大 TCP 连接数为 ~4706。\n保证连接关闭从 RFC 793 对 TIME_WAIT 状态的定义中，我们可以发现该状态的另一个重要作用，等待足够长的时间以确定远程的 TCP 连接接收到了其发出的终止连接消息 FIN 对应的 ACK：\n\n\n\n\n\n\n\n\n\nTIME-WAIT - represents waiting for enough time to pass to be sure the remote TCP received the acknowledgment of its connection termination request.\n如果客户端等待的时间不够长，当服务端还没有收到 ACK 消息时，客户端就重新与服务端建立 TCP 连接就会造成以下问题 — 服务端因为没有收到 ACK 消息，所以仍然认为当前连接是合法的，客户端重新发送 SYN 消息请求握手时会收到服务端的 RST 消息，连接建立的过程就会被终止。\n\n图 4 - TIME-WAIT 较短导致的握手终止\n在默认情况下，如果客户端等待足够长的时间就会遇到以下两种情况：\n\n服务端正常收到了 ACK 消息并关闭当前 TCP 连接；\n服务端没有收到 ACK 消息，重新发送 FIN 关闭连接并等待新的 ACK 消息；\n\n只要客户端等待 2 MSL 的时间，客户端和服务端之间的连接就会正常关闭，新创建的 TCP 连接收到影响的概率也微乎其微，保证了数据传输的可靠性。\n总结在某些场景下，60 秒的等待销毁时间确实是难以接受的，例如：高并发的压力测试。当我们通过并发请求测试远程服务的吞吐量和延迟时，本地就可能产生大量处于 TIME_WAIT 状态的 TCP 连接，在 macOS 上可以使用如下所示的命令查看活跃的连接：\n当我们在主机上通过几千个并发来测试服务器的压力时，这些用于压力测试的连接会迅速消耗主机上的 TCP 连接资源，几乎所有的 TCP 都会处于 TIME_WAIT 状态等待销毁。如果我们真遇到不得不处理单机上的 TIME_WAIT 状态的时候，那么可以通过以下几种方法处理：\n\n使用 SO_LINGER 选项并设置暂存时间 l_linger 为 0，在这时如果我们关闭 TCP 连接，内核就会直接丢弃缓冲区中的全部数据并向服务端发送 RST 消息直接终止当前的连接7；\n使用 net.ipv4.tcp_tw_reuse 选项，通过 TCP 的时间戳选项允许内核重用处于 TIME_WAIT 状态的 TCP 连接8；\n修改 net.ipv4.ip_local_port_range 选项中的可用端口范围，增加可同时存在的 TCP 连接数上限；\n\n\n\n\n\n\n\n\n\n\n需要注意的是，另一个常见的 TCP 配置项 net.ipv4.tcp_tw_recycle 已经在 Linux 4.12 中移除9，所以我们不能再通过该配置解决 TIME_WAIT 设计带来的问题。\nTCP 的 TIME_WAIT 状态有着非常重要的作用，它是保证 TCP 协议可靠性不可缺失的设计，如果能通过加机器解决的话就尽量加机器，如果不能解决的话，我们就需要理解其背后的设计原理并尽可能避免修改默认的配置，就像 Linux 手册中说的一样，在修改这些配置时应该咨询技术专家的建议；在这里，我们再重新回顾一下 TCP 协议中 TIME_WAIT 状态存在的原因，如果客户端等待的时间不够长，那么使用相同端口号重新与远程建立连接时会造成以下问题：\n\n因为数据段的网络传输时间不确定，所以可能会收到上一次 TCP 连接中未被收到的数据段；\n因为客户端发出的 ACK 可能还没有被服务端接收，服务端可能还处于 LAST_ACK 状态，所以它会回复 RST 消息终止新连接的建立；\n\nTIME_WAIT 状态是 TCP 与不确定的网络延迟斗争的结果，而不确定性是 TCP 协议在保证可靠这条路的最大阻碍。到最后，我们还是来看一些比较开放的相关问题，有兴趣的读者可以仔细思考一下下面的问题：\n\nnet.ipv4.tcp_tw_reuse 配置如何通过时间戳保证重用 TCP 连接的相对安全？\nnet.ipv4.tcp_tw_recycle 配置为什么被 Linux 从协议栈中移除？\n\n相关文章\n为什么 TCP 建立连接需要三次握手\n为什么 TCP&#x2F;IP 协议会拆分数据\n\n\n\n“3.5. Closing a Connection · Transmission Control Protocol RFC793” https://tools.ietf.org/html/rfc793#section-3.5 ↩︎\n“Wikipedia: Maximum segment lifetime” https://en.wikipedia.org/wiki/Maximum_segment_lifetime ↩︎\n“Can the time a socket spends in TIMED-WAIT state be reduced?” https://knowledgebase.progress.com/articles/Article/Can-the-time-a-socket-spends-in-TIMED-WAIT-state-be-reduced ↩︎\n“Knowing When to Keep Quiet · Transmission Control Protocol RFC793” https://tools.ietf.org/html/rfc793 ↩︎\n“Setting TIME_WAIT TCP” https://stackoverflow.com/questions/337115/setting-time-wait-tcp ↩︎\n“Coping with the TCP TIME-WAIT state on busy Linux servers” https://vincent.bernat.ch/en/blog/2014-tcp-time-wait-state-linux#connection-table-slot ↩︎\n“SO_LINGER Socket Option · Chapter 7. Socket Options” https://notes.shichao.io/unp/ch7/#so_linger-socket-option ↩︎\n“net.ipv4.tcp_tw_reuse” https://vincent.bernat.ch/en/blog/2014-tcp-time-wait-state-linux#netipv4tcp_tw_reuse ↩︎\n“tcp: remove tcp_tw_recycle” https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=4396e46187ca5070219b81773c4e65088dac50cc ↩︎https://draveness.me/whys-the-design-tcp-time-wait/\n\n","slug":"JAVA/为什么 TCP 协议有 TIME_WAIT 状态","date":"2021-08-31T17:36:55.000Z","categories_index":"TCP,JAVA","tags_index":"https,WAIT,TIME","author_index":"dandeliono"},{"id":"e618b0a0a36dd4779e822db97f59bf1f","title":"kafka启动报错 A broker is already registered on the path brokers ids 1 解决方案","content":"kafka启动报错”A broker is already registered on the path &#x2F;brokers&#x2F;ids&#x2F;1”解决方案问题#kafka 挂掉后，启动报错日志如下\n1234567891011[2020-03-19 17:50:58,123] FATAL Fatal error during KafkaServerStartable startup. Prepare to shutdown (kafka.server.KafkaServerStartable)java.lang.RuntimeException: A broker is already registered on the path /brokers/ids/1. This probably indicates that you either have configured a brokerid that is already in use, or else you have shutdown this broker and restarted it faster than the zookeeper timeout so it appears to be re-registering.        at kafka.utils.ZkUtils.registerBrokerInZk(ZkUtils.scala:408)        at kafka.utils.ZkUtils.registerBrokerInZk(ZkUtils.scala:394)        at kafka.server.KafkaHealthcheck.register(KafkaHealthcheck.scala:71)        at kafka.server.KafkaHealthcheck.startup(KafkaHealthcheck.scala:51)        at kafka.server.KafkaServer.startup(KafkaServer.scala:269)        at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:39)        at kafka.Kafka$.main(Kafka.scala:67)        at kafka.Kafka.main(Kafka.scala)[2020-03-19 17:50:58,123] INFO [Kafka Server 1], shutting down (kafka.server.KafkaServer)\n\n分析#从This probably indicates that you either have configured a brokerid that is already in use提示可知，zookeeper 中可能已经注册了此 broker id，正常情况下，你应该不会启动两个相同 broker id 的 kafka server(除非你没注意弄错了使得两个 kafka server 用了相同的 broker id)\n于是我用$KAFKA安装包下带有的zookeeper client 连接了 zk server，看一下 kafka broker 的注册情况\n1$KAFKA/bin/zookeeper-shell.sh 192.168.0.1:2181 ls /brokers/ids\n\n执行后显示\n123456Connecting to 192.168.0.1:2181WATCHER::WatchedEvent state:SyncConnected type:None path:null[1, 2, 3]\n\n然而实际情况是，broker id 为 1 的 kafka server 并没有启动起来。原因是这台机器之前因为卡死被物理重启，kafka broker 没有正常下线，zk 上还保留着它的 broker id。\n解决方案#找到原因后，解决就很简单了，把注册在zookeeper上的这个 broker id delete掉就行了\n1$KAFKA/bin/zookeeper-shell.sh 192.168.0.1:2181 delete  /brokers/ids/1\n\n然后再启动观察日志就正常了。 https://www.cnblogs.com/bytesfly/p/12527596.html\n","slug":"MIDDLEWARE/kafka启动报错 A broker is already registered on the path brokers ids 1 解决方案","date":"2021-08-31T15:48:56.000Z","categories_index":"broker,MIDDLEWARE","tags_index":"server,kafka,解决方案","author_index":"dandeliono"},{"id":"2f5dd4c8f4fc8dc651e872f7c4bd5927","title":"Linux 设置开机启动项的几种方法","content":"Linux 设置开机启动项的几种方法方法一：编辑 rc.loacl 脚本Ubuntu 开机之后会执行 &#x2F; etc&#x2F;rc.local 文件中的脚本。\n所以我们可以直接在 &#x2F; etc&#x2F;rc.local 中添加启动脚本。\n12$ vim /etc/rc.local\n\n方法二：添加一个开机启动服务。将你的启动脚本复制到 &#x2F;etc&#x2F;init.d 目录下，并设置脚本权限, 假设脚本为 test\n123$ mv test /etc/init.d/test$ sudo chmod 755 /etc/init.d/test\n\n将该脚本放倒启动列表中去\n123$ cd .etc/init.d$ sudo update-rc.d test defaults 95\n\n注：其中数字 95 是脚本启动的顺序号，按照自己的需要相应修改即可。在你有多个启动脚本，而它们之间又有先后启动的依赖关系时你就知道这个数字的具体作用了。\n将该脚本从启动列表中剔除\n123$ cd /etc/init.d$ sudo update-rc.d -f test remove\n","slug":"LINUX/Linux 设置开机启动项的几种方法","date":"2021-08-30T15:35:09.000Z","categories_index":"etc,LINUX","tags_index":"local,Linux,设置开机启动项的几种方法","author_index":"dandeliono"},{"id":"6080594d045265903c518febf773294c","title":"Redis key 过期","content":"Redis key 过期*Keys with an expire 有过期的 keyNormally Redis keys are created without an associated time to live. The key will simply live forever, unless it is removed by the user in an explicit way, for instance using the DEL command.\n通常，Redis 键是在没有相关生存时间的情况下创建的。密钥将永远存在，除非用户以显式的方式删除它，例如使用 DEL 命令。\nThe EXPIRE family of commands is able to associate an expire to a given key, at the cost of some additional memory used by the key. When a key has an expire set, Redis will make sure to remove the key when the specified amount of time elapsed.\n终止命令系列能够将终止期限关联到给定的密钥，代价是该密钥所使用的一些额外内存。如果密钥设置了过期设置，Redis 将确保在指定的时间过期时删除密钥。\nThe key time to live can be updated or entirely removed using the EXPIRE and PERSIST command (or other strictly related commands).\n可以使用 EXPIRE 和 PERSIST 命令 (或其他严格相关的命令) 更新或完全删除关键存活时间。\n*Expire accuracy 过期精度In Redis 2.4 the expire might not be pin-point accurate, and it could be between zero to one seconds out.\n在 Redis 2.4 中，过期值可能不精确，而且可能在 0 到 1 秒之间。\nSince Redis 2.6 the expire error is from 0 to 1 milliseconds.\n由于 Redis 2.6，过期错误从 0 毫秒到 1 毫秒。\n*Expires and persistence 过期和持久性Keys expiring information is stored as absolute Unix timestamps (in milliseconds in case of Redis version 2.6 or greater). This means that the time is flowing even when the Redis instance is not active.\n到期的键信息以绝对 Unix 时间戳的形式存储 (在 Redis 2.6 或更大版本中，以毫秒为单位)。这意味着即使 Redis 实例不活动，时间也是流动的。\nFor expires to work well, the computer time must be taken stable. If you move an RDB file from two computers with a big desync in their clocks, funny things may happen (like all the keys loaded to be expired at loading time).\n为了使计算机正常工作，计算机时间必须稳定。如果您将 RDB 文件从两台计算机的时钟中带有大的 desync 的计算机中移动，可能会发生有趣的事情 (比如加载时加载的所有密钥都过期了)。\nEven running instances will always check the computer clock, so for instance if you set a key with a time to live of 1000 seconds, and then set your computer time 2000 seconds in the future, the key will be expired immediately, instead of lasting for 1000 seconds.\n即使是正在运行的实例也总是会检查计算机时钟，所以举例来说，如果你设置一个时间为 1000 秒的密钥，然后在未来设置你的计算机时间为 2000 秒，这个密钥将立即过期，而不是持续 1000 秒。\n*How Redis expires keys 如何 Redis 过期键Redis keys are expired in two ways: a passive way, and an active way.\nRedis 密钥有两种过期方式: 被动过期和主动过期。\nA key is passively expired simply when some client tries to access it, and the key is found to be timed out.\n当某个客户端试图访问密钥时，密钥被动地过期，并且发现密钥已超时。\nOf course this is not enough as there are expired keys that will never be accessed again. These keys should be expired anyway, so periodically Redis tests a few keys at random among keys with an expire set. All the keys that are already expired are deleted from the keyspace.\n当然，这是不够的，因为有过期的密钥将永远不会再次访问。这些密钥无论如何都应该过期，所以 Redis 会定期在具有过期设置的密钥之间随机测试一些密钥。所有已过期的密钥都将从密钥空间中删除。\nSpecifically this is what Redis does 10 times per second:\n具体来说，这就是 Redis 每秒钟做的 10 次:\n\nTest 20 random keys from the set of keys with an associated expire. 测试来自带有关联过期值的键集合的 20 个随机键\nDelete all the keys found expired. 删除所有过期的密钥\nIf more than 25% of keys were expired, start again from step 1. 如果超过 25% 的密钥过期，从第一步开始重新启动\n\nThis is a trivial probabilistic algorithm, basically the assumption is that our sample is representative of the whole key space, and we continue to expire until the percentage of keys that are likely to be expired is under 25%\n这是一个简单的概率算法，基本上假设我们的样本代表了整个键空间，我们继续过期，直到可能过期的键的百分比低于 25%\nThis means that at any given moment the maximum amount of keys already expired that are using memory is at max equal to max amount of write operations per second divided by 4.\n这意味着在任何给定时刻，正在使用内存的已过期键的最大数量等于每秒写操作的最大数量除以 4。\n*How expires are handled in the replication link and AOF file 在复制链接和 AOF 文件中如何处理过期In order to obtain a correct behavior without sacrificing consistency, when a key expires, a DEL operation is synthesized in both the AOF file and gains all the attached replicas nodes. This way the expiration process is centralized in the master instance, and there is no chance of consistency errors.\n为了在不牺牲一致性的情况下获得正确的行为，当一个密钥失效时，在 AOF 文件中合成一个 DEL 操作，并获得所有附加的副本节点。通过这种方式，过期过程集中在主实例中，不会出现一致性错误。\nHowever while the replicas connected to a master will not expire keys independently (but will wait for the DEL coming from the master), they’ll still take the full state of the expires existing in the dataset, so when a replica is elected to master it will be able to expire the keys independently, fully acting as a master.\n然而，虽然连接到主控制器的副本不会独立地过期密钥 (但是会等待主控制器发出 DEL) ，但是它们仍然会接受数据集中已经存在的过期的完整状态，因此当一个副本被选择为主控制器时，它将能够独立地过期密钥，充当主控制器。 https://redis.io/commands/expire#how-redis-expires-keys\n","slug":"JAVA/Redis key 过期","date":"2021-08-26T18:26:40.000Z","categories_index":"the,JAVA","tags_index":"Redis,keys,expire","author_index":"dandeliono"},{"id":"d258daee60faadc2bf7311dc982adc47","title":"ConditionalOnProperty控制Bean对象加载","content":"@ConditionalOnProperty控制Bean对象加载1、简介\n 　　SpringBoot 通过 @ConditionalOnProperty 来控制 @Configuration 是否生效\n2、说明\n@Retention(RetentionPolicy.RUNTIME)\n@Target(&#123; ElementType.TYPE, ElementType.METHOD &#125;)\n@Documented\n@Conditional(OnPropertyCondition.class) public @interface ConditionalOnProperty &#123;\n\n   String\\[\\] value() default &#123;&#125;; //数组，获取对应property名称的值，与name不可同时使用 \n String prefix() default &quot;&quot;;//property名称的前缀(最后一个属性前的一串。比如aaa.bbb.ccc，则prefix为aaa.bbb)，可有可无 \n String\\[\\] name() default &#123;&#125;;//数组，property完整名称或部分名称（可与prefix组合使用，组成完整的property名称），与value不可同时使用 \n String havingValue() default &quot;&quot;;//可与name组合使用，比较获取到的属性值与havingValue给定的值是否相同，相同才加载配置 \n boolean matchIfMissing() default false;//缺省配置。配置文件没有配置时，表示使用当前property。配置文件与havingValue匹配时，也使用当前property\n boolean relaxedNames() default true;//是否可以松散匹配，至今不知道怎么使用的 \n&#125;\n\n3、单个示例\n　a、建立一个 Configuration 配置类:\n1234567@Data@Configuration@ConditionalOnProperty(value \\= &quot;test.conditional&quot;, havingValue = &quot;abc&quot;) public class TestConfiguration &#123; /\\*\\* 默认值 \\*/    private String field = &quot;default&quot;;&#125;\n\n　b、建立一个测试类：\n@RestController\n@RequestMapping(&quot;test&quot;) public class TestController &#123;\n\n    @Autowired(required \\= false) private TestConfiguration test1Configuration;\n\n    @GetMapping(&quot;testConfiguration&quot;) public String testConfiguration() &#123; if (test1Configuration != null) &#123; return test1Configuration.getField();\n        &#125; return &quot;OK&quot;;\n    &#125;\n\n&#125;\n\nc、配置文件 application.yml：\n d、通过 postman 或者其它工具发送请求。结果如下：\n以上表明 TestConfiguration 配置文件生效了\n4、多种示例\n接下来，改变 @ConditionalOnProperty 中的各个属性，然后通过查看返回结果来判断 TestConfiguration 是否生效。\n1、不配置 @ConditionalOnProperty，直接生效。\n2、只有 value 属性，没有 havingValue 属性。如果 application.yml 配置了 test.conditional 则生效，否则不生效。  \n@ConditionalOnProperty(value = &quot;test.conditional&quot;)\n\n3、prefix + name 相当于 value 属性 (两者不可同时使用)。如果 application.yml 配置了 test.conditional 则生效，否则不生效\n@ConditionalOnProperty(prefix = &quot;test&quot;, name = &quot;conditional&quot;)\n\n4、name 属性为一个数组，当要匹配多个值时，如果 application.yml 的配置与 name 属性中的值一一匹配则生效，否则不生效\n@ConditionalOnProperty(prefix = &quot;test&quot;, name = &#123; &quot;conditional&quot;, &quot;other&quot; &#125;)\n\ntest:\n  conditional: abc\n  other: edf\n\n5、当 matchIfMissing&#x3D;true 时：\n a、如果 application.yml 配置了 test.conditional 则生效 (此时 matchIfMissing 可有可无)，否则不生效\n b、如果 application.yml 啥都没配置则生效\n@ConditionalOnProperty(prefix = &quot;test&quot;, name = &quot;conditional&quot;, matchIfMissing = true)\n\n6、加上 havingValue 属性。当 havingValue 的值与 application.yml 文件中 test.conditional 的值一致时则生效，否则不生效\n@ConditionalOnProperty(prefix = &quot;test&quot;, name = &quot;conditional&quot;, havingValue = &quot;abc&quot;)\n\n7、加上 havingValue 属性。name 属性为数组时，如果 application.yml 文件中配置了相关属性且值都一致时则生效，否则不生效\n@ConditionalOnProperty(prefix = &quot;test&quot;, name = &#123; &quot;conditional&quot;, &quot;other&quot; &#125;, havingValue = &quot;abc&quot;)\n\ntest:\n  conditional: abc\n  other: abc\n\n https://www.cnblogs.com/xuwenjin/p/12603232.html\n","slug":"JAVA/@ConditionalOnProperty控制Bean对象加载","date":"2021-08-25T15:27:06.000Z","categories_index":"application,JAVA","tags_index":"yml,如果,否则不生效","author_index":"dandeliono"},{"id":"549d64a763c7476fdfa760218054955e","title":"top linux下的任务管理器","content":"top linux下的任务管理器Linux Tools Quick Tutorial\ntop 命令是 Linux 下常用的性能分析工具，能够实时显示系统中各个进程的资源占用状况，类似于 Windows 的任务管理器。top 是一个动态显示过程, 即可以通过用户按键来不断刷新当前状态. 如果在前台执行该命令, 它将独占前台, 直到用户终止该程序为止. 比较准确的说, top 命令提供了实时的对系统处理器的状态监视. 它将显示系统中 CPU 最 “敏感” 的任务列表. 该命令可以按 CPU 使用. 内存使用和执行时间对任务进行排序；而且该命令的很多特性都可以通过交互式命令或者在个人定制文件中进行设定。\n$top    top - 09:14:56 up 264 days, 20:56,  1 user,  load average: 0.02, 0.04, 0.00    Tasks:  87 total,   1 running,  86 sleeping,   0 stopped,   0 zombie    Cpu(s):  0.0%us,  0.2%sy,  0.0%ni, 99.7%id,  0.0%wa,  0.0%hi,  0.0%si,  0.2%st    Mem:    377672k total,   322332k used,    55340k free,    32592k buffers    Swap:   397308k total,    67192k used,   330116k free,    71900k cached    PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND    1 root      20   0  2856  656  388 S  0.0  0.2   0:49.40 init    2 root      20   0     0    0    0 S  0.0  0.0   0:00.00 kthreadd    3 root      20   0     0    0    0 S  0.0  0.0   7:15.20 ksoftirqd&#x2F;0    4 root      RT   0     0    0    0 S  0.0  0.0   0:00.00 migration&#x2F;0\n\n第一行\n\n09:14:56 ： 系统当前时间\n264 days, 20:56 ： 系统开机到现在经过了多少时间\n1 users ： 当前 2 用户在线\nload average: 0.02, 0.04, 0.00： 系统 1 分钟、5 分钟、15 分钟的 CPU 负载信息\n\n\n第二行\n\nTasks：任务;\n87 total：很好理解，就是当前有 87 个任务，也就是 87 个进程。\n1 running：1 个进程正在运行\n86 sleeping：86 个进程睡眠\n0 stopped：停止的进程数\n0 zombie：僵死的进程数\n\n\n第三行\n\nCpu(s)：表示这一行显示 CPU 总体信息\n0.0%us：用户态进程占用 CPU 时间百分比，不包含 renice 值为负的任务占用的 CPU 的时间。\n0.7%sy：内核占用 CPU 时间百分比\n0.0%ni：改变过优先级的进程占用 CPU 的百分比\n99.3%id：空闲 CPU 时间百分比\n0.0%wa：等待 I&#x2F;O 的 CPU 时间百分比\n0.0%hi：CPU 硬中断时间百分比\n0.0%si：CPU 软中断时间百分比\n注：这里显示数据是所有 cpu 的平均值，如果想看每一个 cpu 的处理情况，按 1 即可；折叠，再次按 1；\n\n\n第四行\n\nMen：内存的意思\n8175320kk total：物理内存总量\n8058868k used：使用的物理内存量\n116452k free：空闲的物理内存量\n283084k buffers：用作内核缓存的物理内存量\n\n\n第五行\n\nSwap：交换空间\n6881272k total：交换区总量\n4010444k used：使用的交换区量\n2870828k free：空闲的交换区量\n4336992k cached：缓冲交换区总量\n\n\n进程信息\n\n再下面就是进程信息：\nPID：进程的 ID\nUSER：进程所有者\nPR：进程的优先级别，越小越优先被执行\nNInice：值\nVIRT：进程占用的虚拟内存\nRES：进程占用的物理内存\nSHR：进程使用的共享内存\nS：进程的状态。S 表示休眠，R 表示正在运行，Z 表示僵死状态，N 表示该进程优先值为负数\n%CPU：进程占用 CPU 的使用率\n%MEM：进程使用的物理内存和总内存的百分比\nTIME+：该进程启动后占用的总的 CPU 时间，即占用 CPU 使用时间的累加值。\nCOMMAND：进程启动命令名称\n\n\n\n8.1. top 命令交互操作指令¶下面列出一些常用的 top 命令操作指令\n\n\n\n\n\n\n\n\n\n\nq：退出 top 命令\n：立即刷新\ns：设置刷新时间间隔\nc：显示命令完全模式\nt:：显示或隐藏进程和 CPU 状态信息\nm：显示或隐藏内存状态信息\nl：显示或隐藏 uptime 信息\nf：增加或减少进程显示标志\nS：累计模式，会把已完成或退出的子进程占用的 CPU 时间累计到父进程的 MITE+\nP：按 %CPU 使用率排行\nT：按 MITE + 排行\nM：按 %MEM 排行\nu：指定显示用户进程\nr：修改进程 renice 值\nkkill：进程\ni：只显示正在运行的进程\nW：保存对 top 的设置到文件 ^&#x2F;.toprc，下次启动将自动调用 toprc 文件的设置。\nh：帮助命令。\nq：退出\n\n注：强调一下，使用频率最高的是 P、T、M，因为通常使用 top，我们就想看看是哪些进程最耗 cpu 资源、占用的内存最多； 注：通过”shift + &gt;” 或”shift + &lt;” 可以向右或左改变排序列 如果只需要查看内存：可用 free 命令。只查看 uptime 信息（第一行），可用 uptime 命令；\n8.2. 实例¶实例 1：多核 CPU 监控¶在 top 基本视图中，按键盘数字 “1”，可监控每个逻辑 CPU 的状况；\n[rdtfr@bl685cb4-t ^]$ toptop - 09:10:44 up 20 days, 16:51,  4 users,  load average: 3.82, 4.40, 4.40Tasks: 1201 total,  10 running, 1189 sleeping,   0 stopped,   2 zombieCpu0  :  1.3%us,  2.3%sy,  0.0%ni, 96.4%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%stCpu1  :  1.3%us,  2.6%sy,  0.0%ni, 96.1%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%stCpu2  :  1.0%us,  2.0%sy,  0.0%ni, 92.5%id,  0.0%wa,  0.0%hi,  4.6%si,  0.0%stCpu3  :  3.9%us,  7.8%sy,  0.0%ni, 83.2%id,  0.0%wa,  0.0%hi,  5.2%si,  0.0%stCpu4  :  4.2%us, 10.4%sy,  0.0%ni, 63.8%id,  0.0%wa,  0.0%hi, 21.5%si,  0.0%stCpu5  :  6.8%us, 12.7%sy,  0.0%ni, 80.5%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%stCpu6  :  2.9%us,  7.2%sy,  0.0%ni, 85.3%id,  0.0%wa,  0.0%hi,  4.6%si,  0.0%stCpu7  :  6.2%us, 13.0%sy,  0.0%ni, 75.3%id,  0.0%wa,  0.0%hi,  5.5%si,  0.0%stMem:  32943888k total, 32834216k used,   109672k free,   642704k buffersSwap: 35651576k total,  5761928k used, 29889648k free, 16611500k cached\n实例 2：高亮显示当前运行进程¶在 top 基本视图中, 按键盘 “b”（打开 &#x2F; 关闭加亮效果）；\n实例 3：显示完整的程序命令¶命令：top -c\n[rdtfr@bl685cb4-t ^]$ top -ctop - 09:14:35 up 20 days, 16:55,  4 users,  load average: 5.77, 5.01, 4.64Tasks: 1200 total,   5 running, 1192 sleeping,   0 stopped,   3 zombieCpu(s):  4.4%us,  6.0%sy,  0.0%ni, 83.8%id,  0.2%wa,  0.0%hi,  5.5%si,  0.0%stMem:  32943888k total, 32842896k used,   100992k free,   591484k buffersSwap: 35651576k total,  5761808k used, 29889768k free, 16918824k cachedPID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND2013 apache    18   0  403m  88m 5304 S 25.0  0.3   6:37.44 &#x2F;usr&#x2F;sbin&#x2F;httpd18335 pubtest   22   0 65576  996  728 R  7.8  0.0   0:00.24 netstat -naltp16499 rdtfare   15   0 13672 2080  824 R  2.6  0.0   0:00.38 top -c29684 rdtfare   15   0 1164m 837m  14m S  2.3  2.6 148:47.54 .&#x2F;autodata data1.txt12976 pubtest   18   0  238m 9000 1932 S  1.6  0.0 439:28.44 tscagent -s TOEV_P\n实例 4：显示指定的进程信息¶命令：top -p pidid\n&#x2F;opt&#x2F;app&#x2F;tdv1&#x2F;config#top -p 17265top - 09:17:34 up 455 days, 17:55,  2 users,  load average: 3.76, 4.56, 4.46Tasks:   1 total,   0 running,   1 sleeping,   0 stopped,   0 zombieCpu(s):  7.8%us,  1.9%sy,  0.0%ni, 89.2%id,  0.0%wa,  0.1%hi,  1.0%si,  0.0%stMem:   8175452k total,  8103988k used,    71464k free,   268716k buffersSwap:  6881272k total,  4275424k used,  2605848k free,  6338184k cachedPID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND17265 tdv1      15   0 56504  828  632 S  0.0  0.0 195:53.25 redis-server\n指定进程信息有多个时，需要结合其它工具将回车替换为,（-p 支持 pid,pid,pid 语法）\n命令：top -p pgrep MULTI_PROCESS | tr “\\n” ”,” | sed ‘s&#x2F;,$&#x2F;&#x2F;’\n&#x2F;opt&#x2F;app&#x2F;tdv1$top -p `pgrep java | tr “\\\\n””,” | sed ‘s&#x2F;,$&#x2F;&#x2F;‘`top - 14:05:31 up 53 days,  2:43,  9 users,  load average: 0.29, 0.34, 0.22Tasks:   3 total,   0 running,   3 sleeping,   0 stopped,   0 zombieCpu(s):  5.9%us,  8.2%sy,  0.0%ni, 86.0%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%stMem:  66082088k total, 29512860k used, 36569228k free,   756352k buffersSwap: 32767992k total,  1019900k used, 31748092k free, 15710284k cached\n  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND                                          27855 rdtfare   20   0 4454m 1.3g 5300 S  0.7  2.0 338:31.37 java 2034 jenkins   20   0 18.3g 5.2g 5284 S  0.3  8.2  56:02.38 java                                             12156 rdtfare   20   0 4196m 1.2g  12m S  0.3  2.0  86:34.62 java\n8.3. 更强大的工具¶htop¶htop 是一个 Linux 下的交互式的进程浏览器，可以用来替换 Linux 下的 top 命令。\n与 Linux 传统的 top 相比，htop 更加人性化。它可让用户交互式操作，支持颜色主题，可横向或纵向滚动浏览进程列表，并支持鼠标操作。\n与 top 相比，htop 有以下优点：\n\n可以横向或纵向滚动浏览进程列表，以便看到所有的进程和完整的命令行。\n在启动上，比 top 更快。\n杀进程时不需要输入进程号。\nhtop 支持鼠标操作。https://linuxtools-rst.readthedocs.io/zh_CN&#x2F;latest&#x2F;tool&#x2F;top.html\n\n","slug":"LINUX/top linux下的任务管理器","date":"2021-08-23T17:12:55.000Z","categories_index":"top,LINUX","tags_index":"CPU,total,Cpu","author_index":"dandeliono"},{"id":"8cbbd86cd3c93044bc5b65f7c8f71a1c","title":"InfluxDB 通识篇","content":"InfluxDB 通识篇什么是时序数据库？时序数据库全称时间序列数据库，英文名 Time Series DataBase，缩写 TSDB。\n这种数据库专门用作处理时间序列数据。\n那什么是时间序列数据呢？就是随着时间变化而源源不断产生的数据。\n举个例子，Mac 电脑上的活动监视器就是一种时间序列数据，每隔几秒它都会获取电脑上各个部件最新的数据。\n\n为什么需要时序数据库？随着物联网和大数据时代的到来，全球每天产生的数据量大到令人难以想象。这些数据受到业务场景的限制分为不同的种类，每个种类对存储都有不同的要求。单凭传统的 RDBMS 很难完成各种复杂场景下的数据存储。\n这时我们就需要根据不同的数据特性和业务场景的要求，选择不同的数据库。\n一般选择使用哪个数据库，要从低响应时间（Low Response Time）、高可用性（High Availability）、高并发（High Concurrency）、海量数据（Big Data）和可承担成本（Affordable Cost）五个维度去权衡。\n数据库的种类非常繁多，举几个常见的类型来对比一下各自的特点。\n关系型数据库主流代表有 MySQL、Oracle 等，优点是具有 ACID 的特性，各方面能力都比较均衡。缺点是查询、插入和修改的性能都很一般。\nKV 数据库主流带代表有 Redis、Memcached 等，优点是存储简单、读写性能极高。缺点是存储成本非常高，不适合海量数据存储。\n文档型数据库最流行的是 MongoDB，相比 MySQL，数据结构灵活、更擅长存储海量数据，在海量数据的场景下读写性能很强。缺点是占用空间会很大。\n搜索引擎数据库最流行的是 ElasticSearch，非常擅长全文检索和复杂查询，性能极强，并且天生集群。缺点是写入性能低、字段类型无法修改、硬件资源消耗严重。\n而时序数据库，最初诞生的目的很大程度上是在对标 MongoDB，因为在时序数据库出现之前，存储时序数据这项领域一直被 MongoDB 所占据。\n时序数据库一哥 InfluxDB 的公司 InfluxData，曾在 2018 年发表了一篇关于 InfluxDB vs MongoDB 的博客。文中使用 InfluxDB v1.7.2 和 MongoDB v4.0.4 做对比，得出 InfluxDB 比 MongoDB 快 2.4 倍的结论。当然可信度有待考量。\n总之，时序数据库的特点是：持续高性能写入、高性能查询、低存储成本、支持海量时间线、弹性。\n为什么选择 InfluxDB？虽然时序数据库早在 13 年左右就已经出现，但真正流行的时间非常晚，一直到了 17、18 年才稍微普及，即使到了今天，时序数据库在 DB Engiens 的数据库排名中仍然很落后，最靠前的 InfluxDB 也仅仅排在了 28 位。\n选择 InfluxDB 的原因非常简单，因为它目前是最流行的时序数据库。\nInfluxDB 之所以能够从众多时序数据库中脱颖而出，除了自身强大以外，活跃的社区、合理的商业模式和营销功不可没。\n\n既然时序数据库很好，为什么排名如此考后？原因是应用场景少，记录运维监控和物联网实时数据是时序数据库的用武之地。而大多数的系统使用 MySQL、MongoDB 和 Redis 这些主流数据库就可以很好地支撑。\n除了 InfluxDB 以外，还有几个比较流行的时序数据库，比如基于 PostgreSQL 的 TimeScaleDB，目前排在 97 位。基于 HBase 的 OpenTSDB，排在 122 位。基于 Cassandra 的 KairosDB，目前排在 201 位。\nInfluxDB 简介InfluxDB 是 InfluxData 公司在 2013 年开源的数据库，是为了存储物联网设备、DevOps 运维这类场景下大量带有时间戳数据而设计的。\nInfluxDB 源码采用 Go 语言编写，在 InfluxDB OSS 的版本中，部署方式上又分为两个版本，单机版和集群版。单机版开源，目前在 github 上有 21k+ star。集群版闭源，走商业路线。\n个人认为单机版的 InfluxDB 比较鸡肋。因为一旦选择使 InfluxDB，那么数据量肯定一定达到了某个很高的程度。这时候必须使用集群版。而在数据量不够高的情况下，InfluxDB 并不会比 MongoDB 或者 ElasticSearch 有更明显的优势。\n考虑到学习成本、简化上手难度，InfluxDB1.x 采用了一种类似 SQL 的 InfluxQL 语言来操作数据。2019 年 1 月推出了 InfluxDB2.0 alpha 版本。受到 2018 年最流行的脚本语言 JavaScript 影响，推出了全新的查询语言 Flux。并在 2020 年底推出了 InfluxDB 2.0 正式版本，该版本又分为两个系列，云模式的 InfluxDB Cloud 和独立部署的 InfluxDB OSS。\nFlux 不是绑定在 InfluxDB 上的查询脚本语言，它是一个独立的项目，图灵完备，便于处理数据，也可以用作 InfluxDB 以外。\n由于 InfluxDB 的流行程度不高，而且 2.0 版本也推出不久，所以在国内搜索到的很多 InfluxDB 相关资料都是在讲述 1.x 的内容，参考意义不大，目前最好的学习路径是 InfluxDB 官方文档。本文内容全部基于 InfluxDB OSS 2.0 版本。\nTICK 与 InfluxDBTICK 是 InfluxData 平台的组件集的缩写，分别代表四大组件：Telegraf（数据收集器）、InfluxDB（时序数据库）、Chronograf（可视化 UI）和 Kapacitor（监控服务）。\nInfluxData 公司的愿景是帮助人们处理时序数据，仅依靠一个时序数据库是不够的，还需要解决因为时序数据自身产生的一系列问题。因此 InfluxData 决定设计并开发 TICK。\n在早期 Kapacitor 的脚本语言是 TICKScript，但是并不好用，遭受到社区中很多人的诟病。因此出现了 Flux。Flux 的功能性比 InfluxQL 更强，比 TICKScript 更易用。\n随着 Flux 的逐渐发展，InfluxDB 的能力范围也在逐步扩展。\n基本概念InfluxDB 的数据模型概念和 RDBMS 稍有不同，下面是和 MySQL 的概念对照表。\n\n\n\nInfluxDB\nMySQL\n解释\n\n\n\nBuckets\nDatabase\n数据桶 - 数据库，即存储数据的命名空间。\n\n\nMeasurement\nTable\n度量 - 表。\n\n\nPoint\nRecord\n数据点 - 记录。\n\n\nField\nField\n未设置索引的字段。\n\n\nTag\nIndex\n设置了索引的字段。\n\n\n安装部署安装方式有好几种，这里介绍如何使用 Docker 进行安装。\n首先从 Docker 拉取镜像：\n1docker pull influxdb\n\n然后快速创建容器：\n1docker run -d --name influxdb -p 8086:8086 influxdb\n\n启动成功后访问 http://127.0.0.1:8086/ 就可以看到页面了。\n\n之后填写初始化信息，完成初始化。\n\n基本配置将数据持久化到 Docker 容器之外首先创建一个目录。\n1mkdir influxdb_docker_data_volume &amp;&amp; cd $_\n\n在这个目录下运行启动命令，并添加 volume 参数。\n1docker run -d --name influxdb -p 8086:8086 --volume $PWD:/root/.influxdbv2 influxdb\n\n这样容器中的数据就会存储到当前目录。\n写数据InfluxDB 写数据的方式有两种，一是使用不同语言的客户端库，二是使用 Telegraf 插件。\n这里介绍使用客户端库来进行写数据。\n在 Load Data 页面上，有 Sources 标签，其中又有 Client Libraries 和 Telegraf Plugins 两个分类。\n\n这里选择 Go 语言，点开后会有示例代码。\n写数据最少需要 4 个基础信息。\n\n组织 ID（org ID）\n存储桶（bucket ID）\n身份认证令牌（authentication token）\n数据库地址（InfluxDB URL）\n\n写数据的数据格式有两种，第一种是 InfluxDB Line Protocol 格式。\nInfluxDB Line Protocol完整代码如下：\n123456789101112131415161718192021222324252627package mainimport (\t&quot;fmt&quot;\tinfluxdb2 &quot;github.com/influxdata/influxdb-client-go/v2&quot;)func main() &#123;\t\tconst token = &quot;vgqVL_p-qbSpQO0DIzU4QcRgaEyQM-wcEmK2cOkDUHAiQYwLOba7qEZr9Xcq3YvZ2UH-ovu9RG7XkMwChO6eeA==&quot;\tconst bucket = &quot;test&quot;\tconst org = &quot;lzq&quot;\tclient := influxdb2.NewClient(&quot;http://127.0.0.1:8086&quot;, token)\t\tdefer client.Close()\t\twriteAPI := client.WriteAPI(org, bucket)\t\twriteAPI.WriteRecord(fmt.Sprintf(&quot;stat,unit=temperature avg=%f,max=%f&quot;, 23.5, 45.0))\twriteAPI.WriteRecord(fmt.Sprintf(&quot;stat,unit=temperature avg=%f,max=%f&quot;, 22.5, 45.0))\t\twriteAPI.Flush()&#125;\n\nInfluxDB Line Protocol 本质上是一个具有约定格式的字符串。通过这个字符串形成一个记录，这个字符串必须包含一个测量（measurement）和一组字段（field），同时可能会包含 N 个标签（tag）和一个时间戳（timestamp）。\n如果不附带时间戳，那么 InfluxDB 会使用其主机的当前系统时间，单位默认为纳秒。\nData Point另一种是数据点（Data Point）的数据格式。\n这种格式又分为两种风格的 API。\n第一种风格是一个函数，传递 N 个参数。\n示例代码：\n123456p := influxdb2.NewPoint(&quot;stat&quot;,  map[string]string&#123;&quot;unit&quot;: &quot;temperature&quot;&#125;,  map[string]interface&#123;&#125;&#123;&quot;avg&quot;: 24.5, &quot;max&quot;: 45&#125;,  time.Now())writeAPI.WritePoint(p)\n\n第二种风格是类似 DSL 的 API。\n代码如下：\n12345678p = influxdb2.NewPointWithMeasurement(&quot;stat&quot;).  AddTag(&quot;unit&quot;, &quot;temperature&quot;).  AddField(&quot;avg&quot;, 23.2).  AddField(&quot;max&quot;, 45).  SetTime(time.Now())writeAPI.WritePoint(p)\n\n删数据InfluxDB 不支持修改数据，但是可以删除数据。\n删数据的方式有两种，一种是 InfluxDB CLI，一种是 HTTP API。\nInfluxDB CLI运行 influx delete  命令即可删除数据，需要附带参数。\n\nbucket：指定某个数据桶。\nstart 和 stop：指定删除的数据时间戳范围。\npredicate：可选项，删除符合某种条件的数据。\n\n示例：\n123influx delete --bucket example-bucket \\  --start 2020-03-01T00:00:00Z \\  --stop 2020-11-14T00:00:00Z\n\n带有条件的示例：\n1234influx delete --bucket example-bucket \\  --start &#x27;1970-01-01T00:00:00Z&#x27; \\  --stop $(date +&quot;%Y-%m-%dT%H:%M:%SZ&quot;) \\  --predicate &#x27;_measurement=&quot;example-measurement&quot; AND exampleTag=&quot;exampleTagValue&quot;&#x27;\n\nHTTP API和 CLI 的方式类似。调用 api/v2/delete  即可。\n需要符合以下条件。\n\n请求方式（Method）：POST。\n请求头（Headers）：带有 Authorization  用于验证身份，请求格式为 application/json。\n查询参数（QueryParams）：org&#x2F;orgID，指定组织。bucket&#x2F;bucketID，指定数据桶。\n请求体（Body）：start：表示开始时间，stop：表示结束时间，predicate（可选）：表示删除条件。\n\n请求示例：\n1234567curl --request POST http://localhost:8086/api/v2/delete/?org=example-org&amp;bucket=example-bucket \\  --header &#x27;Authorization: Token &lt;YOURAUTHTOKEN&gt;&#x27; \\  --header &#x27;Content-Type: application/json&#x27; \\  --data &#x27;&#123;    &quot;start&quot;: &quot;2020-03-01T00:00:00Z&quot;,    &quot;stop&quot;: &quot;2020-11-14T00:00:00Z&quot;  &#125;&#x27;\n\n我不建议在 InfluxDB 中对数据进行删除操作。\n时序数据库更适合存储完整的原始数据，而经过分析和提炼后的价值更高的数据，可以存入 MongoDB 或者 MySQL。\n读数据InfluxDB 读数据的方式有 5 种。\n\nInfluxDB UI\nInfluxDB HTTP API\nFlux REPL\nInfluxDB CLI\nClient libraries\n\n这里介绍第 5 种，使用客户端的方式来读取数据。\n下面是代码演示：\n123456789101112131415161718192021222324client := influxdb2.NewClient(url, token)defer client.Close()query := fmt.Sprintf(&quot;from(bucket:\\&quot;%v\\&quot;)|&gt; range(start: -1h) |&gt; filter(fn: (r) =&gt; r._measurement == \\&quot;stat\\&quot;)&quot;, bucket)queryAPI := client.QueryAPI(org)result, err := queryAPI.Query(context.Background(), query)if err == nil &#123;        for result.Next() &#123;                if result.TableChanged() &#123;            fmt.Printf(&quot;table: %s\\n&quot;, result.TableMetadata().String())        &#125;                fmt.Printf(&quot;value: %v\\n&quot;, result.Record().Value())    &#125;        if result.Err() != nil &#123;        fmt.Printf(&quot;query parsing error: %\\n&quot;, result.Err().Error())    &#125;&#125; else &#123;    panic(err)&#125;\n\n其中关键的是第 3 行名为 query 的字符串变量，它是使用 flux 语法编写的一段脚本。\nfrom(bucket:&quot;test&quot;)\n  |&gt; range(start: -1h)\n  |&gt; filter(fn: (r) =&gt; r._measurement == &quot;stat&quot;)\n复制代码\n\nfrom  表示从哪个数据源检索数据。\nrange  表示根据时间范围过滤数据。start: -1 表示当前时间减掉 1 个小时。\nfilter  表示自定义过滤条件，其中 fn 是一个函数，在函数内定义规则，语法和 JavaScript 中 Array 的 filter 函数极其类似。\n|&gt;  表示管道前移符，将数据通过管道的形式传递到下一个操作中。\n https://juejin.cn/post/6947575345570643981\n","slug":"JAVA/InfluxDB 通识篇","date":"2021-08-23T15:09:05.000Z","categories_index":"InfluxDB,JAVA","tags_index":"https,com,raw","author_index":"dandeliono"},{"id":"142536715563d02ed2a04093bf771493","title":"async-profiler","content":"async-profiler​简介async-profiler 是一款采集分析 java 性能的工具，翻译一下 github 上的项目介绍：\n\n\n\n\n\n\n\n\n\n❝\nasync-profiler 是一款没有Safepoint bias problem的低开销java 采集分析器，它利用HotSpot特殊的 api 来收集栈信息以及内存分配信息，可以在OpenJDK,Oracle JDK以及一些其他的基于 HotSpot 的 java 虚拟机。async-profiler 可以追踪以下几种事件：\n\ncpu 周期\n硬件和软件性能计数器，例如高速缓存未命中，分支未命中，页面错误，上下文切换等\nJava 堆中内存的分配\n锁尝试，包括 Java 对象监视器和 ReentrantLock\n\n❞\n使用方法首先下载 async-profiler，github 主页（https://github.com/jvm-profiling-tools/async-profiler）上有已经编译好的文件，找到对应的平台下载即可\n\n基本用法下载好的文件解压后，有一个profiler.sh脚本，运行脚本即可对 java 进程进行 cpu 分析，例如 java 进程 id 为 1232\n./profiler.sh start 1232``./profiler.sh stop 1232  \n或者可以用-d指定剖析的时间（秒）\n执行完成后会输出采集的信息：\n\n通常我们会用更加直观的火焰图来绘制输出的信息，使用参数-f &#123;$file_name&#125;.svg  \n命令执行完成后会生成一个 svg 格式的文件，用浏览器打开\n\n火焰图怎么看？可以参考阮一峰的文章\n\n\n\n\n\n\n\n\n\n❝\n《如何读懂火焰图》 http://www.ruanyifeng.com/blog/2017/09/flame-graph.html\n❞\n简单来说：x 轴是抽样数，x 轴越长说明被这个方法被抽样的次数越多，消耗 cpu 时间也越长 y 轴是栈的深度，通常越往上越细，如果发现往上有一个平顶，且平顶很宽，则说明它可能有问题，消耗 cpu 时间较多。\n其他参数介绍\n-e event\n\nevent 可选参数用这个命令来查看\n输出样例：\n\nevent 默认为 cpu，也可以用 alloc 来查看内存分配\n\nlock 查看锁情况：\n\n其他的模式这里就不一一尝试，用的最多的还是 cpu 模式。\n\n-i N 设置采样频率，默认是 10ms，可使用ms，us，s作为单位，无单位默认为纳秒，如\n\n-j N 设置栈采样深度\n\n-o fmt 设置输出格式：可选的有 summary、traces、flat、jfr、collapsed、svg、tree，最常用的是 svg\n\n\n更多命令可参考 async-profiler github 主页\n一次网关性能压测实例笔者有一次在压测全链路异步网关时，RPS 在 2000 左右，始终上不去，cpu 消耗比较高，于是使用 async-profiler 进行 cpu 剖析，生成的火焰图如下\n\n可以看到这里有个宽又深的栈，它消耗了很多 cpu，限于图像大小，再往上拉，看下这块到底是什么\n\n从类名就能看出是log4j，猜测是代码某处打日志过于频繁导致，找到打日志的地方后先去掉日志，压测了一下，果然 RPS 提升到了 5000，一处小小的日志竟有如此大的影响。\n原理介绍看到这里相信你应该会用 async-profiler 来进行 cpu 剖析了，如果感兴趣可以了解下 async-profiler 实现的原理，这块有一篇文章介绍的很详细，可以参考\n\n\n\n\n\n\n\n\n\n❝\n《JVM CPU Profiler 技术原理及源码深度解析》 https://mp.weixin.qq.com/s/RKqmy8dw7B7WtQc6Xy2CLA\n❞\n简单总结一下：\n\ncpu profiler 实现一般有两种方案：（1）Sampling，采样（2）Instrumentation，埋点；采样对性能损耗较小但没那么准确，埋点（类似 AOP）精准但性能影响大\nSampling 采样只能在Safe Ponit处采样，会导致统计结果偏差，也就是开头说的Safepoint bias problem，例如某些方法执行时间短，但执行频率高，正真占用了 cpu，这部分如果 Sampling 的采样频率不能足够小，可能就采样不到，但采样频率过高又会影响性能，这就是一般的基于采样的 cpu profiler 的缺点\nasync-profiler 是基于采样实现的，但它又没有Safepoint bias problem，它是通过一种叫做AsyncGetCallTrace的方式来采样，这种采样不需要在安全点处采样，但这个函数不是那么容易就调用到的，需要使用一些技巧（黑科技）的方式来获取\n\n正是第三点原因我才写了这篇文章来推荐这款cpu 分析利器，比如我用了一款其他的分析器（uber-common&#x2F;jvm-profiler）来分析上面网关的 cpu，得出了如下的火焰图\n\n https://xie.infoq.cn/article/58d2a013a045d2ff3d542a6a1\n","slug":"JAVA/async-profiler","date":"2021-08-17T15:15:33.000Z","categories_index":"profiler,JAVA","tags_index":"https,com,cpu","author_index":"dandeliono"},{"id":"dbb584f30bfc75da44387b6e286da8d6","title":"从Java多线程可见性谈Happens-Before原则","content":"从Java多线程可见性谈Happens-Before原则Happens-Before 是一个非常抽象的概念，然而它又是学习 Java 并发编程不可跨域的部分。本文会先阐述 Happens-Before 在并发编程中解决的问题——_多线程可见性_，然后再详细讲解 Happens-Before 原则本身。\nJava 多线程可见性在现代操作系统上编写并发程序时，除了要注意线程安全性 (多个线程互斥访问临界资源) 以外，还要注意多线程对共享变量的可见性，而后者往往容易被人忽略。可见性是指当一个线程修改了共享变量的值，其它线程能够适时得知这个修改。在单线程环境中，如果在程序前面修改了某个变量的值，后面的程序一定会读取到那个变量的新值。这看起来很自然，然而当变量的写操作和读操作在不同的线程中时，情况却并非如此。\npublic class NoVisibility &#123;\n    private static boolean ready; \n    private static int number;\n    \n    private static class ReaderThread extends Thread &#123;\n        public void run() &#123;\n            while(!ready) &#123;\n                Thread.yield();\n            &#125;\n            System.out.println(number);\n        &#125;\n    &#125;\n    \n    public static void main(String\\[\\] args) &#123;\n        new ReaderThread().start(); \n        number = 42;\n        ready = true;\n    &#125;\n&#125;\n\n上面的代码中，主线程和读线程都访问共享变量 ready 和 number。程序看起来会输出 42，但事实上很可能会输出 0，或者根本无法终止。这是因为上面的程序缺少_线程间变量可见性_的保证，所以在主线程中写入的变量值，可能无法被读线程感知到。\n为什么会出现线程可见性问题要想解释_为什么会出现线程可见性问题_，需要从计算机处理器结构谈起。我们都知道计算机运算任务需要 CPU 和内存相互配合共同完成，其中 CPU 负责逻辑计算，内存负责数据存储。CPU 要与内存进行交互，如读取运算数据、存储运算结果等。由于内存和 CPU 的计算速度有几个数量级的差距，为了提高 CPU 的利用率，现代处理器结构都加入了一层读写速度尽可能接近 CPU 运算速度的高速缓存来作为内存与 CPU 之间的缓冲：将运算需要使用的数据复制到缓存中，让 CPU 运算可以快速进行，计算结束后再将计算结果从缓存同步到主内存中，这样处理器就无须等待缓慢的内存读写了。高速缓存的引入解决了 CPU 和内存之间速度的矛盾，但是在多 CPU 系统中也带来了新的问题：缓存一致性。在多 CPU 系统中，每个 CPU 都有自己的高速缓存，所有的 CPU 又共享同一个主内存。如果多个 CPU 的运算任务都涉及到主内存中同一个变量时，那同步回主内存时以哪个 CPU 的缓存数据为准呢？这就需要各个 CPU 在数据读写时都遵循同一个协议进行操作。\n参考上图，假设有两个线程 A、B 分别在两个不同的 CPU 上运行，它们共享同一个变量 X。如果线程 A 对 X 进行修改后，并没有将 X 更新后的结果同步到主内存，则变量 X 的修改对 B 线程是不可见的。所以 CPU 与内存之间的高速缓存就是导致_线程可见性问题_的一个原因。CPU 和主内存之间的高速缓存还会导致另一个问题——重排序。假设 A、B 两个线程共享两个变量 X、Y，A 和 B 分别在不同的 CPU 上运行。在 A 中先更改变量 X 的值，然后再更改变量 Y 的值。这时有可能发生 Y 的值被同步回主内存，而 X 的值没有同步回主内存的情况，此时对于 B 线程来说是无法感知到 X 变量被修改的，或者可以认为对于 B 线程来说，Y 变量的修改被重排序到了 X 变量修改的前面。上面的程序 NoVisibility 类中有可能输出 0 就是这种情况，虽然在主线程中是先修改 number 变量，再修改 ready 变量，但对于读线程来说，ready 变量的修改有可能被重排序到 number 变量修改之前。此外，为了提高程序的执行效率，编译器在生成指令序列时和 CPU 执行指令序列时，都有可能对指令进行重排序。_Java 语言规范_要求 JVM 只在单个线程内部维护一种类似串行的语义，即只要程序的最终结果与严格串行环境中执行的结果相同即可。所以在单线程环境中，我们无法察觉到重排序，因为程序重排序后的执行结果与严格按顺序执行的结果相同。就像在类 NoVisibility 的主线程中，先修改 ready 变量还是先修改 number 变量对于主线程自己的执行结果是没有影响的，但是如果 number 变量和 ready 变量的修改发生重排序，对读线程是有影响的。所以在编写并发程序时，我们一定要注意重排序对多线程执行结果的影响。看到这里大家一定会发现，我们所讨论的 CPU 高速缓存、指令重排序等内容都是计算机体系结构方面的东西，并不是 Java 语言所特有的。事实上，很多主流程序语言 (如 C&#x2F;C++) 都存在_多线程可见性_的问题，这些语言是借助物理硬件和操作系统的内存模型来处理_多线程可见性_问题的，因此不同平台上内存模型的差异，会影响到程序的执行结果。Java 虚拟机规范定义了自己的内存模型 JMM(Java Memory Model) 来屏蔽掉不同硬件和操作系统的内存模型差异，以实现让 Java 程序在各种平台下都能达到一致的内存访问结果。所以对于 Java 程序员，无需了解底层硬件和操作系统内存模型的知识，只要关注 Java 自己的内存模型，就能够解决 Java 语言中的内存可见性问题了。\nHappens-Before 原则上面讨论了 Java 中多线程共享变量的可见性问题及产生这种问题的原因。下面我们看一下如何解决这个问题，即当一个多线程共享变量被某个线程修改后，如何让这个修改被需要读取这个变量的线程感知到。为了方便程序员开发，将底层的烦琐细节屏蔽掉，JMM 定义了 Happens-Before 原则。只要我们理解了 Happens-Before 原则，无需了解 JVM 底层的内存操作，就可以解决在并发编程中遇到的变量可见性问题。JVM 定义的 Happens-Before 原则是一组偏序关系：对于两个操作 A 和 B，这两个操作可以在不同的线程中执行。如果 A Happens-Before B，那么可以保证，当 A 操作执行完后，A 操作的执行结果对 B 操作是可见的。Happens-Before 的规则包括：\n\n程序顺序规则\n锁定规则\nvolatile 变量规则\n线程启动规则\n线程结束规则\n中断规则\n终结器规则\n传递性规则\n\n下面我们将详细讲述这 8 条规则的具体内容。\n程序顺序规则在一个线程内部，按照程序代码的书写顺序，书写在前面的代码操作 Happens-Before 书写在后面的代码操作。这时因为_Java 语言规范_要求 JVM 在单个线程内部要维护类似严格串行的语义，如果多个操作之间有先后依赖关系，则不允许对这些操作进行重排序。\n锁定规则对锁 M 解锁之前的所有操作 Happens-Before 对锁 M 加锁之后的所有操作。\nclass HappensBeforeLock &#123;\n    private int value = 0;\n    \n    public synchronized void setValue(int value) &#123;\n        this.value = value;\n    &#125;\n    \n    public synchronized int getValue() &#123;\n        return value;\n    &#125;\n&#125;\n\n上面这段代码，setValue 和 getValue 两个方法共享同一个监视器锁。假设 setValue 方法在线程 A 中执行，getValue 方法在线程 B 中执行。setValue 方法会先对 value 变量赋值，然后释放锁。getValue 方法会先获取到同一个锁后，再读取 value 的值。所以根据锁定原则，线程 A 中对 value 变量的修改，可以被线程 B 感知到。如果这个两个方法上没有 synchronized 声明，则在线程 A 中执行 setValue 方法对 value 赋值后，线程 B 中 getValue 方法返回的 value 值并不能保证是最新值。本条锁定规则对显示锁 (ReentrantLock) 和内置锁 (synchronized) 在加锁和解锁等操作上有着相同的内存语义。对于锁定原则，可以像下面这样去理解：同一时刻只能有一个线程执行锁中的操作，所以锁中的操作被重排序外界是不关心的，只要最终结果能被外界感知到就好。除了重排序，剩下影响变量可见性的就是 CPU 缓存了。在锁被释放时，A 线程会把释放锁之前所有的操作结果同步到主内存中，而在获取锁时，B 线程会使自己 CPU 的缓存失效，重新从主内存中读取变量的值。这样，A 线程中的操作结果就会被 B 线程感知到了。\nvolatile 变量规则对一个 volatile 变量的写操作及这个写操作之前的所有操作 Happens-Before 对这个变量的读操作及这个读操作之后的所有操作。\nMap configOptions;\nchar\\[\\] configText; \n\nvolatile boolean initialized = false;\n\n\n\nconfigText = readConfigFile(fileName);\nprocessConfigOptions(configText, configOptions);\ninitialized = true;\n\nwhile (!initialized) &#123;    \n    sleep();\n&#125;\n\ndoSomethingWithConfig();\n\n上面这段代码，读取配置文件的操作和使用配置信息的操作分别在两个不同的线程 A、B 中执行，两个线程通过共享变量 configOptions 传递配置信息，并通过共享变量 initialized 作为初始化是否完成的通知。initialized 变量被声明为 volatile 类型的，根据 volatile 变量规则，volatile 变量的写入操作 Happens-Before 对这个变量的读操作，所以在线程 A 中将变量 initialized 设为 true，线程 B 中是可以感知到这个修改操作的。但是更牛逼的是，volatile 变量不仅可以保证自己的变量可见性，还能保证书写在 volatile 变量写操作之前的操作对其它线程的可见性。考虑这样一种情况，如果 volatile 变量仅能保证自己的变量可见性，那么当线程 B 感知到 initialized 已经变成 true 然后执行 doSomethingWithConfig 操作时，可能无法获取到 configOptions 最新值而导致操作结果错误。所以 volatile 变量不仅可以保证自己的变量可见性，还能保证书写在 volatile 变量写操作之前的操作 Happens-Before 书写在 volatile 变量读操作之后的那些操作。可以这样理解 volatile 变量的写入和读取操作流程：首先，volatile 变量的操作会禁止与其它普通变量的操作进行重排序，例如上面代码中会禁止 initialized &#x3D; true 与它上面的两行代码进行重排序 (但是它上面的代码之间是可以重排序的)，否则会导致程序结果错误。volatile 变量的写操作就像是一条基准线，到达这条线之后，不管之前的代码有没有重排序，反正到达这条线之后，前面的操作都已完成并生成好结果。然后，在 volatile 变量写操作发生后，A 线程会把 volatile 变量本身和书写在它之前的那些操作的执行结果一起同步到主内存中。最后，当 B 线程读取 volatile 变量时，B 线程会使自己的 CPU 缓存失效，重新从主内存读取所需变量的值，这样无论是 volatile 本身，还是书写在 volatile 变量写操作之前的那些操作结果，都能让 B 线程感知到，也就是上面程序中的 initialized 和 configOptions 变量的最新值都可以让线程 B 感知到。原子变量与 volatile 变量在读操作和写操作上有着相同的语义。\n线程启动规则Thread 对象的 start 方法及书写在 start 方法前面的代码操作 Happens-Before 此线程的每一个动作。start 方法和新线程中的动作一定是在两个不同的线程中执行。_线程启动规则_可以这样去理解：调用 start 方法时，会将 start 方法之前所有操作的结果同步到主内存中，新线程创建好后，需要从主内存获取数据。这样在 start 方法调用之前的所有操作结果对于新创建的线程都是可见的。\n线程终止规则线程中的任何操作都 Happens-Before 其它线程检测到该线程已经结束。这个说法有些抽象，下面举例子对其进行说明。假设两个线程 s、t。在线程 s 中调用 t.join() 方法。则线程 s 会被挂起，等待 t 线程运行结束才能恢复执行。当 t.join() 成功返回时，s 线程就知道 t 线程已经结束了。所以根据本条原则，在 t 线程中对共享变量的修改，对 s 线程都是可见的。类似的还有 Thread.isAlive 方法也可以检测到一个线程是否结束。可以猜测，当一个线程结束时，会把自己所有操作的结果都同步到主内存。而任何其它线程当发现这个线程已经执行结束了，就会从主内存中重新刷新最新的变量值。所以结束的线程 A 对共享变量的修改，对于其它检测了 A 线程是否结束的线程是可见的。\n中断规则一个线程在另一个线程上调用 interrupt,Happens-Before 被中断线程检测到 interrupt 被调用。假设两个线程 A 和 B，A 先做了一些操作 operationA，然后调用 B 线程的 interrupt 方法。当 B 线程感知到自己的中断标识被设置时 (通过抛出 InterruptedException，或调用 interrupted 和 isInterrupted),operationA 中的操作结果对 B 都是可见的。\n终结器规则一个对象的构造函数执行结束 Happens-Before 它的 finalize() 方法的开始。“结束”和 “开始” 表明在时间上，一个对象的构造函数必须在它的 finalize()方法调用时执行完。根据这条原则，可以确保在对象的 finalize 方法执行时，该对象的所有 field 字段值都是可见的。\n传递性规则如果操作 A Happens-Before B，B Happens-Before C，那么可以得出操作 A Happens-Before C。\n再次思考 Happens-Before 规则的真正意义到这里我们已经讨论了线程的可见性问题和导致这个问题的原因，并详细阐述了 8 条 Happens-Before 原则和它们是如何帮助我们解决变量可见性问题的。下面我们在深入思考一下，Happens-Before 原则到底是如何解决变量间可见性问题的。我们已经知道，导致多线程间可见性问题的两个 “罪魁祸首” 是_CPU 缓存_和_重排序_。那么如果要保证多个线程间共享的变量对每个线程都及时可见，一种极端的做法就是禁止使用所有的重排序和 CPU 缓存。即关闭所有的编译器、操作系统和处理器的优化，所有指令顺序全部按照程序代码书写的顺序执行。去掉 CPU 高速缓存，让 CPU 的每次读写操作都直接与主存交互。当然，上面的这种极端方案是绝对不可取的，因为这会极大影响处理器的计算性能，并且对于那些非多线程共享的变量是不公平的。_重排序_和_CPU 高速缓存_有利于计算机性能的提高，但却对多 CPU 处理的一致性带来了影响。为了解决这个矛盾，我们可以采取一种折中的办法。我们用分割线把整个程序划分成几个程序块，在每个程序块内部的指令是可以重排序的，但是分割线上的指令与程序块的其它指令之间是不可以重排序的。在一个程序块内部，CPU 不用每次都与主内存进行交互，只需要在 CPU 缓存中执行读写操作即可，但是当程序执行到分割线处，CPU 必须将执行结果同步到主内存或从主内存读取最新的变量值。那么，Happens-Before 规则就是定义了这些程序块的分割线。下图展示了一个使用_锁定原则_作为分割线的例子：\n如图所示，这里的 unlock M 和 lock M 就是划分程序的分割线。在这里，红色区域和绿色区域的代码内部是可以进行重排序的，但是 unlock 和 lock 操作是不能与它们进行重排序的。即第一个图中的红色部分必须要在 unlock M 指令之前全部执行完，第二个图中的绿色部分必须全部在 lock M 指令之后执行。并且在第一个图中的 unlock M 指令处，红色部分的执行结果要全部刷新到主存中，在第二个图中的 lock M 指令处，绿色部分用到的变量都要从主存中重新读取。在程序中加入分割线将其划分成多个程序块，虽然在程序块内部代码仍然可能被重排序，但是保证了程序代码在宏观上是有序的。并且可以确保在分割线处，CPU 一定会和主内存进行交互。Happens-Before 原则就是定义了程序中什么样的代码可以作为分隔线。并且无论是哪条 Happens-Before 原则，它们所产生分割线的作用都是相同的。\n小结在写作本文时，我主要参考的是《Java 并发编程实战》和《深入理解 Java 虚拟机》的最后一章，此外有部分内容是我自己对并发编程的一些浅薄理解，希望能够对阅读的人有所帮助。如有错误的地方，欢迎大家指正。 https://segmentfault.com/a/1190000011458941\n","slug":"JAVA/从Java多线程可见性谈Happens-Before原则","date":"2021-08-16T16:25:22.000Z","categories_index":"CPU,JAVA","tags_index":"volatile,Happens,Before","author_index":"dandeliono"},{"id":"55a6aea8b4cb4859de314d4babde9ef4","title":"ThreadLocal系列（一）-ThreadLocal的使用及原理解析","content":"ThreadLocal系列（一）-ThreadLocal的使用及原理解析一、基本使用先来看下基本用法：\n123456789101112131415    private static ThreadLocal tl = new ThreadLocal&lt;&gt;();    public static void main(String[] args) throws Exception &#123;        tl.set(1);        System.out.println(String.format(&quot;当前线程名称: , main方法内获取线程内数据为: &quot;,                Thread.currentThread().getName(), tl.get()));        fc();        new Thread(ThreadLocalTest::fc).start();    &#125;IT    private static void fc() &#123;        System.out.println(String.format(&quot;当前线程名称: , fc方法内获取线程内数据为: &quot;,                Thread.currentThread().getName(), tl.get()));    &#125;\n\n运行结果：\n1234567891011121314151617181920212223242526272829303132333435363738当前线程名称: main, main方法内获取线程内数据为: 1当前线程名称: main, fc方法内获取线程内数据为: 1当前线程名称: Thread-0, fc方法内获取线程内数据为: null```可以看到，main线程内任意地方都可以通过ThreadLocal获取到当前线程内被设置进去的值，而被异步出去的fc调用，却由于替换了执行线程，而拿不到任何数据值，那么我们现在再来改造下上述代码，在异步发生之前，给Thread-0线程也设置一个上下文数据：```javaprivate static ThreadLocal tl = new ThreadLocal&lt;&gt;();    public static void main(String[] args) throws Exception &#123;        tl.set(1);        System.out.println(String.format(&quot;当前线程名称: , main方法内获取线程内数据为: &quot;,                Thread.currentThread().getName(), tl.get()));        fc();        new Thread(()-&gt;&#123;            tl.set(2);             fc();        &#125;).start();        Thread.sleep(1000L);         fc();     &#125;    private static void fc() &#123;        System.out.println(String.format(&quot;当前线程名称: , fc方法内获取线程内数据为: &quot;,                Thread.currentThread().getName(), tl.get()));    &#125;````运行结果为：```java当前线程名称: main, main方法内获取线程内数据为: 1当前线程名称: main, fc方法内获取线程内数据为: 1当前线程名称: Thread-0, fc方法内获取线程内数据为: 2当前线程名称: main, fc方法内获取线程内数据为: 1\n\n可以看到，主线程和子线程都可以获取到自己的那份上下文里的内容，而且互不影响。\n二、原理分析ok，上面通过一个简单的例子，我们可以了解到 ThreadLocal（以下简称 TL）具体的用法，这里先不讨论它实质上能给我们带来什么好处，先看看其实现原理，等这些差不多了解完了，我再通过我曾经做过的一个项目，去说明 TL 的作用以及在企业级项目里的用处。\n我以前在不了解 TL 的时候，想着如果让自己实现一个这种功能的轮子，自己会怎么做，那时候的想法很单纯，觉得通过一个 Map 就可以解决，Map 的 key 设置为 Thread.currentThread()，value 设置为当前线程的本地变量即可，但后来想想就觉得不太现实了，实际项目中可能存在大量的异步线程，对于内存的开销是不可估量的，而且还有个严重的问题，线程是运行结束后就销毁的，如果按照上述的实现方案，map 内是一直持有这个线程的引用的，导致明明执行结束的线程对象不能被 jvm 回收，造成内存泄漏，时间久了，会直接 OOM。\n所以，java 里的实现肯定不是这么简单的，下面，就来看看 java 里的具体实现吧。\n先来了解下，TL 的基本实现，为了避免上述中出现的问题，TL 实际上是把我们设置进去的值以 k-v 的方式放到了每个 Thread 对象内（TL 对象做 k，设置的值做 v），也就是说，TL 对象仅仅起到一个标记、对 Thread 对象维护的 map 赋值的作用。\n先从 set 方法看起：\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135public void set(T value) &#123;        Thread t = Thread.currentThread();         ThreadLocal.ThreadLocalMap map = getMap(t);         if (map != null)            map.set(this, value);         else            createMap(t, value);     &#125;    ThreadLocal.ThreadLocalMap getMap(Thread t) &#123;        return t.threadLocals;     &#125;    private void set(ThreadLocal&lt;?&gt; key, Object value) &#123;        ThreadLocal.ThreadLocalMap.Entry[] tab = table;         int len = tab.length;        int i = key.threadLocalHashCode &amp; (len-1);         for (ThreadLocal.ThreadLocalMap.Entry e = tab[i];             e != null;             e = tab[i = nextIndex(i, len)]) &#123;             ThreadLocal&lt;?&gt; k = e.get();             if (k == key) &#123;                 e.value = value;                return;            &#125;            if (k == null) &#123;                 replaceStaleEntry(key, value, i);                return;            &#125;        &#125;                tab[i] = new ThreadLocal.ThreadLocalMap.Entry(key, value);        int sz = ++size;        if (!cleanSomeSlots(i, sz) &amp;&amp; sz &gt;= threshold)             rehash();     &#125;        void createMap(Thread t, T firstValue) &#123;        t.threadLocals = new ThreadLocal.ThreadLocalMap(this, firstValue);    &#125;    ThreadLocalMap(ThreadLocal&lt;?&gt; firstKey, Object firstValue) &#123;        table = new ThreadLocal.ThreadLocalMap.Entry[INITIAL_CAPACITY];         int i = firstKey.threadLocalHashCode &amp; (INITIAL_CAPACITY - 1);         table[i] = new ThreadLocal.ThreadLocalMap.Entry(firstKey, firstValue);         size = 1;        setThreshold(INITIAL_CAPACITY);     &#125;```通过上述代码，我们大致了解了TL在set值的时候发生的一些操作，结合之前说的，我们可以确定的是，TL其实对于线程来说，只是一个标识，而真正线程的**本地变量**被保存在每个线程对象的**ThreadLocalMap**里，这个map里维护着一个**Entry\\[\\]的数组（散列表）**，Entry是个k-v结构的对象（如图1-1），k为TL对象，v为对应TL保存在该线程内的本地变量值，值得注意的是，这里的k针对TL对象的引用是个**弱引用**，来看下源码：```javastatic class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; &#123;                        Object value;            Entry(ThreadLocal&lt;?&gt; k, Object v) &#123;                super(k);                value = v;            &#125;        &#125;````为什么这里需要弱引用呢？我们先来看一张图，结合上面的介绍和这张图，来了解 TL 和 Thread 间的关系：![](https://raw.githubusercontent.com/dandeliono/img/main/resource/1ab684fa5a180984ec313bbe716435b2.png)**图 1-1**图中虚线表示弱引用，那么为什么要这么做呢？简单来说，一个 TL 对象被创建出来，并且被一个线程放到自己的 ThreadLocalMap 里，假如 TL 对象失去原有的强引用，但是该线程还没有死亡，如果 k 不是弱引用，那么就意味着 TL 并不能被回收，现在 k 为弱引用，那么在 TL 失去强引用的时候，gc 可以直接回收掉它，弱引用失效，这就是上面代码里会进行检查，k=null 的清除释放内存的原因（这个可以参考下面**expungeStaleEntry**方法，而且 set、get、remove 都会调用该方法，这也是 TL 防止内存泄漏所做的处理）。综上，简单来说这个**弱引用**就是用来解决由于使用 TL 不当导致的**内存泄漏**问题的，假如没有弱引用，那么你又用到了线程池（**池化后线程不会被销毁**），然后 TL 对象又是**局部**的，那么就会导致线程池内线程里的**ThreadLocalMap**存在大量的无意义的 TL 对象引用，造成过多无意义的 Entry 对象，因为即便调用了 set、get 等方法检查 k=null，也没有作用，这就导致了**内存泄漏**，长时间这样最终可能导致 OOM，所以 TL 的开发者为了解决这种问题，就将 ThreadLocalMap 里对 TL 对象的引用改为**弱引用**，一旦 TL 对象失去**强引用**，TL 对象就会被回收，那么这里的**弱引用**指向的值就为 null，结合上面说的，调用操作方法时会检查 k=null 的 Entry 进行回收，从而避免了内存泄漏的可能性。因为 TL 解决了内存泄漏的问题，因此即便是局部变量的 TL 对象且启用线程池技术，也比较难造成内存泄漏的问题，而且我们经常使用的场景就像一开始的示例代码一样，会初始化一个**全局的 static 的 TL 对象**，这就意味着该对象在程序运行期间都不会存在强引用消失的情况，我们可以利用不同的 TL 对象给不同的 Thread 里的 ThreadLocalMap 赋值，通常会 set 值（覆盖原有值），因此在使用线程池的时候也不会造成问题，异步开始之前 set 值，用完以后 remove，TL 对象可以多次得到使用，启用线程池的情况下如果不这样做，很可能业务逻辑也会出问题（一个线程存在之前执行程序时遗留下来的本地变量，一旦这个线程被再次利用，get 时就会拿到之前的脏值）；说完了 set，我们再来看下 get：```javapublic T get() &#123;        Thread t = Thread.currentThread();        ThreadLocal.ThreadLocalMap map = getMap(t);         if (map != null) &#123;            ThreadLocal.ThreadLocalMap.Entry e = map.getEntry(this);             if (e != null) &#123;                @SuppressWarnings(&quot;unchecked&quot;)                T result = (T)e.value;                return result;             &#125;        &#125;        return setInitialValue();     &#125;    private ThreadLocal.ThreadLocalMap.Entry getEntry(ThreadLocal&lt;?&gt; key) &#123;        int i = key.threadLocalHashCode &amp; (table.length - 1);         ThreadLocal.ThreadLocalMap.Entry e = table[i];        if (e != null &amp;&amp; e.get() == key)             return e;        else            return getEntryAfterMiss(key, i, e);     &#125;    private ThreadLocal.ThreadLocalMap.Entry getEntryAfterMiss(ThreadLocal&lt;?&gt; key, int i, ThreadLocal.ThreadLocalMap.Entry e) &#123;        ThreadLocal.ThreadLocalMap.Entry[] tab = table;        int len = tab.length;        while (e != null) &#123;            ThreadLocal&lt;?&gt; k = e.get();             if (k == key)                 return e;            if (k == null)                 expungeStaleEntry(i);            else                 i = nextIndex(i, len);            e = tab[i];        &#125;        return null;     &#125;        private static int nextIndex(int i, int len) &#123;        return ((i + 1 &lt; len) ? i + 1 : 0);    &#125;\n\n最后再来看看 remove 方法：\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public void remove() &#123;        ThreadLocal.ThreadLocalMap m = getMap(Thread.currentThread());        if (m != null)            m.remove(this);     &#125;    private void remove(ThreadLocal&lt;?&gt; key) &#123;        ThreadLocal.ThreadLocalMap.Entry[] tab = table;        int len = tab.length;        int i = key.threadLocalHashCode &amp; (len-1);         for (ThreadLocal.ThreadLocalMap.Entry e = tab[i];             e != null;             e = tab[i = nextIndex(i, len)]) &#123;            if (e.get() == key) &#123;                 e.clear();                 expungeStaleEntry(i);                 return;            &#125;        &#125;    &#125;    private int expungeStaleEntry(int staleSlot) &#123;         ThreadLocal.ThreadLocalMap.Entry[] tab = table;        int len = tab.length;        tab[staleSlot].value = null;         tab[staleSlot] = null;        size--;                ThreadLocal.ThreadLocalMap.Entry e;        int i;                for (i = nextIndex(staleSlot, len);             (e = tab[i]) != null;             i = nextIndex(i, len)) &#123;            ThreadLocal&lt;?&gt; k = e.get();            if (k == null) &#123;                 e.value = null;                tab[i] = null;                size--;            &#125; else &#123;                int h = k.threadLocalHashCode &amp; (len - 1);                if (h != i) &#123;                    tab[i] = null;                                                            while (tab[h] != null)                        h = nextIndex(h, len);                    tab[h] = e;                &#125;            &#125;        &#125;        return i;    &#125;\n","slug":"JAVA/ThreadLocal系列（一）-ThreadLocal的使用及原理解析","date":"2021-08-16T16:17:55.000Z","categories_index":"set,JAVA","tags_index":"Thread,ThreadLocalMap,ThreadLocal","author_index":"dandeliono"},{"id":"a03cc162cee41bf570dc5f1147b3943c","title":"Nginx gzip参数详解及常见问题","content":"Nginx gzip参数详解及常见问题1、Nginx gzip 功能Nginx 实现资源压缩的原理是通过 ngx_http_gzip_module 模块拦截请求，并对需要做 gzip 的类型做 gzip，ngx_http_gzip_module 是 Nginx 默认集成的，不需要重新编译，直接开启即可。\n2、参数详解gzip on这个没的说，打开或关闭 gzip\ngzip_buffers设置用于处理请求压缩的缓冲区数量和大小。比如 32 4K 表示按照内存页（one memory page）大小以 4K 为单位（即一个系统中内存页为 4K），申请 32 倍的内存空间。建议此项不设置，使用默认值。\ngzip_comp_level设置 gzip 压缩级别，级别越底压缩速度越快文件压缩比越小，反之速度越慢文件压缩比越大\n我们以一个大小为 92.6K 的脚本文件为例，如下所示。其中最后三个数值分别表示压缩比、包大小、平均处理时间（使用 ab 压测，100 用户并发下， ./ab -n 10000 -c 100 -H &#39;Accept-Encoding: gzip&#39; http://10.27.180.75/jquery.js ）以及 CPU 消耗。\n从这我们可以得出结论：\n\n随着压缩级别的升高，压缩比有所提高，但到了级别 6 后，很难再提高；\n随着压缩级别的升高，处理时间明显变慢;\ngzip 很消耗 cpu 的性能，高并发情况下 cpu 达到 100%;\n\n因此，建议：\n\n不是压缩级别越高越好，其实 gzip_comp_level 1 的压缩能力已经够用了，后面级别越高，压缩的比例其实增长不大，反而很吃处理性能。\n压缩一定要和静态资源缓存相结合，缓存压缩后的版本，否则每次都压缩高负载下服务器肯定吃不住。\n\ngzip_disable通过表达式，表明哪些 UA 头不使用 gzip 压缩\ngzip_min_length当返回内容大于此值时才会使用 gzip 进行压缩, 以 K 为单位, 当值为 0 时，所有页面都进行压缩。\ngzip_http_version用于识别 http 协议的版本，早期的浏览器不支持 gzip 压缩，用户会看到乱码，所以为了支持前期版本加了此选项。默认在 http&#x2F;1.0 的协议下不开启 gzip 压缩。\n因为浏览器基本上都支持 HTTP&#x2F;1.1。然而这里面却存在着一个很容易掉入的坑，也是笔者从生产环境中一个诡异问题中发现的：\n明明开启 gzip 功能，但是未生效。 \n原因定位： 为什么这样呢？在应用服务器前，公司还有一层 Nginx 的集群作为七层负责均衡，在这一层上，是没有开启 gzip 的。\n如果我们使用了 proxy_pass 进行反向代理，那么 nginx 和后端的 upstream server 之间默认是用 HTTP&#x2F;1.0 协议通信的。如果我们的 Cache Server 也是 nginx，而前端的 nginx 没有开启 gzip。同时，我们后端的 nginx 上没有设置 gzip_http_version 为 1.0，那么 Cache 的 url 将不会进行 gzip 压缩。\n\n\n我相信，以后还有人会入坑，比如你用 Apache ab 做压测，如果不是设置 gzip_http_version 为 1.0，你也压不出 gzip 的效果（同样的道理）。希望写在这里对大家有帮助\ngzip_proxiedNginx 做为反向代理的时候启用：\n\noff – 关闭所有的代理结果数据压缩\nexpired – 如果 header 中包含”Expires” 头信息，启用压缩\nno-cache – 如果 header 中包含”Cache-Control:no-cache” 头信息，启用压缩\nno-store – 如果 header 中包含”Cache-Control:no-store” 头信息，启用压缩\nprivate – 如果 header 中包含”Cache-Control:private” 头信息，启用压缩\nno_last_modified – 启用压缩，如果 header 中包含”Last_Modified” 头信息，启用压缩\nno_etag – 启用压缩，如果 header 中包含 “ETag” 头信息，启用压缩\nauth – 启用压缩，如果 header 中包含 “Authorization” 头信息，启用压缩\nany – 无条件压缩所有结果数据\n\ngzip_vary增加响应头”Vary: Accept-Encoding”\ngzip_types设置需要压缩的 MIME 类型, 如果不在设置类型范围内的请求不进行压缩\n参考文章\n所以 MIME-TYPE 中应该新增字体类型：\n| 字体类型扩展名 | Content-type || .eot | application&#x2F;vnd.ms-fontobject || .ttf | font&#x2F;ttf || .otf | font&#x2F;opentype || .woff | font&#x2F;x-woff || .svg | image&#x2F;svg+xml |\n","slug":"MIDDLEWARE/Nginx gzip参数详解及常见问题","date":"2021-08-16T16:16:06.000Z","categories_index":"gzip,MIDDLEWARE","tags_index":"如果,http,启用压缩","author_index":"dandeliono"},{"id":"db9816c4607e7d8e795e08923e1430dc","title":"Java内存访问重排序的研究","content":"Java内存访问重排序的研究什么是重排序请先看这样一段代码：\n12345678910111213141516171819202122public class PossibleReordering &#123;static int x = 0, y = 0;static int a = 0, b = 0;public static void main(String[] args) throws InterruptedException &#123;    Thread one = new Thread(new Runnable() &#123;        public void run() &#123;            a = 1;            x = b;        &#125;    &#125;);        Thread other = new Thread(new Runnable() &#123;        public void run() &#123;            b = 1;            y = a;        &#125;    &#125;);    one.start();other.start();    one.join();other.join();    System.out.println(“(” + x + “,” + y + “)”);&#125;\n\n很容易想到这段代码的运行结果可能为 (1,0)、(0,1) 或(1,1)，因为线程 one 可以在线程 two 开始之前就执行完了，也有可能反之，甚至有可能二者的指令是同时或交替执行的。\n然而，这段代码的执行结果也可能是 (0,0). 因为，在实际运行时，代码指令可能并不是严格按照代码语句顺序执行的。得到(0,0) 结果的语句执行过程，如下图所示。值得注意的是，a&#x3D;1 和 x&#x3D;b 这两个语句的赋值操作的顺序被颠倒了，或者说，发生了指令“重排序”(reordering)。（事实上，输出了这一结果，并不代表一定发生了指令重排序，内存可见性问题也会导致这样的输出，详见后文）\n\n重排序图解\n对重排序现象不太了解的开发者可能会对这种现象感到吃惊，但是，笔者开发环境下做的一个小实验证实了这一结果 2。\n\n重排序实验\n实验代码是构造一个循环，反复执行上面的实例代码，直到出现 a&#x3D;0 且 b&#x3D;0 的输出为止。实验结果说明，循环执行到第 13830 次时输出了 (0,0).\n大多数现代微处理器都会采用将指令乱序执行（out-of-order execution，简称 OoOE 或 OOE）的方法，在条件允许的情况下，直接运行当前有能力立即执行的后续指令，避开获取下一条指令所需数据时造成的等待 3。通过乱序执行的技术，处理器可以大大提高执行效率。\n除了处理器，常见的 Java 运行时环境的 JIT 编译器也会做指令重排序操作 4，即生成的机器指令与字节码指令顺序不一致。\nas-if-serial 语义As-if-serial 语义的意思是，所有的动作 (Action)5 都可以为了优化而被重排序，但是必须保证它们重排序后的结果和程序代码本身的应有结果是一致的。Java 编译器、运行时和处理器都会保证单线程下的 as-if-serial 语义。 比如，为了保证这一语义，重排序不会发生在有数据依赖的操作之中。\n\nint a &#x3D; 1;\nint b &#x3D; 2;\nint c &#x3D; a + b;\n\n将上面的代码编译成 Java 字节码或生成机器指令，可视为展开成了以下几步动作（实际可能会省略或添加某些步骤）。\n\n对 a 赋值 1\n对 b 赋值 2\n取 a 的值\n取 b 的值\n将取到两个值相加后存入 c\n\n在上面 5 个动作中，动作 1 可能会和动作 2、4 重排序，动作 2 可能会和动作 1、3 重排序，动作 3 可能会和动作 2、4 重排序，动作 4 可能会和 1、3 重排序。但动作 1 和动作 3、5 不能重排序。动作 2 和动作 4、5 不能重排序。因为它们之间存在数据依赖关系，一旦重排，as-if-serial 语义便无法保证。\n为保证 as-if-serial 语义，Java 异常处理机制也会为重排序做一些特殊处理。例如在下面的代码中，y &#x3D; 0 &#x2F; 0 可能会被重排序在 x &#x3D; 2 之前执行，为了保证最终不致于输出 x &#x3D; 1 的错误结果，JIT 在重排序时会在 catch 语句中插入错误代偿代码，将 x 赋值为 2，将程序恢复到发生异常时应有的状态。这种做法的确将异常捕捉的逻辑变得复杂了，但是 JIT 的优化的原则是，尽力优化正常运行下的代码逻辑，哪怕以 catch 块逻辑变得复杂为代价，毕竟，进入 catch 块内是一种 “异常” 情况的表现。6\n12345678910111213public class Reordering &#123;    public static void main(String[] args) &#123;        int x, y;        x = 1;        try &#123;            x = 2;            y = 0 / 0;            &#125; catch (Exception e) &#123;        &#125; finally &#123;            System.out.println(&quot;x = &quot; + x);        &#125;    &#125;&#125;\n\n内存访问重排序与内存可见性计算机系统中，为了尽可能地避免处理器访问主内存的时间开销，处理器大多会利用缓存 (cache) 以提高性能。其模型如下图所示：\n\n处理器 Cache 模型\n在这种模型下会存在一个现象，即缓存中的数据与主内存的数据并不是实时同步的，各 CPU（或 CPU 核心）间缓存的数据也不是实时同步的。这导致在同一个时间点，各 CPU 所看到同一内存地址的数据的值可能是不一致的。从程序的视角来看，就是在同一个时间点，各个线程所看到的共享变量的值可能是不一致的。\n有的观点会将这种现象也视为重排序的一种，命名为 “内存系统重排序”。因为这种内存可见性问题造成的结果就好像是内存访问指令发生了重排序一样。\n这种内存可见性问题也会导致章节一中示例代码即便在没有发生指令重排序的情况下的执行结果也还是 (0, 0)。\n内存访问重排序与 Java 内存模型Java 的目标是成为一门平台无关性的语言，即 Write once, run anywhere. 但是不同硬件环境下指令重排序的规则不尽相同。例如，x86 下运行正常的 Java 程序在 IA64 下就可能得到非预期的运行结果。为此，JSR-1337 制定了 Java 内存模型 (Java Memory Model, JMM)，旨在提供一个统一的可参考的规范，屏蔽平台差异性。从 Java 5 开始，Java 内存模型成为 Java 语言规范的一部分。\n根据 Java 内存模型中的规定，可以总结出以下几条 happens-before 规则 8。Happens-before 的前后两个操作不会被重排序且后者对前者的内存可见。\n\n程序次序法则：线程中的每个动作 A 都 happens-before 于该线程中的每一个动作 B，其中，在程序中，所有的动作 B 都能出现在 A 之后。\n监视器锁法则：对一个监视器锁的解锁 happens-before 于每一个后续对同一监视器锁的加锁。\nvolatile 变量法则：对 volatile 域的写入操作 happens-before 于每一个后续对同一个域的读写操作。\n线程启动法则：在一个线程里，对 Thread.start 的调用会 happens-before 于每个启动线程的动作。\n线程终结法则：线程中的任何动作都 happens-before 于其他线程检测到这个线程已经终结、或者从 Thread.join 调用中成功返回，或 Thread.isAlive 返回 false。\n中断法则：一个线程调用另一个线程的 interrupt happens-before 于被中断的线程发现中断。\n终结法则：一个对象的构造函数的结束 happens-before 于这个对象 finalizer 的开始。\n传递性：如果 A happens-before 于 B，且 B happens-before 于 C，则 A happens-before 于 C\n\nHappens-before 关系只是对 Java 内存模型的一种近似性的描述，它并不够严谨，但便于日常程序开发参考使用，关于更严谨的 Java 内存模型的定义和描述，请阅读 JSR-133 原文或 Java 语言规范章节 17.4。\n除此之外，Java 内存模型对 volatile 和 final 的语义做了扩展。对 volatile 语义的扩展保证了 volatile 变量在一些情况下不会重排序，volatile 的 64 位变量 double 和 long 的读取和赋值操作都是原子的。对 final 语义的扩展保证一个对象的构建方法结束前，所有 final 成员变量都必须完成初始化（的前提是没有 this 引用溢出）。\nJava 内存模型关于重排序的规定，总结后如下表所示：\n\n重排序示意表\n表中 “第二项操作” 的含义是指，第一项操作之后的所有指定操作。如，普通读不能与其之后的所有 volatile 写重排序。另外，JMM 也规定了上述 volatile 和同步块的规则尽适用于存在多线程访问的情景。例如，若编译器（这里的编译器也包括 JIT，下同）证明了一个 volatile 变量只能被单线程访问，那么就可能会把它做为普通变量来处理。\n留白的单元格代表允许在不违反 Java 基本语义的情况下重排序。例如，编译器不会对对同一内存地址的读和写操作重排序，但是允许对不同地址的读和写操作重排序。\n除此之外，为了保证 final 的新增语义。JSR-133 对于 final 变量的重排序也做了限制。\n\n构建方法内部的 final 成员变量的存储，并且，假如 final 成员变量本身是一个引用的话，这个 final 成员变量可以引用到的一切存储操作，都不能与构建方法外的将当期构建对象赋值于多线程共享变量的存储操作重排序。例如对于如下语句：\n\n\n\n\n\n\n\n\n\n\nx.finalField &#x3D; v; … ; 构建方法边界 sharedRef &#x3D; x; v.afield &#x3D; 1; x.finalField &#x3D; v; … ; 构建方法边界 sharedRef &#x3D; x;\n这两条语句中，构建方法边界前后的指令都不能重排序。\n\n初始读取共享对象与初始读取该共享对象的 final 成员变量之间不能重排序。例如对于如下语句：\n\n\n\n\n\n\n\n\n\n\nx &#x3D; sharedRef; … ; i &#x3D; x.finalField;\n前后两句语句之间不会发生重排序。由于这两句语句有数据依赖关系，编译器本身就不会对它们重排序，但确实有一些处理器会对这种情况重排序，因此特别制定了这一规则。\n内存屏障内存屏障（Memory Barrier，或有时叫做内存栅栏，Memory Fence）是一种 CPU 指令，用于控制特定条件下的重排序和内存可见性问题。Java 编译器也会根据内存屏障的规则禁止重排序。\n内存屏障可以被分为以下几种类型：\n\nLoadLoad 屏障：对于这样的语句 Load1; LoadLoad; Load2，在 Load2 及后续读取操作要读取的数据被访问前，保证 Load1 要读取的数据被读取完毕。\nStoreStore 屏障：对于这样的语句 Store1; StoreStore; Store2，在 Store2 及后续写入操作执行前，保证 Store1 的写入操作对其它处理器可见。\nLoadStore 屏障：对于这样的语句 Load1; LoadStore; Store2，在 Store2 及后续写入操作被刷出前，保证 Load1 要读取的数据被读取完毕。\nStoreLoad 屏障：对于这样的语句 Store1; StoreLoad; Load2，在 Load2 及后续所有读取操作执行前，保证 Store1 的写入对所有处理器可见。它的开销是四种屏障中最大的。在大多数处理器的实现中，这个屏障是个万能屏障，兼具其它三种内存屏障的功能。\n\n有的处理器的重排序规则较严，无需内存屏障也能很好的工作，Java 编译器会在这种情况下不放置内存屏障。 为了实现上一章中讨论的 JSR-133 的规定，Java 编译器会这样使用内存屏障。\n\n内存屏障示意表\n为了保证 final 字段的特殊语义，也会在下面的语句加入内存屏障。\n\n\n\n\n\n\n\n\n\nx.finalField &#x3D; v; StoreStore; sharedRef &#x3D; x;\nIntel 64&#x2F;IA-32 架构下的内存访问重排序Intel 64 和 IA-32 是我们较常用的硬件环境，相对于其它处理器而言，它们拥有一种较严格的重排序规则。Pentium 4 以后的 Intel 64 或 IA-32 处理的重排序规则如下。9\n在单 CPU 系统中：\n\n读操作不与其它读操作重排序。\n写操作不与其之前的写操作重排序。\n写内存操作不与其它写操作重排序，但有以下几种例外\nCLFLUSH 的写操作\n带有 non-temporal move 指令 (MOVNTI, MOVNTQ, MOVNTDQ, MOVNTPS, and MOVNTPD) 的 streaming 写入。\n字符串操作\n读操作可能会与其之前的写不同位置的写操作重排序，但不与其之前的写相同位置的写操作重排序。\n读和写操作不与 I&#x2F;O 指令，带锁的指令或序列化指令重排序。\n读操作不能重排序到 LFENCE 和 MFENCE 之前。\n写操作不能重排序到 LFENCE、SFENCE 和 MFENCE 之前。\nLFENCE 不能重排序到读操作之前。\nSFENCE 不能重排序到写之前。\nMFENCE 不能重排序到读或写操作之前。\n\n在多处理器系统中：\n\n各自处理器内部遵循单处理器的重排序规则。\n单处理器的写操作对所有处理器可见是同时的。\n各自处理器的写操作不会重排序。\n内存重排序遵守因果性 (causality)（内存重排序遵守传递可见性）。\n任何写操作对于执行这些写操作的处理器之外的处理器来看都是一致的。\n带锁指令是顺序执行的。\n\n值得注意的是，对于 Java 编译器而言，Intel 64&#x2F;IA-32 架构下处理器不需要 LoadLoad、LoadStore、StoreStore 屏障，因为不会发生需要这三种屏障的重排序。\n一例 Intel 64&#x2F;IA-32 架构下的代码性能优化现在有这样一个场景，一个容器可以放一个东西，容器支持 create 方法来创建一个新的东西并放到容器里，支持 get 方法取到这个容器里的东西。我们可以较容易地写出下面的代码：\n1234567891011121314151617181920212223242526public class Container &#123;    public static class SomeThing &#123;        private int status;        public SomeThing() &#123;            status = 1;        &#125;        public int getStatus() &#123;            return status;        &#125;    &#125;    private SomeThing object;    public void create() &#123;        object = new SomeThing();    &#125;    public SomeThing get() &#123;        while (object == null) &#123;            Thread.yield();         &#125;        return object;    &#125;&#125;\n\n在单线程场景下，这段代码执行起来是没有问题的。但是在多线程并发场景下，由不同的线程 create 和 get 东西，这段代码是有问题的。问题的原因与普通的双重检查锁定单例模式 (Double Checked Locking,DCL)10 类似，即 SomeThing 的构建与将指向构建中的 SomeThing 引用赋值到 object 变量这两者可能会发生重排序。导致 get 中返回一个正被构建中的不完整的 SomeThing 对象实例。为了解决这一问题，通常的办法是使用 volatile 修饰 object 字段。这种方法避免了重排序，保证了内存可见性，摒弃比使用同步块导致的性能损失更小。但是，假如使用场景对 object 的内存可见性并不敏感的话（不要求一个线程写入了 object，object 的新值立即对下一个读取的线程可见），在 Intel 64&#x2F;IA-32 环境下，有更好的解决方案。\n根据上一章的内容，我们知道 Intel 64&#x2F;IA-32 下写操作之间不会发生重排序，即在处理器中，构建 SomeThing 对象与赋值到 object 这两个操作之间的顺序性是可以保证的。这样看起来，仅仅使用 volatile 来避免重排序是多此一举的。但是，Java 编译器却可能生成重排序后的指令。但令人高兴的是，Oracle 的 JDK 中提供了 Unsafe. putOrderedObject，Unsafe. putOrderedInt，Unsafe. putOrderedLong 这三个方法，JDK 会在执行这三个方法时插入 StoreStore 内存屏障，避免发生写操作重排序。而在 Intel 64&#x2F;IA-32 架构下，StoreStore 屏障并不需要，Java 编译器会将 StoreStore 屏障去除。比起写入 volatile 变量之后执行 StoreLoad 屏障的巨大开销，采用这种方法除了避免重排序而带来的性能损失以外，不会带来其它的性能开销。\n我们将做一个小实验来比较二者的性能差异。一种是使用 volatile 修饰 object 成员变量。\n123456789101112131415161718192021222324252627public class Container &#123;    public static class SomeThing &#123;        private int status;        public SomeThing() &#123;            status = 1;        &#125;        public int getStatus() &#123;            return status;        &#125;    &#125;    private volatile  SomeThing object;    public void create() &#123;        object = new SomeThing();    &#125;    public SomeThing get() &#123;        while (object == null) &#123;            Thread.yield();         &#125;        return object;    &#125;&#125;\n\n一种是利用 Unsafe. putOrderedObject 在避免在适当的位置发生重排序。\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class Container &#123;    public static class SomeThing &#123;        private int status;        public SomeThing() &#123;            status = 1;        &#125;        public int getStatus() &#123;            return status;        &#125;    &#125;    private SomeThing object;    private Object value;    private static final Unsafe unsafe = getUnsafe();    private static final long valueOffset;    static &#123;        try &#123;            valueOffset = unsafe.objectFieldOffset(Container.class.getDeclaredField(&quot;value&quot;));        &#125; catch (Exception ex) &#123; throw new Error(ex); &#125;    &#125;    public void create() &#123;        SomeThing temp = new SomeThing();        unsafe.putOrderedObject(this, valueOffset, null);\t        object = temp;    &#125;    public SomeThing get() &#123;        while (object == null) &#123;            Thread.yield();        &#125;        return object;    &#125;    public static Unsafe getUnsafe() &#123;        try &#123;            Field f = Unsafe.class.getDeclaredField(&quot;theUnsafe&quot;);            f.setAccessible(true);            return (Unsafe)f.get(null);        &#125; catch (Exception e) &#123;        &#125;        return null;    &#125;&#125;\n\n由于直接调用 Unsafe.getUnsafe() 需要配置 JRE 获取较高权限，我们利用反射获取 Unsafe 中的 theUnsafe 来取得 Unsafe 的可用实例。unsafe.putOrderedObject(this, valueOffset, null) 这句仅仅是为了借用这句话功能的防止写重排序，除此之外无其它作用。\n利用下面的代码分别测试两种方案的实际运行时间。在运行时开启 - server 和 -XX:CompileThreshold&#x3D;1 以模拟生产环境下长时间运行后的 JIT 优化效果。\n1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public static void main(String[] args) throws InterruptedException &#123;    final int THREADS_COUNT = 20;    final int LOOP_COUNT = 100000;    long sum = 0;    long min = Integer.MAX_VALUE;    long max = 0;    for(int n = 0;n &lt;= 100;n++) &#123;        final Container basket = new Container();        List&lt;Thread&gt; putThreads = new ArrayList&lt;Thread&gt;();        List&lt;Thread&gt; takeThreads = new ArrayList&lt;Thread&gt;();        for (int i = 0; i &lt; THREADS_COUNT; i++) &#123;            putThreads.add(new Thread() &#123;                @Override                public void run() &#123;                    for (int j = 0; j &lt; LOOP_COUNT; j++) &#123;                        basket.create();                    &#125;                &#125;            &#125;);            takeThreads.add(new Thread() &#123;                @Override                public void run() &#123;                    for (int j = 0; j &lt; LOOP_COUNT; j++) &#123;                        basket.get().getStatus();                    &#125;                &#125;            &#125;);        &#125;        long start = System.nanoTime();        for (int i = 0; i &lt; THREADS_COUNT; i++) &#123;            takeThreads.get(i).start();            putThreads.get(i).start();        &#125;        for (int i = 0; i &lt; THREADS_COUNT; i++) &#123;            takeThreads.get(i).join();            putThreads.get(i).join();        &#125;        long end = System.nanoTime();        long period = end - start;        if(n == 0) &#123;            continue;\t        &#125;        sum += (period);        System.out.println(period);        if(period &lt; min) &#123;            min = period;        &#125;        if(period &gt; max) &#123;            max = period;        &#125;    &#125;    System.out.println(&quot;Average : &quot; + sum / 100);    System.out.println(&quot;Max : &quot; + max);    System.out.println(&quot;Min : &quot; + min);&#125;\n\n在笔者的计算机上运行测试，采用 volatile 方案的运行结果如下：\n123Average : 62535770Max : 82515000Min : 45161000\n\n采用 unsafe.putOrderedObject 方案的运行结果如下：\n123Average : 50746230Max : 68999000Min : 38038000\n\n从结果看出，unsafe.putOrderedObject 方案比 volatile 方案平均耗时减少 18.9%，最大耗时减少 16.4%，最小耗时减少 15.8%. 另外，即使在其它会发生写写重排序的处理器中，由于 StoreStore 屏障的性能损耗小于 StoreLoad 屏障，采用这一方法也是一种可行的方案。但值得再次注意的是，这一方案不是对 volatile 语义的等价替换，而是在特定场景下做的特殊优化，它仅避免了写写重排序，但不保证内存可见性。\n序\n样例选自《Java 并发编程实践》章节 16.1\n实验代码见附 1\nhttp://en.wikipedia.org/wiki/Out-of-order_execution\nOracle Java Hotspot https://wikis.oracle.com/display/HotSpotInternals/PerformanceTacticIndex IBM JVM http://publib.boulder.ibm.com/infocenter/javasdk/v1r4m2/index.jsp?topic=%2Fcom.ibm.java.doc.diagnostics.142j9%2Fhtml%2Fhowjitopt.html\nJava 语言规范中对 “动作” 这个词有一个明确而具体的定义，详见http://docs.oracle.com/javase/specs/jls/se7/html/jls-17.html#jls-17.4.2。\nhttps://community.oracle.com/thread/1544959\nhttp://www.cs.umd.edu/~pugh&#x2F;java&#x2F;memoryModel&#x2F;jsr133.pdf\n参见《Java 并发编程实践》章节 16.1\nIntel® 64 and IA-32 Architectures Software Developer’s Manual Volume 3 (3A, 3B &amp; 3C): System Programming Guide 章节 8.2\nhttp://www.cs.umd.edu/~pugh&#x2F;java&#x2F;memoryModel&#x2F;DoubleCheckedLocking.html\n\n附 1 复现重排序现象实验代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class Test &#123;    private static int x = 0, y = 0;    private static int a = 0, b =0;    public static void main(String[] args) throws InterruptedException &#123;        int i = 0;        for(;;) &#123;            i++;            x = 0; y = 0;            a = 0; b = 0;            Thread one = new Thread(new Runnable() &#123;                public void run() &#123;                                        shortWait(100000);                    a = 1;                    x = b;                &#125;            &#125;);            Thread other = new Thread(new Runnable() &#123;                public void run() &#123;                    b = 1;                    y = a;                &#125;            &#125;);            one.start();other.start();            one.join();other.join();            String result = &quot;第&quot; + i + &quot;次 (&quot; + x + &quot;,&quot; + y + &quot;）&quot;;            if(x == 0 &amp;&amp; y == 0) &#123;                System.err.println(result);                break;            &#125; else &#123;                System.out.println(result);            &#125;        &#125;    &#125;    public static void shortWait(long interval)&#123;        long start = System.nanoTime();        long end;        do&#123;            end = System.nanoTime();        &#125;while(start + interval &gt;= end);    &#125;&#125;\n","slug":"JAVA/Java内存访问重排序的研究","date":"2021-08-15T23:36:56.000Z","categories_index":"Java,JAVA","tags_index":"com,volatile,before","author_index":"dandeliono"},{"id":"a3c579c0425bdc8d839cf9bbfcb91047","title":"Spring Boot 排除自动配置的 4 种方法","content":"Spring Boot 排除自动配置的 4 种方法Spring Boot 提供的自动配置非常强大，某些情况下，自动配置的功能可能不符合我们的需求，需要我们自定义配置，这个时候就需要排除 &#x2F; 禁用 Spring Boot 某些类的自动化配置了。\n比如：数据源、邮件，这些都是提供了自动配置的，我们需要排除 Spring Boot 的自动化配置，交给我们自己来自定义，该如何做呢？\n今天栈长给你介绍 4 种排除方式，总有一种能帮到你！\n方法 1使用 @SpringBootApplication 注解的时候，使用 exclude 属性进行排除指定的类：\n@SpringBootApplication(exclude = &#123;DataSourceAutoConfiguration.class, MailSenderAutoConfiguration.class&#125;)\npublic class Application &#123;\n    \n&#125;\n\n自动配置类不在类路径下的时候，使用 excludeName 属性进行排除指定的类名全路径：\n@SpringBootApplication(excludeName = &#123;&quot;org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration&quot;, &quot;org.springframework.boot.autoconfigure.mail.MailSenderAutoConfiguration&quot;&#125;)\npublic class Application &#123;\n    \n&#125;\n\n方法 2单独使用 @EnableAutoConfiguration 注解的时候：\n@...\n@EnableAutoConfiguration\n(exclude = &#123;DataSourceAutoConfiguration.class, MailSenderAutoConfiguration.class&#125;)\npublic class Application &#123;\n    \n&#125;\n\n自动配置类不在类路径下的时候，使用 excludeName 属性进行排除指定的类名全路径：\n@...\n@EnableAutoConfiguration &#123;&quot;org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration&quot;, &quot;org.springframework.boot.autoconfigure.mail.MailSenderAutoConfiguration&quot;&#125;)\npublic class Application &#123;\n    \n&#125;\n\n方法 3使用 Spring Cloud 和 @SpringCloudApplication 注解的时候：\n@...\n@EnableAutoConfiguration\n(exclude = &#123;DataSourceAutoConfiguration.class, MailSenderAutoConfiguration.class&#125;)\n@SpringCloudApplication\npublic class Application &#123;\n    \n&#125;\n\nSpring Cloud 必须建立在 Spring Boot 应用之上，所以这个不用多解释了。\n方法 4终极方案，不管是 Spring Boot 还是 Spring Cloud 都可以搞定，在配置文件中指定参数 spring.autoconfigure.exclude 进行排除：\nspring.autoconfigure.exclude=org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration,\\\\\n    org.springframework.boot.autoconfigure.mail.MailSenderAutoConfiguration\n\n或者还可以这样写：\nspring.autoconfigure.exclude\\[0\\]\\=org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration\nspring.autoconfigure.exclude\\[1\\]\\=org.springframework.boot.autoconfigure.mail.MailSenderAutoConfiguration\n\n如果你用的是 yaml 配置文件，可以这么写：\nspring:     \n  autoconfigure:\n    exclude:\n      - org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration\n      - org.springframework.boot.autoconfigure.mail.MailSenderAutoConfiguration\n\n","slug":"JAVA/Spring Boot 排除自动配置的 4 种方法","date":"2021-08-09T23:05:31.000Z","categories_index":"Spring,JAVA","tags_index":"Boot,使用,方法","author_index":"dandeliono"},{"id":"24d9916b82108a815fcb896c7754eb78","title":"Java异步编程","content":"Java异步编程随着RxJava、Reactor等异步框架的流行，异步编程受到了越来越多的关注，尤其是在 IO 密集型的业务场景中，相比传统的同步开发模式，异步编程的优势越来越明显。\n那到底什么是异步编程？异步化真正的好处又是什么？如何选择适合自己团队的异步技术？在实施异步框架落地的过程中有哪些需要注意的地方？\n本文从以下几个方面结合真实项目异步改造经验对异步编程进行分析，希望能给大家一些客观认识：\n\n使用 RxJava 异步改造后的效果\n什么是异步编程？异步实现原理\n异步技术选型参考\n异步化真正的好处是什么？\n异步化落地的难点及解决方案\n扩展: 异步其他解决方案 - 协程\n\n使用 RxJava 异步改造后的效果下图是我们后端 java 项目使用 RxJava 改造成异步前后的 RT(响应时长) 效果对比：\n\nimage\n\nimage\n统计数据基于 App 端的 gateway，以 75 线为准，还有 80、85、90、99 线，从图中可以看出改成异步后接口整体的平均响应时长降低了 **40%**左右。\n(响应时间是以发送请求到收到后端接口响应数据的时长，上图改造的这个后端 java 接口内部流程比较复杂，因为公司都是微服务架构，该接口内部又调用了 6 个其他服务的接口，最后把这些接口的数据汇总在一起返回给前端)\n这张图是同步接口和改造成异步接口前后的 CPU 负载情况对比\n改造前 cpu load : 35.46\n\nimage\n改造后 cpu load : 14.25\n\nimage\n改成异步后 CPU 的负载情况也有明显下降，但 CPU 使用率并无影响 (一般情况下异步化后 cpu 的利用率会有所提高，但要看具体的业务场景)\nCPU LoadAverage 是指：一段时间内处于可运行状态和不可中断状态的进程平均数量。(可运行分为正在运行进程和正在等待 CPU 的进程；不可中断则是它正在做某些工作不能被中断比如等待磁盘 IO、网络 IO 等)\n而我们的服务业务场景大部分都是 IO 密集型业务，功能实现很多需要依赖底层接口，会进行频繁的 IO 操作。\n下图是 2019 年在全球架构师峰会上阿里分享的异步化改造后的 RT 和 QPS 效果：\n\nimage\n（图片来源：淘宝应用架构升级——反应式架构的探索与实践）\n什么是异步编程？响应式编程 + NIO1. 异步和同步的区别：我们先从 I&#x2F;O 的角度看下同步模式下接口 A 调用接口 B 的交互流程:\n下图是传统的同步模式下 io 线程的交互流程，可以看出 io 是阻塞的，即 bio 的运行模式\n\nimage\n接口 A 发起调用接口 B 后，这段时间什么事情也不能做，主线程阻塞一直等到接口 B 数据返回，然后才能进行其他操作，可想而知如果接口 A 调用的接口不止 B 的话 (A-&gt;B-&gt;C-&gt;D-&gt;E。。。)，那么等待的时间也是递增的，而且这期间 CPU 也要一直占用着，白白浪费资源，也就是上图看到的 cpu load 高的原因。\n而且还有一个隐患就是如果调用的其他服务中的接口比如 C 超时，或接口 C 挂掉了，那么对调用方服务 A 来说，剩余的接口比如 D、E 都会无限等待下去。。。\n其实大部分情况下我们收到数据后内部的处理逻辑耗时都很短，这个可以通过埋点执行时间统计，大部分时间都浪费在了 IO 等待上。\n下面这个视频演示了同步模式下我们线上环境真实的接口调用情况，即接口调用的线程执行和变化情况，(使用的工具是 JDK 自带的 jvisual 来监控线程变化情况)\n这里先交代下大致背景：服务端 api 接口 A 内部一共调用了 6 个其他服务的接口，大致交互是这样的：\nA 接口（B -&gt; C -&gt; D -&gt; E -&gt; F -&gt; G）返回聚合数据\n背景：使用 Jemter 测试工具压测 100 个线程并发请求接口，以观察线程的运行情况（可以全屏观看）：\nhttp-nio-8080-exec*开头的是 tomcat 线程池中的线程，即前端请求我们后端接口时要通过 tomcat 服务器接收和转发的线程，因为我们后端 api 接口内部又调用了其他服务的 6 个接口（B、C、D、E、F、G），同步模式下需要等待上一个接口返回数据才能继续调用下一个接口，所以可以从视频中看出，大部分的 http 线程耗时都在 8 秒以上 (绿色线条代表线程是 “运行中” 状态，8 秒包括等待接口返回的时间和我们内部逻辑处理的总时间，因为是本地环境测试，受机器和网络影响较大)\n然后我们再看下异步模式的交互流程，即 nio 方式：\n\nimage\n大致流程就是接口 A 发起调用接口 B 的请求后就立即返回，而不用阻塞等待接口 B 响应，这样的好处是http-nio-8080-exec*线程可以马上得到复用，接着处理下一个前端请求的任务，如果接口 B 处理完返回数据后，会有一个回调线程池处理真正的响应，即这种模式下我们的业务流程是 http 线程只处理请求，回调线程处理接口响应。\n这个视频演示了异步模式下接口 A 的线程执行情况，同样也是使用 Jemter 测试工具压测 100 个线程并发请求接口，以观察线程的运行情况（可以全屏观看）：\n模拟的条件和同步模式一样，同样是 100 个线程并发请求接口，但这次http-nio-8080-exec*开头的线程只处理请求任务，而不再等待全部的接口返回，所以 http 的线程运行时间普遍都很短 (大部分在 1.8 秒左右完成)，AsfThread-executor-*是我们系统封装的回调线程池，处理底层接口的真正响应数据。\n演示视频中的AsfThread-executor-*的回调线程只创建了 30 多个，而请求的 http 线程有 100 个，也就是说这 30 多个回调线程处理了接口 B 的 100 次响应 (其实应该是 600 次，因为接口 B 内部又调用了 6 个其他接口，这 6 次也都是在异步线程里处理响应的)，因为每个接口返回的时间不一样，加上网络传输的时间，所以可以利用这个时间差充分复用线程即 cpu 资源，视频中回调线程AsfThread-executor-*的绿色运行状态是多段的，表示复用了多次，也就是少量回调线程处理了全部 (600 次) 的响应，这正是 IO 多路复用的机制。\nnio 模式下虽然http-nio-8080-exec*线程和回调线程AsfThread-executor-*的运行时间都很短，但是从 http 线程开始到 asf 回调处理完返回给前端结果的时间和 bio 即同步模式下的时间差异不大（在相同的逻辑流程下），并不是 nio 模式下服务响应的整体时间就会缩短，而是会提升 CPU 的利用率，因为 CPU 不再会阻塞等待（不可中断状态减少），这样 CPU 就能有更多的资源来处理其他的请求任务，相同单位时间内能处理更多的任务，所以 nio 模式带来的好处是：\n\n提升 QPS（用更少的线程资源实现更高的并发能力）\n降低 CPU 负荷, 提高利用率\n\n2. Nio 原理\nimage\n结合上面的接口交互图可知，接口 B 通过网络返回数据给调用方 (接口 A) 这一过程，对应底层实现就是网卡接收到返回数据后，通过自身的 DMA（直接内存访问）将数据拷贝到内核缓冲区，这一步不需要 CPU 参与操作，也就是把原先 CPU 等待的事情交给了底层网卡去处理，这样 CPU 就可以专注于我们的应用程序即接口内部的逻辑运算。\n3. Nio In Java\nimage\nnio 在 java 里的实现主要是上图中的几个核心组件：channel、buffer、selector，这些组件组合起来即实现了上面所讲的多路复用机制，如下图所示：\n\nimage\n响应式编程1. 什么是响应式编程？它和传统的编程方式有什么区别？响应式可以简单的理解为收到某个事件或通知后采取的一系列动作，如上文中所说的响应操作系统的网络数据通知，然后以回调的方式处理数据。\n传统的命令式编程主要由：顺序、分支、循环 等控制流来完成不同的行为\n响应式编程的特点是：\n\n以逻辑为中心转换为以数据为中心\n从命令式到声明式的转换\n\n2. Java.Util.Concurrent.Future在 Java 使用 nio 后无法立即拿到真实的数据，而且先得到一个 “future“，可以理解为邮戳或快递单，为了获悉真正的数据我们需要不停的通过快递单号查询快递进度，所以 J.U.C 中的 Future 是 Java 对异步编程的第一个解决方案，通常和线程池结合使用，伪代码形式如下：\nExecutorService executor = Executors.newCachedThreadPool(); \n\n复制代码\nFuture的缺点很明显：\n\n无法方便得知任务何时完成\n无法方便获得任务结果\n在主线程获得任务结果会导致主线程阻塞\n\n3. ListenableFutureGoogle 并发包下的listenableFuture对 Java 原生的 future 做了扩展，顾名思义就是使用监听器模式实现的回调机制，所以叫可监听的 future。\nFutures.addCallback(listenableFuture, new FutureCallback&lt;String&gt;() &#123;\n\n复制代码\n回调机制的最大问题是：Callback Hell（回调地狱）\n试想如果调用的接口多了，而且接口之间有依赖的话，最终写出来的代码可能就是下面这个样子：\n\nimage\n\n代码的字面形式和其所表达的业务含义不匹配\n业务的先后关系在代码层面变成了包含和被包含的关系\n大量使用 Callback 机制，使应该是先后的业务逻辑在代码形式上表现为层层嵌套, 这会导致代码难以理解和维护。\n\n那么如何解决 Callback Hell 问题呢？\n响应式编程\n其实主要是以下两种解决方式：\n\n事件驱动机制\n链式调用 (Lambda)\n\n4. CompletableFutureJava8 里的CompletableFuture和 Java9 的Flow Api勉强算是上面问题的解决方案：\nCompletableFuture&lt;String&gt; f1 = CompletableFuture.supplyAsync(() -&gt;\n\n复制代码\n但CompletableFuture处理简单的任务可以使用，但并不是一个完整的反应式编程解决方案，在服务调用复杂的情况下，存在服务编排、上下文传递、柔性限流 (背压) 方面的不足\n如果使用CompletableFuture面对这些问题可能需要自己额外造一些轮子，Java9 的Flow虽然是基于 Reactive Streams 规范实现的，但没有 RxJava、Project Reactor 这些异步框架丰富和强大和完整的解决方案。\n当然如果接口逻辑比较简单，完全可以使用listenableFuture或CompletableFuture，关于他们的详细用法可参考之前的一篇文章：Java 异步编程指南  \n5. Reactive Streams在网飞推出 RxJava1.0 并在 Android 端普及流行开后，响应式编程的规范也呼之欲出：\nhttps://www.reactive-streams.org/  \n包括后来的 RxJava2.0、Project Reactor 都是基于 Reactive Streams 规范实现的。\n关于他们和listenableFuture、 CompletableFuture的区别通过下面的例子大家应该就会清楚。\n比如下面的基于回调的代码示例：获取用户的 5 个收藏列表功能\n\nimage\n图中标注序号的步骤对应如下：\n\n根据 uid 调用用户收藏列表接口userService.getFavorites  \n成功的回调逻辑\n如果用户收藏列表为空\n调用推荐服务suggestionService.getSuggestions  \n推荐服务成功后的回调逻辑\n取前 5 条推荐并展示 (Java8 Stream api)\n推荐服务失败的回调, 展示错误信息\n如果用户收藏列表有数据返回\n取前 5 条循环调用详情接口favoriteService.getDetails 成功回调则展示详情, 失败回调则展示错误信息\n\n可以看出主要逻辑都是在回调函数（onSuccess()、onError()）中处理的，在可读性和后期维护成本上比较大。\n基于 Reactive Streams 规范实现的响应式编程解决方案如下：\n\nimage\n\n调用用户收藏列表接口\n压平数据流调用详情接口\n如果收藏列表为空调用推荐接口\n取前 5 条\n切换成异步线程处理上述声明接口返回结果)\n成功则展示正常数据, 错误展示错误信息\n\n可以看出因为这些异步框架提供了丰富的 api，所以我们可以把主要精力放在数据的流转上，而不是原来的逻辑控制上。这也是异步编程带来的思想上的转变。 \n下图是 RxJava 的operator api：\n\nimage\n（如果这些操作符满足不了你的需求，你也可以自定义操作符）\n所以说异步最吸引人的地方在于资源的充分利用，不把资源浪费在等待的时间上 (nio)，代价是增加了程序的复杂度，而 Reactive Program 封装了这些复杂性，使其变得简单。 \n所以我们无论使用哪种异步框架，尽量使用框架提供的 api，而不是像上图那种基于回调业务的代码，把业务逻辑都写在 onSuccess、onError 等回调方法里，这样无法发挥异步框架的真正作用：\n\n\n\n\n\n\n\n\n\nCodes Like Sync，Works Like Async\n即以同步的方式编码，达到异步的效果与性能, 兼顾可维护性与可伸缩性。\n异步框架技术选型\n（图片来源：淘宝应用架构升级——反应式架构的探索与实践）\n上面这张图也是阿里在 2019 年的深圳全球架构师峰会上分享的 PPT 截图（文章末尾有链接），供大家参考，选型标准主要是基于稳定性、普及性、成本这 3 点考虑\n如果是我个人更愿意选择 Project Reactor 作为首选异步框架，（具体差异网上很多分析，大家可以自行百度谷歌），还有一点是因为 Netflix 的尿性，推出的开源产品渐渐都不维护了，而且 Project Reactor 提供了reactor-adapter组件，可以方便的和 RxJava 的 api 转换。\n其实还有 Vert.x 也算异步框架 (底层使用 netty 实现 nio, 最新版已支持 reactive stream 规范)\n异步化真正的好处Scalability伸缩性主要体现在以下两个方面：\n\nelastic 弹性\nresilient 容错性\n\n（异步化在平时不会明显降低 RT、提高 QPS，文章开头的数据也是在大促这种流量高峰下的体现出的异步效果）\n从架构和应用等更高纬度看待异步带来的好处则会提升系统的两大能力：弹性 和 容错性\n前者反映了系统应对压力的表现，后者反映了系统应对故障的表现\n1. 容错性像 RxJava，Reactor 这些异步框架处理回调数据时一般会切换线程上下文，其实就是使用不同的线程池来隔离不同的数据流处理逻辑，下图说明了这一特性的好处：\n\nimage\n即利用异步框架支持线程池切换的特性实现服务 &#x2F; 接口隔离，进而提高系统的高可用。\n2. 弹性\nimage\nback-pressure 是一种重要的反馈机制，相比于传统的熔断限流等方式，是一种更加柔性的自适应限流。使得系统得以优雅地响应负载，而不是在负载下崩溃。\n异步化落地的难点及解决方案还是先看下淘宝总结的异步改造中难点问题：\n\nimage\n（图片来源：淘宝应用架构升级——反应式架构的探索与实践）\n中间件全异步牵涉到到公司中台化战略或框架部门的支持，包括公司内部常用的中间件比如 MQ、redis、dal 等，超出了本文讨论的范围，感兴趣的可以看下文章末尾的参考资料。\n线程模型统一的背景在上一节异步化好处时有提到过，其实主要还是对线程池的管理，做好服务隔离，线程池设置和注意事项可以参考之前的两篇文章：Java 踩坑记系列之线程池 、线程池 ForkJoinPool 简介  \n这里主要说下上下文传递和阻塞检测的问题：\n1. 上下文传递改造成异步服务后，不能再使用ThreadLocal传递上下文 context，因为异步框架比如 RxJava 一般在收到通知后会先调用observeOn()方法切换成另外一个线程处理回调，比如我们在请求接口时在ThreadLocal的 context 里设置了一个值，在回调线程里从 context 里取不到这个值的，因为此时已经不是同一个ThreadLocal了，所以需要我们手动在切换上下文的时候传递 context 从一个线程到另一个线程环境，伪代码如下：\nContext context = ThreadLocalUtils.get(); \n\n复制代码\n在observeOn()方法切换成另外一个线程后调用doOnEvent方法将原来的 context 赋给新的线程ThreadLocal  \n注意：这里的代码只是提供一种解决思路，实际在使用前和使用后还要考虑清空ThreadLocal，因为线程有可能会回收到线程池下次复用，而不是立即清理，这样就会污染上下文环境。\n可以将传递上下文的方法封装成公共方法，不需要每次都手动切换。\n2. 阻塞检测阻塞检测主要是要能及时发现我们某个异步任务长时间阻塞的发生，比如异步线程执行时间过长进而影响整个接口的响应，原来同步场景下我们的日志都是串行记录到 ES 或 Cat 上的，现在改成异步后，每次处理接口数据的逻辑可能在不同的线程中完成，这样记录的日志就需要我们主动去合并（依据具体的业务场景而定），如果日志无法关联起来，对我们排查问题会增加很多难度。所幸的是随着异步的流行，现在很多日志和监控系统都已支持异步了。\nProject Reactor 自己也有阻塞检测功能，可以参考这篇文章：BlockHound  \n3. 其他问题除了上面提到的两个问题外，还有一些比如 RxJava2.0 之后不支持返回 null，如果我们原来的代码或编程习惯所致返回结果有 null 的情况，可以考虑使用 java8 的Optional.ofNullable()包装一下，然后返回的 RxJava 类型是这样的：Single&lt;Optional&gt;，其他异步框架如果有类似的问题同理。\n异步其他解决方案：纤程 &#x2F; 协程\nQuasar\nKilim\nKotlin\nOpen JDK Loom\nAJDK wisp2\n\n协程并不是什么新技术，它在很多语言中都有实现，比如 Python、Lua、Go 都支持协程。\n协程与线程不同之处在于，线程由内核调度，而协程的调度是进程自身完成的。这样就可以不受操作系统对线程数量的限制，一个线程内部可以创建成千上万个协程。因为上文讲到的异步技术都是基于线程的操作和封装，Java 中的线程概念对应的就是操作系统的线程。\n1. Quasar、Kilim开源的 Java 轻量级线程（协程）框架，通过利用Java instrument技术对字节码进行修改，使方法挂起前后可以保存和恢复 JVM 栈帧，方法内部已执行到的字节码位置也通过增加状态机的方式记录，在下次恢复执行可直接跳转至最新位置。\n2. KotlinKotlin Coroutine 协程库，因为 Kotlin 的运行依赖于 JVM，不能对 JVM 进行修改，因此 Kotlin 不能在底层支持协程。同时 Kotlin 是一门编程语言，需要在语言层面支持协程，所以 Kotlin 对协程支持最核心的部分是在编译器中完成，这一点其实和 Quasar、Kilim 实现原理类似，都是在编译期通过修改字节码的方式实现协程\n3. Project LoomProject Loom 发起的原因是因为长期以来 Java 的线程是与操作系统的线程一一对应的，这限制了 Java 平台并发能力提升，Project Loom 是从 JVM 层面对多线程技术进行彻底的改变。\nOpenJDK 在 2018 年创建了 Loom 项目，目标是在 JVM 上实现轻量级的线程，并解除 JVM 线程与内核线程的映射。其实 Loom 项目的核心开发人员正是从 Quasar 项目过来的，目的也很明确，就是要将这项技术集成到底层 JVM 里，所以 Quasar 项目目前已经不维护了。。。\n4. AJDK Wisp2Alibaba Dragonwell 是阿里巴巴的 Open JDK 发行版，提供长期支持。dragonwell8 已开源协程功能（之前的版本是不支持的），开启 jvm 命令：-XX:+UseWisp2 即支持协程。\n总结\nFuture 在异步方面支持有限\nCallback 在编排能力方面有 Callback Hell 的短板\nProject Loom 最新支持的 Open JDK 版本是 16，目前还在测试中\nAJDK wisp2 需要换掉整个 JVM，需要考虑改动成本和收益比\n\n","slug":"JAVA/Java异步编程","date":"2021-08-08T23:36:51.000Z","categories_index":"com,JAVA","tags_index":"https,raw,githubusercontent","author_index":"dandeliono"},{"id":"dd1cc8cb9e1a5d913e3dce35e0f864ee","title":"Spring Boot配置文件指定包外文件","content":"Spring Boot配置文件指定包外文件通过命令行指定#SpringApplication 会默认将命令行选项参数转换为配置信息例如，启动时命令参数指定：\njava -jar myproject.jar --server.port = 9000 \n从命令行指定配置项的优先级最高，不过你可以通过 setAddCommandLineProperties 来禁用\nSpringApplication.setAddCommandLineProperties(false). \n外置配置文件#Spring 程序会按优先级从下面这些路径来加载 application.properties 配置文件\n\n当前目录下的 &#x2F; config 目录\n当前目录\nclasspath 里的 &#x2F; config 目录\nclasspath 跟目录\n\n因此，要外置配置文件就很简单了，在 jar 所在目录新建 config 文件夹，然后放入配置文件，或者直接放在配置文件在 jar 目录\n自定义配置文件#如果你不想使用 application.properties 作为配置文件，怎么办？完全没问题\njava -jar myproject.jar --spring.config.location=classpath:/default.properties,classpath:/override.properties \n或者\njava -jar -Dspring.config.location=D:\\config\\config.properties springbootrestdemo-0.0.1-SNAPSHOT.jar \n当然，还能在代码里指定\n`@SpringBootApplication@PropertySource(value&#x3D;{“file:config.properties”})public class SpringbootrestdemoApplication {\npublic static void main(String[] args) &#123;\n    SpringApplication.run(SpringbootrestdemoApplication.class, args);\n&#125;\n\n}` \n按 Profile 不同环境读取不同配置#不同环境的配置设置一个配置文件，例如：\n\ndev 环境下的配置配置在 application-dev.properties 中；\nprod 环境下的配置配置在 application-prod.properties 中。\n\n在 application.properties 中指定使用哪一个文件\nspring.profiles.active = dev \n当然，你也可以在运行的时候手动指定：\njava -jar myproject.jar --spring.profiles.active = prod https://www.cnblogs.com/xiaoqi/p/6955288.html\n","slug":"JAVA/Spring Boot配置文件指定包外文件","date":"2021-08-07T15:16:44.000Z","categories_index":"jar,JAVA","tags_index":"properties,config,application","author_index":"dandeliono"},{"id":"bd22547b0a0201a36c471a9e283ced81","title":"ES 操作之批量写-BulkProcessor 原理浅析","content":"ES 操作之批量写-BulkProcessor 原理浅析\nBulkProcessor\n\n创建流程\n\n[内部逻辑实现]\n\n\n123456最近对线上业务进行重构，涉及到ES同步这一块，在重构过程中，为了ES 写入 性能考虑，大量的采取了 bulk的方式，来保证整体的一个同步速率，针对BulkProcessor 来深入一下，了解下 是如何实现，基于请求数，请求数据量大小 和 固定时间，刷新写入ES 的原理针对ES 批量写入， 提供了3种方式，在 high-rest-client 中分别是 bulk bulkAsync  bulkProcessor 3种方式。本文主要针对 bulkProcessor 来进行一些讲述\n\nBulkProcessor官方介绍\n12345BulkProcessor是一个线程安全的批量处理类,允许方便地设置 刷新 一个新的批量请求 (基于数量的动作,根据大小,或时间),容易控制并发批量的数量请求允许并行执行。\n\n创建流程How To use ？\n来看个 demo 创建 BulkProcessor\n12345678910111213141516171819202122232425262728293031323334353637@Bean(name = &quot;bulkProcessor&quot;) // 可以封装为一个bean，非常方便其余地方来进行 写入 操作  public BulkProcessor bulkProcessor()&#123;        BiConsumer&lt;BulkRequest, ActionListener&lt;BulkResponse&gt;&gt; bulkConsumer =                (request, bulkListener) -&gt; Es6XServiceImpl.getClient().bulkAsync(request, RequestOptions.DEFAULT, bulkListener);        return BulkProcessor.builder(bulkConsumer, new BulkProcessor.Listener() &#123;            @Override            public void beforeBulk(long executionId, BulkRequest request) &#123;              \t\t// todo do something                int i = request.numberOfActions();                log.error(&quot;ES 同步数量&#123;&#125;&quot;,i);            &#125;            @Override            public void afterBulk(long executionId, BulkRequest request, BulkResponse response) &#123;\t\t\t\t\t\t// todo do something                Iterator&lt;BulkItemResponse&gt; iterator = response.iterator();                while (iterator.hasNext())&#123;                    System.out.println(JSON.toJSONString(iterator.next()));                &#125;            &#125;            @Override            public void afterBulk(long executionId, BulkRequest request, Throwable failure) &#123;\t\t\t\t\t\t\t\t// todo do something                log.error(&quot;写入ES 重新消费&quot;);            &#125;        &#125;).setBulkActions(1000) //  达到刷新的条数                .setBulkSize(new ByteSizeValue(1, ByteSizeUnit.MB)) // 达到 刷新的大小                .setFlushInterval(TimeValue.timeValueSeconds(5)) // 固定刷新的时间频率                .setConcurrentRequests(1) //并发线程数                .setBackoffPolicy(BackoffPolicy.exponentialBackoff(TimeValue.timeValueMillis(100), 3)) // 重试补偿策略                .build();     &#125;\n\n使用 BulkProcessor\n12bulkProcessor.add(xxxRequest)\n\n创建过程做了些什么？\n\n创建一个 consumer 对象用来封装传递参数，和请求操作BiConsumer&lt;BulkRequest, ActionListener&gt; bulkConsumer &#x3D;(request, bulkListener) -&gt; Es6XServiceImpl.getClient().bulkAsync(request, RequestOptions.DEFAULT, bulkListener);\n我们可以看到用了 java 8 的函数式编程接口 BiConsumer 关于 BiConsumer 的用法，可以自行百度，因为也是采取的 异步刷新策略， 所以，是一个返回结果的 Listener ActionListener\n\n构建并 BulkProcessreturn BulkProcessor.builder(bulkConsumer, new BulkProcessor.Listener() {****\n}).setBulkActions(1000).setBulkSize(new ByteSizeValue(1, ByteSizeUnit.MB)).setFlushInterval(TimeValue.timeValueSeconds(5)).setConcurrentRequests(1).setBackoffPolicy(BackoffPolicy.exponentialBackoff(TimeValue.timeValueMillis(100), 3)).build();\n}\n可以很清楚的看到，在 build 操作中，我们看到，在 build 中，除了 之前定义的 consumer，还实现了一个 Listener 接口 （稍后会具体讲到），用来做一些 在批量求情之前和请求之后的处理。\n\n\n至此为止，BulkProcessor 创建，就 OK 啦～。\n内部逻辑实现先不说话，我们先上张类图\n\n可以看到，在 BulkProcessor 中，有这样的一些类和接口\n1234567ListenerBuilderBulkProcessorFlush===== 华丽的分界线BulkProcessor 实现了 Closeable --&gt; 继承自  AutoCloseable （关于AutoCloseable 本文不做过多说明，具体的可以百度，或者等待后续）\n\n那么先从构建开始，我们来看下 Builder\n123456789101112131415161718192021222324252627282930313233343536373839404142/*** 简单的构建，可以看到，就是一个client 和 listener 这个不会做刷新策略，*/public static Builder builder(Client client, Listener listener) &#123;        Objects.requireNonNull(client, &quot;client&quot;);        Objects.requireNonNull(listener, &quot;listener&quot;);        return new Builder(client::bulk, listener, client.threadPool(), () -&gt; &#123;&#125;);    &#125;/*** 所有功能的builder 实现方法* ScheduledThreadPoolExecutor 用来实现 按照时间频率，来进行 刷新，如 每5s **/    public static Builder builder(BiConsumer&lt;BulkRequest, ActionListener&lt;BulkResponse&gt;&gt; consumer, Listener listener) &#123;        Objects.requireNonNull(consumer, &quot;consumer&quot;);        Objects.requireNonNull(listener, &quot;listener&quot;);        final ScheduledThreadPoolExecutor scheduledThreadPoolExecutor = Scheduler.initScheduler(Settings.EMPTY); // 接口静态方式，来实现 Executor 的初始化        return new Builder(consumer, listener,                (delay, executor, command) -&gt; scheduledThreadPoolExecutor.schedule(command, delay.millis(), TimeUnit.MILLISECONDS), //                () -&gt; Scheduler.terminate(scheduledThreadPoolExecutor, 10, TimeUnit.SECONDS));    &#125;/*** 构造函数 * @param consumer 前文定义的consumer request response action* @param listener listener  BulkProcessor 内置监听器* @param scheduler elastic 定时调度 类scheduler  * @paran onClose 关闭时候的运行*/    private Builder(BiConsumer&lt;BulkRequest, ActionListener&lt;BulkResponse&gt;&gt; consumer, Listener listener,                        Scheduler scheduler, Runnable onClose) &#123;            this.consumer = consumer;            this.listener = listener;            this.scheduler = scheduler;            this.onClose = onClose;        &#125;\n\n通过上述的代码片段，可以很明显的看到，关于初始化构建的一些关键点和要素\n看完 builder 接下来，我们看下 bulkprocessor 是如何工作的\n先看下构造方法\n12345678910111213BulkProcessor(BiConsumer&lt;BulkRequest, ActionListener&lt;BulkResponse&gt;&gt; consumer, BackoffPolicy backoffPolicy, Listener listener,               int concurrentRequests, int bulkActions, ByteSizeValue bulkSize, @Nullable TimeValue flushInterval,               Scheduler scheduler, Runnable onClose) &#123;     this.bulkActions = bulkActions;     this.bulkSize = bulkSize.getBytes();     this.bulkRequest = new BulkRequest();     this.scheduler = scheduler;     this.bulkRequestHandler = new BulkRequestHandler(consumer, backoffPolicy, listener, scheduler, concurrentRequests); // BulkRequestHandler 批量执行 handler 操作     // Start period flushing task after everything is setup     this.cancellableFlushTask = startFlushTask(flushInterval, scheduler); //开始刷新任务     this.onClose = onClose; &#125;\n\nstartFlushTask 如何进行工作\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960private Scheduler.Cancellable startFlushTask(TimeValue flushInterval, Scheduler scheduler) &#123;      // 如果 按照时间刷新 为空，则直接返回 任务为取消状态   if (flushInterval == null) &#123;            return new Scheduler.Cancellable() &#123;                @Override                public void cancel() &#123;&#125;                @Override                public boolean isCancelled() &#123;                    return true;                &#125;            &#125;;        &#125;        final Runnable flushRunnable = scheduler.preserveContext(new Flush());        return scheduler.scheduleWithFixedDelay(flushRunnable, flushInterval, ThreadPool.Names.GENERIC);    &#125;    private void executeIfNeeded() &#123;        ensureOpen();        if (!isOverTheLimit()) &#123;            return;        &#125;        execute();    &#125;// 刷新线程class Flush implements Runnable &#123;        @Override        public void run() &#123;            synchronized (BulkProcessor.this) &#123;                if (closed) &#123;                    return;                &#125;                if (bulkRequest.numberOfActions() == 0) &#123;                    return;                &#125;                execute(); // 下面方法            &#125;        &#125;    &#125;/***  刷新执行**/ // (currently) needs to be executed under a lock    private void execute() &#123;        final BulkRequest bulkRequest = this.bulkRequest;        final long executionId = executionIdGen.incrementAndGet();\t\t\t\t// 刷新 bulkRequest 为下一批做准备        this.bulkRequest = new BulkRequest();        this.bulkRequestHandler.execute(bulkRequest, executionId);    &#125;\n\n看到这里，关于时间的定时调度，我们其实是很清楚了，那么 关于数据量 和 大小的判断策略在哪儿？\n123456789101112131415161718192021222324252627282930313233343536373839404142434445/*** 各种添加操作*/public BulkProcessor add(DocWriteRequest request, @Nullable Object payload) &#123;        internalAdd(request, payload);        return this;    &#125;\t/**\t* 我们可以看到，在添加之后，会做一个操作\t* executeIfNeeded 如果需要，则进行执行\t*/    private synchronized void internalAdd(DocWriteRequest request, @Nullable Object payload) &#123;        ensureOpen();        bulkRequest.add(request, payload);        executeIfNeeded();    &#125;/*** 如果超过限制，则执行刷新操作*/ private void executeIfNeeded() &#123;        ensureOpen();        if (!isOverTheLimit()) &#123;            return;        &#125;        execute();    &#125;\t/**\t* 这这儿，我们终于看到了 关于action 和 size 的判断操作，\t*\t*/    private boolean isOverTheLimit() &#123;        if (bulkActions != -1 &amp;&amp; bulkRequest.numberOfActions() &gt;= bulkActions) &#123;            return true;        &#125;        if (bulkSize != -1 &amp;&amp; bulkRequest.estimatedSizeInBytes() &gt;= bulkSize) &#123;            return true;        &#125;        return false;    &#125;\n\n通过上述的分析，关于按照时间，数据 size，大小来进行 flush 执行的入口我们都已经很清楚了\n123针对数据大小的设置。在每次添加的时候，做判断是否 超过限制针对 时间的频次控制，交由ScheduledThreadPoolExecutor 来去做监控\n\n下来，让我们看下具体的执行以及重试策略，和 返回值的处理\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public void execute(BulkRequest bulkRequest, long executionId) &#123;        Runnable toRelease = () -&gt; &#123;&#125;;        boolean bulkRequestSetupSuccessful = false;        try &#123;          // listener 填充 request 和执行ID            listener.beforeBulk(executionId, bulkRequest);          \t//通过信号量来进行资源的控制 来自于我们设置的 setConcurrentRequests            semaphore.acquire();            toRelease = semaphore::release;            CountDownLatch latch = new CountDownLatch(1);          // 进行执行并按照补偿重试策略如果失败            retry.withBackoff(consumer, bulkRequest, new ActionListener&lt;BulkResponse&gt;() &#123;\t\t\t\t\t\t \t//结果写入  ActionListener --&gt; BulkProcessor.Listener 的转换              @Override                public void onResponse(BulkResponse response) &#123;                    try &#123;                        listener.afterBulk(executionId, bulkRequest, response);                    &#125; finally &#123;                        semaphore.release();                        latch.countDown();                    &#125;                &#125;                @Override                public void onFailure(Exception e) &#123;                    try &#123;                        listener.afterBulk(executionId, bulkRequest, e);                    &#125; finally &#123;                        semaphore.release();                        latch.countDown();                    &#125;                &#125;            &#125;, Settings.EMPTY);            bulkRequestSetupSuccessful = true;            if (concurrentRequests == 0) &#123;                latch.await();            &#125;        &#125; catch (InterruptedException e) &#123;            Thread.currentThread().interrupt();            logger.info(() -&gt; new ParameterizedMessage(&quot;Bulk request &#123;&#125; has been cancelled.&quot;, executionId), e);            listener.afterBulk(executionId, bulkRequest, e);        &#125; catch (Exception e) &#123;            logger.warn(() -&gt; new ParameterizedMessage(&quot;Failed to execute bulk request &#123;&#125;.&quot;, executionId), e);            listener.afterBulk(executionId, bulkRequest, e);        &#125; finally &#123;            if (bulkRequestSetupSuccessful == false) &#123;  // if we fail on client.bulk() release the semaphore                toRelease.run();            &#125;        &#125;    &#125;\n\n最终的执行，在 RetryHandler 中，继续往下看\n12345public void execute(BulkRequest bulkRequest) &#123;            this.currentBulkRequest = bulkRequest;            consumer.accept(bulkRequest, this);        &#125;\n\n对，没错，只有一个操作， consumer.accept(bulkRequest, this);\n再一次展现了 java 8 函数式接口的功能强大之处 此处 consumer.accept(bulkRequest, this);\n执行的操作即 Es6XServiceImpl.getClient().bulkAsync(request, RequestOptions.DEFAULT, bulkListener);\n如何设置重试策略，以及数据的筛选\n12345678910111213141516171819202122232425262728293031323334@Override        public void onResponse(BulkResponse bulkItemResponses) &#123;            if (!bulkItemResponses.hasFailures()) &#123;                // we&#x27;re done here, include all responses                addResponses(bulkItemResponses, (r -&gt; true));                finishHim();            &#125; else &#123;                if (canRetry(bulkItemResponses)) &#123;                    addResponses(bulkItemResponses, (r -&gt; !r.isFailed()));                    retry(createBulkRequestForRetry(bulkItemResponses));                &#125; else &#123;                    addResponses(bulkItemResponses, (r -&gt; true));                    finishHim();                &#125;            &#125;        &#125;/*** 只针对失败的请求，放入到重试策略中*/   private void addResponses(BulkResponse response, Predicate&lt;BulkItemResponse&gt; filter) &#123;            for (BulkItemResponse bulkItemResponse : response) &#123;                if (filter.test(bulkItemResponse)) &#123;                    // Use client-side lock here to avoid visibility issues. This method may be called multiple times                    // (based on how many retries we have to issue) and relying that the response handling code will be                    // scheduled on the same thread is fragile.                    synchronized (responses) &#123;                        responses.add(bulkItemResponse);                    &#125;                &#125;            &#125;        &#125;\n\n再一次展示了函数式接口的强大之处\n补充一张流程图\n\n","slug":"JAVA/ES 操作之批量写-BulkProcessor 原理浅析","date":"2021-08-05T23:32:23.000Z","categories_index":"BulkProcessor,JAVA","tags_index":"https,com,consumer","author_index":"dandeliono"},{"id":"23169170d295b05eedfba9152181b026","title":"JVM上的实时监控类库-Metrics","content":"JVM上的实时监控类库-Metrics一、使用 Metrics系统开发到一定的阶段，线上的机器越来越多，就需要一些监控了，除了服务器的监控，业务方面也需要一些监控服务。Metrics作为一款监控指标的度量类库，提供了许多工具帮助开发者来完成自定义的监控工作。\n通过构建一个Spring Boot的基本应用来演示Metrics的工作方式。\n在 Maven 的pom.xml中引入Metrics：\n12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758&lt;groupId&gt;io.dropwizard.metrics&lt;/groupId&gt;&lt;artifactId&gt;metrics-core&lt;/artifactId&gt;&lt;version&gt;$&#123;metrics.version&#125;&lt;/version&gt;```目前`Metrics`的最新版本是`3.1.2`。二、Metrics的基本工具--------------`Metrics`提供了五个基本的度量类型：1.  Gauges（度量）2.  Counters（计数器）3.  Histograms（直方图数据）4.  Meters（TPS计算器）5.  Timers（计时器）`Metrics`中`MetricRegistry`是中心容器，它是程序中所有度量的容器，所有新的度量工具都要注册到一个`MetricRegistry`实例中才可以使用，尽量在一个应用中保持让这个`MetricRegistry`实例保持单例。### 2.1 MetricRegistry 容器在代码中配置好这个`MetricRegistry`容器：```nullpublic MetricRegistry metrics() &#123;return new MetricRegistry();```### 2.2 Meters TPS计算器`TPS计算器`这个名称并不准确，`Meters`工具会帮助我们统计系统中某一个事件的速率。比如每秒请求数（TPS），每秒查询数（QPS）等等。这个指标能反应系统当前的处理能力，帮助我们判断资源是否已经不足。`Meters`本身是一个自增计数器。通过`MetricRegistry`可以获得一个`Meter`：```nullpublic Meter requestMeter(MetricRegistry metrics) &#123;return metrics.meter(&quot;request&quot;);```在请求中调用`mark()`方法，来增加计数，我们可以在不同的请求中添加不同的`Meter`，针对自己的系统完成定制的监控需求。```null@RequestMapping(&quot;/hello&quot;)public String helloWorld() &#123;```应用运行的过程中，在console中反馈的信息：```null-- Meters ----------------------------------------------------------------------         mean rate = 133.35 events/second1-minute rate = 121.66 events/second5-minute rate = 36.99 events/second15-minute rate = 13.33 events/second```从以上信息中可以看出`Meter`可以为我们提供平均速率，以及采样后的1分钟，5分钟，15分钟的速率。### 2.3 Histogram 直方图数据直方图是一种非常常见的统计图表，`Metrics`通过这个`Histogram`这个度量类型提供了一些方便实时绘制直方图的**数据**。和之前的`Meter`相同，我们可以通过`MetricRegistry`来获得一个`Histogram`。```nullpublic Histogram responseSizes(MetricRegistry metrics) &#123;return metrics.histogram(&quot;response-sizes&quot;);```在应用中，需要统计的位置调用`Histogram`的`update()`方法。```nullresponseSizes.update(new Random().nextInt(10));\n\n比如我们需要统计某个方法的网络流量，通过Histogram就非常的方便。\n在 console 中Histogram反馈的信息：\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990-- Histograms ------------------------------------------------------------------````Histogram`为我们提供了最大值，最小值和平均值等数据，利用这些数据，我们就可以开始绘制自定义的直方图了。### 2.4 Counter 计数器`Counter`的本质就是一个`AtomicLong`实例，可以增加或者减少值，可以用它来统计队列中Job的总数。通过`MetricRegistry`也可以获得一个`Counter`实例。```nullpublic Counter pendingJobs(MetricRegistry metrics) &#123;return metrics.counter(&quot;requestCount&quot;);```在需要统计数据的位置调用`inc()`和`dec()`方法。console的输出非常简单：```null-- Counters --------------------------------------------------------------------```只是输出了当前度量的值。### 2.5 Timer 计时器`Timer`是一个`Meter`和`Histogram`的组合。这个度量单位可以比较方便地统计请求的速率和处理时间。对于接口中调用的延迟等信息的统计就比较方便了。如果发现一个方法的`RPS（请求速率）`很低，而且平均的处理时间**很长**，那么这个方法八成出问题了。同样，通过`MetricRegistry`获取一个`Timer`的实例：```nullpublic Timer responses(MetricRegistry metrics) &#123;return metrics.timer(&quot;executeTime&quot;);```在需要统计信息的位置使用这样的代码：```nullfinal Timer.Context context = responses.time();```console中就会实时返回这个`Timer`的信息：```null-- Timers ----------------------------------------------------------------------mean rate = 133.39 calls/second1-minute rate = 122.22 calls/second5-minute rate = 37.11 calls/second15-minute rate = 13.37 calls/secondstddev = 0.00 millisecondsmedian = 0.00 milliseconds99.9% &lt;= 0.01 milliseconds```### 2.6 Gauges 度量除了`Metrics`提供的几个度量类型，我们可以通过`Gauges`完成自定义的度量类型。比方说很简单的，我们想看我们缓存里面的数据大小，就可以自己定义一个`Gauges`。```null                MetricRegistry.name(ListManager.class, &quot;cache&quot;, &quot;size&quot;),                (Gauge&lt;Integer&gt;) () -&gt; cache.size()```这样`Metrics`就会一直监控`Cache`的大小。除此之外有时候，我们需要计算自己定义的一直单位，比如消息队列里面**消费者(consumers)**消费的**速率**和**生产者(producers)**的生产**速率**的比例，这也是一个度量。```nullpublic class CompareRatio extends RatioGauge &#123;private final Meter consumers;private final Meter producers;    public CacheHitRatio(Meter consumers, Meter producers) &#123;this.consumers = consumers;this.producers = producers;protected Ratio getRatio() &#123;return Ratio.of(consumers.getOneMinuteRate(),                producers.getOneMinuteRate());```把这个类也注册到`Metrics`容器里面：```nullpublic CompareRatio cacheHitRatio(MetricRegistry metrics, Meter requestMeter, Meter producers) &#123;    CompareRatio compareRatio = new CompareRatio(consumers, producers);    metrics.register(&quot;生产者消费者比率&quot;, compareRatio);```三、Reporter 报表-------------`Metrics`通过报表，将采集的数据展现到不同的位置,这里比如我们注册一个`ConsoleReporter`到`MetricRegistry`中，那么console中就会打印出对应的信息。```nullpublic ConsoleReporter consoleReporter(MetricRegistry metrics) &#123;return ConsoleReporter.forRegistry(metrics).convertRatesTo(TimeUnit.SECONDS).convertDurationsTo(TimeUnit.MILLISECONDS)```除此之外`Metrics`还支持`JMX`、`HTTP`、`Slf4j`等等，可以访问 [http://metrics.dropwizard.io/3.1.0/manual/core/#reporters](https://link.jianshu.com/?t=http://metrics.dropwizard.io/3.1.0/manual/core/#reporters) 来查看`Metrics`提供的报表，如果还是不能满足自己的业务，也可以自己继承`Metrics`提供的`ScheduledReporter`类完成自定义的报表类。四、完整的代码-------这个demo是在一个很简单的spring boot下运行的，关键的几个类完整代码如下。配置类`MetricConfig.java````nullpackage demo.metrics.config;import com.codahale.metrics.*;import org.slf4j.LoggerFactory;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import java.util.concurrent.TimeUnit;public class MetricConfig &#123;public MetricRegistry metrics() &#123;return new MetricRegistry();public ConsoleReporter consoleReporter(MetricRegistry metrics) &#123;return ConsoleReporter.forRegistry(metrics)                .convertRatesTo(TimeUnit.SECONDS)                .convertDurationsTo(TimeUnit.MILLISECONDS)public Slf4jReporter slf4jReporter(MetricRegistry metrics) &#123;return Slf4jReporter.forRegistry(metrics)                .outputTo(LoggerFactory.getLogger(&quot;demo.metrics&quot;))                .convertRatesTo(TimeUnit.SECONDS)                .convertDurationsTo(TimeUnit.MILLISECONDS)public JmxReporter jmxReporter(MetricRegistry metrics) &#123;return JmxReporter.forRegistry(metrics).build();public ListManager listManager(MetricRegistry metrics) &#123;return new ListManager(metrics);public Meter requestMeter(MetricRegistry metrics) &#123;return metrics.meter(&quot;request&quot;);public Histogram responseSizes(MetricRegistry metrics) &#123;return metrics.histogram(&quot;response-sizes&quot;);public Counter pendingJobs(MetricRegistry metrics) &#123;return metrics.counter(&quot;requestCount&quot;);public Timer responses(MetricRegistry metrics) &#123;return metrics.timer(&quot;executeTime&quot;);```接收请求的类`MainController.java````nullpackage demo.metrics.action;import com.codahale.metrics.Counter;import com.codahale.metrics.Histogram;import com.codahale.metrics.Meter;import com.codahale.metrics.Timer;import demo.metrics.config.ListManager;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.ResponseBody;public class MainController &#123;private Meter requestMeter;private Histogram responseSizes;private Counter pendingJobs;private ListManager listManager;@RequestMapping(&quot;/hello&quot;)public String helloWorld() &#123;        responseSizes.update(new Random().nextInt(10));        listManager.getList().add(1);final Timer.Context context = responses.time();```项目启动类`DemoApplication.java`：```nullimport com.codahale.metrics.ConsoleReporter;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.context.ApplicationContext;import java.util.concurrent.TimeUnit;public class DemoApplication &#123;public static void main(String[] args) &#123;ApplicationContext ctx = SpringApplication.run(DemoApplication.class, args);        ConsoleReporter reporter = ctx.getBean(ConsoleReporter.class);        reporter.start(1, TimeUnit.SECONDS);```  [https://blog.csdn.net/yz18931904/article/details/105612166](https://blog.csdn.net/yz18931904/article/details/105612166)\n","slug":"JAVA/JVM上的实时监控类库-Metrics","date":"2021-08-04T23:26:15.000Z","categories_index":"Metrics,JAVA","tags_index":"MetricRegistry,Histogram,Meter","author_index":"dandeliono"},{"id":"0b57f40a39dc96ad8c50ffa681e75949","title":"CompletableFuture使用","content":"使用异步神器CompletableFuture前提概要\n\n\n\n\n\n\n\n\n在 java8 以前，我们使用 java 的多线程编程, 一般是通过 Runnable 中的 run 方法来完成, 这种方式，有个很明显的缺点, 就是，没有返回值。这时候，大家可能会去尝试使用 Callable 中的 call 方法，然后用 Future 返回结果，如下:\npublic static void main(String[] args) throws Exception &#123;\n\n\n通过观察控制台, 我们发现先打印 main thread , 一秒后打印 async thread，似乎能满足我们的需求。但仔细想我们发现一个问题，当调用 future 的 get() 方法时，当前主线程是堵塞的，这好像并不是我们想看到的。\n另一种获取返回结果的方式是先轮询, 可以调用 isDone，等完成再获取，但这也不能让我们满意.\n\n\n很多个异步线程执行时间可能不一致, 我的主线程业务不能一直等着, 这时候我可能会想要只等最快的线程执行完或者最重要的那个任务执行完, 亦或者我只等 1 秒钟, 至于没返回结果的线程我就用默认值代替.\n我两个异步任务之间执行独立, 但是第二个依赖第一个的执行结果.\n\njava8 的 CompletableFuture, 就在这混乱且不完美的多线程江湖中闪亮登场了. CompletableFuture 让 Future 的功能和使用场景得到极大的完善和扩展, 提供了函数式编程能力, 使代码更加美观优雅, 而且可以通过回调的方式计算处理结果, 对异常处理也有了更好的处理手段.\nCompletableFuture 源码中有四个静态方法用来执行异步任务:\n创建任务public static &lt;U&gt; CompletableFuture&lt;U&gt; supplyAsync(Supplier&lt;U&gt; supplier)&#123;..&#125;\n\n\n如果有多线程的基础知识，我们很容易看出，run 开头的两个方法，用于执行没有返回值的任务，因为它的入参是 Runnable 对象。\n而 supply 开头的方法显然是执行有返回值的任务了，至于方法的入参，如果没有传入 Executor 对象将会使用 ForkJoinPool.commonPool() 作为它的线程池执行异步代码. 在实际使用中, 一般我们使用自己创建的线程池对象来作为参数传入使用，这样速度会快些.\n\n执行异步任务的方式也很简单, 只需要使用上述方法就可以了:\nCompletableFuture&lt;String&gt; future = CompletableFuture.supplyAsync(() -&gt; &#123;\n\n接下来看一下获取执行结果的几个方法。\nV get();\n\n\n上面两个方法是 Future 中的实现方式，get() 会堵塞当前的线程，这就造成了一个问题, 如果执行线程迟迟没有返回数据, get() 会一直等待下去, 因此, 第二个 get() 方法可以设置等待的时间.\ngetNow() 方法比较有意思，表示当有了返回结果时会返回结果，如果异步线程抛了异常会返回自己设置的默认值.\n\n接下来以一些场景的实例来介绍一下 CompletableFuture 中其他一些常用的方法\nthenAccept()\n\n\n功能：当前任务正常完成以后执行，当前任务的执行结果可以作为下一任务的输入参数，无返回值.\n\n场景：执行任务 A, 同时异步执行任务 B，待任务 B 正常返回后，B 的返回值执行任务 C，任务 C 无返回值\nCompletableFuture futureA &#x3D; CompletableFuture.supplyAsync(() -&gt; “任务A”);\n\n功能：对不关心上一步的计算结果，执行下一个操作\n\n场景：执行任务 A, 任务 A 执行完以后, 执行任务 B, 任务 B 不接受任务 A 的返回值 (不管 A 有没有返回值), 也无返回值\nCompletableFuture futureA &#x3D; CompletableFuture.supplyAsync(() -&gt; “任务A”);\n\n功能：当前任务正常完成以后执行，当前任务的执行的结果会作为下一任务的输入参数, 有返回值\n\n场景：多个任务串联执行, 下一个任务的执行依赖上一个任务的结果, 每个任务都有输入和输出\n\n\n\n\n\n\n\n\n\n\n\n异步执行任务 A，当任务 A 完成时使用 A 的返回结果 resultA 作为入参进行任务 B 的处理，可实现任意多个任务的串联执行\nCompletableFuture&lt;String&gt; futureA = CompletableFuture.supplyAsync(() -&gt; &quot;hello&quot;);\n\n上面的代码，我们当然可以先调用 future.join() 先得到任务 A 的返回值, 然后再拿返回值做入参去执行任务 B, 而 thenApply 的存在就在于帮我简化了这一步, 我们不必因为等待一个计算完成而一直阻塞着调用线程，而是告诉 CompletableFuture 你啥时候执行完就啥时候进行下一步. 就把多个任务串联起来了.\nthenCombine(..)  thenAcceptBoth(..)  runAfterBoth(..)\n\n\n功能：结合两个 CompletionStage 的结果，进行转化后返回\n\n场景：需要根据商品 id 查询商品的当前价格, 分两步, 查询商品的原始价格和折扣, 这两个查询相互独立, 当都查出来的时候用原始价格乘折扣, 算出当前价格. 使用方法: thenCombine(..)\n CompletableFuture futurePrice &#x3D; CompletableFuture.supplyAsync(() -&gt; 100d);\n\nthenCombine(..) 是结合两个任务的返回值进行转化后再返回, 那如果不需要返回呢, 那就需要\n\nthenAcceptBoth(..), 同理, 如果连两个任务的返回值也不关心呢, 那就需要 runAfterBoth 了, 如果理解了上面三个方法, thenApply,thenAccept,thenRun, 这里就不需要单独再提这两个方法了, 只在这里提一下.\nthenCompose(..)\n\n\n功能：这个方法接收的输入是当前的 CompletableFuture 的计算值，返回结果将是一个新的 CompletableFuture\n这个方法和 thenApply 非常像, 都是接受上一个任务的结果作为入参, 执行自己的操作, 然后返回. 那具体有什么区别呢?\n\nthenApply(): 它的功能相当于将 CompletableFuture转换成 CompletableFuture, 改变的是同一个 CompletableFuture 中的泛型类型\n\nthenCompose(): 用来连接两个 CompletableFuture，返回值是一个新的 CompletableFuture\nCompletableFuture futureA &#x3D; CompletableFuture.supplyAsync(() -&gt; “hello”);\napplyToEither(..)  acceptEither(..)  runAfterEither(..)\n\n\n功能: 执行两个 CompletionStage 的结果, 那个先执行完了, 就是用哪个的返回值进行下一步操作场景: 假设查询商品 a, 有两种方式, A 和 B, 但是 A 和 B 的执行速度不一样, 我们希望哪个先返回就用那个的返回值.\nCompletableFuture&lt;String&gt; futureA = CompletableFuture.supplyAsync(() -&gt; &#123;\n\n\n\n\n\n\n\n\n\n\n同样的道理, applyToEither 的兄弟方法还有 acceptEither(),runAfterEither(), 我想不需要我解释你也知道该怎么用了.\nexceptionally(..)\n\n\n功能: 当运行出现异常时, 调用该方法可进行一些补偿操作, 如设置默认值.\n\n场景: 异步执行任务 A 获取结果, 如果任务 A 执行过程中抛出异常, 则使用默认值 100 返回.\nCompletableFuture futureA &#x3D; CompletableFuture.\n\n\n上面代码展示了正常流程和出现异常的情况, 可以理解成 catch, 根据返回值可以体会下.\nwhenComplete(..)\n\n\n\n\n\n\n\n\n\n\n功能: 当 CompletableFuture 的计算结果完成，或者抛出异常的时候，都可以进入 whenComplete 方法执行, 举个栗子\nCompletableFuture&lt;String&gt; futureA = CompletableFuture.\n\n根据控制台, 我们可以看出执行流程是这样, supplyAsync-&gt;whenComplete-&gt;exceptionally, 可以看出并没有进入 thenApply 执行, 原因也显而易见, 在 supplyAsync 中出现了异常, thenApply 只有当正常返回时才会去执行. 而 whenComplete 不管是否正常执行, 还要注意一点, whenComplete 是没有返回值的.\n上面代码我们使用了函数式的编程风格并且先调用 whenComplete 再调用 exceptionally, 如果我们先调用 exceptionally, 再调用 whenComplete 会发生什么呢, 我们看一下: CompletableFuture futureA &#x3D; CompletableFuture.supplyAsync(() -&gt; “执行结果:” + (100 &#x2F; 0)).thenApply(s -&gt; “apply result:” + s).exceptionally(e -&gt; {System.out.println(“ex:”+e.getMessage()); &#x2F;&#x2F;ex:java.lang.ArithmeticException: &#x2F; by zeroreturn “futureA result: 100”;}).whenComplete((s, e) -&gt; {if (e &#x3D;&#x3D; null) {System.out.println(s);&#x2F;&#x2F;futureA result: 100} else {System.out.println(e.getMessage());&#x2F;&#x2F; 未执行}});System.out.println(futureA.join());&#x2F;&#x2F;futureA result: 100\n代码先执行了 exceptionally 后执行 whenComplete, 可以发现, 由于在 exceptionally 中对异常进行了处理, 并返回了默认值, whenComplete 中接收到的结果是一个正常的结果, 被 exceptionally 美化过的结果, 这一点需要留意一下.\nhandle(..)\n\n功能: 当 CompletableFuture 的计算结果完成，或者抛出异常的时候，可以通过 handle 方法对结果进行处理\n CompletableFuture&lt;String&gt; futureA = CompletableFuture.\n\n通过控制台, 我们可以看出, 最后打印的是 handle result:futureA result: 100, 执行 exceptionally 后对异常进行了 “美化”, 返回了默认值, 那么 handle 得到的就是一个正常的返回, 我们再试下, 先调用 handle 再调用 exceptionally 的情况.\n CompletableFuture&lt;String&gt; futureA = CompletableFuture.\n\n根据控制台输出, 可以看到先执行 handle, 打印了异常信息, 并对接过设置了默认值 500,exceptionally 并没有执行, 因为它得到的是 handle 返回给它的值, 由此我们大概推测 handle 和 whenComplete 的区别\n\n都是对结果进行处理, handle 有返回值, whenComplete 没有返回值\n\n由于 1 的存在, 使得 handle 多了一个特性, 可在 handle 里实现 exceptionally 的功能\nallOf(..)  anyOf(..)\n\n\n\nallOf: 当所有的 CompletableFuture 都执行完后执行计算\nanyOf: 最快的那个 CompletableFuture 执行完之后执行计算\n\n场景二: 查询一个商品详情, 需要分别去查商品信息, 卖家信息, 库存信息, 订单信息等, 这些查询相互独立, 在不同的服务上, 假设每个查询都需要一到两秒钟, 要求总体查询时间小于 2 秒.\npublic static void main(String[] args) throws Exception &#123;\n\n发布于: 2021 年 08 月 01 日阅读数: 956 https://xie.infoq.cn/article/dc37b55efda4e2cc3b966fa38\n","slug":"JAVA/CompletableFuture使用","date":"2021-08-04T23:08:01.000Z","categories_index":"CompletableFuture,JAVA","tags_index":"whenComplete,exceptionally,handle","author_index":"dandeliono"},{"id":"fd2c15dc5d41ab5172de4dd08fd40ea9","title":"Java集合","content":"","slug":"OTHER/interview-java-collection","date":"2021-01-11T21:09:12.000Z","categories_index":"OTHER","tags_index":"","author_index":"dandeliono"},{"id":"967cedbbb844e0a50d0c9c5adb15d090","title":"spring-resource注解","content":"Spring不但支持自己定义的@Autowired注解，还支持几个由JSR-250规范定义的注解，它们分别是@Resource、@PostConstruct以及@PreDestroy。这里只说@Autowired和@Resource注解的区别。\n\n@Autowired与@Resource都可以用来装配bean. 都可以写在字段上,或写在setter方法上。\n\n@Autowired默认按类型装配（这个注解是属于spring的），默认情况下必须要求依赖对象必须存在，如果要允许null值，可以设置它的required属性为false，如：@Autowired(required&#x3D;false) ，如果我们想使用名称装配可以结合@Qualifier注解进行使用，如下：\n\n\n123@Autowired()@Qualifier(&quot;baseDao&quot;)privateBaseDao baseDao;\n\n\n@Resource（这个注解属于J2EE的），默认按照名称进行装配，名称可以通过name属性进行指定，如果没有指定name属性，当注解写在字段上时，默认取字段名进行安装名称查找，如果注解写在setter方法上默认取属性名进行装配。当找不到与名称匹配的bean时才按照类型进行装配。但是需要注意的是，如果name属性一旦指定，就只会按照名称进行装配。\n\n12@Resource(name=&quot;baseDao&quot;)privateBaseDao baseDao;\n\n@Resource装配顺序  \n\n如果同时指定了name和type，则从Spring上下文中找到唯一匹配的bean进行装配，找不到则抛出异常  \n如果指定了name，则从上下文中查找名称（id）匹配的bean进行装配，找不到则抛出异常  \n如果指定了type，则从上下文中找到类型匹配的唯一bean进行装配，找不到或者找到多个，都会抛出异常  \n如果既没有指定name，又没有指定type，则自动按照byName方式进行装配；如果没有匹配，则回退为一个原始类型进行匹配，如果匹配则自动装配；\n\n@Autowired装配顺序\n\n如果没有指定@Qualifier，默认安装类型注入，找不到则抛出异常。  \n如果指定了@Qualifier，则按照名称注入，找不到则抛出异常。\n\n一般@Autowired和@Qualifier一起用，@Resource单独用。当然没有冲突的话@Autowired也可以单独用。值得注意的是Spring官方并不建议直接在类的field上使用@Autowired注解。参考《Why field injection is evil》\n","slug":"JAVA/spring-resource","date":"2020-09-24T21:56:07.000Z","categories_index":"Autowired,JAVA","tags_index":"name,Resource,bean","author_index":"dandeliono"},{"id":"2c671df6ac43c352607865f709c91620","title":"Kadane算法","content":"Kadane算法扫描一次整个数列的所有数值，在每一个扫描点计算以该点数值为结束点的子数列的最大和（正数和）。该子数列由两部分组成：以前一个位置为结束点的最大子数列、该位置的数值。因为该算法用到了“最佳子结构”（以每个位置为终点的最大子数列都是基于其前一位置的最大子数列计算得出），该算法可看成动态规划的一个例子。\n算法可用如下Python代码实现：\n123456def max_subarray(A):    max_ending_here = max_so_far = A[0]    for x in A[1:]:        max_ending_here = max(x, max_ending_here + x)        max_so_far = max(max_so_far, max_ending_here)    return max_so_far\n\n该问题的一个变种是：如果数列中含有负数元素，允许返回长度为零的子数列。该问题可用如下代码解决：\n123456def max_subarray(A):    max_ending_here = max_so_far = 0    for x in A:        max_ending_here = max(0, max_ending_here + x)        max_so_far = max(max_so_far, max_ending_here)    return max_so_far\n","slug":"ALG/Kadane","date":"2020-09-22T22:01:24.000Z","categories_index":"Kadane,ALG","tags_index":"算法扫描一次整个数列的所有数值,在每一个扫描点计算以该点数值为结束点的子数列的最大和,正数和","author_index":"dandeliono"},{"id":"83a6d625bad3243ff35a5af336d58b53","title":"regex","content":"匹配中文:\n1[\\u4e00-\\u9fa5]\n\n英文字母:\n1[a-zA-Z]\n\n数字:\n1[0-9]\n\n\n匹配中文，英文字母和数字及_:\n1^[\\u4e00-\\u9fa5_a-zA-Z0-9]+$\n\n\n同时判断输入长度：\n1[\\u4e00-\\u9fa5_a-zA-Z0-9_]&#123;4,10&#125;\n\n\n1^[\\w\\u4E00-\\u9FA5\\uF900-\\uFA2D]*$\n\n一个正则表达式，只含有汉字、数字、字母、下划线不能以下划线开头和结尾：\n1^(?!_)(?!.*?_$)[a-zA-Z0-9_\\u4e00-\\u9fa5]+$\n其中：\n12345^与字符串开始的地方匹配(?!_)　　不能以_开头(?!.*?_$)　　不能以_结尾[a-zA-Z0-9_\\u4e00-\\u9fa5]+　　至少一个汉字、数字、字母、下划线$　　与字符串结束的地方匹配\n放在程序里前面加@，否则需要\\进行转义 \n1&quot;^(?!_)(?!.*?_$)[a-zA-Z0-9_\\u4e00-\\u9fa5]+$&quot;\n或者：&quot;^(?!_)\\w*(? \n\n只含有汉字、数字、字母、下划线，下划线位置不限：\n1^[a-zA-Z0-9_\\u4e00-\\u9fa5]+$\n\n由数字、26个英文字母或者下划线组成的字符串\n1^\\w+$\n\n2~4个汉字\n1&quot;^[\\u4E00-\\u9FA5]&#123;2,4&#125;$&quot;;\n\n","slug":"JAVA/regex","date":"2020-09-07T22:13:46.000Z","categories_index":"数字,JAVA","tags_index":"匹配中文,只含有汉字,字母","author_index":"dandeliono"}]