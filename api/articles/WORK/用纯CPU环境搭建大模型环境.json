{"title":"用纯CPU环境搭建大模型环境","uid":"889ca5b7a321750970fca61562d5cfb3","slug":"WORK/用纯CPU环境搭建大模型环境","date":"2024-03-05T09:40:56.000Z","updated":"2025-09-30T03:27:03.205Z","comments":true,"path":"api/articles/WORK/用纯CPU环境搭建大模型环境.json","keywords":"XuGuangSheng","cover":"/covers/cpu.jpg","content":"<h1 id=\"用纯CPU环境搭建大模型环境\"><a href=\"#用纯CPU环境搭建大模型环境\" class=\"headerlink\" title=\"用纯CPU环境搭建大模型环境\"></a>用纯CPU环境搭建大模型环境</h1><p>配套视频：</p>\n<p>大模型现在正处于高速发展时期，各厂商都推出了自己的大模型，各种开源模型也层出不穷。尽管各厂商都有应用接入 API，但在使用过程中还是存在一些问题：</p>\n<ol>\n<li><p>数据安全：接入其他厂商的大模型，就需要将自己的数据交给对方处理，这些数据可能存在一些敏感信息，存在安全风险。</p>\n</li>\n<li><p>费用：大模型应用通常是按照 Token 数量收费的，对于普通开发者来说，长时间使用也是一个不小的开销。</p>\n</li>\n</ol>\n<p>既然开源大模型那么多，是否可以在本地搭建一个大模型的应用呢？目前开源的很多大模型，如 ChatGLM、Baichuan 等部署都需要 GPU 支持，这对个人开发者来说是一个小门槛，对于一个不玩游戏、不做图像处理开发的开发者来说，可能还真没有一块像样点的 GPU，但是可能有一颗强劲的 CPU，那么，是否可以在 CPU 上跑大模型呢？当然可以！今天就介绍一个开源项目，将大模型跑在 CPU 上。</p>\n<p>ChatGPM.cpp 是一个纯 C++实现的大模型运行时，可以将经过转换的开源大模型放在 CPU 上运行。这里是开源地址：<a href=\"https://xie.infoq.cn/link?target=https://github.com/li-plus/chatglm.cpp\">https://github.com/li-plus/chatglm.cpp</a>  </p>\n<p>ChatGLM.cpp 具有以下特点：</p>\n<ul>\n<li><p>基于 ggml，纯 C++实现</p>\n</li>\n<li><p>具有 int4&#x2F;int8 量化、优化 KV 缓存和并行计算的加速内存高效 CPU 推理</p>\n</li>\n<li><p>具有打字机效果的流式生成</p>\n</li>\n<li><p>Python 绑定，Web 演示，API 服务器和更多的可能性</p>\n</li>\n</ul>\n<p>部署流程基本上就是按照项目 readme 进行，但是其中有一些隐藏的坑需要注意，接下来就一步步进行，中间的注意点会着重描述。</p>\n<h2 id=\"纯源码物理机部署\"><a href=\"#纯源码物理机部署\" class=\"headerlink\" title=\"纯源码物理机部署\"></a>纯源码物理机部署</h2><p>纯源码物理机部署是指不使用 docker 虚拟化的情况下部署。</p>\n<h3 id=\"源码获取\"><a href=\"#源码获取\" class=\"headerlink\" title=\"源码获取\"></a>源码获取</h3><p>从 github 下载源码：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git <span class=\"built_in\">clone</span> --recursive https://github.com/li-plus/chatglm.cpp.git &amp;&amp; <span class=\"built_in\">cd</span> chatglm.cpp</span><br></pre></td></tr></table></figure>\n\n<p>这里的<code>--recursive</code>参数是为了将子模块也拉取下来。</p>\n<p>如果拉取的时候没有带此参数，也可以在拉取之后使用以下命令拉取子模块。</p>\n<figure class=\"highlight brainfuck\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">git submodule update</span> <span class=\"literal\">--</span><span class=\"comment\">init</span> <span class=\"literal\">--</span><span class=\"comment\">recursive</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"依赖安装\"><a href=\"#依赖安装\" class=\"headerlink\" title=\"依赖安装\"></a>依赖安装</h3><p>ChatGLM.cpp 依赖 python 转换模型，因此需要安装 python。</p>\n<p>此处有一个坑，对 python 版本是有要求的，因为转换过程中还需要用到 torch 包，而某些版本的 python 没有对应的 torch 包。</p>\n<p>这里推荐使用 Anaconda 来安装环境。</p>\n<p>Anaconda 就是可以便捷获取包且对包能够进行管理，同时对环境可以统一管理的发行版本。Anaconda 包含了 conda、Python 在内的超过 180 个科学包及其依赖项。</p>\n<p>这里是官网：<a href=\"https://xie.infoq.cn/link?target=https://www.anaconda.com/\">https://www.anaconda.com/</a>  </p>\n<p>安装配置好 conda 之后，创建并切换到虚拟环境：</p>\n<figure class=\"highlight applescript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda create -n chatglm-cpp</span><br><span class=\"line\">conda <span class=\"built_in\">activate</span> chatglm-cpp</span><br></pre></td></tr></table></figure>\n\n<p>查找 pytorch 纯 cpu 版本支持的 python 版本：</p>\n<figure class=\"highlight vim\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda <span class=\"built_in\">search</span> pytorch | <span class=\"keyword\">grep</span> cpu</span><br></pre></td></tr></table></figure>\n\n<p>以下是输出：</p>\n<figure class=\"highlight apache\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attribute\">pytorch</span>                        <span class=\"number\">1</span>.<span class=\"number\">2</span>.<span class=\"number\">0</span> cpu_py27h00be3c6_0  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                        <span class=\"number\">1</span>.<span class=\"number\">2</span>.<span class=\"number\">0</span> cpu_py36h00be3c6_0  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                        <span class=\"number\">1</span>.<span class=\"number\">2</span>.<span class=\"number\">0</span> cpu_py37h00be3c6_0  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                        <span class=\"number\">1</span>.<span class=\"number\">3</span>.<span class=\"number\">1</span> cpu_py27h62f834f_0  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                        <span class=\"number\">1</span>.<span class=\"number\">3</span>.<span class=\"number\">1</span> cpu_py36h62f834f_0  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                        <span class=\"number\">1</span>.<span class=\"number\">3</span>.<span class=\"number\">1</span> cpu_py37h62f834f_0  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                        <span class=\"number\">1</span>.<span class=\"number\">4</span>.<span class=\"number\">0</span> cpu_py36h7e40bad_0  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                        <span class=\"number\">1</span>.<span class=\"number\">4</span>.<span class=\"number\">0</span> cpu_py37h7e40bad_0  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                        <span class=\"number\">1</span>.<span class=\"number\">4</span>.<span class=\"number\">0</span> cpu_py38h7e40bad_0  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                        <span class=\"number\">1</span>.<span class=\"number\">5</span>.<span class=\"number\">0</span> cpu_py37hd91cbb3_0  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                        <span class=\"number\">1</span>.<span class=\"number\">7</span>.<span class=\"number\">1</span> cpu_py37h6a09485_0  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                        <span class=\"number\">1</span>.<span class=\"number\">7</span>.<span class=\"number\">1</span> cpu_py38h6a09485_0  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                        <span class=\"number\">1</span>.<span class=\"number\">7</span>.<span class=\"number\">1</span> cpu_py39h6a09485_0  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                        <span class=\"number\">1</span>.<span class=\"number\">8</span>.<span class=\"number\">1</span> cpu_py37h60491be_0  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                        <span class=\"number\">1</span>.<span class=\"number\">8</span>.<span class=\"number\">1</span> cpu_py38h60491be_0  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                        <span class=\"number\">1</span>.<span class=\"number\">8</span>.<span class=\"number\">1</span> cpu_py39h60491be_0  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                       <span class=\"number\">1</span>.<span class=\"number\">10</span>.<span class=\"number\">2</span> cpu_py310h6894f24_0  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                       <span class=\"number\">1</span>.<span class=\"number\">10</span>.<span class=\"number\">2</span> cpu_py37hfa7516b_0  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                       <span class=\"number\">1</span>.<span class=\"number\">10</span>.<span class=\"number\">2</span> cpu_py38hfa7516b_0  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                       <span class=\"number\">1</span>.<span class=\"number\">10</span>.<span class=\"number\">2</span> cpu_py39hfa7516b_0  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                       <span class=\"number\">1</span>.<span class=\"number\">12</span>.<span class=\"number\">1</span> cpu_py310h9dbd814_1  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                       <span class=\"number\">1</span>.<span class=\"number\">12</span>.<span class=\"number\">1</span> cpu_py310hb1f1ab4_1  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                       <span class=\"number\">1</span>.<span class=\"number\">12</span>.<span class=\"number\">1</span> cpu_py310he8d8e81_0  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                       <span class=\"number\">1</span>.<span class=\"number\">12</span>.<span class=\"number\">1</span> cpu_py37h9dbd814_1  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                       <span class=\"number\">1</span>.<span class=\"number\">12</span>.<span class=\"number\">1</span> cpu_py37hb1f1ab4_1  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                       <span class=\"number\">1</span>.<span class=\"number\">12</span>.<span class=\"number\">1</span> cpu_py37he8d8e81_0  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                       <span class=\"number\">1</span>.<span class=\"number\">12</span>.<span class=\"number\">1</span> cpu_py38h9dbd814_1  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                       <span class=\"number\">1</span>.<span class=\"number\">12</span>.<span class=\"number\">1</span> cpu_py38hb1f1ab4_1  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                       <span class=\"number\">1</span>.<span class=\"number\">12</span>.<span class=\"number\">1</span> cpu_py38he8d8e81_0  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                       <span class=\"number\">1</span>.<span class=\"number\">12</span>.<span class=\"number\">1</span> cpu_py39h9dbd814_1  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                       <span class=\"number\">1</span>.<span class=\"number\">12</span>.<span class=\"number\">1</span> cpu_py39hb1f1ab4_1  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                       <span class=\"number\">1</span>.<span class=\"number\">12</span>.<span class=\"number\">1</span> cpu_py39he8d8e81_0  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                       <span class=\"number\">1</span>.<span class=\"number\">13</span>.<span class=\"number\">1</span> cpu_py310h92724a6_0  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                       <span class=\"number\">1</span>.<span class=\"number\">13</span>.<span class=\"number\">1</span> cpu_py310h9dc8d95_0  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                       <span class=\"number\">1</span>.<span class=\"number\">13</span>.<span class=\"number\">1</span> cpu_py310ha02dd7b_1  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                       <span class=\"number\">1</span>.<span class=\"number\">13</span>.<span class=\"number\">1</span> cpu_py311h92724a6_0  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                       <span class=\"number\">1</span>.<span class=\"number\">13</span>.<span class=\"number\">1</span> cpu_py311h9dc8d95_0  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                       <span class=\"number\">1</span>.<span class=\"number\">13</span>.<span class=\"number\">1</span> cpu_py311ha02dd7b_1  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                       <span class=\"number\">1</span>.<span class=\"number\">13</span>.<span class=\"number\">1</span> cpu_py38h92724a6_0  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                       <span class=\"number\">1</span>.<span class=\"number\">13</span>.<span class=\"number\">1</span> cpu_py38h9dc8d95_0  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                       <span class=\"number\">1</span>.<span class=\"number\">13</span>.<span class=\"number\">1</span> cpu_py38ha02dd7b_1  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                       <span class=\"number\">1</span>.<span class=\"number\">13</span>.<span class=\"number\">1</span> cpu_py39h92724a6_0  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                       <span class=\"number\">1</span>.<span class=\"number\">13</span>.<span class=\"number\">1</span> cpu_py39h9dc8d95_0  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                       <span class=\"number\">1</span>.<span class=\"number\">13</span>.<span class=\"number\">1</span> cpu_py39ha02dd7b_1  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                        <span class=\"number\">2</span>.<span class=\"number\">0</span>.<span class=\"number\">1</span> cpu_py310hab5cca8_0  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                        <span class=\"number\">2</span>.<span class=\"number\">0</span>.<span class=\"number\">1</span> cpu_py310hdc00b08_0  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                        <span class=\"number\">2</span>.<span class=\"number\">0</span>.<span class=\"number\">1</span> cpu_py311h53e38e9_0  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                        <span class=\"number\">2</span>.<span class=\"number\">0</span>.<span class=\"number\">1</span> cpu_py311h6d93b4c_0  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                        <span class=\"number\">2</span>.<span class=\"number\">0</span>.<span class=\"number\">1</span> cpu_py38hab5cca8_0  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                        <span class=\"number\">2</span>.<span class=\"number\">0</span>.<span class=\"number\">1</span> cpu_py38hdc00b08_0  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                        <span class=\"number\">2</span>.<span class=\"number\">0</span>.<span class=\"number\">1</span> cpu_py39hab5cca8_0  pkgs/main</span><br><span class=\"line\"><span class=\"attribute\">pytorch</span>                        <span class=\"number\">2</span>.<span class=\"number\">0</span>.<span class=\"number\">1</span> cpu_py39hdc00b08_0  pkgs/main</span><br></pre></td></tr></table></figure>\n\n<p>可以看到最新支持到 python 3.11 了。</p>\n<p>安装 python 3.11</p>\n<figure class=\"highlight abnf\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">conda install python<span class=\"operator\">=</span><span class=\"operator\">=</span><span class=\"number\">3.11</span></span><br></pre></td></tr></table></figure>\n\n<p>安装依赖包：</p>\n<figure class=\"highlight apache\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attribute\">pip</span> install torch tabulate tqdm transformers==<span class=\"number\">4</span>.<span class=\"number\">33</span>.<span class=\"number\">2</span> accelerate sentencepiece aiohttp</span><br></pre></td></tr></table></figure>\n\n<p>这里需要注意的是，transformers 包的版本指定到 4.33.2，因为更高版本的 transformers 的 api 发生了变化，导致模型转换中出错。</p>\n<p>另外增加了 aiohttp 包，因为 openai 包依赖这个包，但是没有被自动安装。</p>\n<p>至此，转换模型的环境已经搭建好了。</p>\n<h3 id=\"模型转换\"><a href=\"#模型转换\" class=\"headerlink\" title=\"模型转换\"></a>模型转换</h3><p>模型转换过程中需要和模型相当大小的内存（chatglm 模型需要 25G 内存），如果本机没有这么大的内存，可以使用以下方法解决：</p>\n<ul>\n<li><p>找一台大内存的机器</p>\n</li>\n<li><p>使用在其他机器转换好的模型。我这里有两个转换后的模型：</p>\n</li>\n<li><p>链接：<a href=\"https://xie.infoq.cn/link?target=https://pan.baidu.com/s/1nXcd6NMOtIvPGX_i_rIaAQ\">https://pan.baidu.com/s/1nXcd6NMOtIvPGX_i_rIaAQ</a>提取码：s7g9</p>\n</li>\n<li><p>在一些免费的云服务器上转换，如：<a href=\"https://gitpod.io(gitpod/\">https://gitpod.io（gitpod</a> 创建环境的时候选择大存储，有 50G 存储空间，否则默认只有 30G，可能不太够）</p>\n</li>\n</ul>\n<h4 id=\"下载\"><a href=\"#下载\" class=\"headerlink\" title=\"下载\"></a>下载</h4><p>接下来下载大模型。</p>\n<p>当前支持的大模型如下：</p>\n<ul>\n<li><p>ChatGLM-6B: <code>THUDM/chatglm-6b</code>, <code>THUDM/chatglm-6b-int8</code>, <code>THUDM/chatglm-6b-int4</code>  </p>\n</li>\n<li><p>ChatGLM2-6B: <code>THUDM/chatglm2-6b</code>, <code>THUDM/chatglm2-6b-int4</code>  </p>\n</li>\n<li><p>ChatGLM3-6B: <code>THUDM/chatglm3-6b</code>  </p>\n</li>\n<li><p>CodeGeeX2: <code>THUDM/codegeex2-6b</code>, <code>THUDM/codegeex2-6b-int4</code>  </p>\n</li>\n<li><p>Baichuan &amp; Baichuan2: <code>baichuan-inc/Baichuan-13B-Chat</code>, <code>baichuan-inc/Baichuan2-7B-Chat</code>, <code>baichuan-inc/Baichuan2-13B-Chat</code></p>\n</li>\n</ul>\n<p>可以使用 git 直接从 Hugging Face 直接下载模型，这里需要注意的是，git 需要支持 lfs，才能将大模型完整下载下来。各软件仓库应该都有 git-lfs 包，安装过程这里就省略了。</p>\n<p>安装配置完 lfs 后，就可以下载大模型了，这里以 chatglm 模型举例：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git <span class=\"built_in\">clone</span> https://huggingface.co/THUDM/chatglm-6b</span><br></pre></td></tr></table></figure>\n\n<p>其他模型类似。</p>\n<p>注意一点是这个命令会卡住，实际上不是卡住了，是 lfs 正在拉取二进制的模型。可以另开一个终端使用<code>du -h</code>命令查看目录的大小变化。</p>\n<p>chatglm-6b 模型大约 25G，因此这个下载时间取决于网速，git 不支持断点续传，所以一定不要断开网络。还有一些其他下载模型的方式，可以自行搜索。</p>\n<h4 id=\"转换\"><a href=\"#转换\" class=\"headerlink\" title=\"转换\"></a>转换</h4><p>使用以下命令转换模型为 ggm 模型：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python3 chatglm_cpp/convert.py -i THUDM/chatglm-6b -t q4_0 -o chatglm-ggml.bin</span><br></pre></td></tr></table></figure>\n\n<p>其中 THUDM&#x2F;chatglm-6b 是模型路径（上一步模型 git 库的路径），chatglm-ggml.bin 是输出的 CPU 模型文件。</p>\n<p>q4_0 为转换类型，解释如下：</p>\n<ul>\n<li><p>q4_0: 使用 fp16 标度的 4 位整数量化。</p>\n</li>\n<li><p>q4_1: 具有 fp16 标度和最小值的 4 位整数量化。</p>\n</li>\n<li><p>q5_0: 基于 fp16 标度的 5 位整数量化。</p>\n</li>\n<li><p>q5_1: 具有 fp16 标度和最小值的 5 位整数量化。</p>\n</li>\n<li><p>q8_0:8 位整数量化与 fp16 尺度。</p>\n</li>\n<li><p>f16: 无量化的半精度浮点权重。</p>\n</li>\n<li><p>f32: 无量化的单精度浮点权重。</p>\n</li>\n</ul>\n<p>不同类型效果不同，消耗的资源也不同，可以多尝试几个。</p>\n<p>这里选用了资源消耗最小的 q4_0 类型。</p>\n<p>执行后生成 chatglm-ggml.bin 文件。</p>\n<h3 id=\"构建可执行文件\"><a href=\"#构建可执行文件\" class=\"headerlink\" title=\"构建可执行文件\"></a>构建可执行文件</h3><p>进入到 ChatGLM.cpp 的源码所在目录，执行：</p>\n<figure class=\"highlight mipsasm\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cmake -<span class=\"keyword\">B </span><span class=\"keyword\">build</span></span><br><span class=\"line\"><span class=\"keyword\"></span>cmake --<span class=\"keyword\">build </span><span class=\"keyword\">build </span>-<span class=\"keyword\">j </span>--<span class=\"built_in\">config</span> Release</span><br><span class=\"line\">python setup.py <span class=\"keyword\">build_ext </span>--inplace</span><br></pre></td></tr></table></figure>\n\n<p>编译后生成 <code>./build/bin/main</code>  </p>\n<h3 id=\"命令行运行\"><a href=\"#命令行运行\" class=\"headerlink\" title=\"命令行运行\"></a>命令行运行</h3><p>使用命令行：</p>\n<figure class=\"highlight stylus\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./build/bin/<span class=\"selector-tag\">main</span> -m chatglm-ggml<span class=\"selector-class\">.bin</span> -<span class=\"selector-tag\">p</span> <span class=\"string\">&quot;你好，介绍一下你自己&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>chatglm-ggml.bin 就是前面转换出来的 ggml 模型。</p>\n<p>我这边得到的输出如下：</p>\n<figure class=\"highlight subunit\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">你好，我是 ChatGLM<span class=\"string\">-6</span>B，是清华大学KEG实验室和智谱AI公司共同训练的语言模型。我的目标是通过回答用户提出的问题来帮助他们解决问题。由于我是一个计算机程序，所以我没有自我意识，也不能像人类一样感知世界。我只能通过分析我所学到的信息来回答问题。</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"交互式运行\"><a href=\"#交互式运行\" class=\"headerlink\" title=\"交互式运行\"></a>交互式运行</h3><p>将<code>-p</code>参数替换为<code>-i</code>就可以进入交互式运行：</p>\n<figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">./build/bin/<span class=\"selector-tag\">main</span> -m chatglm-ggml<span class=\"selector-class\">.bin</span> -<span class=\"selector-tag\">i</span></span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight awk\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">________          __  ________    __  ___</span><br><span class=\"line\">   <span class=\"regexp\">/ ____/</span> <span class=\"regexp\">/_  ____ _/</span> <span class=\"regexp\">/_/</span> ____<span class=\"regexp\">/ /</span>   <span class=\"regexp\">/  |/</span>  /_________  ____</span><br><span class=\"line\">  <span class=\"regexp\">/ /</span>   <span class=\"regexp\">/ __ \\/ __ `/</span> __<span class=\"regexp\">/ /</span> __<span class=\"regexp\">/ /</span>   <span class=\"regexp\">/ /</span>|_<span class=\"regexp\">/ /</span><span class=\"regexp\">/ ___/</span> __ \\/ __ \\</span><br><span class=\"line\"> <span class=\"regexp\">/ /</span>___<span class=\"regexp\">/ /</span> <span class=\"regexp\">/ /</span> <span class=\"regexp\">/_/</span> <span class=\"regexp\">/ /</span>_<span class=\"regexp\">/ /</span>_<span class=\"regexp\">/ /</span> <span class=\"regexp\">/___/</span> <span class=\"regexp\">/  /</span> <span class=\"regexp\">//</span> <span class=\"regexp\">/__/</span> <span class=\"regexp\">/_/</span> <span class=\"regexp\">/ /</span>_<span class=\"regexp\">/ /</span></span><br><span class=\"line\"> \\____<span class=\"regexp\">/_/</span> <span class=\"regexp\">/_/</span>\\__,_<span class=\"regexp\">/\\__/</span>\\____<span class=\"regexp\">/_____/</span>_<span class=\"regexp\">/  /</span>_(_)___<span class=\"regexp\">/ .___/</span> .___/</span><br><span class=\"line\">                                              <span class=\"regexp\">/_/</span>   <span class=\"regexp\">/_/</span></span><br><span class=\"line\"></span><br><span class=\"line\">Welcome to ChatGLM.cpp! Ask whatever you want. Type <span class=\"string\">&#x27;clear&#x27;</span> to clear context. Type <span class=\"string\">&#x27;stop&#x27;</span> to <span class=\"keyword\">exit</span>.</span><br><span class=\"line\"></span><br><span class=\"line\">Prompt  &gt; 你好</span><br><span class=\"line\">ChatGLM &gt; 你好👋！我是人工智能助手 ChatGLM-<span class=\"number\">6</span>B，很高兴见到你，欢迎问我任何问题。</span><br><span class=\"line\">Prompt  &gt; 介绍一下你自己</span><br><span class=\"line\">ChatGLM &gt; 我是一个大型语言模型，被训练用来回答人类提出的问题。我使用了大规模数据和算法来生成回答，可以帮助人们解决各种问题。我的能力覆盖了许多主题，包括历史、科学、技术、文化、语言和娱乐等等。</span><br><span class=\"line\">Prompt  &gt; GPT的原理是什么</span><br><span class=\"line\">ChatGLM &gt; GPT(Generative Pretrained Transformer)是一种基于Transformer架构的自然语言处理模型。它的原理是使用大量文本数据进行预训练，以便在后续任务中能够生成自然语言文本。在预训练期间，GPT学习到了大量的语言模式和规律，这些知识和模式可以用</span><br><span class=\"line\">于生成自然语言文本。在后续任务中，GPT可以根据这些知识和模式生成文本，帮助人们完成各种任务，例如回答问题、撰写文章、翻译文本等等。GPT是由 OpenAI开发的，它被广泛应用于自然语言处理领域。</span><br><span class=\"line\">Prompt  &gt;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"web-执行\"><a href=\"#web-执行\" class=\"headerlink\" title=\"web 执行\"></a>web 执行</h3><p>前面介绍了在命令行运行大模型的步骤，如果想要在 web 中运行，需要安装一些额外的包。</p>\n<p>安装 chatglm_cpp 包：</p>\n<p>或者直接从仓库安装：</p>\n<figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip <span class=\"keyword\">install</span> -U chatglm-cpp</span><br></pre></td></tr></table></figure>\n\n<p>安装 chatglm-cpp 包之后，就可以使用 python 直接调用 ChatGLM.cpp 了：</p>\n<figure class=\"highlight stylus\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import chatglm_cpp</span><br><span class=\"line\"></span><br><span class=\"line\">pipeline = chatglm_cpp<span class=\"selector-class\">.Pipeline</span>(<span class=\"string\">&quot;../chatglm-ggml.bin&quot;</span>)</span><br><span class=\"line\">pipeline<span class=\"selector-class\">.chat</span>(<span class=\"selector-attr\">[<span class=\"string\">&quot;你好&quot;</span>]</span>)</span><br></pre></td></tr></table></figure>\n\n<p>作者提供了一些 demo，位于 examples 目录下，运行其中的 web demo：</p>\n<figure class=\"highlight vim\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install -U gradio</span><br><span class=\"line\"><span class=\"keyword\">python3</span> examples/web_demo.<span class=\"keyword\">py</span> -<span class=\"keyword\">m</span> chatglm-ggml.bin</span><br></pre></td></tr></table></figure>\n\n<p>打开浏览器，访问<code>http://127.0.0.1:7860</code>  </p>\n<p><img src=\"https://static001.geekbang.org/infoq/a1/a177c24cb1178bb3ae03e98210fdf21a.png\"></p>\n<h3 id=\"开放-API\"><a href=\"#开放-API\" class=\"headerlink\" title=\"开放 API\"></a>开放 API</h3><p>可以将聊天能力开放出来，供其他应用程序使用。</p>\n<p>要想使用开放 API，需要安装 chatglm-app[api] 包：</p>\n<figure class=\"highlight nginx\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attribute\">pip</span> install <span class=\"string\">&#x27;chatglm-cpp[api]&#x27;</span></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"LangChain-API\"><a href=\"#LangChain-API\" class=\"headerlink\" title=\"LangChain API\"></a>LangChain API</h4><p>启动 API Server：</p>\n<figure class=\"highlight stylus\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MODEL=./chatglm-ggml<span class=\"selector-class\">.bin</span> uvicorn chatglm_cpp<span class=\"selector-class\">.langchain_api</span>:app <span class=\"attr\">--host</span> <span class=\"number\">127.0</span>.<span class=\"number\">0.1</span> <span class=\"attr\">--port</span> <span class=\"number\">8000</span></span><br></pre></td></tr></table></figure>\n\n<p>测试：</p>\n<figure class=\"highlight ada\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl http://<span class=\"number\">127.0</span>.<span class=\"number\">0.1</span>:<span class=\"number\">8000</span> -H <span class=\"symbol\">&#x27;Content</span>-<span class=\"keyword\">Type</span>: application/json&#x27; -d &#x27;&#123;<span class=\"string\">&quot;prompt&quot;</span>: <span class=\"string\">&quot;你好&quot;</span>&#125;&#x27;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"OpenAI-API\"><a href=\"#OpenAI-API\" class=\"headerlink\" title=\"OpenAI API\"></a>OpenAI API</h4><p>启动 API Server：</p>\n<figure class=\"highlight stylus\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">MODEL=./chatglm-ggml<span class=\"selector-class\">.bin</span> uvicorn chatglm_cpp<span class=\"selector-class\">.openai_api</span>:app <span class=\"attr\">--host</span> <span class=\"number\">127.0</span>.<span class=\"number\">0.1</span> <span class=\"attr\">--port</span> <span class=\"number\">8000</span></span><br></pre></td></tr></table></figure>\n\n<p>测试：</p>\n<figure class=\"highlight awk\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl http:<span class=\"regexp\">//</span><span class=\"number\">127.0</span>.<span class=\"number\">0.1</span>:<span class=\"number\">8000</span><span class=\"regexp\">/v1/</span>chat<span class=\"regexp\">/completions -H &#x27;Content-Type: application/</span>json<span class=\"string\">&#x27; \\</span></span><br><span class=\"line\"><span class=\"string\">    -d &#x27;</span>&#123;<span class=\"string\">&quot;messages&quot;</span>: [&#123;<span class=\"string\">&quot;role&quot;</span>: <span class=\"string\">&quot;user&quot;</span>, <span class=\"string\">&quot;content&quot;</span>: <span class=\"string\">&quot;你好&quot;</span>&#125;]&#125;<span class=\"string\">&#x27;</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"docker-部署\"><a href=\"#docker-部署\" class=\"headerlink\" title=\"docker 部署\"></a>docker 部署</h2><h3 id=\"本地构建镜像\"><a href=\"#本地构建镜像\" class=\"headerlink\" title=\"本地构建镜像\"></a>本地构建镜像</h3><p>使用以下命令构建并运行本地镜像：</p>\n<figure class=\"highlight stylus\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker build . <span class=\"attr\">--network</span>=host -t chatglm<span class=\"selector-class\">.cpp</span></span><br><span class=\"line\"></span><br><span class=\"line\">docker run -it <span class=\"attr\">--rm</span> -v <span class=\"variable\">$PWD</span>:/opt chatglm<span class=\"selector-class\">.cpp</span> ./build/bin/<span class=\"selector-tag\">main</span> -m /opt/chatglm-ggml<span class=\"selector-class\">.bin</span> -<span class=\"selector-tag\">p</span> <span class=\"string\">&quot;你好&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">docker run -it <span class=\"attr\">--rm</span> -v <span class=\"variable\">$PWD</span>:/opt chatglm<span class=\"selector-class\">.cpp</span> python3 examples/cli_chat<span class=\"selector-class\">.py</span> -m /opt/chatglm-ggml<span class=\"selector-class\">.bin</span> -<span class=\"selector-tag\">p</span> <span class=\"string\">&quot;你好&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">docker run -it <span class=\"attr\">--rm</span> -v <span class=\"variable\">$PWD</span>:/opt -<span class=\"selector-tag\">p</span> <span class=\"number\">8000</span>:<span class=\"number\">8000</span> -e MODEL=/opt/chatglm-ggml<span class=\"selector-class\">.bin</span> chatglm<span class=\"selector-class\">.cpp</span> \\</span><br><span class=\"line\">    uvicorn chatglm_cpp<span class=\"selector-class\">.langchain_api</span>:app <span class=\"attr\">--host</span> <span class=\"number\">0.0</span>.<span class=\"number\">0.0</span> <span class=\"attr\">--port</span> <span class=\"number\">8000</span></span><br><span class=\"line\"></span><br><span class=\"line\">docker run -it <span class=\"attr\">--rm</span> -v <span class=\"variable\">$PWD</span>:/opt -<span class=\"selector-tag\">p</span> <span class=\"number\">8000</span>:<span class=\"number\">8000</span> -e MODEL=/opt/chatglm-ggml<span class=\"selector-class\">.bin</span> chatglm<span class=\"selector-class\">.cpp</span> \\</span><br><span class=\"line\">    uvicorn chatglm_cpp<span class=\"selector-class\">.openai_api</span>:app <span class=\"attr\">--host</span> <span class=\"number\">0.0</span>.<span class=\"number\">0.0</span> <span class=\"attr\">--port</span> <span class=\"number\">8000</span></span><br></pre></td></tr></table></figure>\n\n<p>启用 CUDA 支持：</p>\n<figure class=\"highlight dockerfile\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker build . --network=host -t chatglm.cpp-cuda \\</span><br><span class=\"line\">    --build-<span class=\"keyword\">arg</span> BASE_IMAGE=nvidia/cuda:<span class=\"number\">12.2</span>.<span class=\"number\">0</span>-devel-ubuntu20.<span class=\"number\">04</span> \\</span><br><span class=\"line\">    --build-<span class=\"keyword\">arg</span> CMAKE_ARGS=<span class=\"string\">&quot;-DGGML_CUBLAS=ON&quot;</span></span><br><span class=\"line\">docker <span class=\"keyword\">run</span><span class=\"language-bash\"> -it --<span class=\"built_in\">rm</span> --gpus all -v <span class=\"variable\">$PWD</span>:/chatglm.cpp/models chatglm.cpp-cuda ./build/bin/main -m models/chatglm-ggml.bin -p <span class=\"string\">&quot;你好&quot;</span></span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"使用预构建的镜像\"><a href=\"#使用预构建的镜像\" class=\"headerlink\" title=\"使用预构建的镜像\"></a>使用预构建的镜像</h3><p>使用以下命令使用预构建的镜像：</p>\n<figure class=\"highlight dockerfile\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker <span class=\"keyword\">run</span><span class=\"language-bash\"> -it --<span class=\"built_in\">rm</span> -v <span class=\"variable\">$PWD</span>:/opt liplusx/chatglm.cpp:main \\</span></span><br><span class=\"line\"><span class=\"language-bash\">    ./build/bin/main -m /opt/chatglm-ggml.bin -p <span class=\"string\">&quot;你好&quot;</span></span></span><br></pre></td></tr></table></figure>\n\n<p>或者：</p>\n<figure class=\"highlight dockerfile\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker <span class=\"keyword\">run</span><span class=\"language-bash\"> -it --<span class=\"built_in\">rm</span> -v <span class=\"variable\">$PWD</span>:/opt ghcr.io/li-plus/chatglm.cpp:main \\</span></span><br><span class=\"line\"><span class=\"language-bash\">    ./build/bin/main -m /opt/chatglm-ggml.bin -p <span class=\"string\">&quot;你好&quot;</span></span></span><br></pre></td></tr></table></figure>\n\n<p>本教程基本是按照 Chat GLM.cpp 项目中的 README 编写的，但是中间有一些坑在 README 中没有提到，按照本教程可以有效避坑。</p>\n","text":"用纯CPU环境搭建大模型环境配套视频： 大模型现在正处于高速发展时期，各厂商都推出了自己的大模型，各种开源模型也层出不穷。尽管各厂商都有应用接入 API，但在使...","permalink":"/post/WORK/用纯CPU环境搭建大模型环境","photos":[],"count_time":{"symbolsCount":"11k","symbolsTime":"10 mins."},"categories":[{"name":"chatglm","slug":"chatglm","count":1,"path":"api/categories/chatglm.json"},{"name":"WORK","slug":"chatglm/WORK","count":1,"path":"api/categories/chatglm/WORK.json"}],"tags":[{"name":"https","slug":"https","count":44,"path":"api/tags/https.json"},{"name":"cpp","slug":"cpp","count":1,"path":"api/tags/cpp.json"},{"name":"THUDM","slug":"THUDM","count":1,"path":"api/tags/THUDM.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E7%94%A8%E7%BA%AFCPU%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%8E%AF%E5%A2%83\"><span class=\"toc-text\">用纯CPU环境搭建大模型环境</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E7%BA%AF%E6%BA%90%E7%A0%81%E7%89%A9%E7%90%86%E6%9C%BA%E9%83%A8%E7%BD%B2\"><span class=\"toc-text\">纯源码物理机部署</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%BA%90%E7%A0%81%E8%8E%B7%E5%8F%96\"><span class=\"toc-text\">源码获取</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E4%BE%9D%E8%B5%96%E5%AE%89%E8%A3%85\"><span class=\"toc-text\">依赖安装</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%A8%A1%E5%9E%8B%E8%BD%AC%E6%8D%A2\"><span class=\"toc-text\">模型转换</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E4%B8%8B%E8%BD%BD\"><span class=\"toc-text\">下载</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E8%BD%AC%E6%8D%A2\"><span class=\"toc-text\">转换</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%9E%84%E5%BB%BA%E5%8F%AF%E6%89%A7%E8%A1%8C%E6%96%87%E4%BB%B6\"><span class=\"toc-text\">构建可执行文件</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%91%BD%E4%BB%A4%E8%A1%8C%E8%BF%90%E8%A1%8C\"><span class=\"toc-text\">命令行运行</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E4%BA%A4%E4%BA%92%E5%BC%8F%E8%BF%90%E8%A1%8C\"><span class=\"toc-text\">交互式运行</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#web-%E6%89%A7%E8%A1%8C\"><span class=\"toc-text\">web 执行</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E5%BC%80%E6%94%BE-API\"><span class=\"toc-text\">开放 API</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#LangChain-API\"><span class=\"toc-text\">LangChain API</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#OpenAI-API\"><span class=\"toc-text\">OpenAI API</span></a></li></ol></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#docker-%E9%83%A8%E7%BD%B2\"><span class=\"toc-text\">docker 部署</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%9C%AC%E5%9C%B0%E6%9E%84%E5%BB%BA%E9%95%9C%E5%83%8F\"><span class=\"toc-text\">本地构建镜像</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E4%BD%BF%E7%94%A8%E9%A2%84%E6%9E%84%E5%BB%BA%E7%9A%84%E9%95%9C%E5%83%8F\"><span class=\"toc-text\">使用预构建的镜像</span></a></li></ol></li></ol></li></ol>","author":{"name":"dandeliono","slug":"blog-author","avatar":"https://avatars.githubusercontent.com/u/29496357","link":"/","description":"永远相信美好的事情即将发生","socials":{"github":"https://github.com/dandeliono","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"hidden":false,"prev_post":{"title":"直观感受 TLS 握手流程(上)","uid":"18647109cd1271b82271b0b5bf74c52c","slug":"NETWORK/直观感受 TLS 握手流程(上)","date":"2024-03-08T10:07:56.000Z","updated":"2025-09-30T03:26:56.529Z","comments":true,"path":"api/articles/NETWORK/直观感受 TLS 握手流程(上).json","keywords":"XuGuangSheng","cover":"/covers/tls.jpg","text":"直观感受 TLS 握手流程(上)在 HTTPS 开篇的文章中，笔者分析了 HTTPS 之所以安全的原因是因为 TLS 协议的存在。TLS 能保证信息安全和完整性...","permalink":"/post/NETWORK/直观感受 TLS 握手流程(上)","photos":[],"count_time":{"symbolsCount":"56k","symbolsTime":"51 mins."},"categories":[{"name":"Server","slug":"Server","count":1,"path":"api/categories/Server.json"},{"name":"NETWORK","slug":"Server/NETWORK","count":1,"path":"api/categories/Server/NETWORK.json"}],"tags":[{"name":"https","slug":"https","count":44,"path":"api/tags/https.json"},{"name":"Client","slug":"Client","count":2,"path":"api/tags/Client.json"},{"name":"TLS","slug":"TLS","count":1,"path":"api/tags/TLS.json"}],"author":{"name":"dandeliono","slug":"blog-author","avatar":"https://avatars.githubusercontent.com/u/29496357","link":"/","description":"永远相信美好的事情即将发生","socials":{"github":"https://github.com/dandeliono","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}},"next_post":{"title":"NullPointerException 没有打印异常栈问题追踪","uid":"f3f0b4e51e77515a8031a4140316d20f","slug":"JAVA/NullPointerException 没有打印异常栈问题追踪","date":"2024-01-31T16:27:01.000Z","updated":"2025-09-30T03:26:14.215Z","comments":true,"path":"api/articles/JAVA/NullPointerException 没有打印异常栈问题追踪.json","keywords":"XuGuangSheng","cover":"/covers/nullpointerexception.jpg","text":"NullPointerException 没有打印异常栈问题追踪今天去服务器后台看日志，发现有很多NullPointerException异常。我下意识的去找异...","permalink":"/post/JAVA/NullPointerException 没有打印异常栈问题追踪","photos":[],"count_time":{"symbolsCount":"1.3k","symbolsTime":"1 mins."},"categories":[{"name":"JVM","slug":"JVM","count":1,"path":"api/categories/JVM.json"},{"name":"JAVA","slug":"JVM/JAVA","count":1,"path":"api/categories/JVM/JAVA.json"}],"tags":[{"name":"https","slug":"https","count":44,"path":"api/tags/https.json"},{"name":"www","slug":"www","count":8,"path":"api/tags/www.json"},{"name":"NullPointerException","slug":"NullPointerException","count":1,"path":"api/tags/NullPointerException.json"}],"author":{"name":"dandeliono","slug":"blog-author","avatar":"https://avatars.githubusercontent.com/u/29496357","link":"/","description":"永远相信美好的事情即将发生","socials":{"github":"https://github.com/dandeliono","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}}}